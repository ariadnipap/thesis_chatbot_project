{
  "qa_pairs": [
    {
      "question": "What is the correct procedure to resolve a failed EXPORT job in DWH_IBank caused by a duplicate key in the loan payment data?",
      "answer": "To resolve the EXPORT job failure due to a duplicate key, first check for duplicate entries in `prod_trlog_ibank_analytical.dwh_details_loan_payment_stg` using Impala Shell. Then, create backup and temp tables, remove the duplicate entry by ID and timestamp, and overwrite the original table. Finally, rerun the export script:\\n```bash\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\\n```",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# mno - BigStreamer - IM2070630 - Failed batch Job on Grafana\\n\\n<b>Description:</b>\\n\\n```bash\\nToday 26/1/2023 the following batch job failure appeared on Grafana\\n\\nApplication: DWH_IBank\\nJob Name: EXPORT\\nComponent: LOAN_PAYMENT\\nStatus: Failed\\nDescription: Code:1\\nThanks\\n```\\n\\n<b>Actions Taken:</b>\\n\\n<b>Root Cause Analysis:</b>\\n\\nAnalysis was performed in collaboration with @lmn and @iaravant\\n\\n1. Check MapReduce logs from YARN UI - App Name: PROD_IBank_DWH_EXPORT_LoanPaymentDetails_*\\n\\n   ![IM2070630_yarn_app](.media/IM2070630_yarn_app.png)\\n\\n   ![IM2070630_yarn_mapreduce](.media/IM2070630_yarn_mapreduce.png)\\n\\n   There was a duplicate entry in prod_trlog_ibank_analytical.dwh_details_loan_payment_stg.\\n   Duplicate Key Value: 0E86AF89-F15C-4B78-8925-08ED8D237805\\n\\n2. Check the tables from Impala Shell with PRODUSER\\n\\n   ```bash\\n   [PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\\n   ```\\n\\n   Check for duplicates in dwh_details_loan_payment_stg and dwh_details_loan_payment with id=0E86AF89-F15C-4B78-8925-08ED8D237805\\n\\n   ![IM2070630_details_duplicates](.media/IM2070630_details_duplicates.PNG)\\n\\n   Check service_audit specifically for id=0E86AF89-F15C-4B78-8925-08ED8D237805\\n\\n   ![IM2070630_service_audit](.media/IM2070630_service_audit.PNG)\\n\\n   Check service_audit for duplicates\\n\\n   ![IM2070630_service_audit_monthly](.media/IM2070630_service_audit_monthly.PNG)\\n\\n   Service_audit contained two entries with the same id=0E86AF89-F15C-4B78-8925-08ED8D237805 and different timestamps.\\n\\n<b>Resolution:</b>\\n\\nSolution provided by @fgh and @adrint\\n\\n```bash\\n[PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\\n```\\n\\n```bash\\n# == DWH LoanPayment ==\\n\\n# Create table with original data\\ncreate table prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig like prod_trlog_ibank_analytical.dwh_details_loan_payment;\\n\\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt=20230125;\\n\\n# Create table and insert only required data\\ncreate table prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp like prod_trlog_ibank_analytical.dwh_details_loan_payment;\\n\\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt = 20230125 and id != '0E86AF89-F15C-4B78-8925-08ED8D237805';\\n\\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt = 20230125 and id = '0E86AF89-F15C-4B78-8925-08ED8D237805' and tr_timestamp = '202301025 09:38:13.072489000' limit 1;\\n\\n# Overwrite normal table with correct data\\ninsert overwrite prod_trlog_ibank_analytical.dwh_details_loan_payment partition(par_dt) select * from prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp where par_dt = 20230125;\\n```\\n\\n```bash\\n# Run Export procedure \\n/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\\n```\\n\\n```bash\\n[PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\\n```\\n\\n```bash\\n# Drop temporary table\\ndrop prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp purge\\n\\n# TODO Check data and drop backup table with initial orginal data\\ndrop prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig purge\\n```\\n\\n<b>Our Ticket Response:</b>\\n\\n```bash\\nGood evening,\\n\\nAfter investigation, it was determined that there is a problem with two different transactions that occurred on 25/01/2023 which were declared with the same id.\\n\\nThe external SQL server LoanPaymentDetails restricts the id field to be UNIQUE, hence the expected error displayed by sqoop-export.\\n\\nSQLException: Violation of PRIMARY KEY constraint 'PK_LoanPaymentDetails'.\\nCannot insert duplicate key in object 'srcib.LoanPaymentDetails'.\\nThe duplicate key value is (0e86af89-f15c-4b78-8925-08ed8d237805)\\n\\nThe id of the problematic transaction is 0E86AF89-F15C-4B78-8925-08ED8D237805 and the first transaction has a timestamp of 09:38:13.072489000 while the second one has a timestamp of 2023-01-25 2023-01-25 09:38:13.476066000.\\n\\nWe suggest deleting one of the two transactions so that we can proceed with the export of the loan payment data for the remaining transactions. The information for both transactions remains in the big data environment\\nso it can be retrieved if needed later.\\n\\nPlease let us know if you agree to proceed with the proposed action.\\n\\nThank you\\n```\\n\\n```bash\\nAfter communication via email, we proceeded to reconstruct the table, keeping only the record with timestamp 09:38:13.072489000 and reran the job. Please, investigate this on your part and take the necessary actions so that duplicate records are not sent to us. Please confirm that the data is in the SQLServer so that we can close the ticket.\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is highly specific and directly answerable using the provided context, which details the exact issue and resolution steps for a failed EXPORT job due to a duplicate key.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The question is extremely relevant for telecom engineers using the BigStreamer platform, as it addresses a common issue in data export jobs that could disrupt analytics workflows.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear and can be understood on its own without needing the context, as it specifies the problem (failed EXPORT job) and the cause (duplicate key in loan payment data)."
    },
    {
      "question": "A batch job fails with Code 6 while trying to EXPORT loan payment data. What could be the cause and how should it be resolved?",
      "answer": "Code 6 indicates a timeout in the sqoop-eval stage due to a SQLServer connection issue. The job did not proceed to sqoop-export. The solution is to rerun the job at a less busy time to avoid timeout:\\n```bash\\n# Example rerun\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\\n```",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# mno - BigStreamer - IM2074270 - Failed Batch Job on Grafana\\n\\n<b>Description:</b>\\n\\n```bash\\nGood evening,\\n\\nThe following failed batch job appeared in the grafana system:\\n\\napplication : DWH_IBank\\njob_name : EXPORT\\ncomponent : LOAN_PAYMENT\\ndate : 30-01-2023\\nstatus : FAILED\\ndescription : code 6\\nhost : -\\n```\\n\\n</b>Actions Taken:</b>\\n\\n1. Check Loan Payment - Export Status from Grafana\\n\\n   ```bash\\n   LOCAL MONITOR -> Batch Jobs DR -> DW Jobs\\n   ```\\n\\n   The script excited with Code 6, which means that it timed-out, waiting for the Sqoop MapReduce job to be in running state.\\n\\n2. Check Impala Query Status\\n\\n   ```bash\\n   Cloudera Manager -> Impala -> Queries -> statement RLIKE '.*details_loan_payment.*' -> Search\\n   ```\\n\\n   ![IM2074270_impala_queries](.media/IM2074270_impala_queries.png)\\n\\n   Impala query ran successfully.\\n\\n3. Check MapReduce job from YARN\\n\\n   ![IM2074270_yarn_apps](.media/IM2074270_yarn_apps.png)\\n\\n   There wasn't any job for Loan Payment,so it had never been submitted.\\n\\n4. Check export logs\\n\\n    ```bash\\n    #from dr1edge01\\n    less /var/log/datawarehouse-ibank/PRODUSER/sched_export.log\\n    ```\\n\\n   ![IM2074270_export_logs](.media/IM2074270_export_logs.png)\\n\\n   There was a Connection Timed Out error from sqoop-eval which indicates a problem with the SQLServer.\\n\\n5. Check export script\\n\\n    ```bash\\n    #from dr1edge01\\n    less /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_to_dwh.sh\\n    ```\\n\\n   ![IM2074270_export_script](.media/IM2074270_export_script.png)\\n\\n   As we can see in the image above, sqoop-export runs after sqoop-eval. In our case sqoop-export did not run because sqoop-eval exited with error.\\n\\n6. Inform Customer and ask for a rerun\\n\\n<b>Our Ticket Response:</b>\\n\\n```bash\\nGood evening,\\n\\nfrom the analysis of the logs we see that sqoop was not submitted because the evaluation for SQL Server was run first, which crashed with a Network I/O exception. (Connection Timed Out)\\n\\nPlease rerun the job.\\n\\nWe also see that the DWH started today at 12. Because at this time we have an increased chance of the SQL Server evaluation crashing as the traffic on it is increased, could you inform us why the execution of the DWH was delayed?\\n\\nThank you\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is directly answerable using the provided context, which clearly explains the cause (SQLServer timeout) and resolution (rerun the job).",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The question is highly relevant for telecom engineers using BigStreamer, as it addresses a critical issue (batch job failure) that could impact data export processes.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear and self-contained, specifying the error (Code 6) and the context (EXPORT loan payment data) without needing additional context."
    },
    {
      "question": "How can you resolve 'OutOfMemoryError' issues on the Spark History Server in Cloudera?",
      "answer": "Inspect the logs on the node and confirm the Java heap size in Cloudera UI. If it is too low (e.g., 512M), increase it to a higher value like 2G to match the configuration of similar roles on other nodes. Then restart the role.",
      "category": "Infrastructure & Deployment",
      "context": "# mno - BigStreamer - IM2098517 - Health issue on dr1edge01\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\n\\nThe following health issue has occurred in the cloudera manager system:\\n\\nSpark on yarn - History Server (dr1node03) - Unexpected Exits\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to Cloudera for DR Site\\n2. We inspected logs for this role for the time that the problem arose: `Cloudera > Diagnostics > Logs` and chose `Service: Spark on Yarn` and `Role: History Server`. We could not identify the root cause by these logs\\n3. ssh to dr1node03 as root, went to `/var/run/process` , and inspected logs from the process that ran at the time of the problem and found out that the process with pid 51291 was killed while a `OutOfMemoryError` occured\\n   ![terminal_screenshot](.media/IM2098517/spark_on_yarn.png)\\n\\n4. We checked the  `java heap size` of the History Server through Cloudera UI configuration tab. It was set to 512M.\\n5. We checked the respective option for the PR Site and it was set to 2G\\n6. We set the `java heap size` of the History Server to 2G at the DR Site\\n7. We proceeded to restart of the role after communication with the customer\\n    \\n\\n\\n<b>Our Ticket Response:</b>\\n\\n```\\n07/03/23 16:28:55 Europe/Eastern (MASTROKOSTA MARIA):\\nWe have restarted the History Server after a phone call. There was no problem during the restart.\\n\\nPlease let us know if we can close the ticket.\\n\\nThank you\\n07/03/23 16:21:21 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe exit occurred due to an out of memory error. We have changed the java heap size of the History Server from 512MB to 2GB as in the PR. We will need to restart the role. There will be no outage.\\n\\nThank you\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for diagnosing and resolving OutOfMemoryError in Spark History Server.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Highly relevant for telecom engineers using BigStreamer, as memory issues in Spark History Server can impact job monitoring and debugging capabilities.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely self-contained, clearly specifying the error (OutOfMemoryError) and component (Spark History Server in Cloudera)."
    },
    {
      "question": "What causes a YARN Node Manager failure and how can it be mitigated?",
      "answer": "A Node Manager failure on pr1node01 was due to excessive disk usage from krb5kdc logs. These logs had grown to 80GB. The mitigation included deleting old logs and changing log rotation policy to weekly, keeping only 7 logs instead of 12.",
      "category": "Infrastructure & Deployment",
      "context": "# mno - BigStreamer - IM2095156 - Alarm on PRDBA  Cloudera Manager\\n\\n</b>Description:</b>\\n\\n```\\nWe have a critical alarm in PRDBA Cloudera Manager -\\n\\nYARM -- Node Manager (pr1node03)\\nProcess Status\\n\\nThank you\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to Cloudera UI for the PR Site\\n2. Cloudera > Yarn\\n3. Upon inspection we noticed that the alert was about pr1node01 (Node Manager) and not pr1node03 (JobHistory Server)\\n4. Ssh pr1node01 and inspect logs at /var/loh/hadoop-yarn. We could not find the root cause from logs\\n5. Restart the Node Manager role for the specific node. After the restart the alert disappeared.\\n6. During further investigation, from Cloudera UI we saw that prior to ```Process Status``` alert there was a ```NODE_MANAGER_LOG_FREE_SPACE``` alert\\n7. From pr1node01 as root `df -h /`. The usage of `/` was at 98% at that time\\n8. Upon inspection we noticed that the krb5kdc logs had increased over the last months peaking the monthly log file to ~80G.\\n9. We proceeded to the removal of krb5kdc log files for 2022.\\n10. As a permanent solution, we implemented changes to retention policy for krb5kdc logs. Specifically, we changed the rotation to weekly from monthly and the storage to 7 old logs from 12 logs files that it was prior the change.  This change was implemented at pr1node02, dr1node01 and dr1node02 as well.\\n\\n\\n    ![logrotate_krb5kdc](.media/IM2095156/IM2095156_logrotate_krb5kdc.PNG)\\n\\n<b>Our Ticket Response:</b>\\n\\n```\\n09/03/23 15:47:41 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nFollowing the investigation, we have changed the retention for krb5kdc logs. Specifically, we have set the rotation to be weekly instead of monthly and to keep 7 log files. Note that the monthly krb5kdc log file had reached 80G.\\n\\nPlease let us know if we can proceed with closing the ticket.\\n\\nThank you\\n\\n01/03/23 07:16:41 Europe/Eastern (MASTROKOSTA MARIA):\\nGood morning,\\n\\nThere was a malfunction in the yarn node manager since 3.42, resulting in the ibank and online streaming topologies falling as recorded in ticket SD2157107.\\n\\nWe proceeded to restart at 4:53 to get it back up. During the restart, the online merge batch crashed, which was resubmitted (related ticket SD2157111).\\n\\nAt this time, yarn and the flows are running normally.\\n\\nFrom the investigation it appears that the root partition on pr1node01 had filled up, which was caused by the local kdc logs. We have proceeded to clean the corresponding log files and are investigating changes to the retention of the logs to avoid future problems.\\n\\nPlease let us know if you consider the workaround acceptable.\\n\\nThank you\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully answerable from the context, which provides a clear root cause (disk usage from logs) and detailed mitigation steps (log cleanup and rotation policy change).",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Extremely relevant for telecom engineers as YARN Node Manager failures can disrupt critical data processing jobs in BigStreamer.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is perfectly clear on its own, specifying both the component (YARN Node Manager) and what information is needed (causes and mitigation)."
    },
    {
      "question": "How should you run a MAN_DATE extract and export job when there is a duplicate key error?",
      "answer": "Use the `-f` flag to force truncation before running extract and export scripts to avoid primary key constraint violations:\\n```bash\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\\n```",
      "category": "Application Functionality & Flow",
      "context": "# mno - BigStreamer - IM2117067 - Grafana Batch Job failed\\n\\n<b>Description:</b>\\n\\nTitle: Batch Job Failed\\nBody:\\n```\\nHello,\\n\\nToday 29/03 in Grafana application a failed Batch Job appeared.\\n\\nApplication : DWH_IBank\\nJob_Name : Extract\\nComponent: MAN_DATE\\nDate: 28/03/23\\nStatus: Failed\\nDescription Code 1\\n```\\n\\nInformation regarding the extract, logs etc. available [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-extract) \\n\\nInformation regarding the export, logs etc. available [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-export) \\n\\n</b>Actions Taken:</b>\\n\\nWe tried to invoke the Extract MAN_DATE script but it failed multiple times. After investigating the logs of the executor using internal firefox and the logs of the DWH_Ibank_MAN_DATE script we saw the following :\\n\\n`Cannot insert dublicate key in object srcib.MandateDetails. The dublicate key value is (e5435435-4354254235-121nfdgd33)`\\n\\n\\nThat means that the table already has records in it, so we have to drop this records or invoke the script with the `-f` option that gives the ability to trancate the table and then insert the records. \\n\\n```\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\\n```\\n\\nAfter executing the extract script, we use the same `-f` to run the export script. The reason that we are running export is because in the above mentioned step we run the extract and trancated the table so the export is neccessary. \\n\\n```\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\\n```\\n\\nThe issue has been resolved by executing the above two commands. The key in this case was to find in the logs that we have dublicate keys. ",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides specific commands and the '-f' flag solution for handling duplicate key errors in MAN_DATE jobs.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses a common issue in data processing workflows that could disrupt critical operations in BigStreamer.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely self-contained, clearly specifying both the operation (MAN_DATE extract/export) and the specific error condition (duplicate key)."
    },
    {
      "question": "How do you rerun a failed Merge Batch ingestion job for IBank and why is it done in patches?",
      "answer": "You rerun the job using the following commands split by time window to manage load:\\n```bash\\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 00:00:00\\\" \\\"2023-02-28 12:00:00\\\"\\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 12:00:00\\\" \\\"2023-02-28 18:00:00\\\"\\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 18:00:00\\\" \\\"2023-03-01 00:00:00\\\"\\n```\\nThis ensures that large record volumes are handled without overloading HBase or Impala.",
      "category": "Application Functionality & Flow",
      "context": "# mno - BigStreamer - IM2095966 - Failed Batch Job on Grafana\\n\\n<b>Description:</b>\\n\\n```\\nGood evening,\\n\\nThe following failed batch job appeared in the Grafana system:\\n\\napplication :  IBank_Ingestion\\njob_name : MergeBatch\\ncomponent : JOB\\ndate : 28-02-2023\\nstatus : FAILED\\ndescription :\\nhost : pr1edge01.mno.gr\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. We identified the failed step using the alarm name. Steps `MSSQL Sqoop Import (Migration)` and `Insert to Service Audit` had been executed successfully. We rerun the `Merge Batch` according to [this](../supportDocuments/applicationFlows/ibank.md#merge-batch).\\n2. The job had not completed at approximately 9.pm on 01/03/2023 we terminated the job after communication with the customer in order for the night flow to run without any problems. We scheduled to rerun the job in the following day after the completion of the daily MergeBatch.\\n3. On 02/03/2023 we reran the job in 3 patches \\n   ```bash\\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 00:00:00\\\" \\\"2023-02-28 12:00:00\\\"\\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 12:00:00\\\" \\\"2023-02-28 18:00:00\\\"\\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 18:00:00\\\" \\\"2023-03-01 00:00:00\\\"\\n   ```\\n4. The `Upsert to HBase` stage that synchronises the `Visible` table caused an Impala problem during which Impala stopped to process this job as well as other requests.\\n5. The problem is described below.\\n\\n<b>Our Ticket Response:</b>\\n\\n```\\n03/03/23 11:17:49 Europe/Eastern (POULAS GIORGOS):\\nAfter investigating yesterday's Impala issue, we found the following:\\n\\nDue to HBase quotas set in the PROD_IBANK namespace, we have limited the parallelism in the Impala query to run on an Impala daemon.\\n\\nThe daemon that ran the query to enrich the Service Audit Visible (pr1node04) encountered a problem as it did not have the resources required to process the large volume of records we had at the end of the month, while at the same time accepting requests from the REST APIs of the live streams.\\n\\nAs a result of the above, the queries from the live systems were not completing and accumulating, exhausting the available connections that Impala can accept. The malfunction of the live streams is also the problem you observed last night.\\n\\nWe propose as a workaround today after 9pm. disable quotas in the PROD_IBANK namespace and rerun the script without the single node limitation, so that the load is shared across all 9 available Impala daemons. We will then examine the alternatives for modifying the flow and re-enabling quotas.\\n\\nThere is no downtime required for the above actions.\\n\\nIf you need further information we can arrange a call.\\n\\nG. Poulas\\n03/03/23 00:53:26 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe service audit has been filled on the PR Site. The job that fills the visible table is pending as it was canceled in the context of ticket SD2159021.\\n\\nThank you\\n03/03/23 00:52:57 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe service audit has been filled on the PR Site. The job that fills the visible table is pending as it was canceled under ticket SD2159021.\\n\\nThank you\\n02/03/23 15:56:38 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe execution on both sites started after the scheduled execution of the Merge Batch for 01/03/2023, which has been completed without a problem. At this time, the DR has processed until 18:00, while the PR has processed the data until 12:00. The executions on both sites are being monitored so that they can be resubmitted in case of a problem.\\n\\nThank you\\n02/03/23 15:54:31 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe execution on both sites started after the scheduled Merge Batch execution on 01/03/2023, which has been completed without any problems. At the moment, the DR has processed until 18:00, while the PR has processed the data until 12:00. The executions on both sites are being monitored so that they can be resubmitted in case of a problem.\\n\\nThank you\\n01/03/23 21:11:19 Europe/Eastern (MASTROKOSTA MARIA):\\nFollowing our telephone communication, the job has been stopped and will be re-executed tomorrow in order to avoid problems with the evening streams.\\n\\nThank you\\n```\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides specific commands for patched reruns and explains the resource management reasons behind this approach.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Extremely relevant for telecom engineers as it addresses a critical data ingestion process and resource optimization in BigStreamer operations.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the action (rerun failed job) and the specific aspect needing explanation (patch approach)."
    },
    {
      "question": "How can Impala concurrency issues related to HBase quotas be resolved for a job using 'Upsert to HBase'?",
      "answer": "Remove the line `set num_nodes = 1` from the script and disable HBase quotas in the PROD_IBANK namespace. This allows parallel execution across all 9 Impala daemons. Afterward, rerun the script:\\n```bash\\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh\\n```",
      "category": "Data Management & Query Execution",
      "context": "# mno - BigStreamer - IM2097021 - Multiple Health issues on PR Impala\\n\\n<b>Description:</b>\\n\\n```\\nToday 02/03/2023 & 23:30 the Following alarms appeared on Cloudera\\n\\n[Impala: Daemon (pr1node01)]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node02)]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node03)]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node04)]\\n[Pause Duration]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node05)]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node06)]\\n[Impala Concurrent Client Connections]\\n```\\n\\n</b>Actions Taken:</b>\\n\\n1. Login to Cloudera for PR Site \\n2. To identify the Impala query from `Upsert to HBase` we can see logs from the script at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log` as `PRODREST` user. We cite a screenshot that shows the query. Also, we see the url where we can monitor the query progress (paste this url on a firefox opened through terminal), as well as the coordinator.\\n   \\n   ![logs_screenshot](.media/upsert_to_hbase_logs_query.PNG)\\n   > Ndef: These are not logs from that specific script execution, just a sample to see where you can find the query information you need.\\n\\n3. From `Cloudera > Impala > Queries` we identified the query and noticed that it had stopped getting processed. In addition, we noticed that Impala had stopped processing other queries as well\\n4. We cancelled the query that ran for `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` execution. We can cancel the query in two ways.\\n   1. From `Cloudera > Impala > Queries` you can click `cancel` at the dropdown next to the query\\n   2. From the url that we monitor the query\\n5. We restarted Impala daemon role for pr1node01. This solved the problem with this specific node, however the service did not correspond\\n6. We restarted Impala daemon role for pr1node04 that was the coordinator for the query. This solved the problem and recovered the service functionality\\n7. Upon investigation, we concluded that the change to `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` that stops the parallel execution of the query by Impala daemons (set num_nodes = 1) was the cause of the problem\\n8. We scheduled to rerun the `Upsert to HBase` stage the following day after reverting the script to use all Impala daemons for parallel execution.\\n9.  On 03/03/2023 \\n   - we disabled HBase quotas for ` PROD_IBANK` namespace on PR Site according to [this](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/22263_mno_HBASE_TUNING.docx) MoP\\n   - we removed `set num_nodes = 1` from `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\\n   - reran the script\\n10. The script ran successfully\\n\\n<b>Action Points:</b>\\n\\n1. We opened [this](https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/67) issue to investigate and deploy a permanent fix for running `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` alongside HBase quotas.\\n\\n<b>Our Ticket Response:</b>\\n\\n```\\n03/03/23 00:49:29 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe root cause is the same as ticket SD2158913. The job that populates the visible table was canceled after consultation with the customer as it was affecting the live streams.\\n\\nThe job will be scheduled to be rerun after consultation\\n\\nThank you\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides specific technical solutions (removing node limitation and disabling quotas) and the exact script to rerun.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses critical performance issues in HBase/Impala integration that could impact data processing workflows.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely self-contained, clearly specifying both the technical context (Impala concurrency) and the specific operation ('Upsert to HBase')."
    },
    {
      "question": "How can Kerberos authentication issues be resolved on RHEL 8 for a new Way4Streams installation?",
      "answer": "1. Remove `sssd-kcm` and ensure krb5.conf has:\\n```conf\\ndefault_ccache_name = FILE:/tmp/krb5cc_%{uid}\\nallow_weak_crypto = true\\n```\\n2. If `klist -kte` shows DEPRECATED encryption, allow weak crypto as above.\\n3. Restart authentication processes or reinitiate with `kinit`.\\n\\nThis allows proper usage of keytabs even with weak encryption schemes.",
      "category": "Data Management & Query Execution",
      "context": "# mno - BigStreamer - way4streams-venia - Kerberos Authentication Errors on new Way4Streams installation\\n\\n<b>Description:</b>\\n\\n```text\\nReporting and investigation for this issue was performed in a teams call, since we did not have access to the server that Way4Streams was installed.\\n\\nThe error we were facing was something along the lines\\n\\n/way4/DEVUSER.keytab does not contain any keys for DEVUSER@BANK.CENTRAL.mno.GR\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. The new server hosting the application is RHEL 8 instead of Solaris. We tried to manually `kinit`\\n\\nFrom the server with `way4`\\n\\n``` bash\\nkinit DEVUSER@BANK.CENTRAL.mno.GR -kt /way4/DEVUSER.keytab\\n```\\n\\nOutput:\\n\\n```bash\\nTicket cache: KCM:1500\\nDefault principal: DEVUSER@BANK.CENTRAL.mno.GR\\n\\nValid starting       Expires              Service principal\\n15/03/2023 12:35:29  16/03/2023 12:35:29  krbtgt/BANK.CENTRAL.mno.GR@BANK.CENTRAL.mno.GR\\n\trenew until 22/03/2023 12:35:29\\n```\\n\\n2. **Anything** but `FILE` ticket caches is sure to create a problem.\\n\\nFrom the server with `root`:\\n\\n``` bash\\nvi /etc/krb5.conf\\n```\\n\\nChange the following under `libdefaults` section:\\n\\n``` conf\\ndefault_ccache_name = FILE:/tmp/krb5cc_%{uid}\\n```\\n\\nAlso, remove `sssd-kcm`\\n\\n```bash\\nyum remove sssd-kcm\\n```\\n\\n3. After that the klist output used a `FILE` cache, but the problem persisted.\\n\\nSince the OS problems were resolved we focused the keytab.\\n\\nFrom the server with `way4`\\n\\n``` bash\\nklist -kte /way4/DEVUSER.keytab\\n```\\n\\nOutput:\\n\\n```\\nKeytab name: FILE:/way4/DEVUSER.keytab\\nKVNO Timestamp           Principal\\n---- ------------------- ------------------------------------------------------\\n  0 01/01/1970 00:00:00 DEVUSER@BANK.CENTRAL.mno.GR (DEPRECATED:arc4-hmac) \\n```\\n\\nThat DEPRECATED flag is not a good sign. \\n\\n4. Searching for `rc4-hmac` and `OpenJDK11` we stumbled upon this link https://bugs.openjdk.org/browse/JDK-8262273\\n\\nFrom the server with `root`:\\n\\n``` bash\\nvi /etc/krb5.conf\\n```\\n\\nAdd the following under `libdefaults` section:\\n\\n``` conf\\nallow_weak_crypto = true\\n```\\n\\nThe issue was resolved!\\n\\n<b>Affected Systems:</b>\\n\\nWay4Streams QA (Not supported by jkl)\\n\\n<b>Action Points:</b>",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for resolving Kerberos authentication issues specific to RHEL 8 and Way4Streams.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses authentication issues that could prevent proper functioning of the Way4Streams application in BigStreamer.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the technical context (Kerberos authentication) and the specific environment (RHEL 8 for Way4Streams)."
    },
    {
      "question": "How do you rerun the 'Enrich SA from SA_old' job after a memory error and ensure it's properly monitored in Grafana?",
      "answer": "Check the logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log` for memory errors. If no data exists for the failed partition (check using Impala), rerun the script:\\n```bash\\n/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20230330\\n```\\nThen, update the monitoring database with:\\n```bash\\npsql -d monitoring\\nINSERT INTO prod.monitoring (...) VALUES (...);\\n```\\nFinally, verify the status from Grafana.",
      "category": "Application Functionality & Flow",
      "context": "# mno - BigStreamer - SD2180781 - Failed job at Grafana \\n\\n</b>Description:</b>\\n\\n```\\nGood evening.\\nThe following failed job appeared in Grafana:\\n\\nApplication: IBank_Migration\\nJob_name: Enrich SA from SA_old\\nComponment: JOB\\nDate: 30-03-2023\\nStatus: FAILED\\nHost: pr1edge01.mno.gr\\n```\\n\\n</b>Actions Taken:</b>\\n\\n1. We have to check first the logs from the failed job `Enrich SA from SA_old` as described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\\n\\nDetailed information from the above link\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\\n\\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n\\n\\n2. The error was the below from logs:\\n```\\nERROR: Memory limit exceeded: Failed to allocate row batch\\nEXCHANGE_NODE (id=5) could not allocate 1.00 MB without exceeding limit.\\nError occurred on backend pr1node05.mno.gr:22000\\nMemory left in process limit: 27.82 GB\\n\\n```\\n3. To verify the below error also checked from Cloudera > Impala > Queries\\n\\n![impala_query_error](.media/SD2180781/SD2180781_IMPALA_QUERY_ERROR.PNG)\\n\\n4. Now we have to go to `troubleshooting steps` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit) in order to check that no records are present in `prod_trlog_ibank.service_audit`\\n\\n  ``` bash\\n  # eg. 09-11-2019\\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \\\"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\\\"\\n  ```\\n\\n!Ndef : The par_dt is -1 from today\\n\\n5. No records exists on `par_dt` `20230330`\\n\\n6. Now we have to run the command `For the previous day:` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\\n\\nIf no records exist and no other process is up, you can ran the script again.\\n  - For the previous day:\\n\\n    ``` bash\\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\\n    ```\\n\\n  - For a specified date:\\n\\n    ``` bash\\n    # e.g. 09-11-2019\\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\\n    ```\\n\\n7. When the job finished `succesfully` we have to updated the monitoring postgres database in order for the entry `Enrich SA from SA_old` to appeared green/success in Grafana.\\n\\n```bash\\nssh Exxxx@pr1edge01.mno.gr\\nsudo -i -u postgres\\npsql -d monitoring\\nselect * from prod.monitoring where par_dt = 20230330;\\nINSERT INTO prod.monitoring (application, job_name,component,status,par_dt,start_time,end_time,description,params,host) VALUES ('IBank_Migration','Enrich SA from SA_old','JOB',0,20230330,'2023-03-31 03:18:30.000','2023-03-31 05:00:42.000','','','pr1edge01.mno.gr') ON CONFLICT (application, job_name,component,par_dt) DO UPDATE SET status=0, start_time='2023-03-31 03:18:30.000', end_time='2023-03-31 05:00:42.000',description='';\\n```\\n8. Check from Grafana that the job `Enrich SA from SA_old` is now `succeded`\\n9. After i execute the below steps mannualy from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md)\\n\\na. [Upsert to HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`\\n\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\\n\\n**Alerts**:\\n\\n- IBank_Migration Enrich hbase tables JOB\\n- IBank_Migration Enrich hbase tables Impala_insert\\n- IBank_Migration Enrich hbase tables Spark\\n\\n**Troubleshooting Steps**:\\n\\n- Use the script logs to identify the cause of the failure\\n\\n  Ndef: If job failed and the following error appears :`ERROR: RetriesExhaustedWithDetailsException: Failed <num> actions: CallTimeoutException: <num> times, servers with issues: [dr/pr]1node02.mno.gr`,  execute script again. The error has to do with HBase merging/spliting on a region server, but a detailed reason is unknown.\\n- The script uses upsert and can be safely run many times.\\n\\n\\n  - For the previous day:\\n\\n    ``` bash\\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\\n    ```\\n\\n  - For a specified date:\\n\\n    ``` bash\\n    # e.g. 09-11-2019\\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20191109  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\\n    ```\\n\\n\\nb. [Duplicates between Impala and Kudu/HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#duplicates-between-impala-and-kuduhbase)\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log`\\n\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\\n\\n**Alerts**:\\n\\n- **Not Monitored**\\n\\n**Troubleshooting Steps**:\\n\\n- Check `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log` for errors\\n- You can safely skip this step if not running for the previous day\\n- Sample execution:\\n\\n  ``` bash\\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_ibank.service_audit_stream prod_trlog_ibank.service_audit_old ibank >> /var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log 2>&1",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for rerunning the job after a memory error and updating Grafana monitoring.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses a critical data processing job that impacts analytics in BigStreamer.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the job ('Enrich SA from SA_old') and the specific issue (memory error and monitoring)."
    },
    {
      "question": "What is the procedure to continue the merge batch and HBase upsert process for failed DWH flows due to a stalled job on February 29?",
      "answer": "Manually execute merge batch steps on PR site using instructions from the application flow documentation. Then use HDFS replication to copy data to DR and run the HBase upsert step using:\\n```bash\\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20240229\\n```\\nEnsure HBase markers and Kudu partitions are cleaned and reset properly using `alter table ... drop range partition` and `add range partition`.",
      "category": "Application Functionality & Flow",
      "context": "# mno - BigStreamer - IM2285747 - merge batch 29/2\\n\\n<b>Description:</b>\\n\\n```\\nGood evening,\\n\\ncould you please check the merge batch in DR/pr for 2/29. It seems to be still running. We have disabled it on 1/31 until this is finished.\\n\\nThanks,\\nThanos\\n```\\n\\n<b>Investigation:</b>\\n\\n**Ndef**: All log paths and query executions are found/executed from pr1edge01/dr1edge01.\\n\\n1. Login to Grafana and make sure that the failed step is the Merge Batch.\\n2. Login to DR/PR edge nodes and through the node's firefox check YARN at https://dr1node03:8090 and https://pr1node03:8090 for the PROD_Ibank merge batch job.\\n3. Check the stages tab for stages that have been completed for this job. At PR the `insert into` stages had completed after 8h. The RDD stages had failed and continued failing.\\n![Yarn UI](.media/IM2285747_1.png)\\n![Spark Stages](.media/IM2285747_2.png)\\n4. The 8 hour mark gives us some clues as to the failure. In essence after 8 hours kerberos tickets are dropped, leading to continuous authentication failures and timeouts as shown in the logs `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`.\\n5. Checked the merge batch logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log` and `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log` and indeed we found authorization errors.\\n6. Checked the size and count of the `service_audit_old` table through Impala and HDFS and it was found to be among the largest ever both in size (over 115 GB) and count (60 mil). The query used is `select  count(*) from prod_trlog_ibank.service_audit_old where par_dt=20240229`.\\n7. Inspecting the submition script we can see that for each individual sub-script the date is generated anew in the sub-script invocation. This means that since the merge in question spanned multiple dates, all steps started after the day's end wouldn't have completed succesfully.\\n8. Inspecting the range partitions through Impala with the query `show range partitions prod_trlog_ibank.service_audit_stream`, we can find multiple leftover range partitions since November. This added further computation time for the already large table.\\n\\nInvestigation for 05/03:\\n\\nFor this day, while the size and count were not unusually large the last step of the process, ie. the\\nhbase upsert took unusually long and couldn't complete on its own. The process seemed to get stuck on 1\\nsingular task as shown by the logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`, which leads us to conclude that a certain record was problematic because all tasks completed\\nnormally except a specific one that kept failling even on re-runs.\\n\\nInvestigation - Summary:\\n\\n- Greater than most end of the month dates size(over 115 GB) and count (60 mil)\\n- Multiple leftover range partitions\\n- Execution spanning multiple days, triggering the dynamic date issue with the submition script\\n- Stage execution surpassing the 8 hour mark, triggering a known kerberos authorization bug\\n- Problematic record for 05/03\\n\\n<b>Resolution:</b>\\n\\n1. Since the data was already loaded into PR for 29/2, the rest of the steps were executed manually on it\\n   according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#batch instructions for\\n   steps beyond the Merge Batch .\\n2. For all the remaining dates since 29/2 and because the cron jobs were stopped, each day was executed\\n   manually in full in each site, half of them (days) in PR and half of them (days) in DR following\\n   all the steps in the sub-steps guide from https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md .\\n3. For each job completed in one site it was replicated over to the other using HDFS replication, through\\n   the destination's Cloudera Manager for both `service_audit` and `service_audit_old` tables. **A similar\\n   procedure for table replication exists in [Table Replication](./20201218-IM1389913.md) but not for HDFS replication.**\\n   - /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit_old/par_dt=$date\\n   - /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit/par_dt=$date\\n4. Once replicated the HBase Upsert step was run on the destination site according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .\\n5. HBase markers were manually set for each job that didn't complete automatically according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .\\n6. Kudu range partitions were cleaned up manually for all previous days and 3 new partitions were created\\n   for the 3 previous days, in order for the cleanup script to continue functioning as nornal. More details\\n   on this can be found at the end of this document.\\n\\n\\nResolution for 05/03 HBase Upsert:\\n\\nFor this operation specifically more resources had to be allocated to the spark-submit job at `/opt/ingestion/PRODREST/ibank/spark/submit/visible_trn_hbase_daily_upsert/submitVisibleTrnToHbaseIndexesDailyUpsert_STABLE.sh`. After its completion resources were reverted back to normal:\\n```\\nIncrease node count to 12\\nDecrease core count to 1\\nIncrease tasks to 40\\n```\\n\\nKudu Range Partitions:\\n\\nRange partitions are created for the `service_audit_stream` table on the `u_timestamp` column. The commands to create and delete them\\ncan be found below for some example dates. Ndef that those partitions are in UTC time, so the time to\\ncreate/drop must be converted to local time, taking into account DST. For winter we are at GMT+2 so in order\\nto include a full day it must range for 22:00 of the previous to 22:00 of the current (where current is the day you want to delete).\\n\\n```sh\\n# Drop\\nalter table prod_trlog_ibank.service_audit_stream drop range partition '2023-02-26T22:00:00.000000Z' <= VALUES < '2023-02-27T22:00:00.000000Z' ;\\n\\n# Create\\nalter table prod_trlog_ibank.service_audit_stream add range partition '1970-01-01T00:00:00.000000Z' <= VALUES < '2023-02-28T22:00:00.000000Z' ;\\n```\\n\\nThe example above includes the first partition, which tracks from 1970 to the date in question. When deleting previous partitions, the first one\\nmust be recreated in order to include the time from 1970 to the first date, and then you can create the daily partitions. For example, lets say we have\\nthe following partitions:\\n\\n```\\n| 1970-01-01T00:00:00.000000Z <= VALUES < 2023-11-29T22:00:00.000000Z | # We want to drop this one in order to include a bigger range\\n| 2023-11-29T22:00:00.000000Z <= VALUES < 2023-11-30T22:00:00.000000Z | # We want to drop this one due to policy\\n| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\\n| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\\n| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\\n| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This must not be touched\\n| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |\\n| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |\\n| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |\\n```\\n\\nThe above will become:\\n\\n```\\n| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # This was created by including all dates from 1970 to this one\\n| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # This was dropped and recreated\\n| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # This was dropped and recreated\\n| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This was not touched\\n| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |\\n| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |\\n| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for manual execution of merge batch, HDFS replication, HBase upsert, and Kudu partition management.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Extremely relevant for telecom engineers as it addresses critical data warehouse flow failures that impact analytics operations in BigStreamer.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the failed process (merge batch and HBase upsert) and the specific date context (February 29)."
    },
    {
      "question": "How do you resolve a Kudu table creation error due to a 'Thrift SASL frame too long' error?",
      "answer": "Add this flag to all Kudu Masters under CM > Kudu > Safety Valve:\\n```bash\\n--hive_metastore_max_message_size_bytes=858993459\\n```\\nRestart the Kudu masters, then restart all tablet servers after stopping active ingestion flows by placing shutdown markers in HDFS. After tablets restart, retry the table creation query.",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# mno - BigStreamer - IM1908793 - Error on creating kudu table\\n\\n<b>Description:</b>\\n\\n```\\nGood morning.\\n\\nI am getting the following error while creating a temporary kudu table,\\n\\nERROR: ImpalaRuntimeException: Error creating Kudu table 'prod_trlog_card_analytical.opticash_dispencing_atm_tmp'\\n\\nCAUSED BY: NonRecoverableException: failed to wait for Hive Metastore notification log listener to catch up: failed to retrieve notification log events: failed to get Hive Metastore next notification: Thrift SASL frame is too long: 338.01M/100.00M\\n```\\n\\n<b>Actions Taken:</b>\\n\\n**Steps in order to investigate and make sure that the table is not created**\\n1. Login to `dr1edge01.mno.gr` with personal account and then to `dr1node01.mno.gr`\\n2. Move to the process folder:\\n```bash\\ncd /var/run/cloudera-scm-agent/process/\\n```\\n3. Find the latest process and go to that folder. In our case is 12200-kudu-KUDU_TSERVER. So move to that folder:\\n```bash\\ncd 12200-kudu-KUDU_TSERVER\\n```\\n4. Use the keytab you just found in that folder:\\n```bash\\nkinit -kt kudu.keytab kudu/`hostname`\\n```\\n5. Check kudu cluster health and specifically for `prod_trlog_card_analytical` database in order to check if the wanted table is created.\\n```bash\\nkudu cluster ksck dr1node04.mno.gr dr1node05.mno.gr dr1node06.mno.gr | grep -i prod_trlog_card_analytical\\n```\\n\\nAs you can see the `prod_trlog_card_analytical.opticash_dispencing_atm_tmp` table is not created.\\n\\n**Optional**: You can also verify that from impala-shell running the following commands:\\n- Login to `dr1edge01.mno.gr` with personal account\\n- impala-shell -i dr1edge01 -k --ssl\\n- `[dr1edge01.mno.gr:21000] default> use prod_trlog_card_analytical;`\\n- `[dr1edge01.mno.gr:21000] default> show tables;`\\n\\nAs you can see the `prod_trlog_card_analytical.opticash_dispencing_atm_tmp` table is not created.\\n\\n6. Login to CM DR with your pesonal account > Go to impala > Queries\\n7. In the search bar type the following in order to find the query:\\n`STATEMENT RLIKE '.*prod_trlog_card_analytical.opticash_dispencing_atm_tmp'.*` and click on the query details for investigation.\\n\\nWe found that the query they try to run is the following:\\n\\n```bash\\nCREATE TABLE IF NOT EXISTS prod_trog_card analytical.opticash ispencing_atm_tmp\\ncashp id, STRING NOT NULL\\n, transaction date STRING NOT NULL\\n,denom id STRING\\n, cassette STRING\\n, crncy id STRING\\n, open Bal BIGINT\\n, norm del BIGINT\\n, norm rtr BIGINT\\n, unpl_ del BIGINT\\n, unpl_tr BIGINT\\n, wthdrwls BIGINT\\n, pre_wdrw BIGINT\\n, deposits BIGINT\\n, clos_bal BIGINT\\n, bal_disp BIGINT\\n, bal_escr BIGINT\\n, bal_unav BIGINT\\n, opr_stat STRING\\n, excld_fl STRING\\n‚PRIMARY KEY (cashp_id, transaction date, denom_id, cassette)\\n) STORED AS KUDU\\n```\\n\\nWe try to rerun the above query and we get the following error:\\n```bash\\nERROR: ImpalaRuntimeException: Error creating Kudu table 'prod_trlog_card_analytical.opticash_dispencing_atm_tmp'\\nCAUSED BY: NonRecoverableException: failed to wait for Hive Metastore notification log listener to catch up: failed to retrieve notification log events: failed to get Hive Metastore next notification: Thrift SASL frame is too long: 338.01M/100.00M\\n```\\n\\n**Time to fix the problem**\\n\\nAs a first step, let's try to fix `Thrift SASL frame is too long: 338.01M/100.00M` error.\\n\\n1. Login to Cloudera Manager in DR site with your personal administrative account:\\n\\n`Kudu > Instances > Click on Master > Select Tab Configuration`\\n\\n2. In `Search` box write safety valvue and at `Master Advanced Configuration Snippet (Safety Valve for gflagfile)` add th following flag:\\n\\n```bash\\n--hive_metastore_max_message_size_bytes=858993459\\n```\\n\\n>**Important Ndef**:  The above step with flag must be set at all three masters\\n\\n3. Restart the three kudu masters (one at a time)\\n4. After rerunning the query the table is not still created but this time we get the following error: \\n```bash\\nSASL decode failed: SASL(-1): generic failure:\\nwO706 15:44:11.242372 109675 hms_notification_log_listener.cc:130] Hive Metastore notification log listener poll failed: Not authorized: failed to ret\\nrieve notification log events: failed to get Hive Metastore next notification: SASL decode failed: SASL(-1): generie failure:\\nw0706 15:44:35.127687 109673 hms_client.cc:345] Time spent get HMS notification events: real 8.885s user 0.000s sys 0.228s\\n```\\n\\n5. Restarting all Tablet Servers (dr1node01-10),one at a time, fixed the problem. \\n\\n**Before Restarting Tablets the following Flows must be stopped !!!**\\n```\\nPROD_IBANK_IngestStream_Visible\\nPROD_Online_IngestStream\\nPROD_IBank_IngestStream\\n```\\n\\nStop the flows:\\n\\n>Ndef: We used following command because flows were working fine. Otherwise, we you should kill the application.\\n\\n```bash\\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown _marker/\\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\\n```\\nWhen Tablets are all up and running make sure you start again the flows. \\n\\nVerify that Tablets and Kudu is up and running by checking graphs and CM UI (CM -> Kudu -> Charts Library)\\n\\nInformation about how to start flows can be found [here](http://https://metis.ghi.com/obss/oss/sysadmin-group/support/-/tree/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows \\\"here\\\")\\n\\nWe verified that the problem is fixed by running the querry and got Table has been created message.\\n\\n<b>Root Cause Analysis:</b>\\n\\nThis problem occurred due to dr1node07 disk replacement.\\n\\nPlease refer to *IM1893876* for more information.\\n\\nThe fact that kudu tablets were offline for more than 1 days resulted in networking issues between Tablets.\\n\\n\\n<b>Affected Systems:</b>\\n\\nDisaster Site\\n\\n\\n**Well Done!!**\\n\\n\\n\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for resolving the specific Kudu table creation error including configuration changes and restart procedures.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses a critical data storage issue that could impact analytics operations in BigStreamer.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the error condition ('Thrift SASL frame too long') and the specific operation (Kudu table creation)."
    },
    {
      "question": "What actions can resolve an Impala memory allocation error during a service audit enrichment job?",
      "answer": "The error 'Memory limit exceeded' from an Impala EXCHANGE_NODE can be resolved by rerunning the job with:\n```bash\n/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20230330\n```\nIf rerun fails again, reduce query parallelism or split the job into smaller subsets.",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# mno - BigStreamer - SD2180781 - Failed job at Grafana \\n\\n</b>Description:</b>\\n\\n```\\nGood evening.\\nThe following failed job appeared in Grafana:\\n\\nApplication: IBank_Migration\\nJob_name: Enrich SA from SA_old\\nComponment: JOB\\nDate: 30-03-2023\\nStatus: FAILED\\nHost: pr1edge01.mno.gr\\n```\\n\\n</b>Actions Taken:</b>\\n\\n1. We have to check first the logs from the failed job `Enrich SA from SA_old` as described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\\n\\nDetailed information from the above link\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\\n\\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n\\n\\n2. The error was the below from logs:\\n```\\nERROR: Memory limit exceeded: Failed to allocate row batch\\nEXCHANGE_NODE (id=5) could not allocate 1.00 MB without exceeding limit.\\nError occurred on backend pr1node05.mno.gr:22000\\nMemory left in process limit: 27.82 GB\\n\\n```\\n3. To verify the below error also checked from Cloudera > Impala > Queries\\n\\n![impala_query_error](.media/SD2180781/SD2180781_IMPALA_QUERY_ERROR.PNG)\\n\\n4. Now we have to go to `troubleshooting steps` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit) in order to check that no records are present in `prod_trlog_ibank.service_audit`\\n\\n  ``` bash\\n  # eg. 09-11-2019\\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \\\"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\\\"\\n  ```\\n\\n!Ndef : The par_dt is -1 from today\\n\\n5. No records exists on `par_dt` `20230330`\\n\\n6. Now we have to run the command `For the previous day:` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\\n\\nIf no records exist and no other process is up, you can ran the script again.\\n  - For the previous day:\\n\\n    ``` bash\\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\\n    ```\\n\\n  - For a specified date:\\n\\n    ``` bash\\n    # e.g. 09-11-2019\\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\\n    ```\\n\\n7. When the job finished `succesfully` we have to updated the monitoring postgres database in order for the entry `Enrich SA from SA_old` to appeared green/success in Grafana.\\n\\n```bash\\nssh Exxxx@pr1edge01.mno.gr\\nsudo -i -u postgres\\npsql -d monitoring\\nselect * from prod.monitoring where par_dt = 20230330;\\nINSERT INTO prod.monitoring (application, job_name,component,status,par_dt,start_time,end_time,description,params,host) VALUES ('IBank_Migration','Enrich SA from SA_old','JOB',0,20230330,'2023-03-31 03:18:30.000','2023-03-31 05:00:42.000','','','pr1edge01.mno.gr') ON CONFLICT (application, job_name,component,par_dt) DO UPDATE SET status=0, start_time='2023-03-31 03:18:30.000', end_time='2023-03-31 05:00:42.000',description='';\\n```\\n8. Check from Grafana that the job `Enrich SA from SA_old` is now `succeded`\\n9. After i execute the below steps mannualy from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md)\\n\\na. [Upsert to HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`\\n\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\\n\\n**Alerts**:\\n\\n- IBank_Migration Enrich hbase tables JOB\\n- IBank_Migration Enrich hbase tables Impala_insert\\n- IBank_Migration Enrich hbase tables Spark\\n\\n**Troubleshooting Steps**:\\n\\n- Use the script logs to identify the cause of the failure\\n\\n  Ndef: If job failed and the following error appears :`ERROR: RetriesExhaustedWithDetailsException: Failed <num> actions: CallTimeoutException: <num> times, servers with issues: [dr/pr]1node02.mno.gr`,  execute script again. The error has to do with HBase merging/spliting on a region server, but a detailed reason is unknown.\\n- The script uses upsert and can be safely run many times.\\n\\n\\n  - For the previous day:\\n\\n    ``` bash\\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\\n    ```\\n\\n  - For a specified date:\\n\\n    ``` bash\\n    # e.g. 09-11-2019\\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20191109  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\\n    ```\\n\\n\\nb. [Duplicates between Impala and Kudu/HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#duplicates-between-impala-and-kuduhbase)\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log`\\n\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\\n\\n**Alerts**:\\n\\n- **Not Monitored**\\n\\n**Troubleshooting Steps**:\\n\\n- Check `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log` for errors\\n- You can safely skip this step if not running for the previous day\\n- Sample execution:\\n\\n  ``` bash\\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_ibank.service_audit_stream prod_trlog_ibank.service_audit_old ibank >> /var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log 2>&1",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is explicitly answerable using the provided context, which includes detailed logs of the error message, specific paths to the script, and even example rerun commands. The answer aligns well with the troubleshooting steps outlined in the context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Resolving memory allocation errors in Impala during service audit jobs is a critical and realistic task for telecom engineers using BigStreamer. This is highly relevant in day-to-day troubleshooting of distributed data processing jobs.",
      "question_standalone_score": "4",
      "judge_feedback_question_standalone": "The question is mostly self-contained and understandable to someone familiar with Impala and service audit jobs. However, a reader unfamiliar with the BigStreamer platform or the specific job might find 'service audit enrichment job' slightly vague without additional context."
    },
    {
      "question": "How do you handle and recover from a failed disk replacement on a DataNode (dr1node02) in HDFS?",
      "answer": "Unmount affected partitions, run `bdadiskutility`, and if cache issues occur, clear preserved cache using:\n```bash\nMegaCli64 -DiscardPreservedCache -L7 -force -a0\n```\nRe-add virtual disk with:\n```bash\nMegaCli64 CfgLdAdd r0[ENCLOSURE:slot] a0\n```\nAfter configuration, remount disks, verify with `lsblk`, and restart the DataNode service.",
      "category": "Infrastructure & Deployment",
      "context": "# mno - BigStreamer - SD2389640 - hdfs - Data Directory Status\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\n\\nThe following alert has appeared in Cloudera Manager (DR):\\nDataNode (dr1node02)\\nData Directory Status\\n\\nThank you.\\n```\\n\\n<b>Actions Taken:</b>\\n\\nThere are references from the similar issue [20220620-SD1951890.md](20220620-SD1951890.md).\\n\\nAfter investigation we saw that the problem occurred due to disk issue on dr1node02.\\n\\nWe communicated with Oracle and disk replacement was scheduled.\\n\\n> Ndef that disk replacement perfomerd online so there was no downtime.\\n\\nWe followed the steps as described at [20220620-SD1951890.md](20220620-SD1951890.md) and [sync_mysql.md](sync_mysql.md), which include the following:\\n\\n1. Stopping the processes that specifically run at the disk slots `s1` and `s7` of the server `dr1node02`. On our case was the hdfs datanode and some yarn applications . We identified them with:\\n\\n2. Stopping the mysql slaves using the command:\\n```\\nmysql -u root -p\\nSHOW SLAVE STATUS\\\\G;\\n```\\n\\n3. Ensuring that the no processes are running at the partitions with the following commands:\\n```bash\\nlsof /u02\\n```\\n\\n```bash\\nlsof /u08\\n```\\n\\n4. Unmounting the two partitions, so the disks can be replaced.\\n\\n```bash\\numount <mountpoint>\\n```\\n\\n5. Once the disks have been replaced we ran the following command for both partitions:\\n\\n```bash\\nbdadiskutility /u02\\n```\\n\\n6. After running the command, we got the following error:\\n```\\nVirtual Drive <VIRTUAL_DRIVE_NUMBER> is incorrectly mapped.\\n<TIMESTAMP> : Error executing 'MegaCli64 CfgLdAdd r0[<ENCLOSURE>:<SLOT>] a0'\\n<TIMESTAMP> : Error code is 84 . Response is <<\\nAdapter 0: Configure Adapter Failed\\n\\nFW error description:\\nThe current operation is not allowed because the controller has data in cache for offline or missing virtual disks.\\n\\nExit Code: 0x54>>\\nFound a disk with a Firmware State of Unconfigured(good).\\nSuccessfully cleared the cache for the logical drive.\\nSuccessfully added the disk to its own RAID(0) volume.\\n```\\n\\n7. After communicating with Oracle Support [SR 3-36895603206 : Wrong disk status after replacement](https://support.oracle.com/epmos/faces/SrDetail?_afrLoop=206254157461870&srNumber=3-36895603206&queryModeName=Technical&needSrDetailRefresh=true&_afrvwxowMode=0&_adf.ctrl-state=iwvcvrye_184), we ran the following commands to solve the issue:\\n\\n\\n- `For s1 # The disk slot 1 of the server that corresponds to mount point /u02`\\n- `For s7 # The disk slot 7 of the server that corresponds to mount point /u08`\\n\\n- Validated if there is a cache pinned for any device, running command:\\n\\n```bash\\nMegaCli64 -GetPreservedCacheList -a0 \\n ```\\n\\nIf the old disk has pinned the cache, the command will return something like:\\n\\n```\\nAdapter #0\\n \\nVirtual Drive(Target ID 07): Missing.\\n \\nExit Code: 0x00\\n```\\n- In this case, the disk in slot 7 had the pinned cache and had to clear.\\n\\nRemove the pinned cache by running command:\\n\\n```bash\\n#MegaCli64 -DiscardPreservedCache -L7 -force -a0 <<<< where -LX should be replaced by the Target ID number reported in previous step.\\n```\\nGet the `ENCLOSURE_NUMBER`\\n```bash\\nMegaCli64 LdPdInfo a0 | more\\n```\\n- Added the virtual disk back\\n\\n```bash\\nMegaCli64 CfgLdAdd r0[ENCLOSURE_NUMBER:slot] a0\\n```\\nOn our case was:\\n\\nFor `s1`\\n```bash\\nMegaCli64 CfgLdAdd r0[252:1] a0\\n```\\n\\nFor `s7`\\n\\n```bash\\nMegaCli64 CfgLdAdd r0[252:7] a0\\n```\\n\\nStarted configuring the disk at `slot1`\\n\\n```bash\\nbdadiskutility -f /u02\\n```\\n\\nWait until the mirroring is finished and after that.\\n\\nStarted configuring the disk at `slot7`\\n\\n```bash\\nbdadiskutility -f /u08\\n```\\n\\n- Checks:\\n\\nFor `s1`:\\n\\n```bash\\nparted /dev/disk/by-hba-slot/s1 -s unit chs print\\niscsi # Check that all disks appeared\\nlsblk # Check that all disks appeared\\n```\\n\\nFor `s7`:\\n\\n```bash\\nparted /dev/disk/by-hba-slot/s7 -s unit chs print\\niscsi # Check that all disks appeared\\nlsblk # Check that all disks appeared\\n```\\n\\n8. We proceed with the start of the `datanode` role of `dr1node02`",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question directly relates to the failure described in the context, which provides a detailed log of the issue, root cause, and the recovery process, including all relevant commands. The answer is fully supported by the context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Handling hardware-level failures like disk replacements is critical for engineers managing HDFS and BigStreamer infrastructure. This question reflects a realistic and high-priority operational issue that engineers may face.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is self-contained, specifying both the failure type (disk replacement) and the component involved (DataNode in HDFS, dr1node02). No additional context is required to understand what the user is asking."
    },
    {
      "question": "What steps should be taken when all Cloudera services appear unhealthy due to NFS issues?",
      "answer": "Check logs like `/var/log/messages` and `cloudera-scm-agent.log` for NFS timeouts. Confirm NFS availability. Once the issue is resolved by the customer (e.g., freeing up space), Cloudera services should return to healthy state automatically.",
      "category": "Infrastructure & Deployment",
      "context": "# mno - BigStreamer - IM2241809 - Every Host on PR1 and DR1 are in critical state\\n\\n<b>Description:</b>\\n\\n```\\nGood evening,\\n\\nAll Hosts in Dr1 and PR1 are in critical state\\n\\nPlease take action.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n**Steps in order to investigate and make sure that the table is not created**\\n1. Login to `PR` and `DR` cloudera manager in order to check the health of each cluster. The status was unhealthy for all services on both clusters.\\n2. Login to `Grafana` in order to check that applications running. All the applications were running without errors.\\n3. ssh to `pr1edge01.mno.gr` with personal account\\n4. sudo to root\\n5. Move to the log folder:\\n```bash\\ncd /var/log\\n```\\n6. Check messages file\\n```bash\\nless messages\\n```\\nThe output was:\\n![image](.media/IM2241809/pr1edge01_messages.png)\\n\\n7. From the above output we saw that at `22:13:02 pr1edge01_kernel: nfs: server 999.999.999.999 not responding`.\\n8. Now lets check the `agent logs` of an internal node.\\n9. ssh to `pr1node03.mno.gr` with personal account\\n10. sudo to root\\n11. Move to the log folder:\\n```bash\\ncd /var/log/cloudera-scm-agent\\n```\\n12. Check `cloudera-scm-agent.log` file\\n```bash\\nless cloudera-scm-agent.log\\n```\\nThe output was:\\n![image](.media/IM2241809/pr1node03_agent_logs.png)\\n\\n13. Due to unavaliability of `nfs storage`(responisibility of the customer to maintain), `Host Monitor` service of Cloudera management services had `timeout` errors because couldn't collect metrics from each filesystem of the nodes.\\n14. Customer informed that `nfs storage` caused the issue on both clusters and the unhealthy state of all services was not real because `Host Monitor` was not able to collect metrics in order to be appeared on `CM`. Also all flows ran without errors during the issue.\\n15. Customer informed us that the `nfs` storage was full and after their actions it's ok. We checked the `CM` and all the services now is healthy.\\n\\n<b>Root Cause Analysis:</b>\\n\\nThis problem occurred due to `nfs` unavaliability.\\n\\n<b>Our Ticket Response:</b>\\n\\n```\\nGood evening,\\n\\nThe issue was caused by the nfs storage used on the nodes of both clusters becoming full. This resulted in the host monitor of the cloudera management services timeouting as it was unable to collect metrics for each filesystem of the nodes.\\n\\nRelevant screenshots are attached showing the above causes of the issue.\\n\\nThroughout the issue, the flows were up and running as seen in grafana as it was a malfunction of the management services resulting in the incorrect image of all services in Cloudera Manager PR & DR respectively.\\n\\nAfter space was freed up on the nfs, both clusters returned to good health.\\n\\nThank you.\\n```\\n\\n<b>Affected Systems:</b>\\n\\nDisaster/Primary Site\\n\\n\\n**Well Done!!**\\n\\n\\n\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context directly supports the question. It describes a scenario where all Cloudera services appear unhealthy due to NFS-related issues and provides detailed troubleshooting steps including log inspection and root cause analysis. The answer is fully derivable from this context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This is a highly relevant operational question for telecom engineers using BigStreamer. Understanding how NFS storage issues can affect Cloudera service health is critical for effective monitoring and incident resolution.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear and self-contained. It specifies the issue (all Cloudera services unhealthy) and the suspected cause (NFS issues), allowing it to be understood without needing to refer to the context."
    },
    {
      "question": "What caused the Merge Batch job failure on 2023-05-31, and how was it resolved?",
      "answer": "The failure was due to increased response sizes for service `/CAMPAIGNMANAGEMENT/GETCAMPAIGNS`, doubling average JSON size post 2023-05-12. The job was split into time chunks and run over three days. After completion, `distcp` was used for replication and upsert to HBase executed.",
      "category": "Data Management & Query Execution",
      "context": "# mno - BigStreamer - IM2158906 - Failed job at Grafana\\n\\n<b>Description:</b>\\n\\n```\\nGood evening\\nFollowing ticket SD2221480, the failed job [IBank_Ingestion]-[MergeBatch]-[JOB] appeared again but for d1edge01.mno.gr\\n\\nThank you.\\n```\\n\\n<b>Actions Taken:</b>\\nAfter communicating with the customer, we proceeded to manually kill the job as it was running for over 14 hours and was affecting live production flows.\\n\\nProceeding to investigate the issue, we saw the following:\\n\\n1. Going to Cloudera Manager => Yarn => Applications =>\\n\\n``\\nname RLIKE '.*PROD_IBank_MergeBatch' and application_duration > 3h\\n``\\n\\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/Yarn_Applications_Merge_Batch_Prod_Ibank.png)\\n\\nWe notice that the Merge batch for `20230531` was running `14 hours` without finishing.\\n\\n2. As a second step, let's check the number of records in `impala` and the space occupied in `hdfs` by each `par_dt` from `20230509` to `20230530`\\n\\nBelow we see the space at the `hdfs` level:\\n\\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/HDFS_du.png)\\n\\nThe number of records in `impala` as an example for the `par_dt` `20230511` and `20230512`:\\n\\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/impala_query_par_dt_count.PNG)\\n\\nWhat we observe above is that while the `par_dt` `20230511` and `20230512` they have no difference in the number of records, they are twice as large. Where did this come from and what impact does it have? We will analyze it in the next steps.\\n\\n3. Let's see how this increase came about:\\n\\nAnalyzing the sum of length for response_text_data for each service, we notice that from `12/05/2023` onwards the service_name `'/CAMPAIGNMANAGEMENT/GETCAMPAIGNS'` takes up much more space as shown below compared to previous days.\\n\\nAfter `20230512`\\n\\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/After_20230512.png)\\n\\nBefore `20230512`\\n\\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/Before_20230512.png)\\n\\nUntil 11/05/2023 the average avg(length(response_text_data)) of prod_trlog_ibank.service_audit is stable at ~12K while from 12/05/2023 we see it approximately doubling.\\n\\nThis resulted in the `Merge Batch` not finishing as since the size of each `json` has doubled it takes much longer to execute the spark job.\\n\\n4. How did we handle it to get it running?\\n\\nAs described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#merge-batch) in the subchapter `If the problem is with resources (out-of-memory errors):` we ran the `Merge Batch` in separate chunks of the day.\\n\\nThe process took 3 days to complete as each chunk of the day took ~9 hours.\\n\\n5. After it was completed on one site, `distcp` was performed as described in [issue](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/issues/20201218-IM1389913.md) for the `prod_trlog_ibank.service_audit` and `prod_trlog_ibank.service_audit_old` tables with `par_dt` `20230530` on the other site. Finally, on the other site, after `distcp` was completed, we ran [upsert-to-hbase-migration](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\\n\\n6. Finally, after all the steps were completed on both sites, the developers manually deleted the old data in `kudu`.\\n\\nNdef:\\n\\nAfter all the above was completed, we should proceed with the consultation with the bank, to execute the `DWH` flows for the days that did not run due to the above issue. The `DWH` flow was removed, so we did not need to take any action.",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context thoroughly explains the root cause of the job failure (increased JSON size for a specific service), the timeline (from May 12 onwards), the exact service involved, the intermediate steps (splitting the job), and the resolution steps including HBase upsert and `distcp`. It fully supports the answer provided.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The question addresses a concrete failure scenario encountered in production, involving performance issues, Spark job execution, and resolution procedures involving distributed systems. This type of issue is highly relevant for engineers operating BigStreamer in telecom environments.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is specific and fully understandable on its own. It clearly refers to a known job failure on a specific date and asks both about the cause and resolution, without vague or context-dependent language."
    },
    {
      "question": "How can you reduce the risk of Kudu scan timeouts during Spark MergeBatch jobs?",
      "answer": "Ensure the correct number of partitions is used. In the case of the Online MergeBatch job, 468 partitions caused timeout errors. It was fixed by adjusting to 180 partitions for `prod_trlog_online.service_audit_stream` and deleting unnecessary Kudu disk data.",
      "category": "Data Management & Query Execution",
      "context": "# mno - BigStreamer - IM2193241 - Failed job in Grafana\\n\\n</b>Description:</b>\\n\\n```text\\nGood morning...\\n\\nThe following failed job appeared in Grafana today 26/07:\\n\\nApplication: Online_Ingestion\\nJob_name: MergeBatch\\nComponment: JOB\\nDate: 25-07-2023\\nHost: pr1edge01.mno.gr\\n\\nPlease take action.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Re-run the failed step as described [here](../supportDocuments/applicationFlows/online.md#batch)\\n2. The flow completed successfully, we proceeded with the investigation\\n\\n   Logs from the application:\\n\\n   ```\\n   Caused by: org.apache.kudu.client.NonRecoverableException: cannot complete before timeout: ScanRequest(scannerId=\\\"22c757bfcf674a05a08f14c316e745e9\\\", tablet=c42b07f18435403297fee37add478c0b, attempt=1, KuduRpc(method=Scan, tablet=c42b07f18435403297fee37add478c0b, attempt=1, TimeoutTracker(timeout=30000, elapsed=30004), Trace Summary(0 ms): Sent(1), Received(0), Delayed(0), MasterRefresh(0), AuthRefresh(0), Truncated: false \\n   ```\\n\\n   Spark UI:\\n\\n   ![Spark UI](.media/IM2193241_1.png)\\n\\n3. Stage 0 should have 180 partitions not 468\\n\\n   ![Spark UI normal](.media/IM2193241_2.png)\\n\\n4. Informed development team to correct the number of partitions for `prod_trlog_online.service_audit_stream`. This deleted unnecessary data from Kudu's disks and next run (see 3) did not have any failed tasks.\\n\\n<b>Affected Systems:</b>\\n\\nmno Primary Site\\n\\n<b>Action Points:</b>",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context clearly describes the cause of the failure (Kudu scan timeout), presents evidence from logs and Spark UI, and shows that reducing the number of partitions and cleaning up disk space resolved the issue. This supports the answer completely.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This is highly relevant to engineers operating Spark jobs on BigStreamer, as it deals with partition tuning, performance bottlenecks, and resolution of critical runtime errors — all typical challenges in real production environments.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is fully self-contained, specifying the system (Kudu), issue (scan timeouts), and job type (Spark MergeBatch) without referring to prior context or using vague language."
    },
    {
      "question": "How is the Radius flow triggered, and what are the key processing steps for radacct data?",
      "answer": "The Radius flow is triggered every 1h and 30 minutes via a coordinator. Files are fetched from the input SFTP server and stored temporarily. Then they are decompressed, loaded into Hive (staging tables), and enriched using reference data. The final enriched CSV files are sent to the Trustcenter SFTP server, and finally inserted into Kudu and Impala tables (`radius.radacct`).",
      "category": "Application Functionality & Flow",
      "context": "# Radius\\n\\n## Main Flow\\n\\n``` mermaid\\n    graph TD\\n    subgraph AA[Startup]\\n      direction TB\\n      AA1(\\\"Cleanup HDFS Folder:\\\" /ez/warehouse/radius.db/tmp)-->\\n      AA2(\\\"Create Local Folder:\\\" ./sftp_files)-->\\n      AA3(\\\"Create Local Folder:\\\" ./exported_files)-->\\n      AA4(\\\"Housekeep Trustcenter SFTP server\\\")\\n    end\\n    subgraph AB[Load]\\n      AB1(\\\"SFTP Server\\\") -->|Excluding:<br>Non-valid filenames<br>Files with suffix .LOADED<br>Files older than `days_back` according to filename | AB2(\\\"Temporary Directory on Nodemanager: ./sftp_files\\\")\\n      AB1 -->|Add suffix .LOADED to<br>filenames in HDFS file:<br>/user/radius/unrenamed_files|AB1\\n      AB2 -->|Decompress| AB3(\\\"Temporary Directory on Nodemanager: ./sftp_files\\\")\\n    end\\n    subgraph AC[radarchive]\\n      AC1(\\\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\\\") -->|Hive LOAD|AC2(\\\"Hive: radius.radarchive_stg\\\")\\n      AC2 -->|Refresh/Impala Insert|AC3(\\\"Impala: radius.radarchive\\\")\\n      AC2 -->|Refresh/Impala Upsert<br>Reduced fields|AC4(\\\"Kudu: radius.radreference\\\")\\n    end\\n    subgraph AD[radacct]\\n      subgraph ADA[File Export]\\n        direction TB\\n        AD1(\\\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\\\") -->|Hive LOAD|AD2(\\\"Hive: radius.radacct_stg\\\")\\n        AD6(\\\"npce.fixed_super_repo<br>Responsibility: abc\\\") -->AD8\\n        AD7(\\\"demo.dummy_radius_dslams<br>Responsibility: abc\\\") -->AD8\\n        AD2 --> AD8(\\\"Join\\\")\\n        AD8 -->|Refresh/Impala Select|AD9(\\\"Temporary Directory on Nodemanager:<br> ./exported_files/radacct_enriched_yearMonthDay_startHour-endHour.csv\\\")\\n        AD9 -->|SFTP Put|AD10(\\\"Trustcenter SFTP\\\")\\n      end\\n      subgraph ADB[Populate Table]\\n        direction TB\\n        AD3(\\\"Kudu: radius.radreference\\\") -->AD4\\n        AD11(\\\"Hive: radius.radacct_stg\\\") -->AD4(\\\"Join on username\\\")\\n        AD4 -->|Refresh/Impala Insert|AD5(\\\"Impala: radius.radacct\\\")\\n      end\\n      AD10 -.->|On successful SFTP PUT<br>Populate the table| AD3\\n    end\\n    subgraph AE[Finish]\\n      AE1(\\\"SFTP Server: Add suffix .LOADED to file\\\")\\n    end\\n    AA4 --> AB1\\n    AB3 -->|Filename: radarchive_yearMonthDay_startHour-endHour.csv|AC1\\n    AB3 -->|Filename: radacct_yearMonthDay_startHour-endHour.csv|AD1\\n    AC4 -->|On Success|AE1\\n    AD5 -->|On Success|AE1\\n```\\n\\n- **User**: `radius`  \\n- **Coordinator**: `Radius_Load_Coordinator`  \\n- **Workflow**: `Radius_Load_Workflow`\\n- **HDFS path**: `/user/radius`\\n- **Runs**: `every 1h and 30mins`\\n- **Config file**: `HDFS: /user/radius/config/settings_prod.ini`\\n- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\\n- **Input SFTP Server**:\\n  - **Host**: `999.999.999.999`\\n  - **Port**: `22`\\n  - **User**: `prdts`\\n  - **Remdef Files Folder**: `/home/prdts/transfer`\\n  - **Port Forward**:\\n    - **Host**: `un-vip.bigdata.abc.gr`\\n    - **Port**: `2222`\\n- **Trustcenter SFTP Server**:\\n  - **Host**: `unc2.bigdata.abc.gr`\\n  - **Port**: `22`\\n  - **User**: `trustcenterftp`\\n  - **Remdef Files Folder**: `/rd`\\n\\n**Alerts**:\\n\\n- Mail\\n  - Subject: Radius Flow failed\\n  - Alerts that indicate problem with Input SFTP server:\\n    - Body starts with: `No upcoming files for more than 3h`\\n    - Body starts with: `Files found with a late timestamp.`\\n    - Body starts with: `Could not rename file`\\n  - Alerts that indicate problem with Trustcenter SFTP Server:\\n    - Body starts with: `Could not perform radacct_enriched housekeeping on sftp server`\\n  - Alerts that indicate general failures without specific cause:\\n    - Body starts with: `Insert data failed` and then lists the status for each file\\n\\n**Troubleshooting Steps**:\\n\\n- If an alert has been received, use the message from the e-mail to determine the root cause for the failure.\\n  - For failures with Input SFTP server:\\n    - Inform abc in order to check the Input SFTP Server\\n    - If abc does not detect any problems, check connectivity:\\n\\n      From `un2.bigdata.abc.gr` with personal user:\\n  \\n      ``` bash\\n      su - radius\\n      sftp prdts@999.999.999.999\\n      # Check for files\\n      sftp> ls -l\\n      ```\\n\\n  - For failures with Trustcenter SFTP Server:\\n    - Check `unc2.bigdata.abc.gr` for any errors\\n- If no alerts have been received or the problem is not specified determine the root cause for the failure with the following steps:\\n  - Check for failed executions\\n\\n    From `un2.bigdata.abc.gr` with personal user:\\n\\n    ```bash\\n    curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=RADIUS&status=FAILED&operativePartition=<date in YYYYMMDD e.g.:20220518>'\\n    ```\\n\\n    For additional query parameters check [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\\n\\n    or check via the corresponding [Grafana dashboard](https://unc1.bigdata.abc.gr:3000/d/J4KPyBoVk/radius-dashboard?orgId=1&from=now-2d&to=now)\\n\\n  - Check the logs for the failed execution.\\n- After the underlying problem has been corrected the next execution of the flow will load any files that have been delayed or failed to be loaded in a previous execution.\\n- If the customer requests to load specific files after they have been processed by the flow. **In order to avoid duplicate data we have to reload the whole partition.**\\n  - Ensure the files are still available on the SFTP server\\n\\n    From `un2.bigdata.abc.gr` with personal user:\\n  \\n    ``` bash\\n    su - radius\\n    sftp prdts@999.999.999.999\\n    # Check for files covering the whole date affected\\n    sftp> ls -l *.LOADED\\n    ```\\n  \\n  - Suspend coordinator `Radius_Load_Coordinator`\\n  - Drop partitions that were not properly loaded from following tables\\n\\n    From `un2.bigdata.abc.gr` with personal user:\\n\\n    ``` bash\\n    su - radius\\n    kinit -kt /home/users/radius/radius.keytab radius\\n    impala-shell -i un-vip.bigdata.abc.gr -k --ssl\\n    ```\\n\\n    - In case of radarchive category of file:\\n\\n      ```sql\\n      alter table radius.radarchive drop partitions (par_dt=\\\"<date in YYYYMMDD e.g.: 20220915>\\\")\\n      ```\\n\\n    - In case of radacct category of file:\\n\\n      ```sql\\n      alter table radius.radacct drop partitions (par_dt=\\\"<date in YYYYMMDD e.g.: 20220915>\\\")\\n      ```\\n\\n  - Connect to SFTP server and rename files with filename timestamp equal to above-mentioned partition(s) and the same category:\\n\\n    From `un2.bigdata.abc.gr` with personal user:\\n\\n    ``` bash\\n    su - radius\\n    sftp prdts@999.999.999.999\\n    # For every file you need to reload\\n    sftp> rename <filename>.LOADED <filename>\\n    ```\\n\\n  - Resume coordinator `Radius_Load_Coordinator`\\n\\n## Kudu Housekeeping and Compute Statistics Flow\\n\\n```mermaid\\n  graph TD\\n  A[Apply 5-day rention to<br>Kudu: radius.radreference]\\n  B[Compute Impala Statistics for<br>radius.radarchive<br>radius.radacct]\\n```\\n\\n- **User**: `radius`  \\n- **Coordinator**: `Radius_Kudu_Retention_Coordinator`  \\n- **Workflow**: `Radius_Kudu_Retention_Workflow`\\n- **HDFS path**: `/user/radius`\\n- **Runs**: `once a day at 2:25 (UTC)`\\n- **Config file**: `hdfs: /user/radius/Kudu_Retention_And_Compute_Stats/settings.ini`\\n- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\\n\\n**Alerts**:\\n\\n- Not monitored\\n\\n**Troubleshooting Steps**:\\n\\n- Check the logs for the failed execution.\\n- After the underlying problem has been corrected the next execution of the flow will apply the correct retention to the Kudu table and compute the missing statistics.",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context provides a detailed mermaid diagram and textual description of the flow, including timing, trigger mechanism (coordinator), and specific steps for handling radacct files. The answer is directly grounded in the details from the context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Understanding how and when the Radius flow runs and how radacct data is processed is critical for telecom engineers managing ingestion pipelines and troubleshooting job execution issues. This is highly relevant to their operations.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear, precise, and fully understandable without any external context. It explicitly asks about the trigger mechanism and steps for a specific data type, making it fully standalone."
    },
    {
      "question": "What is the role of the `210_refData_Load.sh` and `220_refData_Daily_Snapshot.sh` scripts in the Reference Data Flow?",
      "answer": "`210_refData_Load.sh` loads reference CSV files (e.g., cells_20230530.csv.gz) into Hive load tables. It must be executed with the reference type and date:\n```bash\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20230530\n```\nThen `220_refData_Daily_Snapshot.sh` creates a daily snapshot from the most recent partition into snapshot tables for consumption.",
      "category": "Application Functionality & Flow",
      "context": "# Reference Data Flow\\n\\n## Installation info\\n\\n### Data Source File\\n- Local FileSystem Directories\\n  - host :`un2.bigdata.abc.gr (999.999.999.999)`\\n  - user : `vantagerd`\\n  - spool area : `/data/1/vantage_ref-data/REF-DATA/` link points to `/shared/vantage_ref-data/REF-DATA`\\n  - file_types : `<refType>_<refDate>.csv.gz`  \\n*\\\\<refType\\\\>: `cells, crm, devices, services` (i.e. cells_20230530.csv.gz)*\\n  - load_suffix : `<YYYYMMDD>.LOADED` *(i.e. cells_20230530.csv.cells_20230531.LOADED)*\\n\\n- HDFS Directories\\n\t- hdfs landingzone : `/ez/landingzone/REFDATA`\\n\\n### Scripts-Logs Locations\\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\\n- user : `intra`\\n- script path : `/shared/abc/refdata/bin`\\n- script files: \\n\t- `210_refData_Load.sh`\\n\t- `220_refData_Daily_Snapshot.sh`\\n\\n- log path : `/shared/abc/refdata/log`\\n- log files: \\n\t- `210_refData_Load.<YYYYMM>.log`\\n\t- `220_refData_Daily_Snapshot.<YYYYMM>.log`\\n\\n### Crontab Scheduling\\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\\n- user : `intra`  \\n\truns at : Daily at 00:05\\n\t`5 0 * * * /shared/abc/refdata/bin/210_refData_Load.sh CELLS $(date '+\\\\%Y\\\\%m\\\\%d' -d \\\"yesterday\\\")`\\n\t\\nNdef1: The entry above loads reference data for CELLS.  \\nNdef2: For each reference type a new entry of the `210_refData_Load.sh` is required passing the following parameters  \\n\t\\\\<reference Type\\\\> : `cells, crm, devices, services`  \\n\t\\\\<reference Date\\\\> : `yesterday` is the default value  \\n\\n### Hive Tables\\n- Target Database: `refdata`\\n- Target Tables: \\n\t1. `rd_cells_load`\\n\t1. `rd_services_load`\\n\t1. `rd_crm_load`\\n\t1. `rf_devices_load`\\n\\n\\n## Data process\\n### High Level Overview\\n\\n![High_Level_Overview](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/refdata/-/raw/main/docs/ReferenceData.High_Level_Overview.png)\\n\\n##### Steps 1-3: \\nabc is responsible for the preparation/creation of the Reference Data flat files.  \\nThese files are stored into a specific directory in `UN2` node using the SFTP-PUT method as user `vantagerd`  \\n\\n##### Steps 4-5:\\nScript `210_refData_Load.sh` is responsible to read, parse and load the contents of reference files into HIVE tables (aka LOAD tables).  \\nThese tables keep the data of all completed loads. That is, they contain all the historicity of the reference data.  \\nThe data of each load is stored in a separate partition identified by the date of the loading (i.e. par_dt=20230530)  \\n\\n##### Steps 6-7:\\nScript `220_refData_Daily_Snapshot.sh` reads the most recently added data from the LOAD table and store them as a snapshot into a separate table (aka snapshot tables).  \\nThe data of these tables are used for the enrichment of various fact data (i.e. Traffica (SAI))\\n\\n\\n## Manually Run\\n`210_refData_Load.sh` script is responsible for the loading of a reference file.\\n\\nTo run the script two arguments are required  \\n`/shared/abc/refdata/bin/210_refData_Load.sh <refType>  <refDate>`  \\n\\n1st: **\\\\<refType\\\\>**, the Reference Type\\n```\\n- CELLS\\n- CRM\\n- DEVICES\\n- SERVICES\\n```\\n\\n2nd: **\\\\<refDate\\\\>**, the date that the flat file contains in its filename  \\n\ti.e.\\n```\\ncells_20220207.csv.gz\\ncells_20220208.csv.gz\\ncells_20220209.csv.gz\\n\\nservices_20220207.csv.gz\\ndevices_20220208.csv.gz\\n\\ncrm_20220209.csv.gz\\n```\\n\\nIn case of loading the files above we should execute the following commands\\n```\\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20220207\\n/shared/abc/refdata/bin/210_refData_Load.sh cells 20220208\\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20220209\\n\\n/shared/abc/refdata/bin/210_refData_Load.sh services 20220207\\n/shared/abc/refdata/bin/210_refData_Load.sh DEVICES 20220208\\n\\n/shared/abc/refdata/bin/210_refData_Load.sh crm 20220209\\n```\\n\\nOnce the loads completed, the flat files renamed to <CURRENT_YYYYMMDD>.LOADED\\n```\\ncells_20220207.csv.20230531.LOADED\\ncells_20220208.csv.20230531.LOADED\\ncells_20220209.csv.20230531.LOADED\\n\\nservices_20220207.csv.20230531.LOADED\\ndevices_20220208.csv.20230531.LOADED\\n\\ncrm_20220209.csv.20230531.LOADED\\n```\\n\\n## Troubleshooting\\n**Currently, the Reference Data flow does not support the `Monitoring` services.**  \\n\\n- An email will be sent by the system with the point of failure.\\ni.e.\\n```\\nSubject: ALERT: Reference data Loading, Type:CELL,  File:cells_20220207.csv\\nBody: \\n\tReference Type  : CELL\\n\tReference File  : cells_20220207.csv\\n\tReference Scirpt: 210_refData_Load.sh\\n\t------------------------------------------\\n\tERROR:$(date '+%F %T'), ALTER TABLE or LOAD DATA command failed.\\n```\\n\\n- Check the log files for errors/exceptions  \\n\\n```\\negrep -i 'error|fail|exception|problem' /shared/abc/refdata/log/210_refData_Load.YYYYMM.log\\negrep -i 'error|fail|exception|problem' /shared/abc/refdata/log/220_refData_Daily_Snapshot.YYYYMM.log\\n```\\n\\nIn case of failure follow the instructions described in **`Manually Run`**\\n\\n### Common errors  \\n- Reference data file is empty or the contents of the file is not the expected.  \\nIf this is the case, update abc that the file is invalid and ask them to send a new.  \\n\\n- Other factors not related to the specific flow\\n\t- impala/hive availability\\n\t- Kerberos authentication\\n\t*Ndef: The flow checks if the ticket is still active before any HDFS action.  \\n\tIn case of expiration the flow performs a `kinit` command*\\n\\n## Data Check\\n- **Check final tables for new partitions**:\\n\t```\\n\tsu - intra\\n\t\\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k -q \\\"refresh refdata.rd_cells_load; show partitions refdata.rd_cells_load;\\\"\\n\t\\n\t+----------+-----------+--------+---------+\\n\t| par_dt   | #Rows     | #Files | Size    |\\n\t+----------+-----------+--------+---------+\\n\t| 20220227 | 98090     | 1      | 41.88MB |\\n\t| 20220228 | 98021     | 1      | 41.84MB |\\n\t| 20220301 | 97353     | 1      | 41.76MB |\\n\t| Total    | 142404322 | 1500   | 59.63GB |\\n\t+----------+-----------+--------+---------+\\n\t```\\n\\n- **Check the amount of data in final tables**:\\n\t```\\n\tsu - intra\\n\t\\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k -q \\\"select par_dt, count(*) as cnt from refdata.rd_cells_load group by par_dt order by 1;\\\"\\n\t\\n\tpar_dt   | cnt    \\n\t---------+--------\\n\t20221130 | 2784494\\n\t```\\n\t",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context explicitly describes the role of both scripts, including what they load, how they store partitions, and their usage examples. The question is fully and unambiguously answerable from the context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Understanding the role of these scripts is essential for managing reference data ingestion and ensuring data integrity across analytics processes. This is a common and practical task for BigStreamer engineers.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear, self-contained, and understandable without any external information. It names both scripts and asks for their role within the Reference Data Flow, which is unambiguous."
    },
    {
      "question": "How should you troubleshoot the `AppEmptyQueryException` alert caused by missing CPU/MEM interface data in the IPVPN flow?",
      "answer": "Check the logs:\n```bash\ntail -f /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20231012.log\n```\nLook for messages like 'No file found.. sleep and try again..'. Also confirm no files were created on the NNM server by checking:\n```bash\nssh custompoller@nnmprd01\ntail -f /home/custompoller/export_metrics/log/transfer-new-files-cut.20231012.log\n```",
      "category": "Troubleshooting & Issue Resolution",
      "context": "[[[_TOC_]]\\n# Introduction\\nThis support document outlines the steps that compose the IPVPN-SM replacement process that computes Key Performance Indicators (KPIs) regarding the network quality of abc. The ETL process involves a series of scripts and a Spring Boot application that work together to retrieve, transform, and load data from Impala tables into a remdef server.\\n\\nThere are three major KPI categories, *Components*, *SLA* and *Interfaces*, which consist of the following subcategories:\\n|KPI category|Metrics involved| Relevant Impala tables|\\n|---|---|---|\\n|Components|CPU, Memory (MEM)|bigcust.nnm_ipvpn_componentmetrics_hist,<br>  nnmnps.nms_node |\\n|Interfaces|Interfaces (IF)|bigcust.perf_interfacemetrics_ipvpn_hist,<br>  bigcust.pe_interfaces,<br>  nnmnps.nms_node|\\n|SLA|Availability (AV), Quality of Service (QOS)|bigcust.nnmcp_ipvpn_slametrics_hist,<br> bigcust.customer_pl,<br> bigcust.customer_sla_config_ipvpn,<br> bigcust.sla_configurations|\\n# Application Flow\\n## Scripts & IPVPN-SM App operations\\nThe metrics computation of each of the three categories (sla, components, interfaces) is triggered by the `un2:/shared/abc/ip_vpn/run/initiate_export_*.sh` script that runs as part of the IPVPN-SLA [cronjob](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/blob/master/PROD/crontab/crontab_per_user.txt), scheduled to run every 5 minutes, and the computation refers to a full five-minute interval.\\nThe ETL process follows the following flow:\\n\\n### IPVPN-SLA\\n- [IPVPN-SLA Support Doc](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ip_vpn.md)\\n\\n### IPVPN-SM\\n- [IPVPN-SM Dev Wiki](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/home)\\n3. The `export_*_kpis.sh` calls the IPVPN-SM App to execute the Impala queries. The curl request to the IPVPN-SM App is executed asynchronously, allowing the IPVPN-SM pipeline to continue with other scheduled 5 min intervals without waiting for the query results.\\n4. The IPVPN-SM App executes the Impala queries, applies transformations on the result set based on the agreed schema specifications, and sends the results to the SQM server.\\n## IPVPN-SM Application\\nThe IPVPN-SM Application executes the Impala queries specified in the `/shared/abc/ip_vpn/sm-app/deployment/config/field-mappings.yml`, applies transformations on the result set based on the agreed [schema specifications](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Data-Transmission-Schema) , and sends the results to the SQM server.\\n\\n### Ingestion Endpoint\\nThe endpoint for the IPVPN-SM application is `/ingest-query`. This endpoint should be called with a POST request.\\nDetails on the request body fields and examples [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Application-details#request-body-1)\\n\\n### Check application status\\n\\n```\\ncurl --location --request GET 'un-vip:13001/ipvpn-sm/app/status'\\n```\\n\\n## Application Flow Diagram\\n\\n```mermaid\\nflowchart TD;\\n  A(\\\"/shared/abc/ip_vpn/run/initiate_export_*.sh <br> ipvpn@un2\\\") --> B(\\\"/shared/abc/ip_vpn/run/export_*_kpis.sh <br> ipvpn@un2\\\")\\n  B -- 1. Calls Sync ----> C(\\\"/shared/abc/ip_vpn/sm-replacement/scripts/refresh_*_kpis.sh <br> ipvpn@un2\\\")\\n  B -- 2. Calls Async ---> I(\\\"/shared/abc/ip_vpn/sm-replacement/scripts/compute_metrics_via_sm_app.sh <br> ipvpn@un2\\\")--> J(\\\"/shared/abc/ip_vpn/sm-replacement/scripts/query_sm.sh <br> ipvpn@un2\\\")\\n  J -- /ingest-query request--->IPVPN-SM_Replacement_Application\\n  subgraph IPVPN-SM_Replacement_Application[\\\"root@un1/un2\\\"]\\n\t E(Impala Queries)\\n\t F(Data with agreed schema)\\n\\n  end\\n  E -- data transformation --> F\\n  F -- POST --> G[SQM Server]\\n  style A fill:#F8F8F8\\n  style B fill:#F8F8F8\\n  style IPVPN-SM_Replacement_Application fill:#DCDCDC\\n  style I fill:#DCDCDC\\n  style C fill:#DCDCDC\\n  style E fill:#DCDCDC\\n  style F fill:#DCDCDC\\n  style J fill:#DCDCDC\\n  style G fill:darkgray\\n```\\n\\n# Infrastructure\\nThe ETL pipeline infrastructure includes the following components:\\n\\n## Application\\n- un1/un2 files\\n\t- **Deployment path**: `/shared/abc/ip_vpn/sm-app/deployment`\\n\t- **Configuration**: `/shared/abc/ip_vpn/sm-app/deployment/config`\\n\t\t- [application.yml](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/blob/main/PROD/deployment/config/application.yml)\\n\t- **Logs**: `/shared/abc/ip_vpn/sm-app/deployment/logs`\\n\\n- Data sources:\\n\t- **Impala**:  `un-vip.bigdata.abc.gr:21050`\\n\t- **hive database**: `bigcust`\\n\\n- Authentication:\\n  The authentication to Impala is done with Kerberos. The files used are:\\n    - `/etc/krb5.conf`\\n    - `/shared/abc/ip_vpn/sm-app/deployment/config/jaas.conf`\\n    - `/home/users/ipvpn/ipvpn.keytab`\\n\\n- IPVPN-SM app:\\n  The application uses HAProxy, a load balancer that distributes incoming requests to the deployed application nodes.\\n  - **HAProxy**:\\n  Host: `un-vip` ,\\n  Port: `13001`\\n  - **Application**:\\n    Hosts: `un1/un2`,\\n    Active Ports:\\n      - HTTP port : `13000`\\n      - JMX port : `13800`\\n\\n\\n- External SQM server:\\n\t- **host**: `gw-prod-sqm.aksprd.abc.gr`\\n\t- **port**: `443`\\n\t- **endpoint**: `/inventory/measurements`\\n\\n\\n## Bash scripts\\nIn addition to the IPVPN-SM application, IPVPN-SM utilizes certain bash scripts. Their primary purpose is to generate the appropriate input data that is subsequently passed as the body parameter in the POST requests to the IPVPN-SM application.\\nThey are invoked by the IPVPN-SLA `export_*_kpis.sh` [scripts](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/blob/master/PROD/nnmnps/bin/export_component_kpis.sh).\\n- **Node**: `un2`\\n- **User**: `ipvpn`\\n- **Scripts**: `/shared/abc/ip_vpn/sm-replacement/scripts`\\n- **IPVPN-SM App host & port configured in file** : `/shared/abc/ip_vpn/sm-replacement/scripts/config.properties`\\n- **Logs**: `/shared/abc/ip_vpn/sm-replacement/log`\\n\\nThe specific order in which they are called is the following:\\n```mermaid\\ngraph LR\\nA[\\\"export_*_kpis.sh<br><i>IPVPN-SLA</i>\\\"] -->B[compute_metrics_via_sm_app.sh]\\nB -->C[query_sm.sh]\\nC --/ingest-query-->D(SM Application)\\nstyle A fill:darkgray\\nstyle B fill:#ECECEC\\nstyle C fill:#ECECEC\\n```\\n\\n## Deployment Instructions\\n\\nStop/Start guide on [abc_BigStreamer_Apps_MoP_Restart.docx](https://ghi.sharepoint.com/:w:/t/abcBigData/EZsK97GxFyVHkeBoFTFjuRYBtPF7lK86cTA44wMhayNlNA?e=Gjh0ZO)  \\nCheck project wiki [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Application-details#deployment-instructions)\\n\\n\\n## SQM Token Authentication\\nThe authentication on the SQM server is done with the HTTP Token Auth in IPVPN-SM application. It requires providing the keystore file, where the token is stored.\\n\\n# Failure Handling\\n## Logs\\n\\n#### Script Logs on un2\\nThe scripts which refresh the Impala tables and which call the IPVPN-SM application write logs under the directories:\\n1. The `refresh_*_.sh script` Logs\\n`/shared/abc/ip_vpn/sm-replacement/log/refresh_*_kpis`\\n2. The `compute_metrics_via_sm_app.sh` Logs\\n`/shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*`\\n\\nThe asterisk is used to dendef the type of the particular category.\\n\\n#### Application Logs on un1/un2\\nIn case of failure, the logs related to the IPVPN-SM app are the following:\\n1. **Access Logs**\\n`/shared/abc/ip_vpn/sm-app/deployment/logs/tomcat/access_log.log`\\n2. **Application Logs**\\n`/shared/abc/ip_vpn/sm-app/deployment/logs/application.log`\\n3. **Requests sent to SQM Server**\\n`/shared/abc/ip_vpn/sm-app/deployment/logs/sm-server-requests.log`\\n\\n#### IPVPN-SLA Logs on `un2`\\nAside from the above logs, the operation of the IPVPN-SLA scripts is also monitored by the original implementation. Details about the logging of IPVPN-SLA can be found [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ip_vpn.md)\\n\\n### Auto-retry mechanism\\nDuring the schedules runs, if the call to the IPVPN-SM app fails, the call to the application is repeated. If the application fails on all 5 attemps, the script exits. On each failed attempt, an email alert is sent via the monitoring app. This operation is done inside the `un2:/shared/abc/ip_vpn/sm-replacement/scripts/compute_metrics_via_sm_app.sh` script.\\n\\n\\n# Support\\n\\n## Check request status via Monitoring\\n### Grafana\\nCheck grafana dashboard for failed requests:\\n`https://unc1.bigdata.abc.gr:3000/d/HiuaKlU4z/ipvpn-sm-replacement-monitoring?orgId=1&refresh=5s`\\n\\n### Monitoring DB\\nIPVPN-SM App uses Monitoring App in order to record statistics regarding the status of each metric computation request. Querying monitoring DB allows us to investigate pdefntial failed requests.\\n1. Connect to monitoring DB\\n`mysql -umonitoring -p -h 999.999.999.999`\\n2. `use monitoring;`\\n\\n#### See failed requests in the past 24h excluding EmptyQuery Exceptions\\nOften persistent EmptyQuery alerts demand further investigation in the CustomPoller component of IPVPN-SLA application and the associated metrics cannot be retrieved from the Impala tables.   \\nTo identify failed metrics in the past 24h due to errors other than EmptyQuery exceptions, we can use the following query\\n```\\nmysql> select distinct x.job, x.5min_interval_ts, x.sent_to_SQM, y.request_repetitions, x.exception from ( select d.job, DATE_FORMAT( STR_TO_DATE( operative_partition, '%Y%m%d%H%i%s' ), '%Y-%m-%d %H:%i' ) as 5min_interval_ts, c.operative_partition, c.status AS 'sent_to_S\\nQM', DATE_FORMAT( metric_ts_start, '%Y-%m-%d %H:%i:%s' ) as metric_ts_start, DATE_FORMAT( c.system_ts, '%Y-%m-%d %H:%i:%s' ) as metric_ts_end, SUBSTRING_INDEX( c.message, 'com.jkl.bigstreamer.abc.ipvpn.sm.exception.', -1 ) as exception from ( SELECT job, compone\\nnt as start_comp, status as started, operative_partition as start_oppart, system_ts as metric_ts_start FROM jobstatus WHERE application = \\\"IPVPN-SM\\\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_START' and message = 'n_attempt=0' order by system_ts desc,\\njob ) d inner join ( select b.* from ( SELECT MAX(system_ts) as max_ts FROM jobstatus WHERE application = \\\"IPVPN-SM\\\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_END' group by operative_partition, job ) a inner join ( SELECT * FROM jobstatus WHERE applic\\nation = \\\"IPVPN-SM\\\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_END' ) b on a.max_ts = b.system_ts ) c on d.job = c.job and d.start_oppart = c.operative_partition order by 5min_interval_ts desc ) x left join ( select count(*)-1 as 'request_repetitions',\\njob, operative_partition from jobstatus where application = \\\"IPVPN-SM\\\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_START' group by operative_partition, job ) y on x.operative_partition = y.operative_partition and x.job = y.job where sent_to_SQM=\\\"FAILED\\\"\\n and SUBSTRING_INDEX( exception, ':', 1 )!=\\\"AppEmptyQueryException\\\" order by 5min_interval_ts desc;\\n\\n+-----+------------------+-------------+---------------------+-------------------+\\n| job | 5min_interval_ts | sent_to_SQM | request_repetitions | exception         |\\n+-----+------------------+-------------+---------------------+-------------------+\\n| AV  | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\\n| CPU | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\\n| MEM | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\\n| IF  | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\\n| QOS | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\\n| AV  | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\\n| CPU | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\\n| IF  | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\\n| MEM | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\\n| QOS | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\\n+-----+------------------+-------------+---------------------+-------------------+\\n\\n```\\n\\nThese are the requests that should be manually handled following the actions described next.\\n\\n#### Inspect specific metric request\\nIn case of alerts, we should first check the status of the particular requests on the monitoring DB.\\nUsing the metric category and the reference interval associated with a request to IPVPN-SM App we can query the Monitoring DB\\nby passing the corresponding job and operative_partition values.\\n- `select application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\\\"IPVPN-SM\\\" and job=\\\"<category>\\\" and operative_partition=\\\"<YYYYMMDDhhmmss>\\\" order by system_ts;`\\n\\nFor example. if we want to check the status of a request for QOS metric category and `2023-05-10 06:55:00` reference interval:\\n```\\nmysql> select application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\\\"IPVPN-SM\\\" and job=\\\"QOS\\\" and operative_partition=\\\"20230510065500\\\" order by system_ts;\\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\\n| application | job | component  | operative_partition | status  | message                                                                    | system_ts               |\\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\\n| IPVPN-SM    | QOS | MAIN_START | 20230510065500      | SUCCESS | n_attempt=0                                                                | 2023-05-10 07:04:51.000 |\\n| IPVPN-SM    | QOS | MAIN_END   | 20230510065500      | FAILED  | com.jkl.bigstreamer.abc.ipvpn.sm.exception.AppEmptyQueryException | 2023-05-10 07:05:02.000 |\\n| IPVPN-SM    | QOS | MAIN_START | 20230510065500      | SUCCESS | n_attempt=1                                                                | 2023-05-10 07:05:22.000 |\\n| IPVPN-SM    | QOS | MAIN_END   | 20230510065500      | SUCCESS |                                                                            | 2023-05-10 07:05:35.000 |\\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\\n4 rows in set (0.01 sec)\\n```\\nAccording to the above results, we conclude that the computation of the QOS metrics for the selected 5min interval failed on the first attempt with AppEmptyQueryException error. However the request was successful on the second attempt.\\n#### Inspect overall requests' status\\nIf we want to check if any of the requests has failed, for instance in the past 24h\\n```\\nselect application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\\\"IPVPN-SM\\\" and component=\\\"MAIN_END\\\" and status=\\\"FAILED\\\" and system_ts >= now() - INTERVAL 1 DAY;\\n```\\n\\n##  Pdefntial Error Cases\\n### AppEmptyQueryException\\nThis exception occurs when the executed Impala query fails. The underlying reason behind this message can be one the following:\\n1. The query did not retrieve any records for the `specified 5min interval`. For category **IF**, this type of failure of a 5min interval may cause the failure of `the following 5min interval` as well. This occurs because an aggregation between two consecutive intervals is performed. \\n2. Inability to open one of the HDFS files that constitute a table associated with the particular Impala query.\\n3. Unreachable Impala deamon.\\n4. Inability to allocate Memory (OutOfMemoryError: unable to create new native thread).\\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\\n\\nIf the `bigcust` tables are empty, manually adding metrics won't be effective. In such cases, it's crucial to conduct a detailed investigation to pinpoint the specific step within the IPVPN-SLA process where the failure occurred. Such an investigation is documented [here](../procedures/ipvpn_sm_AppEmptyQuery_resolution_MoP.md)\\n### SMSystemException\\nThe above exception is solely due to SQM server's inability to ingest the information sent by IPVPN-SM App.\\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\\n\\n### SMValidationException\\nThis occurs in the case when the schema of the data sent does not comply with the agreed schema.\\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\\n\\n### AppQueryIngestionException\\nThis can occur due to inability to apply the particular transformation on the retrieved data.\\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\\n\\n### SMAuthException\\nThis occurs if the token used as basic HTTP authentication to the SQM endpoint is rejected.\\n**Support Actions:**\\n1. [Update keystore with the new credentials](#update-http-auth-token)\\n2. [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\\n\\n\\n## Actions\\n### Update HTTP Auth Token\\n1. Create keystore\\n```\\nkeytool -importpass -alias sm_token -storetype JCEKS -validity 365000 -keystore credentials.keystore\\n```\\n2. Enter <keystore_pw>\\n2. Save the given credentials in the format: `username:password`\\n#### Instructions to update the file if given new credentials\\n1. Update keystore\\n`keytool -keypasswd -keystore credentials.keystore -alias sm_token`\\n2. Enter <keystore_pw>\\n3. Enter new credentials in the format: `username:password`\\n\\n### Call the IPVPN-SM App manually on un2\\nIn case we want to bypass the scripts running on un2 we can post a request directly on the IPVPN-SM application, either by performing a curl request specifying the [request body](#request-body) details or by running the script `/shared/abc/ip_vpn/sm-replacement/scripts/sm-replacement-call-repeater.sh` providing the arguments:\\n- `category`, one the valid category types {AV,QOS,CPU,MEM,IF}\\n- `reference-start-time` in the format yyyyMMddHHmm\\n- `reference-end-time` in the format yyyyMMddHHmm (inclusive)\\n\\nThe script will successively call the `refresh_*_.sh` and  `compute_metrics_via_sm_app.sh` scripts for the given category and the period specified by the end and start arguments.\\n\\n1. In case we want perform the execution for just **one 5m interval**, we provide the same argument for `reference-start-time` and `reference-end-time` fields. E.g. for 5m period of 18/12/2022 10:10:\\n```\\n./sm-replacement-call-repeater.sh CPU 202212181010 202212181010\\n```\\n2. If we want to perform requests for **5 multiple consecutive** 5min intervals E.g. for 5m periods of 18/12/2022 10:10 to 10:30 (inclusive):\\n\\n```\\n./sm-replacement-call-repeater.sh CPU 202212181010 202212181030\\n```\\nThe logs of this operation are appended to the log files in `/shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*`.\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context provides detailed information on the IPVPN-SM application's flow, failure handling, exception types including AppEmptyQueryException, and log paths. It even mentions specific log files and what to look for, which makes the answer fully grounded in the context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question targets a common operational issue in a production BigStreamer pipeline (missing data due to AppEmptyQueryException in the IPVPN flow). Troubleshooting this directly affects service reliability and data completeness, which is highly relevant for telecom engineers.",
      "question_standalone_score": "4",
      "judge_feedback_question_standalone": "The question is mostly self-contained, but it slightly depends on internal knowledge of what 'AppEmptyQueryException' and 'IPVPN flow' refer to. While likely known by engineers working in this context, these could be obscure to an outsider, preventing a perfect 5."
    },
    {
      "question": "How can you resolve a failed Prometheus import flow if a partition was partially inserted?",
      "answer": "Use Impala to drop the faulty partition:\n```sql\nALTER TABLE prometheus.dwh22 DROP IF EXISTS PARTITION (par_dt='20220401');\n```\nThen, rerun the Oozie workflow `Prometheus-Import-Workflow` for the failed date via Hue interface.",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# Prometheus\\n\\n## Useful Links\\n\\n[Infrastructure](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/Infastructure)  \\n[Monitoring](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/home#monitoring)  \\n\\n\\n## Oozie workflow\\n\\n``` mermaid\\n  graph TD\\n    A[Oracle DB table DWSRC.DWH22  <br> via Port Forward 999.999.999.999:6634] -->|Sqoop Import| B[HDFS Staging Directory]\\n    B -->|Hive Load| C[Hive: prometheus.dwh22]\\n    C -->|Impala Refresh| D[Impala: prometheus.dwh22]\\n```\\n\\nRuns every day at `06:30 AM UTC`\\n\\n**User**: `prometheus`  \\n**Coordinator**: `Prometheus-Coordinator`  \\n**Workflow**: `Prometheus-Import-Workflow`  \\n**Source Database**:  \\n\\n- **Host**: `999.999.999.999`  \\n- **Port**: `1521`  \\n- **SID**: `A7`\\n- **User**: `bigstreamer`  \\n\\n**Target Table**: `prometheus.dwh22`  \\n**HDFS Installation Directory**: `/user/prometheus/flows`  \\n**HDFS Staging Directory**: `/ez/warehouse/prometheus.db/tmp_sqoop_jobs/`\\n\\n**Alerts**:\\n\\n- Mail with subject: `Prometheus Flow failed`\\n\\n**Troubleshooting Steps**:\\n\\n- Check messages written to Monitoring App\\n\\n    - Check monitoring app for successful executions:  \\n        - From `un2` with personal account:\\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'`\\n \\n    - Check monitoring app for failed executions:  \\n  \\n        - From `un2` with personal account:\\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=FAILED&operativePartition=<timestamp e.g.:20220518>'`\\n    - Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\\n    - Grafana link: `https://unc1.bigdata.abc.gr:3000/d/PcKYyfTVz/prometheus-dashboard?orgId=1&from=now-2d&to=now`\\n\\n- Check if partition is loaded:\\n\\n  From `Hue` as `prometheus` in `Impala Editor`:\\n\\n  ``` sql\\n  SHOW PARTITIONS prometheus.dwh22;\\n  SELECT COUNT(*) FROM prometheus.dwh22 WHERE par_dt='<par_dt>';\\n  ```\\n\\n- Check logs for failed steps:  \\n\\n  From `Hue` as `prometheus` in `Workflows`:\\n\\n  - Search for `Prometheus-Import-Workflow` and filter for failed\\n  - Go to logs and check both stdout and stderr\\n\\n- In case a partition has partially been inserted into the final table `prometheus.dwh22` and the error that caused the failure has been resolved:\\n\\n  From `Hue` as `prometheus` in `Impala Editor`:\\n\\n    ``` sql\\n    ALTER TABLE prometheus.dwh22 DROP IF EXISTS PARTITION (par_dt='<par_dt>');\\n    ```\\n\\n  - For the previous day:\\n\\n    From `Hue` as `prometheus` in `Workflows`:\\n\\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\\n    - Re-run it\\n\\n  - For the previous day:\\n\\n    From `Hue` as `prometheus` in `Workflows`:\\n\\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\\n    - Re-run it\\n\\n  - For partitions older than yesterday:\\n\\n    From `Hue` as `prometheus` in `File Browser`:\\n\\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and set `days_back` to the number of days back needed to reach the partition\\n\\n    From `Hue` as `prometheus` in `Workflows`:\\n\\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\\n    - Re-run it\\n\\n    From `Hue` as `prometheus` in `File Browser`:\\n\\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and restore `days_back` to `1`",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context explicitly explains that in case of a partially inserted partition, one should drop the partition and rerun the Prometheus import workflow from Hue. The provided answer matches this instruction exactly, making the question fully answerable from context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Handling failed Prometheus import flows is a routine and critical task for data engineers working with BigStreamer. This scenario (partial partition insert) is realistic and operationally important, so the question is highly relevant.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clearly worded and fully understandable on its own. It mentions a specific exception scenario and names the flow, making it self-contained and unambiguous."
    },
    {
      "question": "What steps are involved in exporting TeMIP alarms from Kudu to Impala?",
      "answer": "The `TeMIP_kudu_2_Impala_CO` Oozie coordinator runs daily at 06:00. It transfers older records from:\n- `temip.temip_kudu_terminated_alarms` to `temip.temip_impala_terminated_alarms`\n- `temip.temip_kudu_historic_events` to `temip.temip_impala_historic_events`\nMonitor the workflow `TeMIP_kudu_2_Impala_WF` through Hue and check logs for errors.",
      "category": "Infrastructure & Deployment",
      "context": "# TeMIP\\n\\n## Overview\\n\\nThe `abc TeMIP alarms live feed to BigStreamer` application is a Java application hosted on a Wildfly application server. The objective of the application is to receive and store (in near real time) the TeMIP alarms (from specific TeMIP Operation Contexts) into the BigStreamer™ ecosystem. The `Apache Kudu` storage engine was selected in order to achieve near real time CRUD operations (Create, Read, Update, Delete). The `Apache Impala` is used for extended data retention (6 months). The `Apache Oozie` scheduler  is used in order to automatically run the necessary scripts.\\n\\n- **Ndef:** All the needed **passwords** can be found [**here**](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx).\\n\\n## Flows\\n\\nThe `TeMIP Flow` consists of 4 components/flows:\\n1. Initialization/Synchronization flow\\n1. Main Application flow\\n1. Move Kudu to Impala flow\\n1. Alert Mail flow\\n\\n### Main Application\\n\\nThe `Main Application Flow` contains our `TeMIP application` deployed to the `Wildfly Server` which receives the TeMIP alarms and stores them into Kudu tables.\\n\\n``` mermaid\\n  flowchart TD\\n  A[TeMIP Server] \\n  B[Wildfly Server]\\n  A --> |Sends TeMIP alarms| B\\n  B --> |Stores TeMIP alarms| D[(Kudu Storage Engine)]\\n  D --- E[Kudu: temip.temip_kudu_active_alarms]\\n  D --- Z[Kudu: temip.temip_kudu_terminated_alarms]\\n  D --- K[Kudu: temip.temip_kudu_historic_events]\\n  style A fill: #45b39d\\n```\\n\\n- **TeMIP Server**\\n  - **Host:** `999.999.999.999`\\n  - **Port:** `7180`\\n- **Wildfly Server**\\n  - **Servers:**\\n    - `temip1 (999.999.999.999)` Standby Server\\n    - `temip2 (999.999.999.999)` Active Server\\n  - **User:** `temip`\\n  - **Installation Path:** `/opt/wf_cdef_temip/`\\n  - **Deployments Path:** `/opt/wf_cdef_temip/standalone/deployments`\\n  - **Application Logs:** `/opt/wf_cdef_temip/standalone/log/server.log`\\n  - **Access Logs:** `/opt/wf_cdef_temip/standalone/log/access.log`\\n  - **Configuration:** `/opt/wf_cdef_temip/standalone/configuration/BigStreamer/config/`\\n    - **File:** `temip.properties`\\n\\n**Alerts:**\\n\\n- **Mail executed by [Alert Mail](#alert-mail)**\\n  - **Subject:** `\\\"[ Temip ] No alarms available.\\\"`\\n  - **Body:** `\\\"There are no Temip alarms  available for the last hour. Corrective action may be needed.\\\"`\\n\\n**Troubleshooting Steps:**\\n\\n1. Check `logs` (application and access) with `temip-tailog` for any `ERROR` message that can occur.  \\nIf TeMIP Server is running correctly, we should see lines like the following:  \\n`INFO [com.jkl.bigstreamer.abc.temip.core.service.TemipService] (default task-173) Counter= 3064020, handle= 968, batchName= batch_2, timesRestartedFromLastSync= 1, aoExtractDataList size= 1`\\n1. Check if `TeMIP Server` is up by executing `ping 999.999.999.999`.\\n1. Contact a `TeMIP admin` to see if there are any server side related issues\\n\\n### Initialization/Synchronization\\n\\nThe `Initialization/Synchronization Flow` consists of an OOZIE Coordinator called `TeMIP_Synchronization_CO`. The coordinator is responsible for establishing the **connection** and **communication** of the `Wildfly Server` (containing our TeMIP Application) with the `TeMIP Server`.\\n\\nEvery time the `Main Application` is successfully deployed to `Wildfly Server` or gets restarted, this `coordinator` **must be run manually** to initiate the above procedure, it does not happen automatically. If the `Wildfly Server` is up and running, the `coordinator` executes on specific days of every month to perform maintenance tasks.\\n\\n``` mermaid\\n  flowchart TD\\n  A[OOZIE Server] -->|SHELL Action as user temip| B[un-vip.bigdata.abc.gr <br> User: temip]\\n  B -->|REST message protocol| C[Main Application]\\n  C <--> |SOAP message protocol| D[TeMIP Server]\\n  style C fill: #45b39d\\n```\\n\\n- **Oozie Coordinator**\\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\\n  - **User:** `temip`\\n  - **Coordinator:** `TeMIP_Synchronization_CO`\\n    - **Execution:** `2,7,12,17,22,27 of every month at 03:00 local time`\\n    - **Approximate Duration:** `45 minutes`\\n    - **Workflow:** `TeMIP_Synchronization_WF`\\n      - **Master Script:** `hdfs:/user/temip/100.TeMIP_Synchronization_Oozie_Main.sh`\\n      - **Remdef Script:** `un-vip:/shared/abc/temip_oozie_production_scripts/101.temip_synchronization_Main.sh`\\n      - **Server:** `un-vip.bigdata.abc.gr`\\n      - **SSH User:** `temip`\\n      - **Logs:** `un-vip:/shared/abc/temip_oozie_production_scripts/log/102.temip_synchronization.$(date '+%Y%m%d').log`\\n\\n**Alerts:**\\n\\n- **Not Monitored**\\n\\n**Troubleshooting Steps:**\\n\\n1. Check logs for any errors\\n1. If workflow `TeMIP_Synchronization_WF` has been run manually, login to `Hue` with `temip` user `after 45 minutes` and execute the following `impala query` editor: `select * from temip.temip_kudu_configs`.  \\nIt should return `15 rows`. If not, re run the `TeMIP_Synchronization_WF` workflow\\n\\n### Move Kudu to Impala\\n\\nThe `Move Kudu to Impala` flow consists of a coordinator called `TeMIP_kudu_2_Impala_CO` which executes once a day and is responsible for moving the alarms from kudu to the equivalent impala table.\\n\\n``` mermaid\\n flowchart TD\\n  A[OOZIE Job] \\n  Z[Kudu: temip.temip_kudu_terminated_alarms]\\n  K[Kudu: temip.temip_kudu_historic_events]\\n  A --> Z\\n  A --> K\\n  Z --> |Move older alarms to impala|H[Impala: temip.temip_impala_terminated_alarms]\\n  K --> |Move older events to impala|L[Impala: temip.temip_impala_historic_events]\\n```\\n\\n- **Oozie Coordinator**\\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\\n  - **User:** `temip`\\n  - **Coordinator:** `TeMIP_kudu_2_Impala_CO`\\n    - **Execution:** `everyday at 06:00 local time`\\n    - **Approximate Duration:** `15 minutes`\\n    - **Workflow:** `TeMIP_kudu_2_Impala_WF`\\n      - **Script:** `hdfs:/user/temip/temip_kudu_to_impala.sh`\\n      - **Logs:** `Through Oozie Job in HUE`\\n\\n**Alerts:**\\n\\n- **Not Monitored**\\n\\n**Troubleshooting Steps:**\\n\\n1. Check that workflow `TeMIP_kudu_2_Impala_WF` runs successfully. This can be done through accessing `Hue` with `temip` user and selecting `Jobs`. Then filter the jobs with `user:temip` and look for a `job` with the below properties:\\n    - **Name:**  `oozie:launcher:T=shell:W=temip_kudu_to_impala:A=shell-661a:*`\\n    - **Type:** `Oozie Launcher`\\n    - **Execution Time:** `06:00 AM`\\n\\n    Select the desired shell execution, view its logs and search for any ERRORS.\\n1. After the root cause of the problem has been identified, re-rerun the failed execution. This can be done through accessing `Hue` with `temip` user.\\n\\n### Alert Mail\\n\\nThe `Alert Mail` flow consists of a coordinator called `TeMIP_Alert_Mail_CO` which runs every hour and checks if the application receives any alarms from the TeMIP Server. The `TeMIP Server` sends alarms continuously. If in the last hour, the application has not detected any new alarms, an email is sent to jkl Engineers to inform that there might be a issue. The check is performed by comparing the number of alarms from the previous execution stored in `temip.temip_alert_table` table with the current one.\\n\\n``` mermaid\\n flowchart TD\\n  A[OOZIE Server] -->|SHELL Action as user temip| B[un-vip.bigdata.abc.gr <br> User: temip]\\n  B --> C[201.temip_alert_mechanism_Main.sh]\\n```\\n\\n- **Oozie Scheduler**\\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\\n  - **User:** `temip`\\n  - **Coordinator:** `TeMIP_Alert_Mail_CO`\\n    - **Execution:** `every hour`\\n    - **Workflow:** `TeMIP_Alert_Mail_WF`\\n      - **Master Script:** `hdfs:/user/temip/200.TeMIP_Alert_Mail_Oozie_Main.sh`\\n      - **Remdef Script:** `un-vip/shared/abc/temip_oozie_production_scripts/201.temip_alert_mechanism_Main.sh`\\n      - **Server:** `un2.bigdata.abc.gr`\\n      - **SSH User:** `temip`\\n      - **Logs:** `un-vip:/shared/abc/temip_oozie_production_scripts/log/202.temip_alert_mechanism.$(date '+%Y%m%d').log`\\n\\n**Alerts:**\\n\\n- **Not Monitored**\\n\\n**Troubleshooting Steps:**\\n\\n1. Check for any failed executions. This can be done through accessing `Hue` with `temip` user\\n1. Check for any cluster related problems during the failed execution.\\n\\n## Manual Actions\\n\\n### Restart Wildfly Server\\n---\\n\\n**_Ndef:_** TEMIP runs only in one node. Second node is in standby mode.\\n\\n1. **Only if requested by TeMip Administrators** In `Hue` with `temip` user, clear table `temip.temip_kudu_active_alarms` by executing `delete from temip.temip_kudu_active_alarms;`\\n1. `Shutdown Wildfly Server`\\n    1. Login as `temip` user in `temip2`.\\n    1. Execute `/bin/bash` and then run `temip-stop` to stop wildfly and check logs with `temip-tailog`.\\n    1. Suspend the temip Coordinators (`Hue` with `temip` user):\\n        - `TeMIP_kudu_2_Impala_CO`\\n        - `TeMIP_Synchronization_CO`\\n        - `TeMIP_Alert_Mail_CO`\\n    1. Clear table `temip.temip_kudu_active_alarms` by executing `delete from temip.temip_kudu_active_alarms;` in `Hue` as `temip` user \\n\\n1. `Startup Wildfly Server`\\n    1. Login as `temip` user in `temip2`.\\n    1. Start wildfly by executing `/bin/bash` and then running `temip-start` and check logs with `temip-tailog`.\\n    1. Resume the three temip coordinators (`Hue` with `temip` user):\\n        - `TeMIP_kudu_2_Impala_CO`\\n        - `TeMIP_Synchronization_CO`\\n        - `TeMIP_Alert_Mail_CO`\\n    1. Workflows:\\n        - The two workflows `TeMIP_kudu_2_Impala_WF` and `TeMIP_Alert_Mail_WF` should run automatically when oozie scheduler detects that it was suspended.\\n        - The third workflow `TeMIP_Synchronization_WF` should be run manually. Specifically, `login` as `temip` to `Hue` and run manually with no parameters. Make sure that it will not also be executed by the corresponding coordinator.\\n    1. At `HUE` with `temip` user, open the impala editor and execute the following command in order to refresh e-mail alert script:  \\n      `insert overwrite temip.temip_alert_table values(1);`\\n\\n1. `Sanity Checks`\\n\\n    1. Login as `temip` user in `temip2`\\n    1. Check `logs` with `temip-tailog` and search for any errors.\\n    1. After `45 minutes`, login to `Hue` with `temip` user and execute the following impala query editor:  \\n    `select * from temip.temip_kudu_configs`  \\n      It should return 15 rows. If not, `re run` the `TeMIP_Synchronization_WF` workflow.\\n    1. Login to `Hue` with `temip` and perform the below impala queries with a temip admin (Ioanna Bekiari) in order to established if everything is running okay. If the results are the same or really similar, the synchronization is considered successful.\\n\\n        ``` sql\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ENM_BASEBAND%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ERICOSS2G%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ERICOSS-LTE%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%NOKIA3G%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%.ATHENS-OC%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%NNM_FIXED%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%U2000-OC%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%1350OMS%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%HUAWEI_IMS%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%AUMS-OC%';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '.def.A5529.A5520_AMS-OC';\\n        select count(*) from temip_kudu_active_alarms where operation_context like '%2000_DSLAM%';\\n        ```\\n\\n### Load Terminated Alarms from TeMIP Oracle Database In case of data loss\\n---\\n\\nIn case there is a loss of alarms for any reason, eg our application or TeMIP outage, we may be asked to load historical data directly from TeMIP Oracle Database into our terminated alarms table. In order to start this operation we must wait for some days, so that all alarms are transferred to the Oracle table. Whole procedure is described in detail below:\\n\\n1. Wait `7 days` from the day you want to `re-load` in order for terminated alarms to be refreshed in Oracle table.\\n\\n1. Connect as `temip` in `un2` and run `ping 999.999.999.999`, in order to see if `Temip Server` is up and running.\\n\\n1. From impala shell `secimp`(as `temip` in `un2`) or `Hue`(as `temip`):\\n\\n\t1. Check missing partitions in `temip.temip_kudu_terminated_alarms` and `temip.temip_impala_terminated_alarm` by running  \\n  `select count(*), par_dt from <database>.<table> where par_dt='<partition>' group by par_dt;` on both tables.  \\n\t\tWe receive TeMIP alarms every day. So if there are general ERRORS(logs) or we have partitions containing less alarms than usual(eg. count), it suggests that there might be problems with the TeMIP server or our application and in need of investigating.\\n\\n\t1. Delete existing wrong partitions that overlap with the required interval, either from kudu table `temip.temip_kudu_terminated_alarms` or from impala table `temip.temip_impala_terminated_alarms`.\\n\t\t- If wrong partitions are contained in kudu table (only 10 most recent days are in kudu), do:  \\n`ALTER table temip.temip_kudu_terminated_alarms DROP IF EXISTS RANGE PARTITION 'v1'<= values < 'v2';`,   \\nwhere v1 and v2 the required interval.\\n\\n\t\t- If wrong partitions are contained in impala table (10 days past the current date), do:  \\n`ALTER table temip.temip_impala_terminated_alarms DROP IF EXISTS PARTITION (par_dt='v');`,   \\nwhere v is the wrong partition.\\n\\n\t1. In order to not tranfer again old data that have remained, run `truncate table temip.temipdb_term_alarms_load_par;`.\\n\\n1. As `temip` in `un2` Run the script with arguments\\n    ``` bash\\n    sh /usr/icom/scripts/Sqoop_Oracle_HDFS_Impala_Load_TeMIP_v832.sh \\\"temipaharchi.alarmobject0\\\" identifier 30 <current-pardt> \\\"terminationtimestamp>='v1' and terminationtimestamp<'v2'\\\"\\n    ```\\n    - **current-pardt:** is the `today` par_dt, the day the script is run. Format `YYYYMMDD`. It has no significant value to the internal process.\\n    - **v1, v2:** Use values for `terminationtimestamp` that are between the start and end of the interval you want to load from Oracle. Format `01-MAY-22`.\\n\\n    Example for day 20220501:\\n    ``` bash\\n    sh /usr/icom/scripts/Sqoop_Oracle_HDFS_Impala_Load_TeMIP_v832.sh \\\"temipaharchi.alarmobject0\\\" identifier 30 20230104 \\\"terminationtimestamp>='01-MAY-22' and terminationtimestamp<'02-MAY-22'\\\"\\n    ```\\n    The data will be **loaded** into table `temip.temipdb_term_alarms_load_par`.\\n\\n1. From impala shell `secimp`(as `temip` in `un2`) or `Hue`(as `temip`):\\n\\n    1. Refresh the staging table in the impala shell:  \\n      `refresh temip.temipdb_term_alarms_load_par;`\\n\\n    1. Run the following sql command, which transfers automatically all data to the right partition (par_dt) of temip.temip_impala_terminated_alarms:\\n\\n        ``` sql\\n        insert overwrite temip.temip_impala_terminated_alarms partition (par_dt)\\n        select concat('OPERATION_CONTEXT ',split_part(upper(ocname),':',2),' ALARM OBJECT ', identifier) outentityspec,\\n        null last_Modification_Timestamp,\\n        split_part(upper(ocname),':',2) operation_context,\\n        cast(identifier as bigint) identifier,\\n        \\\"Terminated-Oracle\\\" state,\\n        \\\"Closed\\\" problem_status,\\n        case when clearancereportflag = \\\"1\\\" then true else false end clearance_report_flag,\\n        acknowledgementuseride as acknowledgement_user_identifier,\\n        handledby as handled_by,\\n        closedby as closed_by,\\n        handleduseridentifier as handled_user_identifier,\\n        releaseuseridentifier as release_user_identifier,\\n        closeuseridentifier as close_user_identifier,\\n        terminationuseridentif as termination_user_identifier,\\n        acknowledgementtimesta as acknowledgement_time_stamp,\\n        handletimestamp as handle_time_stamp,\\n        closetimestamp as close_time_stamp,\\n        terminationtimestamp as termination_time_stamp,\\n        releasetimestamp as release_time_stamp,\\n        null automatic_terminate_on_close,\\n        creationtimestamp as creation_timestamp,\\n        archivetimestamp as archive_time_stamp,\\n        clearancetimestamp as clearance_time_stamp,\\n        null previous_state,\\n        managedobject as managed_object,\\n        targetentities as target_entities,\\n        --targetentities60512 as target_entities,\\n        alarmtype as alarm_type,\\n        eventtime as event_time,\\n        probablecause as probable_cause,\\n        securityalarmcause as security_alarm_cause,\\n        specificproblems as specific_problems,\\n        --specificproblems (id)-8eloume to join kai edw,\\n        null backed_up_status,\\n        backupobject as backup_object,\\n        trendindication as trend_indication,\\n        thresholdinfo as threshold_info,\\n        cast(notificationidentifier as bigint) notification_identifier,\\n        correlnotifinfo as correl_notif_info,\\n        monitoredattributes as monitored_attributes,\\n        proposedrepairactions as proposed_repair_actions,\\n        null additional_information,\\n        domain as domain,\\n        securityalarmdetector as security_Alarm_Detector,\\n        null service_User,\\n        null service_Provider,\\n        ocname as oc_Name,\\n        cast(parentalarmobject as bigint) parent_alarm_object,\\n        null severity_changed_time_stamp,\\n        alarmcomment as alarm_comment,\\n        agentalarmidentifier as agent_alarm_identifier,\\n        agententity as agent_entity,\\n        perceivedseverity as perceived_Severity,\\n        additionaltext as additional_Text,\\n        alarmobjectoperatorno as alarm_Object_Operator_Ndef,\\n        originalseverity as original_Severity,\\n        originaleventtime as original_Event_Time,\\n        0 useridentifier,\\n        usertext as user_Text,\\n        cast(satotal as bigint) sa_total,\\n        null deleted,\\n        from_timestamp(to_timestamp(terminationtimestamp,'yyyy-MM-dd HH:mm:ss'),'yyyyMMdd') as par_dt\\n        --,*\\n        from temip.temipdb_term_alarms_load_par a;\\n        ```\\n\\n        **Ndef:** There are comments that might affect the query if not handled carefully. \\n\\n    1. Check if data transferred successfully by running the command below and comparing the result with the number of retrieved records in the logs produced by the script.  \\n\\n      ``` sql\\n      SELECT par_dt, count(*) FROM temip.temip_impala_terminated_alarms where par_dt > 'v';\\n      Eg. Logs: INFO mapreduce.ImportJobBase: Retrieved 1113488 records.  \\n      +----------+  \\n      | count(*) |  \\n      +----------+  \\n      | 1113488  |  \\n      +----------+  \\n      ```\\n\\n## TeMIP Wildfly Server\\n\\nIn order to change the `logging level` of any of the `categories` of the `TeMIP Wildfly Server` access the `WildFly Management`.\\n\\n### Logging\\n\\nIn order to change the `logging level` of any of the categories of the `TeMIP Wildfly Server` access the `WildFly Management`.\\n\\n- [Home URL](https://999.999.999.999:8888/)\\n- **User:** `admin`\\n- **Password:** [abc-syspasswd.kdbx](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/abc-syspasswd.kdbx)\\n\\n#### Change logging level\\n\\n1. Login to the `WildFly Management` by following the [home url](https://999.999.999.999:10213/console/App.html#home), select `Configuration` ~> `Subsystems` ~> `Logging` and select `View`.\\n1. Select `LOG CATEGORIES`\\n1. From the table select the `category` you desire to change its **logging level** e.g. com.jkl.bigstreamer.abc.temip is our main TeMIP App.\\n1. Press the `Edit` option below the table\\n1. Select the desired `Level` between the given options. \\n1. Click `Save`\\n1. Ssh from `un2` with `temip` to `temip1` or `temip2` with `temip` user and check that in the file `/opt/wf_cdef_temip/standalone/configuration/standalone-full.xml` the level of the previously configured logger has changed successfully. It should be configured **automatically**. \\n\\n## Useful Links\\n\\n- [TeMIP Dir](https://metis.ghi.com/obss/bigdata/abc/temip)\\n- [TeMIP Application Deployment](https://metis.ghi.com/obss/bigdata/abc/temip/temip-devops/-/wikis/Application-Deployment)\\n- [TeMIP Wiki](https://metis.ghi.com/obss/bigdata/abc/temip/temip-devops/-/wikis/home)",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context clearly describes that the TeMIP_kudu_2_Impala_CO coordinator handles moving data from Kudu to Impala, and provides the exact source and destination tables involved. It also confirms the daily schedule and mentions workflow and log monitoring through Hue. The answer aligns precisely with this information.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Understanding how to export TeMIP alarms from Kudu to Impala is a critical task for engineers maintaining data pipelines in BigStreamer, especially for long-term storage and querying. This operation is part of daily ETL processes, making the question highly relevant.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear and fully self-contained. It specifies what is being exported (TeMIP alarms), from where (Kudu), to where (Impala), and asks for the steps involved, without needing additional context."
    },
    {
      "question": "What is the process for replacing hardware on a BigStreamer node and contacting Dell support?",
      "answer": "1. SSH into the node and run:\n```bash\nsudo -i\nipmitool lan print | grep -i 'IP Address'\n```\n2. Access IDRAC via browser, get the service tag.\n3. Call Dell support and provide the tag.\n4. Collect TSR logs from IDRAC and send them to Dell.\n5. Inform abc before any IDRAC operation.",
      "category": "Infrastructure & Deployment",
      "context": "# abc - BigStreamer - How to open a ticket to DELL\\n\\n\\n<b>Description:</b>\\n\\n```\\nBelow is a step-by-step description of the process from opening a ticket to collecting TSR logs from IDRAC.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. ssh with your personal account on the issue node.\\n2. sudo -i;ipmitool lan print | grep -i 'IP Address' # To find the Managment IP. Otherwise e.g grep nodew /etc/hosts \\n```\\nIf the ipmitool package did not exist just install it. yum install ipmitool;\\n```\\n3. connect via vnc. Open firefox and type the `IP Address` from step 2\\n4. From `Server-->Overview-->Server Information` copy the `Service Tag number`\\n5. Call Dell support `2108129800`. They need the `Service Tag number` from step 4\\n6. An engineer will create a case and sent you all the necessary steps. If not the link to collect the TSR logs is `https://www.dell.com/support/kbdoc/el-gr/000126803/export-a-supportassist-collection-via-idrac7-and-idrac8`\\n7. Inform `abc` before any action on the IDRAC.\\n8. Download localy the TSR gz file. ssh on the node with vnc (e.g un4). The downloaded files stored under `/home/cloudera/Downloads/` and the format is `TSRdate_service_tag.zip`\\n9. Send the zip file/files to DELL and wait for their response.\\n\\nDone!",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The answer is fully grounded in the context, which clearly outlines the entire process of hardware replacement and Dell support interaction—from SSH access, identifying the IDRAC IP, locating the service tag, to contacting Dell and handling TSR logs.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This is a highly relevant operational question for telecom engineers managing BigStreamer infrastructure. Hardware failures are critical, and knowing how to interact with Dell support is essential.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely self-contained and unambiguous. It clearly asks for the hardware replacement and support process on a BigStreamer node without needing external information."
    },
    {
      "question": "How can you verify if Traffica’s flow is paused and resume it?",
      "answer": "Check status:\n```bash\ncurl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disabled\"\n```\nIf the output is true, resume with:\n```bash\ncurl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\"\n```",
      "category": "Data Management & Query Execution",
      "context": "# Traffica Flow\\n\\n## Useful links\\n\\n- [Wiki](https://metis.ghi.com/obss/bigdata/abc/etl/traffica/traffica-devops/-/wikis/home)\\n- [Infrastructure](https://metis.ghi.com/obss/bigdata/abc/etl/traffica/traffica-devops/-/wikis/Infrastructure)\\n\\n## SMS\\n\\n``` mermaid\\n     graph TD\\n      A0[\\\"abc Flow <br> User: trafficaftp\\\"]\\n      A1[\\\"Host: cne.def.gr <br> Path: /data/1/trafficaftp/Traffica_XDR\\\"]\\n      A2[\\\"Staging Directory <br> Path: /data/1/traffica_LZ/sms\\\"]\\n      A3(\\\"Staging HDFS Directory <br> Path: /ez/warehouse/sai.db/landing_zone/sms\\\")\\n      A4(\\\"Staging Table <br> Hive: sai.sms_load\\\")\\n      A5(\\\"Staging Table <br> Hive: sai.sms_raw_text\\\")\\n      A6(\\\"Table <br> Impala: sai.sms_raw\\\")\\n      A7(\\\"Cleanup <br> Add .LOADED suffix to local raw files <br> Clean local staging directories <br> Clean HDFS tables/directories\\\")\\n    \\n      A0 -->|SFTP| A1\\n      A1 --> |Merge files| A2\\n      A2 --> |HDFS Load| A3\\n      A3 --> |Hive Load| A4\\n      A4 --> |Hive Insert| A5\\n      A5 --> |Impala Insert| A6\\n      A6 --> |Successful loaded files only| A7\\n```\\n\\n**Schedule**: `every 35 minutes`  \\n\\n**Scheduler**: `Java Springboot Application`  \\n\\n**User**: `traffica`  \\n\\n**Active Node**: `unc2.bigdata.abc.gr`  \\n\\n**Backup Node**: `unc1.bigdata.abc.gr`  \\n\\n**Installation directory**: `/shared/abc/traffica`  \\n\\n**Logs**: `/shared/abc/traffica/logs`  \\n\\n**Configuration File**: `/shared/abc/traffica/config/application.yml`\\n\\n**Start command**: `supervisorctl start traffica_sms`  \\n\\n**Stop command**: `supervisorctl stop traffica_sms`  \\n\\n**Enable command (un-pause)**: `curl -X PUT \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\\\"`\\n\\n**Alerts**:\\n\\n- Mail with subject: `Traffica Application failed`\\n\\nPossible messages:\\n\\n1. `Traffica sms main flow failed.`\\n2. `Traffica sms application has been paused due to multiple sequential failures. Manual actions are needed.`\\n3. `Traffica sms application has been paused due to inability to rename local files. Manual actions are needed.`\\n4. `Traffica sms application has been paused due to inability to clean up files. Manual actions are needed.`\\n\\n**Troubleshooting steps**:\\n\\n- Check to see if the application is running:\\n  \\n  From `unc2` with personal account:\\n\\n  ``` bash\\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\\\"\\n  ```\\n\\n- Check the logs for errors to identify the root cause\\n\\n  From `unc2` as `traffica`:\\n\\n  ``` bash\\n  # For the current log file\\n  grep -i -e error -e exception /shared/abc/traffica/logs/traffica-sms.log\\n  # For older compressed files\\n  zgrep -i -e error -e exception /shared/abc/traffica/logs/<yearMonthFolder>/<name_of_logfile>\\n  ```\\n\\n- Check metrics and error rates from Grafana\\n\\n  Open Firefox using VNC and go to `https://unc1.bigdata.abc.gr:3000/d/qIM5rod4z/traffica`\\n\\n  Use panels ending in `Err` to identify problematic components and steps.\\n\\n  Use `Files`,`Size`,`Rows` to identify if input has changed\\n\\n- If there is a problem renaming files with the `.LOADED` suffix\\n\\n  From `unc2` as `traffica`:\\n\\n  ``` bash\\n  # Get files that where processed correctly\\n  grep 'filelist=' /shared/abc/traffica/logs/traffica-sms.log \\n  # Move files pending rename from the list above\\n  cd /data/1/trafficaftp/Traffica_XDR\\n  mv <file>{,.LOADED}\\n  ```\\n\\n- If the root cause is resolved resume normal operation.\\n\\n  The flow has been designed with auto-recovery. It marks only successfully loaded files as `.LOADED` and handles/cleans up all staging directories on each run.\\n\\n  From `unc2` with personal user:\\n\\n  ``` bash\\n  # Check if scheduling is enabled \\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disabled\\\"\\n  # If the above command returns true\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\\\"\\n  ```\\n\\n  - If `unc2` is down, then manually start the application on `unc1` this requires that VIP `cne.def.gr` has been migrated and SFTP is working on `unc1`\\n\\n**Ndefs**:\\n\\n  From `unc2` with personal account:\\n\\n  ``` bash\\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/info/health\\\" # HTTP 200 if app is up\\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/info/check\\\" # returns message if up\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/shutdown/gracefully\\\" # shutdown application. If flow is running, then wait to finish. App should terminate ONLY with this method.\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disable\\\" # enable flow scheduling\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\\\" # enable flow scheduling\\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disabled\\\" # true if disabled, else false\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/cleanup/all\\\" # Run cleanup on demand\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/main/run\\\" # Run flow on demand\\n  # UI with endpoints available from VNC: http://unc2.bigdata.abc.gr:11483/traffica/swagger-ui/index.html#/\\n  ```\\n\\n## VOICE\\n\\n``` mermaid\\n     graph TD\\n      A0[\\\"abc Flow <br> User: trafficaftp\\\"]\\n      A1[\\\"cne.def.gr <br> Path: /data/1/trafficaftp/Traffica_XDR\\\"]\\n      A2[\\\"Staging Directory <br> Path: /data/1/traffica_LZ/voice\\\"]\\n      A3(\\\"Staging HDFS Directory <br> Path: /ez/warehouse/sai.db/landing_zone/voice\\\")\\n      A4(\\\"Staging Table <br> Hive: sai.voice_load\\\")\\n      A5(\\\"Staging Table <br> Hive: sai.voice_raw_text\\\")\\n      A6(\\\"Staging Table <br> Hive: sai.voice_raw_text_c2c\\\")\\n      A7(\\\"Table <br> Impala: sai.voice_raw\\\")\\n      A8(\\\"Cleanup <br> Add .LOADED suffix to local raw files <br> Clean local staging directories <br> Clean HDFS tables/directories\\\")\\n    \\n      A0 -->|SFTP| A1\\n      A1 --> |Merge files| A2\\n      A2 --> |HDFS Load| A3\\n      A3 --> |Hive Load| A4\\n      A4 --> |Hive Insert| A5\\n      A4 --> |Hive Insert| A6\\n      A5 --> |Impala Insert| A7\\n      A6 --> |Impala Insert| A7\\n      A7 --> |Successful loaded files only| A8\\n```\\n\\n**Schedule**: `every 20 minutes`  \\n\\n**Scheduler**: `Java Springboot Application`  \\n\\n**User**: `traffica`  \\n\\n**Active Node**: `unc2.bigdata.abc.gr`  \\n\\n**Backup Node**: `unc1.bigdata.abc.gr`  \\n\\n**Installation directory**: `/shared/abc/traffica`  \\n\\n**Logs**: `/shared/abc/traffica/logs`  \\n\\n**Configuration File**: `/shared/abc/traffica/config/application.yml`\\n\\n**Start command**: `supervisorctl start traffica_voice`  \\n\\n**Stop command**: `supervisorctl stop traffica_voice`  \\n\\n**Enable command (un-pause)**: `curl -X PUT \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/enable\\\"`\\n\\n**Alerts**:\\n\\n- Mail with subject: `Traffica Application failed`\\n\\nPossible messages:\\n\\n1. `Traffica voice main flow failed.`\\n2. `Traffica voice application has been paused due to multiple sequential failures. Manual actions are needed.`\\n3. `Traffica voice application has been paused due to inability to rename local files. Manual actions are needed.`\\n4. `Traffica voice application has been paused due to inability to clean up files. Manual actions are needed.`\\n\\n**Troubleshooting steps**:\\n\\n- Check to see if the application is running:\\n  \\n  From `unc2` with personal account:\\n\\n  ``` bash\\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\\\"\\n  ```\\n\\n- Check the logs for errors to identify the root cause\\n\\n  From `unc2` as `traffica`:\\n\\n  ``` bash\\n  # For the current log file\\n  grep -i -e error -e exception /shared/abc/traffica/logs/traffica-voice.log\\n  # For older compressed files\\n  zgrep -i -e error -e exception /shared/abc/traffica/logs/<yearMonthFolder>/<name_of_logfile>\\n  ```\\n\\n- Check metrics and error rates from Grafana\\n\\n  Open Firefox using VNC and go to `https://unc1.bigdata.abc.gr:3000/d/qIM5rod4z/traffica`\\n\\n  Use panels ending in `Err` to identify problematic components and steps.\\n\\n  Use `Files`,`Size`,`Rows` to identify if input has changed\\n\\n- If there is a problem renaming files with the `.LOADED` suffix\\n\\n  From `unc2` as `traffica`:\\n\\n  ``` bash\\n  # Get files that where processed correctly\\n  grep 'filelist=' /shared/abc/traffica/logs/traffica-voice.log \\n  # Move files pending rename from the list above\\n  cd /data/1/trafficaftp/Traffica_XDR\\n  mv <file>{,.LOADED}\\n  ```\\n\\n- If the root cause is resolved resume normal operation.\\n\\n  The flow has been designed with auto-recovery. It marks only successfully loaded files as `.LOADED` and handles/cleans up all staging directories on each run.\\n\\n  From `unc2` with personal user:\\n\\n  ``` bash\\n  # Check if scheduling is enabled \\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/disabled\\\"\\n  # If the above command returns true\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/enable\\\"\\n  ```\\n\\n  - If `unc2` is down, then manually start the application on `unc1` this requires that VIP `cne.def.gr` has been migrated and SFTP is working on `unc1`\\n\\n**Ndefs**:\\n\\n  From `unc2` with personal account:\\n\\n  ``` bash\\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/info/health\\\" # HTTP 200 if app is up\\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\\\" # returns message if up\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/shutdown/gracefully\\\" # shutdown application. If flow is running, then wait to finish. App should terminate ONLY with this method.\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/disable\\\" # enable flow scheduling\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/enable\\\" # enable flow scheduling\\n  curl -X GET \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/disabled\\\" # true if disabled, else false\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/cleanup/all\\\" # Run cleanup on demand\\n  curl -X PUT \\\"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/main/run\\\" # Run flow on demand\\n  # UI with endpoints available from VNC: http://unc2.bigdata.abc.gr:11482/traffica/swagger-ui/index.html#/\\n  ```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context clearly outlines the HTTP endpoints used to check the paused status and resume the Traffica flow, making the question fully answerable with no ambiguity.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Verifying and resuming paused flows is a critical operational task for telecom engineers ensuring data pipelines like Traffica remain functional. This is highly relevant to BigStreamer system operations.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely self-contained and specific. It clearly asks about verifying and resuming a paused flow, requiring no additional context to understand."
    },
    {
      "question": "How are LTE data files for TrustCenter generated and validated?",
      "answer": "The Oozie coordinator `Location_Mobility_2Hour_CO` executes `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` every 2 hours. It extracts data from `eea.eea_hour`, creates files like `LM_02_lte_yyyyMMdd_000xx.txt`, and uploads them to `/lm` via SFTP. Validation includes checking `/shared/abc/location_mobility/logging/LM_02_lte_reconciliation.log` for the correct file entries and line counts.",
      "category": "Data Management & Query Execution",
      "context": "# TrustCenter Flows\\n\\n        This is a document that will assist on support. Business documents can be found [here](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/tree/master/mini%20projects/location_mobility/docs).\\n        \\n        ## Location Mobility\\n        \\n        Location Mobility (LM) reffers to extraction of data from BigStreamer into files.  \\n        The output files are transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them.  \\n        These files are:\\n        \\n        - `LM_02_lte_yyyyMMdd_xxx.txt`\\n        - `LM_03_smsIn_yyyyMMdd_xxx.txt`\\n        - `LM_04_smsOut_yyyyMMdd_xxx.txt`\\n        - `LM_05_voiceInOut_yyyyMMdd_xxx.txt`\\n        - `LM_06_voiceIn_yyyyMMdd_xxx.txt`\\n        - `LM_07_voiceOut_yyyyMMdd_xxx.txt`\\n        - `LM_08_cellHist_yyyyMMdd_xxx.txt`\\n        \\n        Along with those, the reconciliation files are produced and sent for each one.  \\n        They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\\n        \\n        ``` bash\\n        cat /shared/abc/location_mobility/logging/LM_05_voiceInOut_reconciliation.log\\n        #e.g for LM_05_voiceInOut and 1st of February 2022\\n        2022-02-01 08:06:33 LM_05_voiceInOut_20220201_00001.txt 20220201 20906\\n        2022-02-01 10:02:36 LM_05_voiceInOut_20220201_00002.txt 20220201 23810\\n        2022-02-01 12:02:45 LM_05_voiceInOut_20220201_00003.txt 20220201 179719\\n        2022-02-01 14:03:45 LM_05_voiceInOut_20220201_00004.txt 20220201 876051\\n        2022-02-01 16:05:13 LM_05_voiceInOut_20220201_00005.txt 20220201 1581201\\n        2022-02-01 20:02:00 LM_05_voiceInOut_20220201_00006.txt 20220201 1606966\\n        ```\\n        \\n        **Reconcilication Files**:  \\n        `/shared/abc/location_mobility/logging/LM_*` on `un2.bigdata.abc.gr`\\n        \\n        **Troubleshooting Steps**:\\n        - Check to see if the file was produced at the right time and contained the expected number of rows.\\n        \\n        ### LM_02_lte\\n        Under normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala table `eea.eea_hour`.  \\n        The filename format is `LM_02_lte_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\n        For example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_02_lte_20220301_00002.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n          B --> C[ Remdef Script ]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `mtuser`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every 2 hours`  \\n        \\n        **Coordinator**: `Location_Mobility_2Hour_CO`\\n        \\n        **Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n        \\n        **Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n          graph TD \\n          A[Impala: eea.eea_hour] -->| Impala Query | B1[location_mobility.lm_lte_exp]\\n          B1 --> |Merge HDFS files to a single file | B2[File: LM_02_lte_yyyyMMdd_000xx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B2 --> |SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/lm`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` on `un2.bigdata.abc.gr`\\n        \\n        **SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/eea_hour.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Find in the failed execution's log the message:  \\n            - login on `un2.bigdata.abc.gr` with personal account  \\n            - `su - mtuser`\\n            ``` logs\\n            # e.g for 2021-02-22\\n            [...] - INFO: end_date=2021-02-22 09:00:00\\n            [...] - INFO: max_date=2021-02-22 09:00:00\\n            ```\\n        \\n            If end_date is newer or equal to max_date, it means that table `eea.eea_hour` does not contain new data and therefore there is nothing to be done during this execution.  \\n                abc should load data in `eea.eea_hour` table first and then execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\n        Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        \\n        - If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\n        This will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\n        This is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \\n        For example if 6 files were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        - If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        ### LM_03_smsIn\\n        Under normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.sms_raw_v, osix.osix_sms_raw` that fulfill some conditions.  \\n        The filename format is `LM_03_smsIn_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\n        For example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_03_smsIn_20220301_00002.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n          B --> C[ Remdef Script ]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `mtuser`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every 2 hours`  \\n        \\n        **Coordinator**: `Location_Mobility_2Hour_CO`\\n        \\n        **Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n        \\n        **Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD\\n          A[Impala: sai.sms_raw_v] --> |union all | D[Impala: osix.osix_sms_raw ] --> | Impala Query | B[File: LM_03_smsIn_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/lm`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh` on `un2.bigdata.abc.gr`\\n        \\n        **SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_in_v2.sql` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/sms_in.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Check if this message exists in the failed execution's log:  \\n            - login on `un2.bigdata.abc.gr` with personal account  \\n            - `su - mtuser`\\n        \\n            ``` logs\\n            [...] - INFO: Nothing to export.\\n            ```\\n        \\n            This means that tables `sai.sms_raw_v` or `osix.osix_sms_raw` do not contain new data and therefore there is nothing to be done during this execution.  \\n        \\n            New data should be loaded in the following tables and then execute the script.  \\n                - `sai.sms_raw`, updated by TRAFFICA flow (`sai.sms_raw_v` is a view on `sai.sms_raw` table).  \\n                - `osix.osix_sms_raw`, responsible abc\\n        \\n        - If failed execution's log contains the message:\\n        \\n            ``` logs\\n            [...] - ERROR: Script is being executed by another process. Exiting..\\n            ```\\n        \\n            and `ps -ef | grep export_lm_sms_in_v2_mon.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/sms_in.lock` and execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\n        Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        \\n        - If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\n        This will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\n        This is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \\n        For example if 6 files were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        - If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag.  \\n        For example if the first 6 files for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        ### LM_04_smsOut\\n        Under normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.sms_raw_v, osix.osix_sms_raw` that fulfill some conditions.  \\n        The filename format is `LM_04_smsOut_yyyyMMdd_xxx.txt` where `xx` is a serial number between `01` and `12`.  \\n        For example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_04_smsOut_20220301_00002.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n          B --> C[ Remdef Script ]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `mtuser`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every 2 hours`  \\n        \\n        **Coordinator**: `Location_Mobility_2Hour_CO`\\n        \\n        **Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n        \\n        **Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: sai.sms_raw_v] --> |union all | D[Impala: osix.osix_sms_raw ] -->| Impala Query | B[File: LM_04_smsOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/lm`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d').log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh` on `un2.bigdata.abc.gr`\\n        \\n        **SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_out_v2.sql` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/sms_out.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Check if this message exists in the failed execution's log:  \\n            - login on `un2.bigdata.abc.gr` with personal account  \\n            - `su - mtuser`\\n        \\n            ``` logs\\n            [...] - INFO: Nothing to export.\\n            ```\\n        \\n            This means that table `sai.sms_raw_v` or `osix.osix_sms_raw` do not contain new data and therefore there is nothing to be done during this execution.  \\n        \\n            New data should be loaded in the following tables and then execute the script.  \\n                - `sai.sms_raw`, updated by TRAFFICA flow (`sai.sms_raw_v` is a view on `sai.sms_raw` table).  \\n                - `osix.osix_sms_raw`, responsible abc\\n        \\n        - If failed execution's log contains the message:\\n        \\n            ``` logs\\n            [...] - ERROR: Script is being executed by another process. Exiting..\\n            ```\\n        \\n            and `ps -ef | grep export_lm_sms_out_v2_mon.sh` return no process means the previous execution was forcefully stopped.  \\n                Delete the lock file `/shared/abc/location_mobility/run/sms_out.lock` and execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\n        Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        \\n        - If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\n        This will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\n        This is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \\n        For example if 6 files were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d') 2>&1\\n            ```\\n        \\n        - If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        ### LM_05_voiceInOut\\n        Under normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions. \\n        The filename format is `LM_05_voiceInOut_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\n        For example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_05_voiceInOut_20220301_00002.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n          B --> C[ Remdef Script ]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `mtuser`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every 2 hours`  \\n        \\n        **Coordinator**: `Location_Mobility_2Hour_CO`\\n        \\n        **Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n        \\n        **Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] --> | Impala Query | B[File: LM_05_voiceInOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/lm`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh` on `un2.bigdata.abc.gr`\\n        \\n        **SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2.sql` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/voice_inout.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Check if this message exists in the failed execution's log:  \\n            - login on `un2.bigdata.abc.gr` with personal account  \\n            - `su - mtuser`\\n        \\n            ``` logs\\n            [...] - INFO: Nothing to export.\\n            ```\\n        \\n            This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution.  \\n        \\n            New data should be loaded in the following tables and then execute the script.  \\n                - `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \\n                - `osix.osix_voice_raw`, responsible abc\\n        \\n        - If failed execution's log contains the message:\\n        \\n            ``` logs\\n            [...] - ERROR: Script is being executed by another process. Exiting..\\n            ```\\n        \\n            and `ps -ef | grep export_lm_voice_inout_v2_mon.sh` return no process means the previous execution was forcefully stopped.  \\n                Delete the lock file `/shared/abc/location_mobility/run/voice_inout.lock` and execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\n        Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\n        This will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\n        This is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \\n        For example if 6 files were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        - If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        ### LM_06_voiceIn\\n        Under normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions.  \\n        The filename format is `LM_06_voiceIn_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\n        For example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_06_voiceIn_20220301_00002.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n          B --> C[ Remdef Script ]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `mtuser`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every 2 hours`  \\n        \\n        **Coordinator**: `Location_Mobility_2Hour_CO`\\n        \\n        **Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n        \\n        **Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] -->| Impala Query | B[File: LM_06_voiceIn_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/lm`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh` on `un2.bigdata.abc.gr`\\n        \\n        **SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_in_v2.sql` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/voice_in.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Check if this message exists in the failed execution's log:  \\n            - login on `un2.bigdata.abc.gr` with personal account  \\n            - `su - mtuser`\\n        \\n            ``` logs\\n            [...] - INFO: Nothing to export.\\n            ```\\n        \\n            This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution. \\n        \\n            New data should be loaded in the following tables and then execute the script.  \\n                - `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \\n                - `osix.osix_voice_raw`, responsible abc\\n        \\n        - If failed execution's log contains the message:\\n        \\n            ``` logs\\n            [...] - ERROR: Script is being executed by another process. Exiting..\\n            ```\\n        \\n            and `ps -ef | grep export_lm_voice_in.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/voice_in.lock` and execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\n        Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\n        This will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\n        This is not needed if 4 or less files were missed in which case the procedure will automatically catch up. For example if 6 files were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        - If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        ### LM_07_voiceOut\\n        Under normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions.  \\n        The filename format is `LM_07_voiceOut_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\n        For example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_07_voiceOut_20220301_00002.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n          B --> C[ Remdef Script ]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `mtuser`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every 2 hours`  \\n        \\n        **Coordinator**: `Location_Mobility_2Hour_CO`\\n        \\n        **Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n        \\n        **Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] -->| Impala Query | B[File: LM_07_voiceOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/lm`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh` on `un2.bigdata.abc.gr`\\n        \\n        **SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_out_v2.sql` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/voice_out.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\n        Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Check if this message exists in the failed execution's log:  \\n            - login on `un2.bigdata.abc.gr` with personal account  \\n            - `su - mtuser`\\n        \\n            ``` logs\\n            [...] - INFO: Nothing to export.\\n            ```\\n        \\n            This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution. \\n        \\n            New data should be loaded in the following tables and then execute the script.  \\n                - `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \\n                - `osix.osix_voice_raw`, responsible abc\\n        \\n        - If failed execution's log contains the message:\\n        \\n            ``` logs\\n            [...] - ERROR: Script is being executed by another process. Exiting..\\n            ```\\n        \\n            and `ps -ef | grep export_lm_voice_out_v2_mon.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/voice_out.lock` and execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\n        Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If 5 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N 2-hour intervals. This is not needed if 4 or less files were missed in which case the procedure will automatically catch up. For example if 6 files were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        - If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        ### LM_08_cellHist\\n        \\n        Under normal circumstances this file is produced every day and contains yesterday's data from the Impala table `refdata.rd_cells_v`.  \\n        The filename format is `LM_08_cellHist_yyyyMMdd_00001.txt`.  \\n        For example, if the file contains data for the 1st of March 2022 the filename will be `LM_08_cellHist_20220301_00001.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie Coord: Location_Mobility_Daily_CO] -->|SHELL| B[Master Script ]\\n          B --> C[ Remdef Script ]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `mtuser`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every day at 07:00`  \\n        \\n        **Coordinator**: `Location_Mobility_Daily_CO`\\n        \\n        **Master Script**: `000.Location_Mobility_Daily_Oozie_Main.sh`\\n        \\n        **Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_daily.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: refdata.rd_cells_v] -->| Impala Query | B[File: LM_08_cellHist_yyyyMMdd_00001.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/lm`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/rd_cells.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Find in the failed execution's log the message:  \\n            - login on `un2.bigdata.abc.gr` with personal account  \\n            - `su - mtuser`\\n        \\n            ``` logs\\n            [...] - INFO: max_date=yyyyMMdd and export_date=yyyyMMdd\\n            ```\\n        \\n            If the desired export_date is newer than max_date, it means that table `refdata.rd_cells_v` does not contain new data and therefore there is nothing to be done during this execution.  \\n                Load table `refdata.rd_cells` first and then execute the script.\\n        \\n        - If failed execution's log contains the message:\\n        \\n            ``` logs\\n            [...] - ERROR: Script is being executed by another process. Exiting..\\n            ```\\n        \\n            and `ps -ef | grep export_lm_rd_cells.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/rd_cells.lock` and execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\n        Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If 5 or more dates weren't exported execute the script with the `--max-files <N>` flag.  \\n        This will instruct the script to catch-up meaning to export files for N dates.  \\n        This is not needed if 4 or less dates were missed in which case the procedure will automatically catch up.  \\n        For example if 6 dates were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        - If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh -t 20220313 >> /shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log 2>&1\\n            ```\\n        \\n        \\n        ## Router Analytics\\n        \\n        Router Analytics (RA) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\\n        \\n        - `RA_01_yyyymmdd_00001_x.gz` \\n        - `RA_02_yyyymmdd_00001_x.gz`\\n        - `RA_03_yyyymmdd.gz`\\n        \\n        \\n        Along with those, the reconciliation files are produced and sent for each one. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\\n        \\n        ``` bash\\n        cat /shared/abc/location_mobility/logging/RA_BS_01_reconciliation.log\\n        #e.g for LM_05_voiceInOut and 31st of January 2022\\n        2022-02-01 09:06:39 RA_01_20220131_00001_[0-5] 20220131 68579162\\n        ```\\n        \\n        **Reconcilication Files**: `/shared/abc/location_mobility/logging/RA_BS_*` on `un2.bigdata.abc.gr`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Check to see if the file was produced at the right time and contained the expected number of rows.\\n        \\n        \\n        ### RA_01\\n        Under normal circumstances this file is produced every day and contains yesterday's data from the Impala table `npce.device_session`. The filename format is `RA_01_yyyymmdd_00001_x.gz` where `x` is a serial number between `1` and `5` as due to size the file is split into subfiles. For example, the files containing data for the 1st of March 2022 will be `RA_01_20220301_00001_[0-5].gz`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n          B -->|sudo to mtuser| C[Master Script]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `intra`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every day at 07:00`  \\n        \\n        **Coordinator**: `export_Router_Analytics_files_daily`\\n        \\n        **Master Script**: `/shared/abc/location_mobility/run/run_ra_exports.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: npce.device_session] -->| Impala Query | B[File: RA_01_yyyymmdd_00001_x.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/ra`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/export_ra_bs_01.sh` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/ra_01.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Find in the failed execution's log the message:\\n        \\n            ``` logs\\n            # e.g for 2021-02-01\\n            [...] - INFO: max_date=20220131 and export_date=20220131\\n            ```\\n        \\n            If the desired export_date is newer than max_date, it means that table `npce.device_session` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\\n        - If failed execution's log contains the message:\\n        \\n            ``` logs\\n            [...] - ERROR: Script is being executed by another process. Exiting..\\n            ```\\n        \\n            and `ps -ef | grep export_ra_bs_01.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_01.lock` and execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If 4 or more dates weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N dates. This is not needed if 3 or less dates were missed in which case the procedure will automatically catch up. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 5 dates were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/export_ra_bs_01.sh --max-files 6 >> /shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\\n            ```\\n        \\n        - If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/export_ra_bs_01.sh -t 20220313 >> /shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\\n            ```\\n        \\n        ### RA_02\\n        Under normal circumstances this file is produced every day and contains yesterday's data from the Impala table `npce.device_traffic`. The filename format is `RA_02_yyyymmdd_00001_x.gz` where `x` is a serial number between `1` and `5` as due to size the file is split into subfiles. For example, the files containing data for the 1st of March 2022 will be `RA_02_20220301_00001_[0-5].gz`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n          B -->|sudo to mtuser| C[Master Script]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `intra`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every day at 07:00`  \\n        \\n        **Coordinator**: `export_Router_Analytics_files_daily`\\n        \\n        **Master Script**: `/shared/abc/location_mobility/run/run_ra_exports.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: npce.device_traffic] -->| Impala Query | B[File: RA_02_yyyymmdd_00001_x.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/ra`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/export_ra_bs_02.sh` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/ra_02.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Find in the failed execution's log the message:\\n        \\n            ``` logs\\n            # e.g for 2021-02-01\\n            [...] - INFO: max_date=20220131 and export_date=20220131\\n            ```\\n        \\n            If the desired export_date is newer than max_date, it means that table `npce.device_traffic` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\\n        - If failed execution's log contains the message:\\n        \\n            ``` logs\\n            [...] - ERROR: Script is being executed by another process. Exiting..\\n            ```\\n        \\n            and `ps -ef | grep export_ra_bs_02.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_02.lock` and execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If 4 or more dates weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N dates. This is not needed if 3 or less dates were missed in which case the procedure will automatically catch up. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 5 dates were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/export_ra_bs_02.sh --max-files 6 >> /shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log 2>&1\\n            ```\\n        \\n        - If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/export_ra_bs_02.sh -t 20220313 >> /shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log 2>&1\\n            ```\\n        \\n        \\n        ### RA_03\\n        Under normal circumstances this file is produced every Wednesday and contains past week's data from the Impala table `npce.device_dms`. The filename format is `RA_03_yyyymmdd.gz`. For example, the files containing data up to the 2nd of March 2022 will be `RA_03_20220302.gz`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n          B -->|sudo to mtuser| C[Master Script]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `intra`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every Wednesday at 16:00`  \\n        \\n        **Coordinator**: `export_Router_Analytics_files_to_mediation_ra_03_weekly`\\n        \\n        **Master Script**: `/shared/abc/location_mobility/run/run_ra_03_export.sh`\\n        \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: npce.device_dms] -->| Impala Query | B[File: RA_03_yyyymmdd.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/ra`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/export_ra_bs_03.sh` on `un2.bigdata.abc.gr`\\n        \\n        **Lock file**: `/shared/abc/location_mobility/run/ra_03.lock`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Find in the failed execution's log the message:\\n        \\n            ``` logs\\n            # e.g for 2021-01-26\\n            [...] - INFO: max_date=20220126 and export_date=20220202\\n            ```\\n        \\n            If the desired export_date is newer than max_date, it means that table `npce.device_dms` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\\n        - If failed execution's log contains the message:\\n        \\n            ``` logs\\n            [...] - ERROR: Script is being executed by another process. Exiting..\\n            ```\\n        \\n            and `ps -ef | grep export_ra_bs_03.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_03.lock` and execute the script.\\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If 2 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N executions. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 2 files were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/export_ra_bs_03.sh --max-files 2 >> /shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log 2>&1\\n            ```\\n        \\n        - If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 16th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/export_ra_bs_03.sh -t 20220316 >> /shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log 2>&1\\n            ```\\n        \\n        \\n        \\n        ## Application Data Usage Insights\\n        \\n        Application Data Usage Insights (AUI) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\\n        \\n        - `AUI_01_yyyymmdd_0000x.txt`\\n        \\n        \\n        Along with those, a reconciliation file are produced and sent. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\\n        \\n        ``` bash\\n        cat /shared/abc/location_mobility/logging/AUI_BS_01_reconciliation.log\\n        #e.g for AUI_01 and 21st of February 2022\\n        2021-02-22 06:00:09 AUI_01_20210221_00005.txt 20210221 15\\n        ```\\n        \\n        **Reconcilication File**: `/shared/abc/location_mobility/logging/AUI_BS_01_reconciliation.log` on `un2.bigdata.abc.gr`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Check to see if the file was produced at the right time and contained the expected number of rows.\\n        \\n        \\n        ### AUI_01\\n        Under normal circumstances this file is produced every 4 hours and contains data from 6 to 2 hours ago of the Impala table `npce.abc_apps_raw_events`. The filename format is `AUI_01_yyyymmdd_0000x.txt` where `x` is a serial number between `1` and `6`. For example, the files containing data for the 1st of March 2022 from 00:00 to 04:00 will be `AUI_01_20220301_00001.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie: export_Application_Data_Usage_Insights_files_4_hours] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n          B -->|sudo to mtuser| C[Master Script]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `intra`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every 4 hours`  \\n        \\n        **Coordinator**: `export_Application_Data_Usage_Insights_files_4_hours`\\n        \\n        **Master Script**: `/shared/abc/location_mobility/run/run_aui_exports.sh`\\n         \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: npce.abc_apps_raw_events] -->| Impala Query | B[File: AUI_01_yyyymmdd_0000x.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /aui]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/data/location_mobility/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/aui`\\n        \\n        **Logs**: ```/shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log```\\n        \\n        **Script**: `/shared/abc/location_mobility/run/export_aui.sh` on `un2.bigdata.abc.gr`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Find in the failed execution's log the message:\\n        \\n            ``` logs\\n            date: invalid date ‘NULL 6 hours ago’\\n            ```\\n        \\n            This means that table `npce.abc_apps_raw_events` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\\n        \\n        \\n        **Ndefs**:\\n        \\n        - If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If 5 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N 4-hour intervals. This is not needed if 4 or less dates were missed in which case the procedure will automatically catch up. For example if 6 dates were not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/export_aui.sh --max-files 6 >> /shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\\n            ```\\n        \\n        - If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/location_mobility/run/export_aui.sh -t 20220313 >> /shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\\n            ```\\n        \\n        \\n        \\n        ## Customer Satisfaction Index\\n        \\n        Customer Satisfaction Index (CSI) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\\n        \\n        - `CSI_fix_mmddyyyy_wXX.txt`\\n        - `CSI_mob_mmddyyyy_mmddyyyy.txt`\\n        \\n        \\n        Along with those, a reconciliation file are produced and sent. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\\n        \\n        ``` bash\\n        cat /shared/abc/export_sai_csi/logging/CSI_mob_reconciliation.log\\n        #e.g for CSI_mob and 30th of January 2022\\n        2022-01-30 09:02:42  CSI_mob_01242022_01302022.txt  20220124  4223904\\n        ```\\n        \\n        **Reconcilication File**: `/shared/abc/export_sai_csi/logging/CSI_*` on `un2.bigdata.abc.gr`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Check to see if the file was produced at the right time and contained the expected number of rows.\\n        \\n        ### CSI_fix\\n        Under normal circumstances this file is produced every 4 hours and contains data from 2 days ago ago of the Impala table `sai.cube_indicators_it`. The filename format is `CSI_fix_mmddyyyy_wXX.txt` where `XX` is a serial number between `1` and `52` for the week of the year. For example, the file containing data for the 2nd of February 2022 which belongs to the 5th week of the year, will be `CSI_fix_02042022_w05.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie: export_CSI_fix_and_mobile_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n          B -->|sudo to mtuser| C[Master Script]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `intra`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every day at 7:00`  \\n        \\n        **Coordinator**: `export_CSI_fix_and_mobile_daily`\\n        \\n        **Master Script**: `/shared/abc/location_mobility/run/run_csi_exports_daily.sh`\\n         \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: sai.cube_indicators_it] -->| Impala Query | B[File: CSI_fix_mmddyyyy_wXX.txt <br> Server: un2.bigdata.abc.gr <br> Path: /shared/abc/export_sai_csi/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /csi]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/shared/abc/export_sai_csi/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/csi`\\n        \\n        **Logs**: ```/shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log```\\n        \\n        **Script**: `/shared/abc/export_sai_csi/run/export_csi_fix.sh` on `un2.bigdata.abc.gr`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Find in the failed execution's log the message:\\n        \\n            ``` logs\\n            # e.g for 2022-01-10\\n            Problem with 20220108.\\n            ```\\n        \\n            This means that table `sai.cube_indicators_it` does not contain new data and therefore there is nothing to be done during this execution. Load table `brond.cube_indicators` first and then execute the script.\\n        \\n        \\n        **Ndefs**:\\n        \\n        - If one date was missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If you need to export file for a specific date execute the script with argument `<yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/export_sai_csi/run/export_csi_fix.sh 20220313 >> /shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log 2>&1\\n            ```\\n        \\n        ### CSI_mob\\n        Under normal circumstances this file is produced every day and contains data for the current week of the Impala table `sai.sub_aggr_csi_it`. The filename format is `CSI_mob_mmddyyyy_mmddyyyy.txt` where the first date is the last loaded Monday and the second the current date. For example, the file containing data for the 2nd of February 2022 will be `CSI_mob_01312022_02022022.txt`.\\n        \\n        ``` mermaid\\n          graph TD\\n          A[Oozie: export_CSI_fix_and_mobile_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n          B -->|sudo to mtuser| C[Master Script]\\n        ```\\n        \\n        The workflow triggers a master script which in turn executes the substeps\\n        \\n        **User**: `intra`\\n        \\n        **Scheduler**: `Oozie`\\n        \\n        **Schedule**: `Every day at 7:00`  \\n        \\n        **Coordinator**: `export_CSI_fix_and_mobile_daily`\\n        \\n        **Master Script**: `/shared/abc/location_mobility/run/run_csi_exports_daily.sh`\\n         \\n        The master script triggers the export procedure.\\n        \\n        ``` mermaid\\n        graph TD \\n          A[Impala: sai.sub_aggr_csi_it] -->| Impala Query | B[File: CSI_mob_mmddyyyy_mmddyyyy.txt <br> Server: un2.bigdata.abc.gr <br> Path: /shared/abc/export_sai_csi/out]\\n          B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /csi]\\n        ```\\n        \\n        **User**: `mtuser`\\n        \\n        **Local path**: `/shared/abc/export_sai_csi/out`\\n        \\n        **SFTP user**: `trustcenterftp`\\n        \\n        **SFTP path**: `/csi`\\n        \\n        **Logs**: ```/shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log```\\n        \\n        **Script**: `/shared/abc/export_sai_csi/run/export_csi_mob_daily.sh` on `un2.bigdata.abc.gr`\\n        \\n        **Troubleshooting Steps**:\\n        \\n        - Identify system or service errors in the log file e.g failed Impala query.\\n        - Find in the failed execution's log the message:\\n        \\n            ``` logs\\n            # e.g for 2022-01-10\\n            Problem with 20220108.\\n            ```\\n        \\n            This means that table `sai.sub_aggr_csi_it` does not contain new data and therefore there is nothing to be done during this execution. Load table `sai.sub_aggr_csi_it` first and then execute the script.\\n        \\n        \\n        **Ndefs**:\\n        \\n        - If one date was missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n        - If you need to export file for a specific date execute the script with argument `<yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\\n        \\n            ``` bash\\n            /shared/abc/export_sai_csi/run/export_csi_mob_daily.sh 20220313 >> /shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log 2>&1\\n            ```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context provides all relevant details for how LTE data files are generated and validated, including scripts used, scheduling, filenames, data source, and validation log paths. The answer is fully grounded in this context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question is highly relevant for telecom engineers using BigStreamer, especially those responsible for ensuring timely data delivery to TrustCenter and verifying data integrity.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely self-contained and unambiguous. It clearly asks for the process of file generation and validation for LTE data without needing any additional context."
    },
    {
      "question": "How do you increase Java heap space for Streamsets and validate the changes?",
      "answer": "1. Go to Cloudera Manager > Streamsets > Configuration > Java Options and set:\n```bash\n-Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow\n```\n2. Remove old configs from 'Data Collector Advanced Configuration Snippet' if present.\n3. Restart Streamsets.\n4. Validate with:\n```bash\nps -ef | grep -i streamsets | grep -i xmx\njmap -heap <pid>\n```",
      "category": "Infrastructure & Deployment",
      "context": "# Streamsets - Java Heap Space\\n\\n        Streamsets Java Heap Memory was increased due to [this](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/102#ndef_95081) issue.\\n        \\n        Actions Taken:\\n        \\n        1. Configure Java Options from CLoudera Manager\\n        \\n           ```bash\\n           cluster -> Streamsets -> Configuration -> Java options: `-Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow`\\n           ```\\n        \\n        2. Remove old configuration\\n        \\n           ```bash\\n           cluster-> Streamsets -> Configuration -> Data Collector Advanced Configuration Snippet (Safety Valve) for sdc-env.sh\\n           ```\\n        \\n           ```bash\\n           #Remove the following line, if exists\\n           export SDC_JAVA_OPTS=\\\"-Xmx16384m -Xms16384m -server -XX:-OmitStackTraceInFastThrow ${SDC_JAVA_OPTS}\\\"\\n           ```\\n        \\n        3. Restart Streamsets\\n        \\n           ```bash\\n           cluster -> Streamsets -> Restart\\n           ```\\n        \\n        4. Check Streamsets Process Options\\n        \\n           ```bash\\n           [root@un2 ~]# ps -ef | grep -i streamsets | grep -i xmx\\n        \\n           sdc      24898 24873 45 12:45 ?        00:40:11 /usr/java/default/bin/java -classpath /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs/main/streamsets-datacollector-bootstrap-3.21.0.jar:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/root-lib/* -Djava.security.manager -Djava.security.policy=file:///var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR/sdc-security.policy -Xmx1024m -Xms1024m -server -XX:-OmitStackTraceInFastThrow -Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow -Dsdc.dist.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0 -Dsdc.resources.dir=/var/lib/sdc/resources -Dsdc.hostname=un2.bigdata.abc.gr -Dsdc.conf.dir=/var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR -Dsdc.data.dir=/shared/sdc/data -Dsdc.log.dir=/shared/sdc/log/ -javaagent:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs/main/streamsets-datacollector-bootstrap-3.21.0.jar -Dsdc.libraries.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs -Dsdc.librariesExtras.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs-extras -Dsdc.rootLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/root-lib -Dsdc.bootstrapLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs -Dsdc.apiLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/api-lib -Dsdc.asterClientLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/aster-client-lib -Dsdc.containerLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/container-lib -Dsdc.libsCommon.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libs-common-lib -Dsdc.userLibs.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/user-libs -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -Djdk.nio.maxCachedBufferSize=262144 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/shared/sdc/log//gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/shared/sdc/log//sdc_heapdump_1675334705.hprof -XX:ErrorFile=/shared/sdc/log//hs_err_1675334705.log com.streamsets.pipeline.BootstrapMain -mainClass com.streamsets.datacollector.main.DataCollectorMain -apiClasspath /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/api-lib/*.jar -containerClasspath /var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/container-lib/*.jar -streamsetsLibrariesDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs -userLibrariesDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/user-libs -configDir /var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR -libsCommonLibDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libs-common-lib -streamsetsLibrariesExtraDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs-extras\\n           ```\\n        \\n           > Tip: When defining Java configuration options, avoid defining duplicate options. If you do define duplicates, the last option passed to the JVM usually takes precedence.\\n        \\n        5. Check Max Java Heap Space for Streamsets via jconsole or jmap\\n        \\n           **jconsole**\\n           > Needs additional Java Options: -Dcom.sun.management.jmxremdef -Dcom.sun.management.jmxremdef.port=3333 -Dcom.sun.management.jmxremdef.local.only=false -Dcom.sun.management.jmxremdef.authenticate=false -Dcom.sun.management.jmxremdef.ssl=false\\\"\\n        \\n           ```bash\\n           [root@un2 ~]# /usr/java/latest/bin/jconsole\\n           ```\\n        \\n           Select `com.streamsets.pipeline.BootstrapMain`,  `Connect` and check the metrics.\\n        \\n           **jmap**\\n        \\n           ```bash\\n           jmap -heap <pid>\\n        \\n           #output example\\n           [root@un2 ~]# jmap -heap 24898\\n           Attaching to process ID 24898, please wait...\\n           Debugger attached successfully.\\n           Server compiler detected.\\n           JVM version is 25.181-b13\\n        \\n           using parallel threads in the new generation.\\n           using thread-local object allocation.\\n           Concurrent Mark-Sweep GC\\n        \\n           Heap Configuration:\\n              MinHeapFreeRatio         = 40\\n              MaxHeapFreeRatio         = 70\\n              MaxHeapSize              = 34359738368 (32768.0MB)\\n              NewSize                  = 2442723328 (2329.5625MB)\\n              MaxNewSize               = 2442723328 (2329.5625MB)\\n              OldSize                  = 31917015040 (30438.4375MB)\\n              NewRatio                 = 2\\n              SurvivorRatio            = 8\\n              MetaspaceSize            = 21807104 (20.796875MB)\\n              CompressedClassSpaceSize = 1073741824 (1024.0MB)\\n              MaxMetaspaceSize         = 17592186044415 MB\\n              G1HeapRegionSize         = 0 (0.0MB)\\n        \\n           Heap Usage:\\n           New Generation (Eden + 1 Survivor Space):\\n              capacity = 2198470656 (2096.625MB)\\n              used     = 1493838840 (1424.6357345581055MB)\\n              free     = 704631816 (671.9892654418945MB)\\n              67.94900063473942% used\\n           Eden Space:\\n              capacity = 1954217984 (1863.6875MB)\\n              used     = 1433160568 (1366.768424987793MB)\\n              free     = 521057416 (496.91907501220703MB)\\n              73.33678124620104% used\\n           From Space:\\n              capacity = 244252672 (232.9375MB)\\n              used     = 60678272 (57.8673095703125MB)\\n              free     = 183574400 (175.0701904296875MB)\\n              24.84241891937215% used\\n           To Space:\\n              capacity = 244252672 (232.9375MB)\\n              used     = 0 (0.0MB)\\n              free     = 244252672 (232.9375MB)\\n              0.0% used\\n           concurrent mark-sweep generation:\\n              capacity = 31917015040 (30438.4375MB)\\n              used     = 12194092928 (11629.193237304688MB)\\n              free     = 19722922112 (18809.244262695312MB)\\n              38.20561826573617% used\\n        \\n           57229 interned Strings occupying 8110512 bytes.\\n           ```\\n        \\n        Reference: https://docs.streamsets.com/platform-datacollector/latest/datacollector/UserGuide/Configuration/DCEnvironmentConfig.html",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully answerable using the provided context. The steps to increase Java heap space in Streamsets and validate those changes using both `ps` and `jmap` are described in detail, including example outputs and where to configure settings in Cloudera Manager.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question is highly relevant for BigStreamer engineers since managing and optimizing Streamsets memory is critical for stable pipeline execution, especially in high-volume telecom environments.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is self-contained and unambiguous. It clearly asks for two specific tasks: increasing Java heap space and validating the configuration, without relying on external context."
    },
    {
      "question": "How can you repair OpenLDAP replication between kerb1 and kerb2 if the Manager password has changed?",
      "answer": "1. Prepare `replication_config.ldif` with updated credentials.\n2. Apply config using:\n```bash\nldapmodify -H ldaps://kerb1.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\nldapmodify -H ldaps://kerb2.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\n```\n3. Test replication by creating a test user on kerb1 and verifying it appears on kerb2 via ldapsearch.",
      "category": "Infrastructure & Deployment",
      "context": "# How to fix openldap replication\\n\\nThe broken replication between `kerb1`/`kerb2` could happened in case any of the below cases appeared/happened.\\n\\n- Case 1: You changed the `Manager` password of openldap instance\\n- Case 2: Replication broken for any other reason (power outage etc) between `kerb1` and `kerb2`\\n\\nFor every ldasearch the password of `Manager` is [here](KnowledgeBase/prodsyspasswd.kdbx)\\n\\n# For Case 1 follow the below steps:\\n\\nLogin into kerb1 node as root\\n\\n```bash\\nssh kerb1\\nsudo -i\\n```\\nBackup:\\n\\n```bash\\nslapcat -n 0 -l config.ldif\\nslapcat -n 2 -l data.ldif\\n```\\nCreate ldif file replication fix\\n\\n```bash\\nvi replication_config.ldif\\n\\ndn: olcDatabase={0}config,cn=config\\nchangetype:modify\\nreplace: olcSyncrepl\\nolcSyncrepl: rid=001\\n  provider=ldaps://kerb1.bigdata.abc.gr/\\n  binddn=\\\"cn=config\\\"\\n  bindmethod=simple\\n  credentials=\\\"new password\\\"\\n  searchbase=\\\"cn=config\\\"\\n  type=refreshAndPersist\\n  retry=\\\"5 5 300 +\\\"\\n  timeout=1\\nolcSyncrepl: rid=002\\n  provider=ldaps://kerb2.bigdata.abc.gr/\\n  binddn=\\\"cn=config\\\"\\n  bindmethod=simple\\n  credentials=\\\"new password\\\"\\n  searchbase=\\\"cn=config\\\"\\n  type=refreshAndPersist\\n  retry=\\\"5 5 300 +\\\"\\n  timeout=1\\n\\nadd: olcMirrorMode\\nolcMirrorMode: TRUE\\n\\ndn: olcDatabase={2}bdb,cn=config\\nchangetype:modify\\nreplace: olcSyncrepl\\nolcSyncrepl: rid=003\\n  provider=ldaps://kerb1.bigdata.abc.gr/\\n  binddn=\\\"cn=Manager,dc=bigdata,dc=abc,dc=gr\\\"\\n  bindmethod=simple\\n  credentials=`new password`\\n  searchbase=\\\"dc=bigdata,dc=abc,dc=gr\\\"\\n  type=refreshAndPersist\\n  retry=\\\"5 5 300 +\\\"\\n  timeout=1\\nolcSyncrepl: rid=004\\n  provider=ldaps://kerb2.bigdata.abc.gr/\\n  binddn=\\\"cn=Manager,dc=bigdata,dc=abc,dc=gr\\\"\\n  bindmethod=simple\\n  credentials=`new password`\\n  searchbase=\\\"dc=bigdata,dc=abc,dc=gr\\\"\\n  type=refreshAndPersist\\n  retry=\\\"5 5 300 +\\\"\\n  timeout=1\\n\\nadd: olcMirrorMode\\nolcMirrorMode: TRUE\\n```\\n\\nFix the replication:\\n\\n```bash\\nldapmodify  -H ldaps://kerb1.bigdata.abc.gr -D \\\"cn=config\\\" -W -f replication_config.ldif\\nldapmodify  -H ldaps://kerb2.bigdata.abc.gr -D \\\"cn=config\\\" -W -f replication_config.ldif\\n```\\n\\nChecks:\\n\\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:\\n\\nLogin into admin node as root:\\n\\nOpen firefox\\n```bash\\nfirefox\\n```\\nphpldapadmin link: (https://kerb1.bigdata.abc.gr/phpldapadmin/)\\n\\n# Steps to create an ldap user\\n\\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\\n2. Expand tree `people`\\n3. Click `create a new entry here`\\n4. The type of account will be `User account`\\n5. Fill all the empty boxes. \\n6. The user will be `no login` and the group/gid `disabled`\\n7. Create object\\n\\nAfter succesfully creation of `testuser` check if exist on `kerb2` from the dropdown menu or via ldapsearch\\n\\n```bash\\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \\\"cn=Manager,dc=bigdata,dc=abc,dc=gr\\\" -W -b \\\"ou=People,dc=bigdata,dc=abc,dc=gr\\\"  'uid=testuser'\\n```\\n\\nIf user exist then replication fixed. Just delete the `testuser`.\\n\\n# Steps to delete an ldap user\\n\\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\\n2. Expand tree `people`\\n3. Check the new user and from the right bar click `delete this entry`\\n\\n\\n# For Case 2 follow the below steps:\\n\\nIdentify which `kerb` ldap instance has the issue. For example check if they had the same ammount of `users` and `groups` with `ldapsearch` commands from checks\\n\\nFrom the `kerb` ldap instance without corruption :\\n\\n```bash\\nslapcat -n 0  config.ldif\\nslapcat -n 1 data.ldif\\n\\nscp *.ldif `ldap_instance_with_corruption`:/tmp\\n```\\nGo to corrupted `kerb` instance:\\n\\nBackup:\\n```bash\\ncp -rp /etc/openldap/ /tmp/openldap.bak\\ncp -rp /var/lib/ldap/ /tmp\\n```\\n\\nClear:\\n```bash\\nsystemctl stop slapd\\nsystemctl status slapd\\ncd /etc/openldap/\\nrm –Rf slapd.d\\ncd /etc/openldap\\nmkdir slapd.d\\ncd /var/lib/ldap/\\nrm –Rf *\\n```\\nRestore:\\n```bash\\nslapadd -n 0 -F /etc/openldap/slapd.d -l /tmp/config.ldif\\nslapadd -n 2 -F /etc/openldap/slapd.d -l /tmp/data.ldif -w\\n```\\nOwnership:\\n```bash\\nchown -R ldap:ldap /var/lib/ldap\\n```\\nStart `slapd` service:\\n\\n```bash\\nsystemctl start slapd\\nsystemctl status slapd\\n```\\n\\nChecks:\\n\\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:\\n\\nThe procedure to create a new user described [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/openldap_change_manager_password.md)\\n\\nLogin into admin node as root:\\n\\nOpen firefox\\n```bash\\nfirefox\\n```\\nphpldapadmin link: (https://kerb1.bigdata.abc.gr/phpldapadmin/)\\n\\nAfter succesfully creation of `testuser` check if exist on `kerb2` from the dropdown menu or via ldapsearch\\n\\n```bash\\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \\\"cn=Manager,dc=bigdata,dc=abc,dc=gr\\\" -W -b \\\"ou=People,dc=bigdata,dc=abc,dc=gr\\\"  'uid=testuser'\\n```\\n\\nIf user exist then replication fixed. Just delete the `testuser`.\\n\\nThe procedure to delete a new user described [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/openldap_change_manager_password.md)",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context provides detailed steps for updating replication credentials in OpenLDAP and validating that replication is restored, fully answering the question.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question addresses a real-world failure scenario (Manager password change) in LDAP replication which is essential for authentication infrastructure in BigStreamer systems.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question clearly presents the problem and environment (OpenLDAP, replication between kerb1 and kerb2, Manager password change), making it fully self-contained and unambiguous."
    },
    {
      "question": "How can you check whether anonymization and retention scripts ran successfully?",
      "answer": "1. For retention:\n```bash\ngrep \"Script Status\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.*.log | tail -n1\n```\nCheck `Status=0`. If not, grep for errors:\n```bash\negrep -i '(error|problem|except|fail)' /shared/abc/cdo/log/Retention/*<SnapshotID>*.log\n```\n2. For anonymization:\n```bash\ngrep \"Script Status\" /shared/abc/cdo/log/100.Anonymize_Data_Main.*.log | tail -n1\negrep '(:ERROR|with errors)' /shared/abc/cdo/log/Anonymize/*<RunID>*\n```",
      "category": "Data Management & Query Execution",
      "context": "#check Retention\\nlogin @un2 as intra\\n1st level:\\n$ grep \\\"Script Status\\\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.202012.log | tail -n1\\n\\nπ.χ. Script Status ==> Scr:203.Retention_Dynamic_Drop_DDL.sh, Dt:2020-12-18 08:13:12, Status:0, Snapshot:1608267602, RunID:1608271202, ExpRows:3327, Secs:790, 00:13:10\\n\\nif Status != 0 we have a problem\\n\\n---\\n\\n2nd level:\\nwe get the Snapshot ID from the above (e.g. Snapshot:1608267602)\\n\\n$ egrep -i '(error|problem|except|fail)' /shared/abc/cdo/log/Retention/*1608267602*.log\\n\\nif it comes out < 10 it doesn't particularly worry us.\\nIf it comes out a lot it's not good.\\n\\n#Anonymization\\n$ grep \\\"Script Status\\\" /shared/abc/cdo/log/100.Anonymize_Data_Main.202012.log | tail -n1\\nex: Script Status ==> Scr:100.Anonymize_Data_Main.sh, Dt:2020-12-17 21:01:03, Status:, RunID:1608228002, Secs:3661, 01:01:01\\n\\nwe take RunID from the above (π.χ. RunID:1608228002)\\n$ egrep '(:ERROR|with errors)' /shared/abc/cdo/log/Anonymize/*1608228002*.log | less\\n\\n> 0 we have a problem",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context explicitly describes both retention and anonymization verification procedures, including the exact commands and what to look for in the output. The answer is entirely grounded in the provided material.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Script verification for anonymization and retention is critical for compliance, data integrity, and operational reliability in telecom data platforms like BigStreamer. This is highly relevant to daily tasks of engineers.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clearly worded and conveys a specific intent — checking the success of two types of scripts — without depending on external context."
    },
    {
      "question": "How do you deduplicate rows in the `energy_efficiency.cell` table for a specific date range?",
      "answer": "1. Backup existing data:\n```sql\nCREATE TABLE energy_efficiency.cell_bak LIKE energy_efficiency.cell;\nINSERT INTO TABLE energy_efficiency.cell_bak PARTITION (par_dt) SELECT * FROM energy_efficiency.cell;\n```\n2. Overwrite with distinct records:\n```sql\nINSERT OVERWRITE TABLE energy_efficiency.cell PARTITION (par_dt)\nSELECT DISTINCT * FROM energy_efficiency.cell WHERE par_dt BETWEEN '20211210' AND '20211215';\n```",
      "category": "Data Management & Query Execution",
      "context": "# Streamsets - Energy Efficiency\\n\\n## Access\\n\\nStreamsets Login Page: https://999.999.999.999:18636/\\n\\nFiles:\\n\\nFrom un2 with sdc user:\\n\\n```bash\\nsftp bigd@999.999.999.999\\ncd /ossrc\\n```\\n\\n## Check for Duplicates\\n\\nExecute the following from Impala\\n\\n```bash\\nselect count(*), par_dt from energy_efficiency.cell where par_dt>'202111201' group by par_dt order by par_dt desc;\\n```\\n\\n```bash\\nselect count(*) from (select distinct * from energy_efficiency.cell where par_dt='20211210') a;\\n```\\n\\n## Solve Duplicates\\n\\nExecute the following from Impala\\n\\nBackup table:\\n```bash\\nCREATE TABLE  energy_efficiency.cell LIKE energy_efficiency.cell;\\nINSERT INTO TABLE energy_efficiency.cell_bak PARTITION (par_dt) SELECT * FROM energy_efficiency.cell;\\n```\\n\\nModify table:\\n```bash\\nINSERT OVERWRITE TABLE energy_efficiency.cell partition (par_dt)\\n\tSELECT DISTINCT * FROM energy_efficiency.cell\\n\tWHERE par_dt between '20211210' and '20211215';\\n```\\n\\nDrop Backup table:\\n```bash\\nDROP TABLE energy_efficiency.cell;\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context contains detailed, explicit SQL commands and procedures for deduplicating rows in the specified table and date range. The answer is entirely based on the provided context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Data deduplication in production tables like `energy_efficiency.cell` is a highly relevant operational task for engineers managing data pipelines in telecom environments.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is fully understandable on its own. It clearly specifies the table and operation (deduplication over a date range) without requiring prior context."
    },
    {
      "question": "What are the steps to create a keytab file for a user in NYMA?",
      "answer": "1. SSH to kerb1 and run:\n```bash\nkadmin.local\nlistprincs <user>@CNE.abc.GR\naddprinc <user>@CNE.abc.GR\nktadd -k /tmp/<user>.keytab -norandkey <user>@CNE.abc.GR\n```\n2. Copy the keytab to `un2`:\n```bash\nscp -p /tmp/<user>.keytab un2:/tmp\n```\n3. On `un2`, move it to the correct user folder:\n```bash\ncp /tmp/<user>.keytab /home/users/skokkoris/\nchown skokkoris. /home/users/skokkoris/<user>.keytab\n```",
      "category": "Application Functionality & Flow",
      "context": "# How to create a keytab in NYMA\\n\\nLogin into kerb1 node as root\\n\\n```bash\\nssh kerb1\\nsudo -i\\n```\\n\\nUse command-line interface to the Kerberos administration system\\n\\n```bash\\nkadmin.local\\n```\\n\\nCheck if there is a principal for the corresponding username\\n\\n```bash\\nlistprincs <username>@CNE.abc.GR\\n```\\n\\nCreate a principal if there is not one\\n\\n```bash\\naddprinc <username>CNE.abc.GR\\n```\\n\\nCreate the keytab\\n\\n```bash\\nktadd -k /tmp/<username>.keytab -norandkey <username>@CNE.abc.GR\\n```\\n\\nCopy the keytab file to un2 node\\n\\n```bash\\nscp -p /tmp/<username>.keytab un2:/tmp\\n```\\n\\nLogin into un2, place keytab file under /home/users/skokkoris/ and change ownership into skokkoris\\n\\n```bash\\nssh un2\\nsudo -i\\ncp -p /tmp/<username>.keytab /home/users/skokkoris/\\nchown skokkoris. /home/users/skokkoris/<username>.keytab\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The steps in the answer are directly and completely derived from the context, which outlines the procedure for creating and deploying a keytab file for a user in NYMA.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Creating keytab files is a common and critical task in authentication management, especially in secured telecom infrastructure environments like BigStreamer.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is fully understandable on its own. It clearly specifies what is needed (keytab creation), for whom (a user), and in which system (NYMA)."
    },
    {
      "question": "How do you set up a Kubernetes service account for RAN.AI access with kubeconfig?",
      "answer": "1. Create `service_account.yml` and `role_binding.yml` and apply:\n```bash\nkubectl apply -f service_account.yml\nkubectl apply -f role_binding.yml\n```\n2. If k8s version >1.24, create a secret:\n```yaml\nkind: Secret\nmetadata:\n  name: <secret>\n  annotations:\n    kubernetes.io/service-account.name: <account>\n```\n3. Get kubeconfig with:\n```bash\nkubectl view-serviceaccount-kubeconfig -n <namespace> <account> > ~/.kube/config\n```",
      "category": "Application Functionality & Flow",
      "context": "# Kubernetes User Environment Setup\\n\\n## Tools\\n\\nThe main tool that needs to be installed is **kubectl**, instructions for which can be found\\n[here](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/). Additionally after\\ninstallation, completion can be enabled by executing:\\n```bash\\nmkdir -p /etc/bash_completion.d\\nkubectl completion bash | sudo tee /etc/bash_completion.d/kubectl\\n```\\nIf possible a nice to have plugin for setting up kubeconfigs can be found at\\n[kubectl-view-serviceaccount-kubeconfig-plugin](https://github.com/superbrothers/kubectl-view-serviceaccount-kubeconfig-plugin/releases). Simply place the binary under `/usr/local/bin/`.\\n\\nAdditionally in order to install **helm**, follow the (instructions](https://helm.sh/docs/intro/install/)\\nand set up completion by executing hte following:\\n```bash\\nhelm completion bash | sudo tee /etc/bash_completion.d/helm\\n```\\n\\n## Service Account\\n\\nCreate the following YAML files, that contain the definition for the service account and its\\nrole binding:\\n- **`service_account.yml`**\\n```yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: <Account Name>\\n  namespace: <RAN.AI Namespace>\\n```\\n- **`role_binding.yml`**\\n```yaml\\nkind: RoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: <Binding Name>\\n  namespace: <RAN.AI Namespace>\\nsubjects:\\n- kind: ServiceAccount\\n  name: <Account Name>\\n  namespace: <RAN.AI Namespace>\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: edit\\n```\\n\\nApply both in sequence:\\n```bash\\nkubectl apply -f service_account.yml\\nkubectl apply -f role_binding.yml\\n```\\n\\n### User Secret\\n\\nFor Kubernetes versions over 1.24 when creating a service account it's secret is not automatically created\\nand mounted, so in that case create the following secret:\\n```yaml\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: <Account Secret Name>\\n  namespace: <RAN.AI Namespace>\\n  annotations:\\n    kubernetes.io/service-account.name: <Account Name>\\ntype: kubernetes.io/service-account-token\\n```\\n\\nExecute the following to get the new users config. If the plugin is not available the config must be constructed manually:\\n```\\nkubectl view-serviceaccount-kubeconfig -n <RAN.AI Namespace> <Account Name> > <User's Home>/.kube/config\\n```\\n\\nFor reference the config looks like this:\\n```yaml\\napiVersion: v1\\nclusters:\\n- cluster:\\n    certificate-authority-data: ...\\n    server: https://<Server>:<Port>\\n  name: <Cluster Name>\\ncontexts:\\n- context:\\n    cluster: <Cluster Name>\\n    namespace: <RAN.AI Namespace>\\n    user: <Account Name>\\n  name: kubernetes-admin@kubernetes\\ncurrent-context: kubernetes-admin@kubernetes\\nkind: Config\\npreferences: {}\\nusers:\\n- name: <Account Name>\\n  user:\\n    token: ...\\n```\\nIn the above segment `certificate-authority-data` and `token` can be obtained by executing:\\n```bash\\n# Find the Account's Secret Token name\\nkubectl get secrets -n <RAN.AI Namespace>\\n\\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.ca\\\\.crt}'\\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.token}' | base64 --decode\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The steps in the answer are directly supported by the context, including detailed YAML configuration and commands for both creating service accounts and retrieving kubeconfig credentials.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Setting up Kubernetes service accounts with kubeconfig is a key task in managing secure access to BigStreamer environments like RAN.AI, making this highly relevant to telecom engineers.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear, specific, and understandable without any need for external information. It states the goal (setup service account with kubeconfig) and the scope (for RAN.AI)."
    },
    {
      "question": "How do you manually execute the Cube Indicators pipeline from the terminal?",
      "answer": "1. SSH into `un1.bigdata.abc.gr` as `intra` and go to:\n```bash\ncd projects/cube_ind\n```\n2. Fetch latest script:\n```bash\nkinit -kt /home/intra/intra.keytab intra\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\n```\n3. Edit and run `run_cube.sh` with the correct date:\n```bash\nvim run_cube.sh\n./run_cube.sh\n```",
      "category": "Troubleshooting & Issue Resolution",
      "context": "### Execute Cube Indicators via Terminal\\n\\n1. Connect to `un1.bigdata.abc.gr` and change user to `intra` using sudo.\\n\\n2. Change directory \\n```\\ncd projects/cube_ind\\n```\\n\\n3. Remove old pyspark script\\n```\\nrm Indicators_Spark_Job.py\\n```\\n\\n4. Get new one from HDFS\\n```\\nkinit -kt /home/intra/intra.keytab intra\\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\\n```\\n\\n6. Edit submit script to change execution date. Execution date equals the missing date + 2. E.g data for 20210101 will be loaded on 20210103.\\n```\\nvim run_cube.sh\\n...verhead=4096 Indicators_Spark_Job.py <date> brond.dsl_s...\\n```\\n\\n7. Run the submit script \\n```\\n./run_cube.sh\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context provides step-by-step instructions for manually executing the Cube Indicators pipeline, matching the answer exactly. All commands and actions are covered.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Running the Cube Indicators pipeline is a core BigStreamer task, relevant for engineers ensuring data pipelines are processed correctly. It directly supports real-world troubleshooting and maintenance.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is specific and clear on its own. It mentions the exact task (manually executing the Cube Indicators pipeline) and the interface (terminal), making it self-contained."
    },
    {
      "question": "What should you check if the `Coord_Cube_Spark_Indicators` job fails?",
      "answer": "Check dependencies:\n- `brond.fixed_radio_matches_unq_inp` via `101_fixed_radius.sh`\n- `radius.radacct_hist` via `radius.pl`\n- `brond.brond_retrains_hist` via `brond_retrains.pl`\nEnsure all source tables are populated before triggering `Coord_Cube_Spark_Indicators`.",
      "category": "Troubleshooting & Issue Resolution",
      "context": "### Cube Indicators Pipeline\\n\\n* Coordinator `1011_Fixed_brond_customers`  populates table `brond.fixed_brond_customers_daily`\\n* Coordinator `Coord_post_BROND_FIXED_CUSTOMERS` populates tables `brond.fixed_brond_customers_daily_unq` & `brond.fixed_customers_brond_latest`\\n* Coordinator `Coord_Cube_Spark_Indicators` for `par_date=date -2 days` populates table `brond.cube_indicators`  and its dependencies are:\\n  * `brond.fixed_radio_matches_unq_inp` populated by script `un2:/shared/abc/brond/bin/101_fixed_radius.sh`\\n  * `brond.fixed_brond_customers_daily_unq`\\n  * `radius.radacct_hist` populated by script `un2:/shared/abc/radius/DataParser/scripts/radius.pl`\\n  * `brond.brond_retrains_hist` populated by script `un2:/shared/abc/brond/DataParser/scripts/brond_retrains.pl`\\n  * `brond.dsl_stats_week_xdsl_hist` populated by Coordinator `coord_brond_load_dsl_daily_stats`",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully answerable using the provided context. The dependencies of the `Coord_Cube_Spark_Indicators` job are clearly listed, along with the scripts responsible for populating them.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This is a highly relevant question for telecom engineers working with BigStreamer, especially those monitoring pipeline execution and debugging job failures.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear and specific. It refers to a well-defined job and asks what checks are necessary if it fails, requiring no prior context to be understood."
    },
    {
      "question": "How can you diagnose and prevent duplicate job submissions in CDSW for scheduled workflows like 'Set_Point_Automation'?",
      "answer": "1. Log in to CDSW Admin panel and inspect the usage and job history.\n2. SSH into `mncdsw1.bigdata.abc.gr` and run:\n```bash\nkubectl get pods\nkubectl logs <cron-pod-name> | grep job=<jobId>\n```\n3. Verify if the CDSW scheduler is submitting the same job twice.\n4. Confirm no duplicated entries in the scheduler DB (access via `psql` inside the db pod) and ensure cron configurations do not overlap.\n5. Adjust the scheduling configuration in the `crons` table if needed.",
      "category": "Application Functionality & Flow",
      "context": "# abc - BigStreamer - IM1900072 - Execution schedule job\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\nIt was observed that there is a problem with the jobs in cloudera data science, specifically the Energy Bills project and the Set_Point_Automation job at the time and date that it is scheduled to run, it was observed that it starts and runs 2 times at the same time.\\nThis is easy to understand from the job history and the cases are the ones I have attached with a screenshot.\\nThank you and for any clarification I am at your disposal\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to CDSW on https://mncdsw1.bigdata.abc.gr with personal account.\\n2. From the Admin tab select usage.\\n3. Confirm duplicate submitted jobs.\\n4. SSH to `mncdsw1.bigdata.abc.gr` with personal account and change to `root` with sudo\\n5. Find the scheduler pod\\n``` bash\\nkubectl get pods\\nNAME                                          READY   STATUS             RESTARTS   AGE\\narchiver-7c6656cf45-nklb2                     1/1     Running            0          159d\\ncdsw-compute-pod-evaluator-849b98f9fd-rvg58   1/1     Running            0          159d\\ncron-7d5f5656c7-ws77j                         1/1     Running            0          159d                 <---- This looks good\\ndb-84f767b84c-tnr2j                           1/1     Running            0          159d\\ndb-migrate-f260849-x6f9z                      0/1     Completed          0          159d\\nds-cdh-client-6bd7476b5b-r268f                1/1     Running            0          159d\\nds-operator-576c7459d6-wr4hc                  1/1     Running            1          159d\\nds-reconciler-5cd476896d-8lnn7                1/1     Running            1          159d\\nds-vfs-7f6578594b-dgnrx                       1/1     Running            0          159d\\nfeature-flags-54f7f97948-zd4gw                1/1     Running            0          159d\\ngrafana-cml-dashboards-f260849-4v7k4          0/1     Completed          0          159d\\ngrafana-core-cd44d8dff-b2hhg                  1/1     Running            0          159d\\nimage-puller-5cxg2                            1/1     Running            1          159d\\nimage-puller-5khss                            1/1     Running            31         159d\\nimage-puller-cgbls                            1/1     Running            35         159d\\nimage-puller-f8876                            1/1     Running            34         159d\\nimage-puller-vjkcp                            1/1     Running            40         159d\\nimage-puller-vs6b7                            1/1     Running            38         159d\\nimage-puller-w8wc2                            1/1     Running            1          159d\\ningress-controller-78fc7d87b8-jntf8           1/1     Running            0          159d\\nkube-state-metrics-656687dd48-zh66p           1/1     Running            0          159d\\nlivelog-85fb8d8974-fnwkr                      1/1     Running            0          159d\\nlivelog-cleaner-1656288000-gjfjw              0/1     Completed          0          2d11h\\nlivelog-cleaner-1656374400-5pqd7              0/1     Completed          0          35h\\nlivelog-cleaner-1656460800-vnjfw              0/1     Completed          0          11h\\nlivelog-publisher-5rhbv                       1/1     Running            39         159d\\nlivelog-publisher-f68qs                       1/1     Running            37         159d\\nlivelog-publisher-j9p22                       1/1     Running            1          159d\\nlivelog-publisher-rp4pp                       1/1     Running            39         159d\\nlivelog-publisher-rv6h5                       1/1     Running            3          159d\\nlivelog-publisher-wb6cn                       1/1     Running            43         159d\\nlivelog-publisher-xc2wm                       1/1     Running            40         159d\\nmodel-proxy-69867f6ff6-ljcdv                  1/1     Running            1          159d\\nprometheus-core-686874bbbc-nzn9p              0/1     CrashLoopBackOff   35926      159d                 <---- This looks not good\\nprometheus-node-exporter-d6n5v                1/1     Running            0          159d\\nprometheus-node-exporter-flhq7                1/1     Running            23         159d\\nprometheus-node-exporter-gxh2h                1/1     Running            0          159d\\nprometheus-node-exporter-kvvjv                1/1     Running            24         159d\\nprometheus-node-exporter-n47w5                1/1     Running            23         159d\\nprometheus-node-exporter-sxtxp                1/1     Running            23         159d\\nprometheus-node-exporter-wb4lf                1/1     Running            23         159d\\nruntime-repo-puller-74f488b875-dj8f8          1/1     Running            0          159d\\ns2i-builder-775cc65845-28k88                  1/1     Running            0          159d\\ns2i-builder-775cc65845-qww92                  1/1     Running            0          159d\\ns2i-builder-775cc65845-t8rp6                  1/1     Running            0          159d\\ns2i-client-7979d87646-skh8m                   1/1     Running            0          159d\\ns2i-git-server-5b6c4c4df9-8jczc               1/1     Running            0          159d\\ns2i-queue-65cc5dd86b-6sckk                    1/1     Running            0          159d\\ns2i-registry-75565bc6d4-zls79                 1/1     Running            0          159d\\ns2i-registry-auth-58c4b8ddb-lgbf5             1/1     Running            0          159d\\ns2i-server-6549bc9f86-zbxl9                   1/1     Running            1          159d\\nsecret-generator-76994558c6-fl8sn             1/1     Running            0          159d\\nspark-port-forwarder-29gfq                    1/1     Running            0          159d\\nspark-port-forwarder-5w9hr                    1/1     Running            0          159d\\nspark-port-forwarder-jss7r                    1/1     Running            23         159d\\nspark-port-forwarder-kpkrh                    1/1     Running            23         159d\\nspark-port-forwarder-r2lrj                    1/1     Running            23         159d\\nspark-port-forwarder-tm757                    1/1     Running            23         159d\\nspark-port-forwarder-zkb2h                    1/1     Running            24         159d\\ntcp-ingress-controller-647b484f4c-fl6tr       1/1     Running            1          159d\\nusage-reporter-d46bcdb59-cswll                1/1     Running            0          159d\\nweb-6c75f94ff4-k2z7m                          1/1     Running            8          159d\\nweb-6c75f94ff4-vfb2h                          1/1     Running            9          159d\\nweb-6c75f94ff4-vl4p4                          1/1     Running            8          159d\\n```\\n6. Confirm that jobs were submitted by the CDSW scheduler\\n``` bash\\nkubectl logs cron-7d5f5656c7-ws77j | grep job=624 \\n# Job ID can be obtained from the URL when inspecting the job from the Web GUI\\n\\n2022-06-29 06:00:00.001\t1\tINFO   \tCron                          \tStart  submitting cron job\tdata = {\\\"jobId\\\":624,\\\"spec\\\":\\\"0 0 9 * * *\\\",\\\"timezone\\\":\\\"Europe/Athens\\\",\\\"url\\\":\\\"http://web.default.svc.cluster.local/api/v1/tasks/start-job?job=624",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context contains clear and detailed steps related to diagnosing duplicate job submissions in CDSW, including logging into pods, checking logs, and inspecting the scheduler database. The question is fully answerable based on this information.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This is highly relevant to telecom engineers working with BigStreamer, particularly those dealing with automation pipelines in CDSW. Identifying and resolving duplicate job submissions is a critical operational task.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is fully self-contained and understandable. It clearly defines the context of 'duplicate job submissions' and specifies the environment ('CDSW' and the job name), requiring no additional information to interpret."
    },
    {
      "question": "Why does the CDSW job 'pollaploi' from the Energy Bills project fail frequently, and how should it be configured?",
      "answer": "The job uses excessive cluster resources: 100 Spark executors × 4GB = 400GB RAM and 200 vcores. Since the cluster has 1TB RAM, this can cause OOM errors. Reduce executors or apply dynamic allocation more efficiently:\n```python\n.config(\"spark.executor.instances\", \"20\")\n.config(\"spark.executor.memory\", \"2g\")\n```",
      "category": "Application Functionality & Flow",
      "context": "# abc - BigStreamer - IM2172470 - abc (492) BigStreamer - CDH - energy_efficiency.pollaploi\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\nwe have a problem with the pollaploi job located in the Energy Bills project in impala\\nthe frequency of it failing has increased quite a bit\\n```\\n\\n<b>Actions Taken:</b>\\n\\nAfter communication with customer we undestand that the issue occurs for job at workbench and not for flow. So:\\n\\n1. Login to Cloudera Data Science Workbench with your personal account (https://mncdsw1.bigdata.abc.gr/)\\n\\n2. Click on the left **Sessions** tab and then on **Scope** select **All Projects** and click on **Energy Bills** Project and find **Pollaploi** job.\\n\\n\\n3. Go on **History** tab and you will see that there are a lot of pollaploi jobs with status Failure\\n\\n4. Click on one job with status Failure and then go to **See job details** and then click on **Script: Energy_Bills_Automation/Energy_Bills_Automation.py**\\n\\n5. When investigated the script we saw below snippet of spark configuration:\\n\\n```bash\\nspark = SparkSession.builder\\\\\\n.master(\\\"yarn\\\")\\\\\\n.config(\\\"spark.submit.deployMode\\\", \\\"client\\\")\\\\\\n.config(\\\"spark.eventLog.enabled\\\", \\\"true\\\")\\\\\\n.config(\\\"spark.executor.instances\\\", \\\"100\\\")\\\\\\n.config(\\\"spark.executor.cores\\\", \\\"2\\\")\\\\\\n.config(\\\"spark.executor.memory\\\", \\\"4g\\\")\\\\\\n.config(\\\"spark.rpc.message.maxSize\\\", \\\"1024\\\")\\\\\\n.config(\\\"spark.executor.memoryOverhead\\\", \\\"800\\\")\\\\\\n.config(\\\"spark.driver.memory\\\", \\\"4g\\\")\\\\\\n.config(\\\"spark.driver.memoryOverhead\\\", \\\"800\\\")\\\\\\n.config(\\\"spark.spark.driver.maxResultSize\\\", \\\"4g\\\")\\\\\\n.config(\\\"spark.executor.dynamicAllocation.initialExecutors\\\", \\\"4\\\")\\\\\\n.config(\\\"spark.executor.dynamicAllocation.minExecutors\\\", \\\"4\\\")\\\\\\n.config(\\\"spark.executor.dynamicAllocation.maxExecutors\\\", \\\"4\\\")\\\\\\n.config(\\\"spark.sql.broadcastTimeout\\\", \\\"1000\\\")\\\\\\n.config(\\\"spark.kryoserializer.buffer.max\\\", \\\"1024m\\\")\\\\\\n.getOrCreate()\\n```\\n\\nSo, there are 100 instances * 2 cores = 200 vcores\\n\\nand 100 instances * 4G ram = 400GB ram\\n\\nThe cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.\\n\\n<b>Our Ticket Response:</b>\\n\\n```bash\\nGood evening,\\n\\nUpon investigation we noticed that the job you mentioned fails with an out-of-memory error.\\n\\nAdditionally, according to the spark configuration snippet below in your job:\\n\\n```\\n.master(\\\"yarn\\\")\\\\\\n.config(\\\"spark.submit.deployMode\\\", \\\"client\\\")\\\\\\n.config(\\\"spark.eventLog.enabled\\\", \\\"true\\\")\\\\\\n.config(\\\"spark.executor.instances\\\", \\\"100\\\")\\\\\\n.config(\\\"spark.executor.cores\\\", \\\"2\\\")\\\\\\n.config(\\\"spark.executor.memory\\\", \\\"4g\\\")\\\\\\n```\\n\\nWe see that you have given 100 instances * 2 cores = 200 vcores and 100 instances * 4Gram = 400GB ram\\n\\nThe cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.\\n\\nTherefore, the problem concerns the specific job. For this reason, jobs should be optimized according to the guidance given in an earlier communication for a similar issue, taking into account the configuration that has already been done in the cluster.\\n\\nIf you don't need anything else please if we can proceed to close the ticket.\\n\\nThanks\\n```\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer CDSW",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context clearly explains the root cause of the job failures (OOM due to excessive Spark resource allocation) and includes the relevant Spark configuration. The answer is fully grounded in the provided context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This is highly relevant to telecom engineers using BigStreamer in production, particularly those managing CDSW workflows and Spark jobs. Preventing job failure is a critical operational concern.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is fully self-contained. It names the job, the platform (CDSW), and the associated project, and clearly describes the failure behavior and the information being requested."
    },
    {
      "question": "What causes the error 'Engine exited with status 34' on jobs in CDSW and how is it resolved?",
      "answer": "The error is caused by the CNI plugin failing to assign IPs due to a stalled weave pod. Restart the Docker daemon indirectly using supervisord:\n```bash\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart <CDSW_DOCKER_ID>\n```",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# abc - IM1923742 - Job's problem\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\nit has been observed that jobs show the problem Engine exited with status 34.\\nsome of them are:\\n• Set_Point_Automation job in the Set Point Automation project (error today 22/7)\\n• Cabins Live Measurements job in the Energy Bills project (error yesterday 21/7)\\n• Flows_update_all_counters_12:00_no_par_dt job in the Monitoring Flows project (error yesterday 7/15)\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'\\n2. Go to last tab(admin).\\n3. Select `Activity` tab.\\n4. Inspect the Jobs in question.\\n\\nThe jobs are in `FAILED` status. The logs for the failed applications are missing.\\n \\n5. Troubleshoot from the command line:\\n\\nFrom `mncdsw1` as root (use personal account and then sudo):\\n\\n```bash\\nkubectl get pods -w -A # Wait a pod to fail (namespace should be like default-user-XXX)\\n\\n# After a while, a pod has failed, describe it\\n\\nkubectl describe pod -n default-user-XXX XXXXXXXX\\n```\\n\\n``` logs\\nEvents\\nWarning  FailedCreatePodSandBox  10s                    kubelet, wrkcdsw4.bigdata.abc.gr  Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \\\"...\\\" network for pod \\\"XXXXXXXX\\\": networkPlugin cni failed to set up pod \\\"XXXXXXXX_default\\\" network: unable to allocate IP address: Post http://127.0.0.1:6784/ip/....: dial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container \\\"....\\\" network for pod \\\"XXXXXXXX\\\": networkPlugin cni failed to teardown pod \\\"XXXXXXXX_default\\\" network: Delete http://127.0.0.1:6784/ip/....: dial tcp 127.0.0.1:6784: connect: connection refused]\\n```\\n\\nThis error points us to the CNI plugin\\n\\nCheck the logs for the weave pods\\n\\n``` bash\\nkubectl logs -n kube-system weave-net-XXXXX\\n# Weave pod in wrkcdsw4 has stopped logging events\\n```\\n\\nThe pod was not responding and could not be deleted.\\n\\n7. Restart the docker daemon to restart all containers on `wrkcdsw4`\\n\\n_At the time of the issue, CDSW had stale configuration that required full restart (outage) which was not desirable_\\n\\nTo avoid applying the settings, restart the service with the same configuration by triggering a restart by `supervisord` deployed as part of the Cloudera agent\\n\\n<details> ![Danger ahead](https://media3.giphy.com/media/vvzMdSygQejBIejeRO/200w.gif?cid=6c09b952aacsm9yssw6k6q0z5v8ejuy82rjpvw6qdhglcwpu&rid=200w.gif&ct=g) </details>\\n\\nFrom wrkcdsw4 as root (use personal account and then sudo):\\n\\n``` bash\\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf status | grep DOCKER\\n# Sample\\n# 145071-cdsw-CDSW_DOCKER          RUNNING   pid 39353, uptime 29 days, 0:40:20\\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart 145071-cdsw-CDSW_DOCKER\\n```\\n\\n8. Check that the node is operational after the restart\\n\\nFrom `mncdsw1` as root (use personal account and then sudo):\\n\\n```bash\\ncdsw status # You might have to wait a few minutes\\n```\\n\\n9. Inform the customer about the problem\\n\\n``` text\\nA component of CDSW on worker node 4 encountered a problem resulting in jobs running on that node not being able to start. The function of the component has been restored and the jobs are now running normally.\\n\\nPlease let us know if you need anything else or if we can proceed to close the ticket.\\n```\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer CDSW\\n\\n<b>Action Points:</b>\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context explicitly identifies the root cause (CNI plugin failure due to a non-responsive weave pod) and provides the exact resolution procedure involving a supervised Docker restart. The answer is fully grounded in this detailed troubleshooting flow.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question addresses a specific and recurring operational issue in BigStreamer’s CDSW environment that affects job execution across multiple projects. Diagnosing and resolving such failures is a critical real-world task for telecom engineers.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is fully understandable without needing the context. It names the error, the platform (CDSW), and the use case (scheduled jobs), and clearly seeks both the cause and the resolution."
    },
    {
      "question": "How do you fix RStudio Connect when LDAP authentication fails after a password reset for bind user `t1-svc-cnebind`?",
      "answer": "1. SSH into unrstudio1 and verify LDAP bind user credentials:\n```bash\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D \\\"t1-svc-cnebind\\\" -W -b \\\"dc=groupnet,dc=gr\\\"\n```\n2. If it fails, update the password in:\n```bash\nvi /etc/rstudio-connect/rstudio-connect.gcfg\n```\n3. Restart the service:\n```bash\nsystemctl restart rstudio-connect\n```",
      "category": "Troubleshooting & Issue Resolution",
      "context": "## Contents:\\n\\n<b>Description:</b>\\n\\n`RStudio user applications not working We would like to inform you that the applications of the user kmpoletis are not running showing the following error. This morning we reset the user's password because it had expired. It can connect normally with the credentials. Also let me know that the applications of the other users (Charisis, Ploskas) are running normally.`\\n\\n<b>Actions Taken:</b>\\n\\n- Ssh to **unrstudio1**\\n\\n- Do an ldap search using `t1-svc-cnebind` as the following in order to check if `t1-svc-cnebind` password is still valid:\\n\\n```\\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D \\\"t1-svc-cnebind\\\" -W -b \\\"dc=groupnet,dc=gr\\\" '(sAMAccountName=t1-svc-cnebind)'\\n```\\n\\n-  In the above case, if you get an error that you can't connect with the ldap server, that means it has an expired `t1-svc-cnebind` password, so you will need to replace it with a new one.\\n\\n- Check the configuration file below for the current ldap t1-svc-cnebind password & replace it with new **(Responsible for that abc Admin)**:\\n```\\nssh unrstudio1\\nvi /etc/rstudio-connect/rstudio-connect.gcfg\\n```\\n\\n\\n\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context fully describes the troubleshooting procedure for LDAP authentication failure due to an expired bind user password and outlines the exact steps included in the answer. The answer is thus completely grounded in the provided context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This issue affects user authentication and application availability on RStudio Connect, which is part of the BigStreamer ecosystem. Ensuring continuous access is essential, making this highly relevant to telecom engineers.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clearly phrased and provides all necessary details (issue, system, user role), making it fully understandable without needing the context."
    },
    {
      "question": "What steps do you take when CDSW refuses to start and reports errors like 'Failed to run CDSW Nodes Check'?",
      "answer": "1. Check health logs:\n```bash\nless /var/log/cdsw/cdsw_health.log\n```\n2. Investigate for control plane pod issues or ETCD probe failures.\n3. Confirm disk I/O isn't maxed out (`dm-7`) using:\n```bash\nlsblk | grep cdsw\n```\n4. Identify if Persistent Volumes are bound correctly with `kubectl get pv`.\n5. Remediate based on impalad host resource usage.",
      "category": "Infrastructure & Deployment",
      "context": "# abc - BigStreamer - IM2073052 - Application not working\\n\\n<b>Description:</b>\\n\\n```bash\\nGood morning,\\n\\nIn CDSW we get error \\\"Unexpected Error. An unexpected error occurred\\\" when connecting. We saw that the node mncdsw1.bigdata.abc.gr was down. We did a restrart, just that, and it now appears to be in good health status.\\n\\nHowever, we still get the same error.\\n\\nIn CDSW status it has the following message\\n\\nFailed to run CDSW Nodes Check. * Failed to run CDSW system pods check. * Failed to run CDSW application pods check. * Failed to run CDSW services check. * Failed to run CDSW secrets check. * Failed to run CDSW persistent volumes check. * Failed to run...\\n\\nPlease for your checks.\\n\\nThanks\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Restart CDSW\\n\\n   The customer had already restarted CDSW, so we tried it once more in order to live monitor it.\\n\\n   ```bash\\n   Cloudera Manager -> CDSW -> Restart\\n   ```\\n\\n2. Check status\\n\\n   We followed the logs until CDSW was available again.\\n\\n   ```bash\\n   #from mncdsw1\\n   cdsw status\\n   ...\\n   Cloudera Data Science Workbench is ready!\\n   ```\\n\\n   Since CDSW was up and running, we continued with root cause analysis.\\n\\n3. Check logs\\n\\n   ```bash\\n   less /var/log/cdsw/cdsw_health.log\\n   ```\\n\\n   Firstly, we noticed an abnormal behavior with some of the control plane pods:\\n\\n   ```bash\\n   2023-01-29 05:50:53,868 ERROR cdsw.status:Pods not ready in cluster kube-system ['component/kube-controller-manager', 'component/kube-scheduler'].\\n   ```\\n\\n   And after that, CDSW lost connection with apiserver pod completely:\\n\\n   ```bash\\n   2023-01-29 05:51:42,392 WARNING urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549bb50>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\\n   2023-01-29 05:51:42,735 WARNING urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b710>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\\n   2023-01-29 05:51:43,065 WARNING urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b050>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\\n   2023-01-29 05:51:43,371 ERROR cdsw.status:Failed to run CDSW Nodes Check.\\n   ```\\n\\n4. Check node resources\\n\\n   From Cloudera Manager we saw that CPU and Memory were not increased but Disk I/O reached 100%.\\n\\n   ![IM2073052_diskio](.media/IM2073052_diskio.png)\\n\\n   From the image above we noticed that the issue occured on dm-7.\\n\\n   ```bash\\n   [root@mncdsw1 ~]# ll /dev/mapper/cdsw-var_lib_cdsw\\n   lrwxrwxrwx 1 root root 7 Dec 16  2021 /dev/mapper/cdsw-var_lib_cdsw -> ../dm-7\\n   ```\\n\\n   ```bash\\n   [root@mncdsw1 ~]# lsblk | grep cdsw-var\\n   └─cdsw-var_lib_cdsw                                                                         253:7    0   931G  0 lvm  /var/lib/cdsw\\n   ```\\n\\n   ```bash\\n   [root@mncdsw1 ~]# kubectl get pv\\n   NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS                 REASON   AGE\\n   0c9df8bb         1Ti        RWX            Retain           Bound    default-user-120/b128af5f   cdsw-storageclass-whiteout            83m\\n   1214923b         1Ti        RWX            Retain           Bound    default-user-98/1ec1e99a    cdsw-storageclass-whiteout            11m\\n   1297834a         1Ti        RWX            Retain           Bound    default-user-9/740094c3     cdsw-storageclass-whiteout            54s\\n   1a2f7a8a         1Ti        RWX            Retain           Bound    default-user-9/92acb87f     cdsw-storageclass-whiteout            55s\\n   1f498fe8         1Ti        RWX            Retain           Bound    default-user-120/588500de   cdsw-storageclass-whiteout            106s \\n   ```\\n\\n   ```bash\\n   [root@mncdsw1 ~]# kubectl get pv 0c9df8bb -o yaml\\n\\n   apiVersion: v1\\n   kind: PersistentVolume\\n   metadata:\\n     name: 0c9df8bb\\n   spec:\\n     accessModes:\\n     - ReadWriteMany\\n     capacity:\\n       storage: 1Ti\\n     mountOptions:\\n     - nfsvers=4.1\\n     nfs:\\n       path: /var/lib/cdsw/current/projects/cdn/4xsyzsv0lnij00ob\\n       server: 10.255.241.130\\n   persistentVolumeReclaimPolicy: Retain\\n     storageClassName: cdsw-storageclass-whiteout\\n     volumeMode: Filesystem \\n   ```\\n\\n   It seems that every CDSW project uses mncdsw1:/var/lib/cdsw for storage.\\n\\n5. Check kubelet logs\\n\\n   ```bash\\n   ll /run/cloudera-scm-agent/process/ | grep -i master\\n   ```\\n\\n   ```bash\\n   [root@mncdsw1 ~]# ll /run/cloudera-scm-agent/process/145081-cdsw-CDSW_MASTER/logs/\\n   total 111880\\n   -rw-r--r-- 1 root root  9658036 Jan 30 10:24 stderr.log\\n   -rw-r--r-- 1 root root 10485841 Jan 30 05:42 stderr.log.1\\n   -rw-r--r-- 1 root root 10485989 Jan  4 19:40 stderr.log.10\\n   -rw-r--r-- 1 root root 10485928 Jan 30 00:20 stderr.log.2\\n   -rw-r--r-- 1 root root 10486166 Jan 29 18:58 stderr.log.3\\n   -rw-r--r-- 1 root root 10485841 Jan 29 13:36 stderr.log.4\\n   -rw-r--r-- 1 root root 10485790 Jan 29 08:06 stderr.log.5\\n   -rw-r--r-- 1 root root 10485858 Jan 25 16:41 stderr.log.6\\n   -rw-r--r-- 1 root root 10485835 Jan 21 08:56 stderr.log.7\\n   -rw-r--r-- 1 root root 10485760 Jan 15 14:47 stderr.log.8\\n   -rw-r--r-- 1 root root 10485805 Jan 10 11:57 stderr.log.9\\n   -rw-r--r-- 1 root root    12055 Nov 21 14:58 stdout.log\\n   ```\\n\\n   In stderr.log.5 file there were many log entries indicating a problem with etcd.\\n\\n   ```bash\\n   I0129 05:50:11.022246   89953 prober.go:117] Liveness probe for \\\"etcd-mncdsw1.bigdata.abc.gr_kube-system(ef618d8c591c98ed7bd7d66b177d34f7):etcd\\\" failed (failure): HTTP probe failed with statuscode: 503\\n   ```\\n\\n   ```bash\\n   E0129 05:51:22.881553   89953 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\\\"\\\", APIVersion:\\\"",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context explicitly describes the issue ('Failed to run CDSW Nodes Check') and all the troubleshooting steps provided in the answer, making the answer fully grounded.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The question addresses a high-impact system-level failure in CDSW, which is integral to BigStreamer environments. It's highly relevant to telecom engineers responsible for system availability and deployment.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is precise, clearly describes the scenario (CDSW failing to start with a specific error), and can be understood independently of the context."
    },
    {
      "question": "How do you recover a Cloudera host like `mn3` that lost contact with Cloudera Manager due to CPU errors?",
      "answer": "1. Access iDRAC (e.g., https://10.255.242.85/), check server logs for hardware faults.\n2. Cold reboot the server via iDRAC.\n3. SSH into the node and verify agent:\n```bash\nsystemctl status cloudera-scm-agent\n```\n4. Confirm host is healthy again from Cloudera Manager.",
      "category": "Infrastructure & Deployment",
      "context": "# abc - BigStreamer - IM2104114 -  mn3 bad health\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\n\\nThe mn3 node is in bad health with the message This host has been out of contact with the Cloudera Manager Server for too long. This host is not in contact with the Host Monitor.\\n\\nPlease for your checks.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. From `admin` tried to reach mn3 but with no response:\\n\\n```bash\\nping mn3\\n```\\n\\n2. Since there is no response from `mn3` time to reboot server.\\n\\nLogin to `admin` from ad hoc connection and then type from command line `firefox`.\\n\\nSelect `default profile` and go to idrac for mn3 `https://10.255.242.85/` with the credentials you can find [here](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/prodsyspasswd.kdbx).\\n\\n3. From left Column click on `Server` and then go to `Logs`.\\n\\nAt that moment we see the following error for CPU:\\n```bash\\nMon Mar 13 2023 08:44:27 CPU 1 machine check error detected.\\nMon Mar 13 2023 08:44:27 CPU 2 machine check error detected.\\nMon Mar 13 2023 08:44:27 CPU 1 machine check error detected.\\n```\\n\\n4. Time to reboot server:\\n\\nFrom `https://10.255.242.85/` go to Server and then click on `Power Cycle System (cold boot)`\\n\\n5. In order to check that server is up and running open java console, otherwise check logs from left Column click on `Server` and then go to `Logs`\\n\\n6. Login to Cloudera Manager with your personal account and then to `Hosts` -> `All hosts` and search for mn3.\\n\\nVerify that it is back again to cluster \\n\\n7. Login to `admin` and then ssh to `mn3` in order to verify that cloudera agent is up and running.\\n\\n```bash\\nsystemctl status cloudera-scm-agent status\\n```\\n\\n<b>Investigation:</b>\\n\\n1. Investigated `/var/log/messages` and we saw bellow info:\\n\\n```bash\\nMar 13 09:10:47 mn3.bigdata.abc.gr systemd-logind[1628]: New session 6590707 of user nagios.\\nMar 13 09:10:47 mn3.bigdata.abc.gr systemd[1]: Started Session 6590707 of user nagios.\\nMar 13 09:10:47 mn3.bigdata.abc.gr systemd-logind[1628]: Removed session 6590707.\\nMar 13 09:10:47 mn3.bigdata.abc.gr systemd[1]: Removed slice User Slice of nagios.\\nMar 13 10:52:20 mn3.bigdata.abc.gr rsyslogd:  [origin software=\\\"rsyslogd\\\" swVersion=\\\"8.24.0-57.el7_9\\\" x-pid=\\\"1848\\\" x-info=\\\"http://www.rsyslog.com\\\"] start\\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] microcode: microcode updated early to revision 0x44, date = 2020-05-27\\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Initializing cgroup subsys cpuset\\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Initializing cgroup subsys cpu\\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Initializing cgroup subsys cpuacct\\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Linux version 3.10.0-1160.15.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) ) #1 SMP Wed Feb 3 15:06:38 UTC 2021\\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Command line: BOOT_IMAGE=/vmlinuz-3.10.0-1160.15.2.el7.x86_64 root=/dev/mapper/vg00-root ro crashkernel=auto rd.lvm.lv=vg00/root rd.lvm.lv=vg00/swap rhgb quiet transparent_hugepage=never ipv6.disable=1 LANG=en_US.UTF-8\\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] e820: BIOS-provided physical RAM map:\\n```\\n\\nAs we see at `09:10:47` mn3 lost contact from Cloudera Manager\\n\\n2. Investigated below logs between time that problem occurred but we didn't find any useful information\\n\\n```bash\\njournalctl -S \\\"2023-03-13 08:00:00\\\" -U \\\"2023-03-13 10:50:00\\\"\\nless /var/log/dmesg\\nless /var/log/cloudera-scm-agent/cloudera-scm-agent.log\\n```\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer Master/Management Services\\n\\n\\n<b>Our Ticket Response:</b>\\n\\n\\n```bash\\nGood morning,\\n\\nAfter investigation we notice that there was a problem with the CPU due to some process and the node had to be rebooted in order to recover.\\n\\nWe will monitor the node and in case the problem appears again we will proceed with a ticket to Dell.\\n\\nIf you don't need anything else please let us know if we can proceed to close the ticket.\\n\\nThank you\\n```\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The steps to recover the node from CPU errors and loss of contact with Cloudera Manager are directly detailed in the context. The answer mirrors these steps accurately.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Recovering a critical node like `mn3` is highly relevant to system stability and operations in BigStreamer environments. This task is essential for telecom engineers handling platform reliability.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear, provides necessary details (node name, error type, and Cloudera Manager), and can be fully understood without referencing the context."
    },
    {
      "question": "How can you confirm that Fraport branch probes are properly loaded into `bigcust.nnmcp_ipvpn_slametrics_hist`?",
      "answer": "1. Connect with Impala:\n```bash\nimpala-shell -i un-vip.bigdata.abc.gr -k --ssl\n```\n2. Run:\n```sql\nSELECT DISTINCT qa_probe_name FROM bigcust.nnmcp_ipvpn_slametrics_hist WHERE par_dt='20230202' AND customer='fraport';\n```\n3. If names are present, verify device types:\n```sql\n... AND device_type IN ('ce', 'cpe', 'nte', ...);\n```",
      "category": "Data Management & Query Execution",
      "context": "# abc - IM2076207 - Πρόβλημα registration μετρήσεων Fraport\\n\\n<b>Description:</b>\\n\\n```\\nGood evening,\\n\\nWe've noticed that we're not getting metrics at the following new points in the Fraport client (there don't seem to be any records at all)\\n\\nFrap-PVK1\\nFrap-KVA1\\nFrap-SKG1\\nFrap-JMK1\\nFrap-RHO1\\n\\nFrap-PVK2\\nFrap-KVA2\\nFrap-SKG2\\nFrap-JMK2\\nFrap-RHO2\\n\\nFrom a check made through the saa-csr5 probe, it appears that the router takes measurements from the specific points.\\n\\nIndicative:\\n\\nsaa-csr5#sh ip sla configuration 4891 | i addre\\nTarget address/Source address: 80.106.132.34/212.205.74.72\\n\\nsaa-csr5#sh ip sla statistics 4891\\nIPSLAs Latest Operation Statistics\\nIPSLA operation id: 4891\\n        Latest RTT: 8 milliseconds\\nLatest operation start time: 12:56:10 EET Thu Feb 2 2023\\nLatest operation return code: OK\\nNumber of successes: 302\\nNumber of failures: 0\\nOperation time to live: Forever\\n\\nFanis also sees that NNM takes the measurements but they are not sent to Bigstreamer.\\n\\nPlease for your checks.\\n\\nThanks\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. As seen from [here](../supportDocuments/applicationFlows/ip_vpn.md) branch metrics are exported by querying `bigcust.nnmcp_ipvpn_slametrics_hist`. We need to investigate if this table contains metrics for the branches mentioned in the description.\\n\\n    From `un2.bigdata.abc.gr` with root (cyberark login):\\n\\n    ``` bash\\n    su - ipvpn\\n    impala-shell -i un-vip.bigdata.abc.gr -k --ssl\\n    ```\\n\\n    ``` sql\\n    select distinct qa_probe_name from bigcust.nnmcp_ipvpn_slametrics_hist where par_dt='20230202' and customer = 'fraport';\\n    ```\\n\\n    |                     qa_probe_name                      |\\n    | :----------------------------------------------------: |\\n    |                     _Other probes_                     |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-RHO1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-RHO2_64_ce   |\\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-fgh1_64_ce |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-HQs1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JSI2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KGS2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CHQ1_64_ce   |\\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-fgh2_64_ce |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-HQs2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-EFL2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CFU1_64_ce   |\\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-LH1_64_ce  |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-EFL1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KVA1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JMK2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CFU2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JSI1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JTR1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JTR2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JMK1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-MJT1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-PVK2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CHQ2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SMI1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-PVK1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KGS1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-MJT2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SKG1_64_ce   |\\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-LH2_64_ce  |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KVA2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SKG2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-ZTH1_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SMI2_64_ce   |\\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-ZTH2_64_ce   |\\n    |                     _Other probes_                     |\\n\\n2. These probes were checked in the CSVs created by `custompoller`\\n\\n    From `un2.bigdata.abc.gr` with root (cyberark login):\\n\\n    ``` bash\\n    su - ipvpn\\n    ssh custompoller@nnmdis01\\n    grep fraport_Frap-ZTH2_64_ce ipvpn/out/*.LOADED | head -10\\n    # Here we see that the probe name is the same as the one we see in bigcust.nnmcp_ipvpn_slametrics_hist\\n    ```\\n\\n3. After reviewing the [business documentation](https://metis.xyztel.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/tree/master/docs) we found that valid device types for export are `'ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes'` and that the `device_type` field is derived from the probe name.\\n\\n    We checked the probe names with valid device type.\\n\\n    From `un2.bigdata.abc.gr` with root (cyberark login):\\n\\n    ``` bash\\n    su - ipvpn\\n    impala-shell -i un-vip.bigdata.abc.gr -k --ssl\\n    ```\\n\\n    ``` sql\\n   select distinct qa_probe_name from bigcust.nnmcp_ipvpn_slametrics_hist where par_dt='20230202' and customer = 'fraport' and  device_type  IN ('ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes');\\n    ```\\n\\n    |                             qa_probe_name                             |\\n    | :-------------------------------------------------------------------: |\\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-HDQ-01_cpe_fraport      |\\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-KGS-01_cpe_fraport      |\\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-KGS-02_cpe_fraport      |\\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-RHO-02_cpe_fraport      |\\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-SMI-01_cpe_fraport      |\\n    |        avail_saa-csr1_ip-sla-probe_fraport_Frap12_cpe_fraport         |\\n    |        avail_saa-csr1_ip-sla-probe_fraport_Frap13_cpe_fraport         |\\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-CFU-01_cpe_fraport      |\\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-CFU-02_cpe_fraport      |\\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-JMK-01_cpe_fraport      |\\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-JSI-01_cpe_fraport      |\\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-SKG-01_cpe_fraport      |\\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-SKG-02_cpe_fraport      |\\n    |        avail_saa-csr2_ip-sla-probe_fraport_Frap15_cpe_fraport         |\\n    |      avail_saa-csr3_ip-sla-probe_fraport_FG4G-CHQ-02_cpe_fraport      |\\n    |      avail_saa-csr3_ip-sla-probe_fraport_FG4G-EFL-01_cpe_fraport      |\\n    |      avail_saa-csr3_ip-sla-probe_fraport_FG4G-RHO-01_cpe_fraport      |\\n    |      avail_saa-csr4_ip-sla-probe_fraport_FG4G-CHQ-01_cpe_fraport      |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap01_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap01_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap02_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap02_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap03_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap03_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap04_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap04_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap05_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap05_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap06_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap06_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap07_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap07_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap09_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap09_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap10_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap10_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap11_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap11_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap12_ce_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap13_ce_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap14_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap14_cpe_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap15_ce_fraport         |\\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap16_ce_fraport         |\\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap16_cpe_fraport         |\\n    |    avail_saa-csr5_ip-sla-probe_fraport_frap-21p2000380_ce_fraport     |\\n    | avail_saa-csr5_ip-sla-probe_fraport_fraport-DIA-21N1003241_ce_fraport |\\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap11-backup_ce_fraport     |\\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap12-backup_ce_fraport     |\\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap13-backup_ce_fraport     |\\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap14-backup_ce_fraport     |\\n    |    rttd-pl_saa-csr2_ip-sla-probe_fraport_Frap15-backup_ce_fraport     |\\n    |    rttd-pl_saa-csr3_ip-sla-probe_fraport_Frap06-backup_ce_fraport     |\\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap01-backup_ce_fraport     |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap01_ce_fraport        |\\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap02-backup_ce_fraport     |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap02_ce_fraport        |\\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap03-backup_ce_fraport     |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap03_ce_fraport        |\\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap04-backup_ce_fraport     |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap04_ce_fraport        |\\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap05-backup_ce_fraport     |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap05_ce_fraport        |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap06_ce_fraport        |\\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap07-backup_ce_fraport     |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap07_ce_fraport        |\\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap09-backup_ce_fraport     |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap09_ce_fraport        |\\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap10-backup_ce_fraport     |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap10_ce_fraport        |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap11_ce_fraport        |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap12_ce_fraport        |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap13_ce_fraport        |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap14_ce_fraport        |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap15_ce_fraport        |\\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap16-backup_ce_fraport     |\\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap16_ce_fraport        |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap01_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap02_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap03_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap04_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap05_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap06_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap07_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap09_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap10_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap11_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap12_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap13_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap14_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap15_ce_fraport       |\\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap16_ce_fraport       |\\n\\n4. By comparing the probe from the two queries, we see that the device type for the new devices is `64` and therefore it is omitted from the CSVs. The probe name is configured by the customer on their end\\n\\n5. Inform the customer about the problem\\n\\n``` text\\nGood evening,\\n\\nWe see that for the new points you mention there are SLA metrics in the respective tables. These points are excluded when generating the CSV as the probe name does not match what is expected. Specifically:\\n\\nThe delimeter in the probe name is the \\\"_\\\" character, the device type is extracted from the probe name, and the measurements included in the CSVs are for the device types 'ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes'.\\n\\nΑυτό το probe είναι valid:\\navail_saa-csr1_ip-sla-probe_fraport_Frap12_cpe_fraport\\nWhile the following belonging to one of the points you mentioned is not:\\navail_saa-csr5_ip-sla-probe_fraport_Frap-PVK1_64_ce\\nAs you can see in the device type position it has the value 64 and is therefore excluded in the generated CSVs.\\n\\nPlease correct the probe names. If there is nothing else please let us know so we can close the ticket.\\n```\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer CDSW\\n\\n<b>Action Points:</b>",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully answerable using only the context, which contains detailed Impala queries targeting the `bigcust.nnmcp_ipvpn_slametrics_hist` table, along with clear explanations about device types and probe name formats.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The question is highly relevant, as confirming the correct ingestion of network SLA metrics into BigStreamer is a core operational task for telecom engineers managing customer data pipelines.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear, unambiguous, and includes all the necessary information for understanding, including the specific table and probe types it refers to."
    },
    {
      "question": "What caused the Location Mobility flow to fail on 2023-04-19 and how was it resolved?",
      "answer": "1. Initial failure was due to unreachable impalad: `sn102.bigdata.abc.gr:22000`, caused by a memory-hogging query.\n2. Later failures related to a missing HDFS file triggered a 'Disk I/O error'.\n3. After dev team corrected the reference config table, the flow resumed. No restart needed.\nUse Impala to verify availability of partitioned files and adjust load info if required.",
      "category": "Data Management & Query Execution",
      "context": "### abc - BigStreamer - IM2131290 - Location Mobility Failure\\n\\n<b>Description:</b>\\n\\nOur team has been assigned with the following issue:\\n\\n```\\nGood morning,\\n\\nAs of yesterday noon at 15:00 we noticed that the creation of the LM_02_LTE file of the location mobility stream fails. From the HUE jobs you can see that the workflow (export_Location_Mobility_files_to_mediat...) is killed.\\n\\nWe also saw from the logs (lm_export_lte_v2_mon.cron.20230419.log) that at 15:00 when the problem starts we have the following error\\n\\nQuery submitted at: 2023-04-19 15:00:31 (Coordinator: http://sn72.bigdata.abc.gr:25000)\\nQuery progress can be monitored at: http://sn72.bigdata.abc.gr:25000/query_plan?query_id=c74df6d614d535ea:4de432ac00000000\\nERROR: Failed due to unreachable impalad(s): sn102.bigdata.abc.gr:22000\\n\\nCould not execute command: SELECT\\nachievable_thr_bytes_down_1,\\nachievable_thr_bytes_up_1,\\nachievable_thr_time_down_1,\\n\\n..................................................................\\n\\n[2023/04/19 15:19:13] - ERROR: Impala shell command for par_msisdn= failed.\\n[2023/04/19 15:19:13] - ERROR: Clean up and exit.\\n% Total % Received % Xferd Average Speed Time Time Time Current\\n\\n\\nFrom there onwards we observe errors of the form:\\n\\nQuery submitted at: 2023-04-19 17:00:31 (Coordinator: http://sn64.bigdata.abc.gr:25000)\\nQuery progress can be monitored at: http://sn64.bigdata.abc.gr:25000/query_plan?query_id=094fdeda997b8d44:5826172200000000\\nERROR: Disk I/O error on sn62.bigdata.abc.gr:22000: Failed to open HDFS file hdfs://nameservice1/ez/warehouse/npce.db/eea_hour/pardt=2023041910/ef4d08b3073e8531-d909e37b0000010\\ne_1525512597_data.0.txt\\nError(2): No such file or directory\\nRoot cause: RemfghException: File does not exist: /ez/warehouse/npce.db/eea_hour/pardt=2023041910/ef4d08b3073e8531-d909e37b0000010e_1525512597_data.0.txt\\nat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:85)\\nat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)\\nat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)\\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1909)\\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:736)\\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:415)\\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)\\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)\\nat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:869)\\n\\n\\nWe have similar errors in today's log file lm_export_lte_v2_mon.cron.20230420.log\\n\\nPlease for your checks.\\n\\nThanks\\n```\\n\\n<b>Root Cause Analysis:</b>\\n\\nFirst thing that we have checked were the comments on the [md](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/trustcenter_flows.md) that exists for this flow.\\nAfter checking the logs of the flow we saw that the flow was running successfully after it had failed for half day. The reason that it was able to run  was the retention that had already taken place the day that we received the ticket.\\n\\nWe were able to identify the issue that caused the problem. After checking the logs of the host we saw that the host didn't have enough memory at the time.\\n![sn102_memory](.media/sn102_memory.JPG) because of the user `ranai-geo` that had run an impala query \\n![query](.media/query.JPG) that took all the resources of the impala deamon on sn102.\\n![query_details](.media/q_details.JPG) \\n\\n<b>Actions Taken:</b>\\n\\nNo restart needed of the flow. There were some adjustments from dev team that took place. \\n\\nDev Resolution:\\n\\n```\\nIt was necessary to make some corrections in the configuration table of the refdata.mediation_loc_mobility_load_info flow to synchronize the data sets that will be exported.\\nAdditionally, due to the following exception a refresh table was added before the select in npce.eea_hour.\\nERROR: Disk I/O error on sn111.bigdata.abc.gr:22000: Failed to open HDFS file hdfs://nameservice1/ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\\nError(2): No such file or directory\\nRoot cause: RemfghException: File does not exist: /ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\\n```\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The answer is entirely supported by the context, which includes a detailed timeline, error messages, root cause diagnosis, and the exact steps that were taken for resolution.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This is highly relevant to engineers maintaining BigStreamer, especially those involved in troubleshooting data pipelines and understanding resource-related job failures.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is fully self-contained and clear, specifying both the system (Location Mobility flow) and the timeframe (2023-04-19) with a clear request for cause and resolution."
    },
    {
      "question": "What steps should be followed if the `RA_Dsession` or `RA_Dtraffic` files are not exported?",
      "answer": "1. Check logs under `/shared/abc/location_mobility/log/`.\n2. Run Impala queries:\n```bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh --query=\"SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201023';\"\n```\n3. If `max(par_dt)` is already past export date, files were exported. To re-export, run:\n```bash\ncd /shared/abc/location_mobility/run\n./export_ra_bs_01.sh -t 20201115\n./export_ra_bs_02.sh -t 20201115\n```",
      "category": "Application Functionality & Flow",
      "context": "# abc - BigStreamer - IM1333238 - abc BigStreamer IT files (RA Dsession / RA Dtraffic)\\n\\n<b>Description:</b>\\n\\n```\\nRA_Dsession/RA_Dtraffic are not exported\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `mtuser` with sudo\\n2. Inspect logs of *RA* flow\\n``` bash\\ncd /shared/abc/location_mobility/log/\\nless ra_export_bs_01.oozie.20201026.log\\nless ra_export_bs_02.oozie.20201026.log\\n```\\n3. Check if max partition of source tables is greater or equal than the export date.\\nIn `ra_export_bs_01.oozie.20201026.log`:\\n``` bash\\nQuery: SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201024'\\n...\\n[2020/10/26 09:00:22] - INFO: max_date=NULL and export_date=20201024\\n```\\nIn `ra_export_bs_02.oozie.20201026.log`:\\n``` bash\\nQuery: SELECT MAX(par_dt) FROM device_traffic WHERE par_dt >= '20201024'\\n...\\n[2020/10/26 09:00:22] - INFO: max_date=NULL and export_date=20201024\\n```\\nThe above messages show that data from the source tables have been exported to files already.\\n\\n4. Validate the results of the query:\\nFor `RA_Dsession`:\\n``` bash\\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\\\"SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201023';\\\";\\n...\\n+-------------+\\n| max(par_dt) |\\n+-------------+\\n| 20201023    |\\n+-------------+\\nFetched 1 row(s) in 1.38s\\n```\\nFor `RA_Dtraffic`:\\n``` bash\\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\\\"SELECT MAX(par_dt) FROM device_traffic WHERE par_dt >= '20201023';\\\";\\n...\\n+-------------+\\n| max(par_dt) |\\n+-------------+\\n| 20201023    |\\n+-------------+\\nFetched 1 row(s) in 1.38s\\n```\\n5. Files will be exported at the next execution if the source tables contain new entries. Due to the size of the exported files runs only for the previous day `par_dt`.\\n\\nIf the customer requests to generate the files for the missing days:\\n\\n``` bash\\ncd /shared/abc/location_mobility/run\\n./export_ra_bs_01.sh -t 20201115 # Run for specific date\\n./export_ra_bs_02.sh -t 20201115 # Run for specific date\\n```\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer Backend\\n\\n<b>Action Points:</b>\\n\\nN/A\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The answer is fully supported by the context, including specific log locations, Impala queries, and shell commands for re-exporting the data. It directly reflects the troubleshooting steps and logic provided.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question is highly relevant for telecom engineers dealing with daily exports of telecom-related session and traffic data in BigStreamer. It concerns production-critical data pipelines.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is fully self-contained and understandable without any external context. It clearly specifies which files are in question and asks about troubleshooting and remediation steps."
    },
    {
      "question": "How do you restore the `osix.sip` topology if data loading has stopped?",
      "answer": "1. SSH into `unosix1`, switch to `osix`, and kinit:\n```bash\nsudo -iu osix\nkinit -kt osix.keytab osix\n```\n2. Check YARN and listeners:\n```bash\nyarn application -list | grep OSIX-SIP-NORM\n```\nIf not running, restart from:\n```bash\n/home/users/osix/topologies/binary-input-impala-output/sip_norm/\n./submit_sip_norm.sh\n```\n3. Confirm with Impala:\n```sql\nSELECT count(*), par_dt FROM osix.sip WHERE par_dt > '20201124' GROUP BY par_dt;\n```",
      "category": "Application Functionality & Flow",
      "context": "# abc - BigStreamer - IM1367129 - osix.sip 25/11/2020\\n\\n<b>Description:</b>\\n\\n```\\nGood evening,\\n\\nWe notice that no data is being loaded from 25/11/2020 at 07:00 on osix.sip.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. ssh unosix1 with your personal account\\n2. sudo -iu osix\\n3. kinit -kt osix.keytab osix\\n4. yarn application -list | grep OSIX-SIP-NORM\\n5. In our case the topology was down and the kudu script didn't resubmit it.\\n6. Check if `coord_OsixStreaming_SIP_MonitorResubmit` is running.\\n7. Check if listeners is healthy and data inserted `http://172.25.37.251/dashboard/#osix_listeners`\\n8. The rate for `listen_sip_core` should be between 12K and 22K messages.If there is an extreme problem e.g. the rate is 0, consider restarting the problematic listener\\n9. Check the logs of monitor script `hdfs dfs -ls /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM` and `hdfs dfs -cat /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM/date_monitor_sip_norm.log`\\n10. Start again the topology `unosix1:/home/users/osix/topologies/binary-input-impala-output/sip_norm/` and execute `./submit_sip_norm.sh` until the state appeared `RUNNING`\\n11. yarn application -list | grep OSIX-SIP-NORM\\n12. Connect to impala-shell or Hue and execute `SELECT count(*), par_dt FROM osix.sip WHERE par_dt>'20201124' group by par_dt;` to check if the data inserted on the table.\\n\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer  \\n\\n<b>Nfgh:<b>\\n\\nRecommended Mop for help `18316_abc_Generic_MOP_CDH_5_16_2_Upgrade`",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully answerable from the context, which contains detailed step-by-step actions for verifying and restarting the topology and validating data ingestion.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Restoring a streaming topology like `osix.sip` is a critical real-world task for telecom engineers using BigStreamer. The issue directly affects data availability and service quality.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear, specific, and self-contained. It provides all necessary information to be understood without referencing additional materials."
    },
    {
      "question": "What caused the Spark job failure in the Geolocation LTE pipeline and how should it be handled?",
      "answer": "Follow the documentation at `http://10.124.161.38/trac/hadoop/wiki/dev/project/abc/geolocation_viavi`. Check for older fail folders under `/ez/warehouse/geolocation.db/geo_<tech>_fail/`. Remove those older than 2 days. Failures are rare and no additional handling is usually necessary.",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# abc - BigStreamer - GI9 - abc Bigstreamer Geolocation mail for Spark job failure\\n\\n<b>Description:</b>\\n\\n```\\nThe following mail arrived:\\n\\nGeolocation ALERT:[WARN] - Phase #4b, Spark job GeoViavi-LTE failed (1605552391)\\n\\nS550.Geo_Tech_Load_Data.sh\\n2020-11-16 20:54:05 --> Phase #4\\nSpark job GeoViavi-LTE failed with return_code 1.\\nApplicationID:application_1599948124043_370289.\\nLoadID:1605552391.\\nStart time of job :1605552487.\\n\\nThis is an automated e-mail.\\nPlease do not reply.\\n```\\n\\n<b>Actions Taken:</b>\\n\\nThe [full documentation](http://10.124.161.38/trac/hadoop/wiki/dev/project/abc/geolocation_viavi) provided by the developers' team\\n\\nFollowed the `Failure Handling Manual Mechanism` section of the guide above\\n\\nIf there are other dates in the `/ez/warehouse/geolocation.db/geo_<technology>_fail/` HDFS folder that are older than two days, they can be deleted. Retention for the geolocation tables is 2 days for the eponymus and 1 day for the anonymous table.\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer Geolocation\\n\\n<b>Action Points:</b>\\n\\nNone, failures of that kind are rare and not worth the extra effort.",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context clearly explains the cause of the Spark job failure and provides instructions on how to handle it. The answer aligns directly with the documented resolution steps.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This is a highly relevant question for telecom engineers managing BigStreamer. It concerns the stability of geolocation data pipelines, which are important for network monitoring and analytics.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear and specific without needing any additional context. It defines both the system (Geolocation LTE pipeline) and the issue (Spark job failure), making it fully standalone."
    },
    {
      "question": "How do you handle stale metadata issues or corrupted Parquet files in `osix.sip`?",
      "answer": "1. Run Impala:\n```sql\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='REGISTER');\n```\n2. If error persists, check file:\n```bash\nparquet-tools meta <file>\n```\n3. If not a valid Parquet file, move it:\n```bash\nhdfs dfs -mv <bad_file> /ez/landingzone/tmp/osix_sip/other\n```\n4. Refresh metadata again.",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# abc - BigStreamer - IM1364500 - abc BigStreamer oozie job hive2script failed / stale metadata\\n\\n<b>Description:</b>\\n\\n```\\nImpala queries finish with error for table osix.sip and partition 20201123\\nMessage:\\nERROR processing query/statement. Error Code: 0, SQL state: File 'hdfs://nameservice1/ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/par_method=REGISTER/part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000' has an invalid version number: .??6\\nThis could be due to stale metadata. Try running \\\"refresh osix.sip\\\".\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Checked that same query results in error using Hive.\\n2. Checked that the problem occurs only with par_hr=08 partition. \\n```bash\\nselect distinct sip.callinguser \\nas callinguser \\nfrom OSIX.sip where par_dt='20201123' \\nAND par_hr != '08' \\nAND sip.callingUser IS NOT NULL;\\n...\\nFetched X rows in X seconds.\\n```\\n\\n2. Inspected logs of Osix SIP application for that time. Login in `unosix1.bigdata.abc.gr`, switch user to `osix` and kinit first.\\n```bash\\n$ sudo su - osix\\n$ cd\\n$ kinit -kt osix.keytab osix\\n$ yarn logs -applicationId application_1599948124043_405502\\n```\\n3. As `sn87.bigdata.abc.gr` was running a Spark executor of this application the time it was forced to shutdown, inspected if there are any corrupt files in the table. Login to any datanode first.\\n``` bash\\n$ cd /var/run/cloudera-scm-agent/process/ \\n$ ls -lahtr | grep -i hdfs\\n$ cd <last directory>\\n$ kinit -kt hdfs.keytab hdfs/`hostname -f`\\n$ hdfs fsck /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08 -includeSnapshots\\n...\\nStatus healthy\\n```\\n4. Inspected format of written files. After communication with the dev team the batch id was retrieved so only a few files were checked. Login to un2.  \\n```\\n$ hdfs dfs -copyToLocal /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/part-*-17ead666-d5cb-437e-a849-c08ef825bec4.* .\\n$ parquet-tools meta part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000\\nfile:/home/users/u15/part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000 is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [46, -19, -49, 54]\\n```\\n4. Some of the files didn't have a correct parquet format so we removed them from the table.\\n``` bash\\n$ hdfs dfs -mv hdfs dfs -mv  /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/par_method=OTHER/part-00005-17ead666-d5cb-437e-a849-c08ef825bec4.c000 /ez/landingzone/tmp/osix_sip/other\\n...\\n```\\n\\n5. Refresh the table and check that problem is fixed.\\n```\\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='REGISTER');\\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='OTHER');\\nselect count(*) from OSIX.sip where par_dt='20201123' ;\\n``` \\n\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer Backend\\n\\n<b>Action Points:</b>\\n\\nN/A",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context explicitly outlines both the metadata issue and the steps taken to identify and resolve corrupted Parquet files in `osix.sip`. The answer reflects these steps clearly.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Stale metadata and file corruption in HDFS are high-priority issues for telecom engineers using BigStreamer. Resolving them is essential for maintaining data pipeline reliability.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is fully self-contained and specific. It clearly defines both the system (`osix.sip`) and the problem (stale metadata or corrupted files), making the query comprehensible without further context."
    },
    {
      "question": "How can you completely disable root SSH access on all nodes while ensuring scripts work with a substitute user?",
      "answer": "1. Create `backup_user`, add it to AllowedGroups, and configure rsync scripts:\n```bash\nHost wrkcdsw1.bigdata.abc.gr\n    User backup_user\n    IdentityFile /root/backup_user_id/id_backup_user_rsa\n```\n2. Disable root login:\n```bash\nsalt '*' cmd.script salt://prepare_sshd_config.sh\nsalt '*' cmd.script salt://disable_root_login.sh\nsalt '*' cmd.run 'service sshd reload'\n```",
      "category": "Infrastructure & Deployment",
      "context": "# abc - BigStreamer - GI7 - abc BigStreamer Configure root ssh login\\n\\n<b>Description:</b>\\n\\n```\\nSecurity requirements from abc mandates that we should change the ssh configuration in all servers in order not to permit root ssh login\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to `admin.bigdata.abc.gr` with personal account and change to root with sudo\\n2. Inspect the status of sshd on all nodes\\n\\n``` bash\\nsalt '*' cmd.run 'grep \\\"^PermitRootLogin\\\" /etc/ssh/sshd_config'\\n```\\n\\n3. Inspect cronjobs of root user on all nodes\\n\\n``` bash\\nsalt '*' cmd.run 'cronjob -l'\\n```\\n\\n4. After collecting the scripts executed by root user, we checked the scripts that contain the words `ssh`, `scp` and `rsync`\\n\\n``` bash\\nfor i in {'/root/disk_balance_with_check.sh' '/usr/local/bin/CM_Config_Backup.sh' '/etc/elasticsearch/elasticsearch_monitoring.sh' '/etc/keepalived/scripts/mysql_check_crontab.sh' '/home/intra/scripts/MySQL_Dump_All_DBs.sh' '/usr/local/bin/krb5prop.sh' '/root/scripts/ldap_check.sh' '/root/send_haproxy_statistics.sh' '/root/send_haproxy_weekly_statistics.sh' '/home/intra/MySQL_Dump_All_DBs.sh' '/home/intra/dfs-backup.sh' '/usr/lib/icom/scripts/cdsw_rsync_backup.sh' '/usr/lib/icom/scripts/cdsw_tar_backup.sh' '/home/intra/MySQL_Dump_spec_DB.sh' '/home/intra/scripts/hue_workflows_all.sh' '/root/hive_logs_retention.sh'}\\ndo\\nsalt \\\"*\\\" cmd.run \\\"if [ -f $i ]; then grep -e ssh -e scp -e rsync $i; fi\\\" | grep -B1 -e ssh -e scp -e rsync\\ndone\\n```\\n\\n5. User `backup_user` was created by abc admins to substitute the root login in the scripts above. From the above investigation we determined that `backup_user` needs to be able to login to nodes `un1.bigdata.abc.gr` and `wrkmncdsw1.bigdata.abc.gr`\\n\\n```bash\\n  # un1.bigdata.abc.gr\\n  setfacl -R -m d:u:backup_user:rwx /data/1/cm_backup/cmdeploys/\\n  setfacl -R -m u:backup_user:rwx /data/1/cm_backup/cmdeploys/\\n  setfacl -R -m d:u:backup_user:rwx /data/1/cm_backup/db-backups/db-vip\\n  setfacl -R -m u:backup_user:rwx /data/1/cm_backup/db-backups/db-vip\\n  setfacl -R -m u:backup_user:rwx /data/1/cm_backup/dfs_backup/\\n  setfacl -R -m d:u:backup_user:rwx /data/1/cm_backup/dfs_backup/\\n\\n  # Add backup_user to AllowedGroups in /etc/ssh/sshd_config\\n```\\n\\n``` bash\\n  # wrkmncdsw1.bigdata.abc.gr\\n  setfacl -R -m u:backup_user:rwx /backup\\n  setfacl -R -m d:u:backup_user:rwx /backup\\n\\n  # Add backup_user to AllowedGroups in /etc/ssh/sshd_config\\n```\\n\\n6. Created `backup user` saltstack state that installs a new private key for passwordless ssh under `/root/backup_user_id/id_backup_user_rsa` on all nodes\\n7. Change the following scripts to use `backup_user` instead of `root`:\\n\\n- /usr/local/bin/CM_Config_Backup.sh\\n- /home/intra/scripts/MySQL_Dump_All_DBs.sh\\n- /home/intra/MySQL_Dump_All_DBs.sh\\n- /home/intra/dfs-backup.sh\\n- /usr/lib/icom/scripts/cdsw_rsync_backup.sh\\n\\n```conf\\n# This script uses rsync. For rsync to use another user than the logged in one create /root/.ssh/config with the following contents:\\nHost wrkcdsw1.bigdata.abc.gr\\n    User backup_user\\n    IdentityFile /root/backup_user_id/id_backup_user_rsa\\nHost wrkcdsw1.bigdata.abc.gr\\n    User root \\n    IdentityFile ~/.ssh/id_rsa\\n```\\n\\n- /home/intra/MySQL_Dump_spec_DB.sh\\n\\n8. Change `PermitRootLogin` on all hosts. The actions mentioned below are executed as `root` from `admin.bigdata.abc.gr`\\n\\n- Get `/etc/ssh/sshd_config to a uniform state\\n\\nContents of `/etc/salt/salt/prepare_sshd_config.sh`:\\n\\n``` bash\\ngrep -e \\\"^PermitRootLogin\\\" /etc/ssh/sshd_config &> /dev/null\\ni=$?\\nif [ $i -eq 1 ]; then\\n  echo \\\"The config file is ok\\\"\\nelse \\n  sed -i -e 's/^#PermitRootLogin/PermitRootLogin/' /etc/ssh/sshd_config # From step 2 we know that on some files the entry was commented\\nfi\\n```\\n\\n- Get `/etc/ssh/sshd_config to a uniform state\\n\\nContents of `/etc/salt/salt/disable_root_login.sh`:\\n\\n``` bash\\ngrep -e \\\"^PermitRootLogin no\\\" /etc/ssh/sshd_config &> /dev/null\\nif [ $i -eq 0 ]; then \\n  echo \\\"The config file is ok\\\"\\nelse \\n  sed -i -e 's/^PermitRootLogin.*yes/PermitRootLogin no/' /etc/ssh/sshd_config\\nfi\\n```\\n\\n- Apply the two scripts on all nodes and reload sshd:\\n\\n``` bash\\nsalt '*' cmd.script salt://prepare_sshd_config.sh\\nsalt '*' cmd.script salt://disable_root_login.sh\\nsalt '*' cmd.run 'service sshd reload'\\n```\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer OS\\n\\n<b>Action Points:</b>\\n\\nWhile investigating the impact of disallowing the root ssh login, we found the following port forwards:\\n\\n``` bash\\n  ssh -g -f gbenet@unekl2 -L 18636:10.255.240.20:3306 -N\\n  ssh -g -f gbenet@admin -L 8889:admin:5900 -N\\n  ssh -g -f u15@admin -L 8888:172.25.37.237:3000 -N\\n  ssh -g -f gbenet@unc1 -L 8743:172.25.37.241:8743 -N\\n  ssh -g -f gbenet@unc1 -L 9743:172.25.37.241:8743 -N\\n  ssh -g -f root@omnm -L 8888:omnm:5901 -N\\n  ssh -g -f root@hedge1 -L 8998:10.255.240.142:8998 -N\\n  ssh -g -f intra@un2 -L 2525:172.18.20.205:25 -N\\n  ssh -g -f intra@un2 -L 22255:un1:22222 -N\\n  ssh -g -f intra@un2 -L 22255:un1:22222 -N\\n  ssh -g -f u15@un2 -L 6536:172.25.150.68:5432 -N\\n  ssh -g -f gbenet@un2 -L 227:undt1:8522 -N\\n  ssh -g -f root@mncdsw1 -L 5555:172.19.53.146:5555 -N\\n  ssh -g -f gbenet@undt2 -L 21050:10.255.241.239:3306 -N\\n  ssh -g -f u3@undt1 -L 9191:10.95.129.200:9191 -N\\n  ssh -g -f gbenet@undt1 -L 9191:10.95.129.200:9191 -N\\n  ssh -g -f gbenet@undt1 -L 9621:10.53.166.37:1521 -N\\n  ssh -g -f gbenet@undt1 -L 4040:10.255.241.220:3306 -N\\n  ssh -g -f gbenet@undt1 -L 9521:10.53.192.187:1521 -N\\n  ssh -g -f gbenet@undt1 -L 21050:10.53.192.187:1521 -N\\n  ssh -g -f gbenet@undt1 -L 3579:172.26.131.15:3579 -N\\n  ssh -g -f gbenet@undt1 -L 8521:10.53.192.192:1521 -N\\n  ssh -g -f gbenet@undt1 -L 8522:10.53.192.191:1521 -N\\n  ssh -g -f gbenet@undt1 -L 8523:10.53.192.190:1521 -N\\n  ssh -g -f gbenet@un1 -L 3579:172.26.131.15:3579 -N\\n  ssh -g -f intra@un1 -L 6654:10.255.240.20:3306 -N\\n  ssh -g -f intra@un1 -L 6634:10.101.1.230:1521 -N\\n  ssh -g -f intra@un1 -L 6433:172.16.109.237:1433 -N\\n  ssh -g -f intra@un1 -L 7536:10.255.241.239:3306 -N\\n  ssh -g -f intra@un1 -L 6721:172.21.4.68:1521 -N\\n  ssh -g -f intra@un1 -L 3389:10.101.6.41:389 -N\\n  ssh -g -f intra@un1 -L 6644:10.101.16.169:1521 -N\\n  ssh -g -f intra@un1 -L 6633:10.255.240.13:6533 -N\\n  ssh -g -f intra@un1 -L 7535:172.24.104.100:5432 -N\\n  ssh -g -f intra@un1 -L 6646:10.95.129.43:1521 -N\\n  ssh -g -f intra@un1 -L 5525:172.18.20.205:25 -N\\n  ssh -g -f intra@un1 -L 6645:10.95.129.41:1521 -N\\n  ssh -g -f intra@un1 -L 21060:sn38:21050 -N\\n  ssh -g -f intra@un1 -L 7183:10.255.243.215:80 -N\\n  ssh -g -f intra@un1 -L 25020:sn88:25020 -N\\n  ssh -g -f ipvpn@un1 -L 7180:undt2:22 -N\\n  ssh -g -f ipvpn@un1 -L 6531:172.25.119.82:1521 -N\\n  ssh -g -f gbenet@un1 -L 8888:undt2:22 -N\\n```\\n\\nBoth `intra` and `root` are no longer allowed to ssh and in a future restart most of the above will not be able to be implemented in the same way.",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The answer is fully grounded in the context, which outlines the specific steps, salt scripts, user setup, file permissions, and SSH configuration changes required to disable root access while enabling `backup_user` as a substitute.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Managing SSH access across BigStreamer infrastructure is highly relevant for compliance and operational security. Ensuring automated scripts function post-change is critical in telecom environments.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is completely self-contained. It clearly defines the goal (disable root access) and the constraint (ensure script continuity), making it easy to understand without needing extra context."
    },
    {
      "question": "What is the proper procedure for updating the BIOS and iDRAC firmware on a Dell PowerEdge C6320 server via CLI?",
      "answer": "1. Download `.BIN` files for both BIOS and iDRAC.\n2. Upload to `/tmp/` on target server.\n3. Run:\n```bash\nchmod +x iDRAC*.BIN && ./iDRAC*.BIN\nchmod +x BIOS*.BIN && ./BIOS*.BIN\n```\n4. If iDRAC update fails, reset:\n```bash\nipmitool -U root -P c0sm0t31 mc reset cold\n```",
      "category": "Infrastructure & Deployment",
      "context": "# abc - BigStreamer - IM1363402 - abc BigStreamer - HW\\n\\n<b>Description:</b>\\n\\n```\\nWe see that sn87 has a problem with the CPU (attached). It has gone out of cluster. Please for your actions and communication with DELL.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Check Idrac logs for the description error `Overview-->Server-->Logs`\\n2. Export the lifecycle logs `Overview-->Server-->Troubleshooting-->Support Assist-->Export Collection` and save the TSR*.zip\\n3. Open a case on DELL SUPPORT(2108129800). Dell need the service tag from `Overview` of Idrac\\n4. Send them the TSR*.zip\\n5. In this case necessary was the update of BIOS & Lifecycle Controller of Idrac\\n6. Dell send us the right update files based on our servers `PowerEdge C6320`\\n7. Updated the BIOS base on the link `https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Updating%20BIOS%20on%20Dell%2013G%20PowerEdge%20Servers.pdf`\\n(to update the BIOS via OS-CLI, see APPENDIX below )\\n8. Updated the Lifecycle Controller base on the link `https://dl.dell.com/topicspdf/idrac_2-75_rn_en-us.pdf`\\n(to update the Lifecycle Controller via OS-CLI, see APPENDIX below )\\n9. After the update of both versions the host was up with the roles stopeed for 1 day.\\n10. After 1 day send the lifecycle logs like `Step 2` and forward the zip file to Dell.\\n11. If any error exist start the roles.\\n\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer HW\\n\\n\\n\\n<b>References:</b>\\nAPPENDIX  - Procedure for BIOS & F/W upgrade for C6320 with CLI (via OS shell)\\n-------------------------------------------------------------------------------\\n\\n\\n- Download new iDRAC FW from link below (Nfgh: download the \\\".bin\\\" format, not the \\\".exe\\\" format):\\n \\n https://www.dell.com/support/home/en-us/drivers/driversdetails?driverid=5hn4r&oscode=naa&productcode=poweredge-c6320\\n \\n  eg: iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \\n \\n \\n- Download new BIOS from (Nfgh: download the \\\".bin\\\" format, not the \\\".efi\\\" format):\\n \\n https://www.dell.com/support/home/en-us/drivers/driversdetails?driverid=cctdp&oscode=naa&productcode=poweredge-c6320\\n \\n eg : BIOS_CCTDP_LN64_2.13.0.BIN\\n\\n\\nProcedure :\\n---------------\\nLogin to C6320 eg sn75 as root\\n\\nStore the downloaded files under /tmp/\\n\\nProcedure executed via OS shell\\n\\n\\nGet current BIOS version \\n---------------------------\\n[root@sn75 /]# ipmitool  mc getsysinfo system_fw_version\\n\\n2.3.4\\n\\n\\nGet current iDRAC version\\n---------------------------\\n[root@sn75 /]# ipmitool   mc info | grep Firmware\\n\\nFirmware Revision         : 2.40\\n\\n\\n\\n\\nUPDATE iDRAC PROCEDURE (mc cold restart is preformed automatically) :\\n------------------------------------------------------------------------\\n```\\n[root@sn75 /]# \\n\\n[root@sn75 tmp]# ll iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \\n\\n  -rw-r--r-- 1 root root 111350247 Dec  6 14:17 iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\\n\\n[root@sn75 tmp]# chmod +x iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \\n\\n[root@sn75 tmp]# ./iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \\n  Update Package 21.04.200 (BLD_1123)\\n  Copyright (C) 2003-2021 Dell Inc. All Rights Reserved.\\n  Release Title:\\n  iDRAC 2.81.81.81, A00\\n  Release Date:\\n  July 02, 2021\\n  Default Log File Name:\\n  5HN4R_A00\\n  Reboot Required:\\n  No\\n  Running validation...\\n  iDRAC\\n  The version of this Update Package is newer than the currently installed version.\\n  Software application name: iDRAC\\n  Package version: 2.81.81.81\\n  Installed version: 2.40.40.40\\n  Continue? Y/N:Y\\n  Executing update...\\n  WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER PRODUCTS WHILE UPDATE IS IN PROGRESS.\\n  THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE!\\n  ...............................................................   USB Device is not found\\n  ..............................................................   USB Device is not found\\n  ...............................................................   USB Device is not found\\n  Device: iDRAC\\n    Application: iDRAC\\n    Failed to reach virtual device. This could be caused by BitLocker or other security software being enabled. For more information, see the\\n    Update Package Userâ€™s Guide.\\n  The update completed successfully.\\n```\\n\\n\\n<b>Nfgh:</b>\\n------\\n  IF THE ABOVE ERROR IS SHOWN, THEN REBOOT THE iDRAC (\\\"#ipmitool  -U root -P c0sm0t31 mc reset cold\\\") \\n  \\n  and REPEAT to get the below correct output, without the \\\"Failed to reach virtual device.\\\" message:!!!\\n  \\n  \\n```  Device: iDRAC\\n  Application: iDRAC\\n  Update Successful.\\n  The update completed successfully.\\n```\\n\\n\\nUpdate BIOS PROCEDURE (REBOOT REQUIRED !!!)\\n----------------------------------------------\\n```\\n[root@sn75 /]# cd /tmp\\n\\n[root@sn75 /]#  ll BIOS_CCTDP_LN64_2.13.0.BIN \\n\\n[root@sn75 /]#  chmod +x BIOS_CCTDP_LN64_2.13.0.BIN \\n\\n[root@sn75 /]#  ./BIOS_CCTDP_LN64_2.13.0.BIN \\n  Running validation...\\n  \\n  PowerEdge C6320 BIOS\\n  \\n  The version of this Update Package is newer than the currently installed version.\\n  Software application name: BIOS\\n  Package version: 2.13.0\\n  Installed version: 2.3.4\\n  Continue? Y/N:Y\\n  Executing update...\\n  WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER PRODUCTS WHILE UPDATE IS IN PROGRESS.\\n  THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE!\\n  ................................................................................................................\\n  Device: PowerEdge C6320 BIOS\\n    Application: BIOS\\n    The BIOS image file is successfully loaded. Do not shut down, cold reboot, power cycle, or turn off the system, till the BIOS update is complete otherwise the\\n    system will be corrupted or damaged. Bios update takes several minutes and it may be unresponsive during that time. Nfgh: If OMSA is installed on the system,\\n    the OMSA data manager service stops if it is already running.\\n  \\n  Would you like to reboot your system now?\\n  \\n  Continue? Y/N:Y \\n``` \\n\\n\\n\\n\\n\\nGet NEW BIOS version \\n------------------------\\n[root@sn75 /]# ipmitool  mc getsysinfo system_fw_version\\n\\n2.3.4\\n\\n\\nGet NEW iDRAC version\\n------------------------\\n[root@sn75 /]# ipmitool   mc info | grep Firmware\\n\\nFirmware Revision         : 2.40\\n\\n\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The answer is fully grounded in the context. The context describes the BIOS and iDRAC update procedures in detail, including exact command sequences and error handling via `ipmitool`, which the assistant answer summarizes accurately.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This task is directly related to hardware maintenance for nodes running BigStreamer. BIOS and iDRAC updates are critical for system stability, especially after hardware faults that take nodes out of the cluster.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear and self-contained. It provides enough detail (hardware, goal, method) to be understandable and actionable without requiring prior context."
    },
    {
      "question": "How can you investigate missing log data from a specific syslog server?",
      "answer": "1. SSH into `un2` as root.\n2. Check `rsyslog.conf` and logrotate:\n```bash\ncat /etc/rsyslog.conf\ncat /etc/logrotate.conf\n```\n3. Verify message sources and delivery servers configured.\n4. Review system logs under `/var/log/messages` or journald.",
      "category": "Data Management & Query Execution",
      "context": "abc\\n\\nBigStreamer\\n\\nSyslog(un2)\\n\\nIssue Number: IM1299104 \\nTitle: Missing logs\\n\\n<b>Description:</b>\\n\\nGood evening, the abc syslog administrators have noticed much lower than expected and irregular log reception times from server 172.25.37.236 for the period 7/23-27.\\nWe would like to investigate the cause and if possible find the missing logs.\\n\\nThank you very much for your immediate actions.\\n\\nKeywords: logs\\nOwner: u27\\nDate: 20200929\\nStatus: Open\\n\\n<b>Actions Taken:</b>\\n\\n1. ssh un2 as root\\n2. cat /etc/rsyslog.conf | more\\n3. check the servers that messages transfered. ( only abc servers on this file)\\n4. cat /etc/logrotate.conf | more ( check the rotate of messages)",
      "question_groundedness_score": "4",
      "judge_feedback_question_groundedness": "The question is mostly answerable from the context, which includes an investigation checklist involving rsyslog and logrotate. However, the answer goes slightly beyond by mentioning journald explicitly, which is not present in the context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question is highly relevant for engineers operating in BigStreamer, where proper log collection is essential for monitoring, debugging, and auditing distributed systems.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear, self-contained, and provides enough detail for someone to understand and address the issue without needing additional context."
    },
    {
      "question": "Why are exported CPU and Memory metrics files missing or delayed, and how can you reduce export latency?",
      "answer": "Delays are due to Impala query lag and incorrect timestamp rounding by Flume Morphline.\n1. Update Morphline rounding:\n```bash\n{ extractTimeBucket { field : file, bucket:5, rounding:mathematical } }\n```\n2. Restart Flume agent.\n3. Investigate query performance and table schema for optimization:\n```sql\nSELECT count(*) FROM bigcust.nnm_ipvpn_componentmetrics_hist WHERE par_dt='20201027';\n```",
      "category": "Data Management & Query Execution",
      "context": "# abc - BigStreamer - IM1333238 - abc BigStreamer SM-MISSING DATA\\n\\n<b>Description:</b>\\n\\n```\\nFiles CPU_LOAD and MEMORY_USAGE are exported in the wrong order or not at all\\nMissing files for 4:25 and 5:05 a.m.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `ipvpn` with sudo\\n2. Inspect logs of *export component files* flow\\n``` bash\\ncd /shared/abc/ip_vpn/log/\\nless initiate_export_components.cron.20201025.log\\n```\\n\\n3. Check messages for files with 04:25 and 05:05 metrics\\n``` bash\\n[2020/10/25 04:32:56] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-25_04.25.00.csv is empty.\\n[2020/10/25 04:32:56] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-25_04.25.00.csv is empty.\\n...\\n[2020/10/25 05:12:57] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-25_05.05.00.csv is empty.\\n[2020/10/25 05:12:57] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-25_05.05.00.csv is empty.\\n```\\n\\n4. Check Impala queries execution for that files\\nFor `CPU_LOAD`:\\n``` bash\\nless compute_cpu_kpis.20201025.log\\n...\\nQuery: SELECT straight_join a.min_5 AS time,\\n...\\n    a.component_type='CPU' AND\\n    a.min_5='2020-10-25 04:25:00' AND\\n    a.par_dt='20201025'\\n...\\nFetched 0 row(s) in 12.20s\\nINFO: CPU file exported.\\n```\\nFor `MEM_USAGE`:\\n``` bash\\nless compute_memory_kpis.20201025.log\\n...\\nQuery: SELECT straight_join a.min_5 AS time,\\n...\\na.component_type='MEMORY' AND\\na.min_5='2020-10-25 04:25:00' AND\\na.par_dt='20201025'\\n...\\nFetched 0 row(s) in 12.03s\\nINFO: Memory file exported.\\n```\\n\\n5. Check input metrics table\\nExecute the Impala query either from Hue or impala-shell\\n```bash\\nSELECT count(*)\\nFROM bigcust.nnm_ipvpn_componentmetrics_hist a\\nWHERE        \\n    a.min_5='2020-10-25 04:25:00' AND\\n    a.par_dt='20201025';\\nResult = 0\\n```\\n\\n6. Inspect logs of input metrics ingestion\\nFiles are transferred from NNM node to a local spool directory every 5 minutes.\\n```bash\\nless /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20201025.log\\n...\\n[2020/10/25 04:31:05] - INFO - Checking file: /data/1/nnm_components_LZ/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.csv.gz\\n...\\n[2020/10/25 04:31:05] - INFO - /bin/mv /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv.tmp /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv\\n```\\n\\n7. Inspect logs of Flume agent\\n```bash\\nless /var/log/flume-ng/flume-cmf-flume5-AGENT-un2.bigdata.abc.gr.log\\n...\\n2020-10-28 22:46:05,308 INFO org.kitesdk.morphline.stdlib.LogInfoBuilder$LogInfo: After Split record:[{IPAddress=[87.203.132.214], ..., file=[/data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv]}]\\n2020-10-28 22:46:05,308 INFO org.kitesdk.morphline.stdlib.LogInfoBuilder$LogInfo: After extractTimeBucket record:[{IPAddress=[87.203.132.214], ..., field_min_5=[2020-10-25 04:30:00], field_par_dt=[20201025], file=[/data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv]}]\\n```\\nThe issue is that field_min_5 should have been `2020-10-25 04:25:00`, not `2020-10-25 04:30:00`.\\n\\n8. Alter Morphline configuration\\nChange rounding to `mathematical` so field_min_5 is the timestamp of the metric rounded to the nearest 5-minute interval. \\n```bash\\nvim /shared/abc/ip_vpn/flume/nnm_component_metrics/morphline_nnmMetricsCsvToRecord_ipvpn_user.conf\\n...\\n          { extractTimeBucket { field : file, bucket:5, rounding:mathematical } }\\n```\\n\\n9. Restart Flume Agent\\nWhen the agent is not processing any files, restart `FLUME-IPVPN` at `un2.bigdata.abc.gr` from Cloudera Manager. Monitor that field_min_5 is rounded to the nearest 5-minute interval.\\n```bash\\ntail -f less /var/log/flume-ng/flume-cmf-flume5-AGENT-un2.bigdata.abc.gr.log\\n```\\n\\n10. Investigate delays in export\\n```bash\\nless initiate_export_components.cron.20201025.log\\n...\\n[2020/10/25 03:12:56] - INFO: Searching for output files..\\n[2020/10/25 03:13:04] - INFO: cpu_output_file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-25_03.05.00.csv\\n[2020/10/25 03:13:04] - INFO: mem_output_file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-25_03.05.00.csv\\n```\\nSearching for the files is taking too long. \\n\\n11. Inspect host resources at that time\\nLogin to Cloudera Manager. CPU, memory and network did not show anomalies, however disk queue was increased for sdc device. Peaks were nfghd every 2 hours. \\n\\n12. Find processes with heavy disk i/o\\nBased on a previous investigation the processes that write to sdc run for Location Mobility. Communication with the development team to change disk/device.\\n\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer Backend\\n\\n<b>Action Points:</b>\\n\\n# abc - BigStreamer - IM1336999 - abc BigStreamer SM-MISSING DATA\\n\\n<b>Description:</b>\\n\\n```\\nSince 27/10/2020 12:40 pm 3 files have not been registered to EEM due to delays. Normal offset is 8 minutes e.g. metrics for 13:05 have to be transferred to the exchange directory before 13:13.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. In the screenshot sent via email there is one file is missing for 13:05 and three files have been delayed from 13:55 to 14:05\\n2. Login to `un2.bigdata.abc.gr` with personal account and change to `ipvpn` with sudo\\n3. Inspect logs of *export component files* flow\\n``` bash\\ncd /shared/abc/ip_vpn/log/\\nless initiate_export_components.cron.20201027.log\\n```\\n\\n3. Check messages for missing file\\n``` bash\\n[2020/10/27 13:13:00] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_13.05.00.csv is empty.\\n[2020/10/27 13:13:00] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_13.05.00.csv is empty.\\n```\\n\\n4. Check Impala queries execution for that file\\nFor `CPU_LOAD`:\\n``` bash\\nless compute_cpu_kpis.20201027.log\\n...\\nQuery: SELECT straight_join a.min_5 AS time,\\n...\\n    a.component_type='CPU' AND\\n   a.min_5='2020-10-27 13:05:00' AND\\n    a.par_dt='20201027'\\n...\\nQuery submitted at: 2020-10-27 13:12:51\\n...\\nFetched 0 row(s) in 8.68s\\nINFO: CPU file exported.\\nTue Oct 27 13:13:00 EET 2020\\n```\\nFor `MEM_USAGE`:\\n``` bash\\nless compute_memory_kpis.20201025.log\\n...\\nQuery: SELECT straight_join a.min_5 AS time,\\n...\\na.component_type='MEMORY' AND\\na.min_5='2020-10-27 13:05:00' AND\\na.par_dt='20201027'\\n...\\nQuery submitted at: 2020-10-27 13:12:50\\n...\\nFetched 0 row(s) in 9.00s\\nINFO: Memory file exported.\\nTue Oct 27 13:13:00 EET 2020\\n```\\n\\n5. Check input metrics table\\nExecute the Impala query either from Hue or impala-shell\\n```bash\\nSELECT count(*)\\nFROM bigcust.nnm_ipvpn_componentmetrics_hist a\\nWHERE        \\n    a.min_5='2020-10-27 13:05:00' AND\\n    a.par_dt='20201027';\\nResult = 4286\\n```\\n\\n6. Inspect logs of input metrics ingestion\\n```bash\\nless /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20201027.log\\n...\\n[2020/10/27 13:11:03] - INFO - /bin/mv /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201027130601110.20201027_111102UTC.csv.tmp /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201027130601110.20201027_111102UTC.csv\\n...\\n```\\nInput metrics file has been loaded before the execution of the export query. So queries in step 4 should have returned about 1260 rows. This needs further investigation on why even after the table has been refreshed the query returns the wrond result. Nfghd as Action Point 1.\\n\\n3. Check messages for delayed files\\n``` bash\\n[2020/10/27 14:03:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_13.55.00.csv at /shared/abc/ip_vpn/out/saismpm\\n[2020/10/27 14:03:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_13.55.00.csv at /shared/abc/ip_vpn/out/saismpm\\n...\\n[2020/10/27 14:08:09] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_14.00.00.csv at /shared/abc/ip_vpn/out/saismpm\\n[2020/10/27 14:08:09] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_14.00.00.csv at /shared/abc/ip_vpn/out/saismpm\\n...\\n[2020/10/27 14:14:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_14.05.00.csv at /shared/abc/ip_vpn/out/saismpm\\n[2020/10/27 14:14:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_14.05.00.csv at /shared/abc/ip_vpn/out/saismpm\\n```\\nSince SFTP GET occurs on 3rd, 8th, 13th, 18th etc minute of every hour, the files above should have been transferred in `/shared/abc/ip_vpn/out/saismpm` before 14:03, 14:08, 14:13 respectively.\\n\\n4. Check Impala queries execution for those files\\nFor `CPU_LOAD`:\\n``` bash\\nless compute_cpu_kpis.20201027.log\\n...\\nTue Oct 27 14:02:02 EET 2020\\nStarting Impala Shell using LDAP-based authentication\\n...\\nINFO: CPU file exported.\\nTue Oct 27 14:03:00 EET 2020\\n...\\n...\\nTue Oct 27 14:07:07 EET 2020\\nStarting Impala Shell using LDAP-based authentication\\n...\\nINFO: CPU file exported.\\nTue Oct 27 14:08:09 EET 2020\\n...\\n...\\nTue Oct 27 14:12:06 EET 2020\\nStarting Impala Shell using LDAP-based authentication\\n...\\nINFO: CPU file exported.\\nTue Oct 27 14:14:03 EET 2020\\n```\\nSame for `MEM_USAGE`.\\nThe cause of the delays is the duration time of the Impala queries. To reduce the times we need to investigate if the schema of bigcust.nnm_ipvpn_componentmetrics_hist can be improved, if we can delete the `REFRESH nnmnps.nms_node ...` queries etc. Nfghs as Action Point 2.\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer Backend\\n\\n<b>Action Points:</b>\\n\\n1. Investigate empty response of Impala query for 13:05 even after refreshing the table\\n2. Investigate slightly increased response times of Impala queries and ways to reduce them.\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully answerable based on the provided context. The logs, Morphline configuration, and Impala query behavior are all described in detail, allowing the answer to directly address causes and solutions.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question is highly relevant for telecom engineers dealing with metric ingestion, processing, and file exports in BigStreamer. It focuses on troubleshooting data delays and optimizing pipeline performance.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is self-contained, with no vague references. It clearly describes the issue (missing or delayed metric files) and asks for root cause and mitigation, which makes it fully understandable without external context."
    },
    {
      "question": "How can you manually re-ingest missing data in the `refdata.rd_cells_load` table when a partition has fewer rows than expected?",
      "answer": "1. Identify the affected partition by checking:\n```bash\nimpala-shell -q 'show partitions refdata.rd_cells_load;'\n```\n2. Verify file size with:\n```bash\nimpala> refresh refdata.rd_cells_load;\nimpala> show files in refdata.rd_cells_load partition (par_dt>='20201110');\n```\n3. Copy data from a correct partition (e.g., from 20201111 to 20201110):\n```bash\nhdfs dfs -cp /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/\n```\n4. Rename old file:\n```bash\nhdfs dfs -mv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/.cells_20201110.csv\n```\n5. Re-check row counts:\n```sql\nSELECT par_dt, count(*) FROM refdata.rd_cells_load WHERE par_dt >= '20201109' GROUP BY par_dt ORDER BY 1;\n```",
      "category": "Application Functionality & Flow",
      "context": "# abc - BigStreamer - IM1353607  - abc BigStreamer (refdata.rd_cells)\\n\\n<b>Description:</b>\\n\\n```\\nPlease load the data for 11/11 and for 10/11\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Check the size of current partition from Impala-Shell\\n``` bash\\nshow partitions refdata.rd_cells_load;\\n```\\nexample output\\n``` bash\\npar_dt   | #Rows     | #Files | Size    | Bytes Cached | Cache Replication | Format | Incremental stats | Location                                                                 \\n---------+-----------+--------+---------+--------------+-------------------+--------+-------------------+--------------------------------------------------------------------------\\n20201109 |    105576 |      1 | 44.82MB | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201109\\n20201110 |         6 |      1 | 191B    | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110\\n20201111 |    105325 |      1 | 45.63MB | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111\\n```\\nWe notisted significant less Rows for par_dt \\\"20201110\\\" .\\n\\n2. We check \\\"20201110\\\" & \\\"20201111\\\" partition files from HDFS.\\n``` bash\\nimpala> refresh refdata.rd_cells_load;\\nimpala> show files in refdata.rd_cells_load partition (par_dt>='20201110');\\n```\\nexample output\\n```\\nPath                                                                                         | Size    | Partition      \\n---------------------------------------------------------------------------------------------+---------+----------------\\nhdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv | 191B    | par_dt=20201110\\nhdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv | 45.63MB | par_dt=20201111\\n```\\n\\n3. We copy partition \\\"20201111\\\" file to \\\"20201110\\\".\\n``` bash\\nhdfs dfs -cp /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/\\n```\\n\\n4. We rename to hide the file for \\\"20201110\\\".\\n``` bash\\nhdfs dfs -mv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/.cells_20201110.csv\\n```\\n\\n5. Repeat Step 2.\\n\\n6. We execute the query bellow to check if the partitions \\\"20201111\\\" & \\\"20201110\\\" have the same number of Rows.\\n``` bash\\nimpala> select par_dt, count(*) cnt from refdata.rd_cells_load where par_dt>='20201109' group by par_dt order by 1;\\n```\\nexample output\\n```\\npar_dt   | cnt   \\n---------+-------\\n20201109 | 105576\\n20201110 | 105325\\n20201111 | 105325\\n```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully answerable using only the context, which clearly describes the issue with the specific partition, and the steps taken to re-ingest data from a neighboring partition.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The question is highly relevant to telecom engineers working on BigStreamer, as it addresses data integrity and recovery procedures within the platform's data ingestion workflows.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is specific, unambiguous, and self-contained. It clearly describes the problem and the table involved, without relying on prior context."
    },
    {
      "question": "What should be checked if the `energy_efficiency.pollaploi` table hasn't updated as expected?",
      "answer": "1. Check the SFTP source for new files:\n```bash\nsftp bigd@172.16.166.30\ncd energypm\nls -ltr\n```\n2. Inspect the Oozie workflow `energy_efficiency_load_pollaploi` via Hue.\n3. Review logs:\n```bash\nless /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log\nless /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.next_date.log\n```\n4. Validate row counts:\n```bash\nwc -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/*.txt\nimpala-shell -q 'SELECT COUNT(*) FROM energy_efficiency.pollaploi;'\n```",
      "category": "Application Functionality & Flow",
      "context": "# abc - IM1382364 - Energy efficiency πληροφορία update\\n\\n<b>Description:</b>\\n\\n```\\nPlease let us know whenever the pollaploi table in schema energy efficiency is updated. \\nAlso to investigate why an update has not been made based on the latest file.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. ssh un2 with your personal account\\n2. sudo -iu intra\\n3. sftp `bigd@172.16.166.30`\\n4. cd energypm\\n5. ls -ltr\\n6. Open HUE dashboard and search for `energy_efficiency_load_pollaploi` Workflow\\n7. Check if workflow failed.\\n8. ssh `un2` with your personal account.\\n9. sudo -i\\n10. less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log` and less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.next_date.log`. The next date should returned no changes.\\n11. At un2 `wc -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/2020_10_pollaploi.txt`\\n12. Connect to impala-shell and execute `select count(*) from energy_efficiency.pollaploi`\\n13. The count should be the same on `11 & 12`\\n14. Check on Impala Queries UI if the queries ran without exception `STATEMENT RLIKE '.*energy_efficiency_load_pollaploi.*'`\\n\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context provides a clear step-by-step investigation log that maps directly to the checks outlined in the answer. The answer is fully grounded in the available context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Monitoring and troubleshooting table updates is a core part of maintaining data integrity and freshness in BigStreamer, especially in telecom-related energy efficiency workflows.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is self-contained and clearly identifies the table, symptom (lack of update), and required investigative goal. It can be understood without any external reference."
    },
    {
      "question": "How do you restore `radius.radacct_hist` when it failed to ingest rows despite having correct data in `radacct_orig_files`?",
      "answer": "1. Locate affected files in:\n```bash\nhdfs dfs -ls /ez/warehouse/radius.db/radacct_orig_files | grep 20210209_07\n```\n2. Copy to load table:\n```bash\nhdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-09_07-30.csv.* /ez/warehouse/radius.db/radacct_load/\n```\n3. Rerun ingestion:\n```bash\n/shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.20210211.log.manual 2>&1\n```",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# abc - IM1443515 - radius.radacct_hist\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\n\\nPlease investigate the three-day reduction in registrations at 06:00 and 07:00 (02/08 - 02/10) and inform us about it.\\n```\\n\\n<b>Actions Taken:</b>\\n- Login to `un2.bigdata.abc.gr` with personal account and change to `intra` with sudo.\\n- Compare the count of the inserted data between the radius.radacct_hist and the original files radius.radacct_orig_files\\n\\n```bash\\n[intra@un2 ~]$ secimp\\n[un-vip.bigdata.abc.gr:22222] > select par_dt,substr(acctupdatetime,1,13),count(*) from radius.radacct_hist where par_dt>'20210209' group by 1,2 order by 1,2;\\n| 20210209 | 2021-02-09 06 | 597 |\\n| 20210209 | 2021-02-09 07 | 697082 |\\n\\n[un-vip.bigdata.abc.gr:22222] > select substr(acctupdatetime,1,13),count(*) from radius.radacct_orig_files where acctupdatetime>'2021-02-09' group by 1 order by 1;\\n| 2021-02-09 06                 | 1430757  |\\n| 2021-02-09 07                 | 1393639  |\\n```\\nSo, the files had been correctly ingested but the radacct_hist table has a problem.\\n\\n- Compare the total ingested lines with the total inserted lines for the provided dates/hours (the provided hours are in UTC time - Impala)\\n\\n```bash\\n[intra@un2 ~]$ for i in {08..09};do grep -E \\\"2021/02/09 ${i}.*Total lines\\\" /shared/abc/radius/DataParser/scripts/log/radius_cron.log;done\\n[2021/02/09 08:12:01] - info - Total lines :  <2130925>\\n[2021/02/09 09:12:37] - info - Total lines :  <2136145>\\n\\n[intra@un2 ~]$ for i in {08..09};do grep -B 5 Modified /shared/abc/radius/log/000_radius_ops.20210209.log | grep -A 6 \\\"insert into radius.radacct_hist\\\" | grep -C 3 \\\"Query submitted at: 2021-02-09 ${i}\\\" | grep Modified;done\\nModified 0 row(s) in 0.58s\\nModified 2136145 row(s) in 12.15s\\n```\\n\\nAs you can see, the data had been correctly inserted into radius.radacct_load (2130925) but the insert into the radius.radacct_hist had insert 0 rows @  09/02/2021 08:18:15.\\n\\nDo the same process for 08/02/2021 and 10/02/2021 and observe the same outcome.\\n\\n- The ingested files have been backed up in the radius.radacct_orig_files. Find the appropriate files for the given dates/hours\\n\\n```bash\\n[intra@un2 ~]$ for i in {08..10};do hdfs dfs -ls /ez/warehouse/radius.db/radacct_orig_files/ | grep 202102${i}_08;done\\n-rwxrwx--x+  3 hive hive  839787710 2021-02-08 08:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-08_07-30.csv.20210208_081002.utc\\n-rwxrwx--x+  3 hive hive  844035825 2021-02-09 08:12 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-09_07-30.csv.20210209_081002.utc\\n-rwxrwx--x+  3 hive hive  844035825 2021-02-09 08:12 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-10_07-30.csv.20210210_081001.utc\\n```\\n\\n- Copy the files to the load table\\n\\n```bash\\n[intra@un2 ~]$ hdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-08_07-30.csv.20210208_081002.utc /ez/warehouse/radius.db/radacct_load/\\n```\\n\\nDo the same for the other two files.\\n\\n- Run the radius procedure again\\n\\n```bash\\n[intra@un2 ~] /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.20210211.log.manual 2>&1\\n```\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer\\n\\n<b>Action Points:</b>\\n\\nResolution of Cloudera Issue 752877 - Hive Metastore innodb lock await time out which is the root cause of this issue.",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context provides detailed logs, HDFS file listings, and exact actions taken to identify the discrepancy between source and target tables. The question is fully answerable based on this information.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The question targets a critical issue in data pipeline reliability within BigStreamer, involving ingestion recovery, which is highly relevant to the day-to-day work of telecom data engineers.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question clearly names the tables involved and the scenario (data present but not ingested), making it understandable and actionable without needing the context."
    },
    {
      "question": "What steps should you follow when CDSW jobs are stuck in 'Scheduled' due to resource exhaustion?",
      "answer": "1. Login to CDSW Admin UI and check the Activity tab for CPU and memory usage.\n2. Remove all pending or stuck pods:\n```bash\nkubectl get pods | grep Pending | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\nkubectl get pods | grep 'Init:0/1' | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n```",
      "category": "Troubleshooting & Issue Resolution",
      "context": "# abc - IM1476278 - CDSW Not enough CPU/GPU/Memory \\n\\n<b>Description:</b>\\n\\n```\\nGood morning, \\n\\nFor 2 days now all jobs in CDSW fail (stay in status schedule) with the message\\n\\\"Unschedulable: No host in the cluster currently has enough CPU, memory and GPU to run the engine\\\"\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'\\n2. Go to last tab(admin).\\n3. Select `Activity` tab.\\n4. Check `CPU` and `Memory` graphs.\\n5. In our case all scheduled jobs stucked at `Scheduling` due to a job that ran `800 times at 14/03 07:00`\\n \\n- Delete all PENDING pods to free resources\\n\\n```bash\\n[root@mncdsw1 ~]# kubectl get pods\\n[root@mncdsw1 ~]# kubectl get pods | grep Pending | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\\n[root@mncdsw1 ~]# kubectl get pods | grep \\\"Init:0/1\\\" | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\\n[root@mncdsw1 ~]# kubectl get pods\\n```\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer CDSW\\n\\n<b>Action Points:</b>\\n\\nIf the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a place can be found. However, it will not be killed for excessive CPU usage.\\n\\nAs an action point we could `monitoring compute & memory resource usage` from the Pod status via our monitoring tool.",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully supported by the provided context, which includes the specific problem (jobs stuck due to lack of resources), diagnostics (UI and kubectl), and exact remediation steps.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This issue is critical for maintaining the operational functionality of CDSW within BigStreamer. Resource exhaustion and pod scheduling problems are highly relevant for engineers working in this environment.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clearly phrased and understandable without requiring additional context. It specifies the component (CDSW), the issue (jobs stuck in 'Scheduled'), and the suspected cause (resource exhaustion)."
    },
    {
      "question": "How do you recover BigStreamer after a NameNode failure affecting HDFS, HBase, and Oozie?",
      "answer": "1. Access Cloudera Manager: https://172.25.37.232:7183\n2. Verify that one NameNode is active. Restart if both are in standby.\n3. After restart, check health of HBase, Oozie, Impala.\n4. Use Kibana to examine system metrics during outage:\n```url\nhttp://10.20.9.82:5601/app/kibana\n```",
      "category": "Infrastructure & Deployment",
      "context": "# abc - IM1391585 - Πρόβλημα λειτουργίας BigStreamer\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\n\\nPlease check immediately if BigStreamer is working properly.\\n\\nWe have received many alerts over the weekend and today regarding HDFS, for various nodes as well as for services (eg oozie). We also noticed problems with geolocation streams and loc mob files.\\n\\nThanks,\\nSK\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Connect with personal creds `https://172.25.37.232:7183` Cloudera Manager\\n2. Namenodes were both on standby mode which caused bad health on HDFS,HBASE,OOZIE,IMPALA. After nn1 restarted nn2 became the Active and nn1 the Standby namenode. All the other services was stable after this manual action except HBASE which restarted.\\n3. Since all services were stable check HUE `https://172.25.37.236:8888/oozie/list_oozie_workflows/` to ensure that all workflows running.\\n4. The specific timeline which namenodes crashed the load,cpu,network,hdfs_read/write,nodes health,,namenodes health,impala queries if something heavy executed,yarn applications if something heavy executed `http://10.20.9.82:5601/app/kibana`\\n5. Opened a case on Cloudera with namenodes diagnostics.\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer\\n\\n<b>Action Points:</b>\\n\\nMonitor the status/health of services and inform with mail/alert when a service/role is down.\\n\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully grounded in the context, which details the NameNode failure, its impact on HDFS, HBase, and Oozie, and the recovery steps taken using Cloudera Manager and Kibana.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The issue is highly relevant for telecom engineers managing BigStreamer, as NameNode failures can disrupt core services. Recovery procedures are critical in such scenarios.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is well-formed and can be fully understood without the context. It specifies the system (BigStreamer), the failure (NameNode), the impact (HDFS, HBase, Oozie), and the desired outcome (recovery steps)."
    },
    {
      "question": "How do you reprocess RADIUS files for missing data during system downtime?",
      "answer": "1. Manually fetch the missing files via SFTP and place under `/shared/radius_repo/cdrs`\n2. Edit `radius.trn` to use `.dat.local`:\n```bash\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n```\n3. Run:\n```bash\n/shared/abc/radius/DataParser/scripts/radius.pl -l -d -D -o >> radius_cron_manual.log\n/shared/abc/radius/bin/000_radius_ops.sh >> radius_ops.manual.log\n```\n4. Validate HDFS ingest:\n```bash\nhdfs dfs -ls /ez/warehouse/radius.db/radacct_hist/par_dt=20201220\n```",
      "category": "Infrastructure & Deployment",
      "context": "# abc - IM1391612 - Data loading\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\n\\nPlease immediately load all the data for all the hours and for all the flows (sai,vantage,radius etc) for the period of time the system was not working and run all the aggregated tables. We should also be sent a summary table with the relevant information so that we know if and in which streams there are data deficiencies.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `intra` with sudo.\\n2. Check files that have not been ingested. As you can see, there is a gap for radacct 20201220 and for radarchive between 3:00 - 16:30\\n```bash\\n[intra@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radacct_orig_files\\n...\\n-rwxrwx--x+  3 hive hive  838634257 2020-12-20 02:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_01-30.csv.20201220_021002.utc\\n-rwxrwx--x+  3 hive hive  837624575 2020-12-20 03:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_03-00.csv.20201220_031003.utc\\n-rwxrwx--x+  3 hive hive  840322537 2020-12-20 17:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_16-30.csv.20201220_171002.utc\\n-rwxrwx--x+  3 hive hive  839948348 2020-12-20 18:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_18-00.csv.20201220_181002.utc\\n-rwxrwx--x+  3 hive hive  840668651 2020-12-20 20:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_19-30.csv.20201220_201002.utc\\n-rwxrwx--x+  3 hive hive  840847248 2020-12-20 21:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_21-00.csv.20201220_211002.utc\\n...\\n[intra@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radarchive_orig_files\\nFound 30 items\\n...\\n-rwxrwx--x+  3 hive hive 1694918420 2020-12-17 05:14 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-16.csv.20201217_051003.utc\\n-rwxrwx--x+  3 hive hive 1635182557 2020-12-18 05:14 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-17.csv.20201218_051002.utc\\n-rwxrwx--x+  3 hive hive 1618497473 2020-12-19 05:14 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-18.csv.20201219_051002.utc\\n-rwxrwx--x+  3 hive hive 1522580860 2020-12-21 05:13 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-20.csv.20201221_051002.utc\\n```\\n\\n3. Change directory and transfer missing files from sftp. \\n``` bash\\n[intra@un2 radius]$ cd /shared/radius_repo/cdrs\\n[intra@un2 cdrs]$ sftp prdts@79.128.178.35\\nConnecting to 79.128.178.35...\\nsftp> get radacct_2020-12-20_04-30.csv.bz2\\nFetching /home/prdts/radacct_2020-12-20_04-30.csv.bz2 to radacct_2020-12-20_04-30.csv.bz2\\n/home/prdts/radacct_2020-12-20_04-30.csv.bz2                                                                                                                              100%  201MB  28.8MB/s   00:07    \\nsftp> get radarchive_2020-12-19.csv.bz2\\nFetching /home/prdts/radarchive_2020-12-19.csv.bz2 to radarchive_2020-12-19.csv.bz2\\n/home/prdts/radarchive_2020-12-19.csv.bz2                                                                                                                                 100%  207MB  25.8MB/s   00:08    \\nsftp> get radacct_2020-12-20_06-00.csv.bz2\\nFetching /home/prdts/radacct_2020-12-20_06-00.csv.bz2 to radacct_2020-12-20_06-00.csv.bz2\\n/home/prdts/radacct_2020-12-20_06-00.csv.bz2                                                                                                                              100%  201MB  25.2MB/s   00:08    \\nsftp> get radacct_2020-12-20_07-30.csv.bz2\\nFetching /home/prdts/radacct_2020-12-20_07-30.csv.bz2 to radacct_2020-12-20_07-30.csv.bz2\\n/home/prdts/radacct_2020-12-20_07-30.csv.bz2                                                                                                                              100%  201MB  33.6MB/s   00:06    \\nsftp> get radacct_2020-12-20_09-00.csv.bz2\\nFetching /home/prdts/radacct_2020-12-20_09-00.csv.bz2 to radacct_2020-12-20_09-00.csv.bz2\\n/home/prdts/radacct_2020-12-20_09-00.csv.bz2                                                                                                                              100%  201MB  25.1MB/s   00:08    \\nsftp> get radacct_2020-12-20_10-30.csv.bz2\\nFetching /home/prdts/radacct_2020-12-20_10-30.csv.bz2 to radacct_2020-12-20_10-30.csv.bz2\\n/home/prdts/radacct_2020-12-20_10-30.csv.bz2                                                                                                                              100%  201MB  28.8MB/s   00:07    \\nsftp> get radacct_2020-12-20_12-00.csv.bz2\\nFetching /home/prdts/radacct_2020-12-20_12-00.csv.bz2 to radacct_2020-12-20_12-00.csv.bz2\\n/home/prdts/radacct_2020-12-20_12-00.csv.bz2                                                                                                                              100%  202MB  25.2MB/s   00:08    \\nsftp> get radacct_2020-12-20_13-30.csv.bz2\\nFetching /home/prdts/radacct_2020-12-20_13-30.csv.bz2 to radacct_2020-12-20_13-30.csv.bz2\\n/home/prdts/radacct_2020-12-20_13-30.csv.bz2                                                                                                                              100%  202MB  28.8MB/s   00:07    \\nsftp> get radacct_2020-12-20_15-00.csv.bz2\\nFetching /home/prdts/radacct_2020-12-20_15-00.csv.bz2 to radacct_2020-12-20_15-00.csv.bz2\\n/home/prdts/radacct_2020-12-20_15-00.csv.bz2 \\nsftp> exit\\n```\\n\\n4. Check that /shared/radius_repo/radius_date.dat.local points to an older file:\\n``` bash\\n[intra@un2 cdrs]$ cat /shared/radius_repo/radius_date.dat.local\\n[File]\\nlatest_file=\\\"/shared/radius_repo/cdrs/radarchive_2019-08-14.csv.bz2\\\"\\n[intra@un2 cdrs]$ ll\\ntotal 2357692\\n-rw-r--r-- 1 intra intra 211140333 Dec 21 16:27 radacct_2020-12-20_04-30.csv.bz2\\n-rw-r--r-- 1 intra intra 211020434 Dec 21 16:28 radacct_2020-12-20_06-00.csv.bz2\\n-rw-r--r-- 1 intra intra 211125062 Dec 21 16:28 radacct_2020-12-20_07-30.csv.bz2\\n-rw-r--r-- 1 intra intra 210696825 Dec 21 16:28 radacct_2020-12-20_09-00.csv.bz2\\n-rw-r--r-- 1 intra intra 211175805 Dec 21 16:29 radacct_2020-12-20_10-30.csv.bz2\\n-rw-r--r-- 1 intra intra 211440564 Dec 21 16:29 radacct_2020-12-20_12-00.csv.bz2\\n-rw-r--r-- 1 intra intra 211670525 Dec 21 16:29 radacct_2020-12-20_13-30.csv.bz2\\n-rw-r--r-- 1 intra intra 211765933 Dec 21 16:29 radacct_2020-12-20_15-00.csv.bz2\\n-rw-r--r-- 1 intra intra 172240773 Jun 19  2019 radarchive_2019-06-12.csv.bz2\\n-rw-r--r-- 1 intra intra 162087027 Jul 30  2019 radarchive_2019-07-27.csv.bz2\\n-rw-r--r-- 1 intra intra 168362647 Aug 16  2019 radarchive_2019-08-14.csv.bz2\\n-rw-r--r-- 1 intra intra 216716584 Dec 21 16:27 radarchive_2020-12-19.csv.bz2\\n```\\nSince we want to load files newer than `/shared/radius_repo/cdrs/radarchive_2019-08-14.csv.bz2` we don't have to edit the file.\\n\\n5. Change configuration file of ingestion script.\\n```bash\\n[intra@un2 cdrs]$ vim /shared/abc/radius/DataParser/scripts/transferlist/radius.trn\\nDefault Status:\\n...\\nfile_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat\\\"\\n#-- local executions --\\n#file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat.local\\\"\\n...\\nWhen local file is used:\\n...\\n#file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat\\\"\\n#-- local executions --\\nfile_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat.local\\\"\\n...\\n```\\n\\n6. Execute ingestion scripts when making sure it is not executed at the moment. When scripts have finished, revert changes in the .trn file.\\n```bash\\n[intra@un2 cdrs]$ tail /shared/abc/radius/DataParser/scripts/log/radius_20201221.log\\n...\\n--------------END------------\\n[intra@un2 cdrs]$ /shared/abc/radius/DataParser/scripts/radius.pl -l -d -D -o >> /shared/abc/radius/DataParser/scripts/log/radius_cron_manual_20201221.log 2>&1\\n[intra@un2 cdrs]$ /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.manual_20201221.log 2>&1\\n[intra@un2 cdrs]$ vim /shared/abc/radius/DataParser/scripts/transferlist/radius.trn\\n```\\n\\n7. Check that files have been loaded to hist tables.\\n```bash\\n[u15@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radacct_hist/par_dt=20201220\\n...\\n-rwxrwx--x+  3 hive hive   65350341 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000013_2054504955_data.0.\\n-rwxrwx--x+  3 hive hive  134217741 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000014_1021750110_data.0.\\n-rwxrwx--x+  3 hive hive  134217750 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000028_924374267_data.0.\\n-rwxrwx--x+  3 hive hive  134217617 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000008_1102168495_data.0.\\n-rwxrwx--x+  3 hive hive  134217769 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000001b_924374267_data.0.\\n-rwxrwx--x+  3 hive hive  134217906 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000000b_1400128216_data.0.\\n-rwxrwx--x+  3 hive hive  134217799 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000005_183542376_data.0.\\n-rwxrwx--x+  3 hive hive  134217146 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000001d_1808120301_data.0.\\n-rwxrwx--x+  3 hive hive  134217812 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000000d_1400128216_data.0.\\n-rwxrwx--x+  3 hive hive  165905440 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000020_2125534478_data.0.\\n-rwxrwx--x+  3 hive hive  166171908 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000026_694498725_data.0.\\n-rwxrwx--x+  3 hive hive  166671557 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000009_1829852461_data.0.\\n-rwxrwx--x+  3 hive hive  134217919 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000027_1432252135_data.0.\\n-rwxrwx--x+  3 hive hive  134217610 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000006_975299616_data.0.\\n-rwxrwx--x+  3 hive hive  134217617 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000000c_361805242_data.0.\\n-rwxrwx--x+  3 hive hive  134217918 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000023_361805242_data.0.\\n-rwxrwx--x+  3 hive hive  134217774 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000002_361805242_data.0.\\n-rwxrwx--x+  3 hive hive  134217914 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000004_2125534478_data.0.\\n-rwxrwx--x+  3 hive hive  134217443 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000011_1541315014_data.0.\\n-rwxrwx--x+  3 hive hive  168449504 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000018_924374267_data.0.\\n-rwxrwx--x+  3 hive hive  134217940 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000007_975299616_data.0.\\n-rwxrwx--x+  3 hive hive  134217515 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000010_2101034182_data.0.\\n-rwxrwx--x+  3 hive hive  134217798 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000001_1541315014_data.0.\\n-rwxrwx--x+  3 hive hive  134217909 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000024_738850578_data.0.\\n-rwxrwx--x+  3 hive hive  134216978 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000017_361805242_data.0.\\n-rwxrwx--x+  3 hive hive  134217450 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000016_559180009_data.0.\\n[u15@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219\\n...\\n-rwxrwx--x+  3 hive hive  134217959 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000001_564661313_data.0.\\n-rwxrwx--x+  3 hive hive  134217962 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000002_564661313_data.0.\\n-rwxrwx--x+  3 hive hive   78601928 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000000_564661313_data.0.\\n-rwxrwx--x+  3 hive hive  134217573 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000006_1446698864_data.0.\\n-rwxrwx--x+  3 hive hive  134217391 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000004_929468556_data.0.\\n-rwxrwx--x+  3 hive hive  134217788 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000007_1446698864_data.0.\\n-rwxrwx--x+  3 hive hive  134218011 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000005_1259058186_data.0.\\n-rwxrwx--x+  3 hive hive  134217788 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000008_1446698864_data.0.\\n-rwxrwx--x+  3 hive hive  134217753 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de0000000a_1960091991_data.0.\\n-rwxrwx--x+  3 hive hive  134217789 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000003_2144981155_data.0.\\n-rwxrwx--x+  3 hive hive  134217904 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000009_1259058186_data.0.\\n-rwxrwx--x+  3 hive hive  134217360 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de0000000b_1446698864_data.0.\\n```\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer\\n\\n<b>Action Points:</b>\\n\\nSection `Radius` in https://edn2.bigdata.intranet.gr/abc/BigStreamer/cluster_monitoring/blob/master/future_steps/flows_applications.md\\n\\n",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is directly grounded in the context, which describes the exact problem (RADIUS data gaps during downtime), the file locations, ingestion script usage, and verification steps.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Reprocessing RADIUS data is critical for ensuring completeness and correctness in BigStreamer’s metrics pipelines, making this highly relevant for telecom engineers.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is self-contained and clearly articulates the scenario (reprocessing RADIUS data during downtime), with no vague references or reliance on prior turns."
    },
    {
      "question": "What should be done if an Agama schema table has missing data for a specific day?",
      "answer": "1. Locate the ingestion script `/shared/abc/agama/bin/<table>.sh` on `un2`.\n2. Edit the script and hardcode the date values:\n```bash\n##dt_sftp=2021/07/07\n##dt=20210707\n```\n3. Rerun the script manually.\n4. After execution, validate data in Impala:\n```sql\nSELECT COUNT(*), par_dt FROM agama.<table> WHERE par_dt >= '20210402' GROUP BY par_dt;\n```\n5. Revert the script to use dynamic values again.",
      "category": "Data Management & Query Execution",
      "context": "# abc - IM1500475 - agama schema missing data 02/04\\n\\n<b>Description:</b>\\n\\n```\\nGood evening,\\n\\nPlease investigate if primary data has been created for 02/04/2021 and if so upload it.\\n\\nThank you\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. ssh with you personal ldap account at `un2` from `admin`\\n2. sudo -iu intra\\n3. cronta -l | grep agama\\n4. Check logs of the script that data is missing.\\n5. Also check if the files exist at `sftp_server:directory`\\n6. If files exist modify script for the <table> un2:/shared/abc/agama/bin/`table`.sh\\n7. comment `yest_sftp`,`yest`,`dt_sftp`,`dt`. Uncomment the static `dt_sftp`,`dt` (e.g `##dt_sftp=2021/07/07 ##dt=20210707`)\\n8. Run the script and when is finished connnect to impala-sheel `select count(*),par_dt from agama.table where par_dt >= '2021xxxx' group by 2;`\\n9. Edit again the script on un2 with the default values to run tomorrow\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context clearly describes the exact steps taken to resolve missing data in an Agama schema table, including script modification and validation. The answer is fully supported by the context.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This is a highly relevant operational task in BigStreamer, ensuring data consistency in ingestion pipelines, which is essential for analytics and downstream processing.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clearly formulated and self-contained, specifying the table, the issue (missing data), and the timeframe, making it easy to understand without additional context."
    },
    {
      "question": "How do you detect and restore a missing hourly RADIUS file that was incorrectly marked as processed?",
      "answer": "1. Use hourly row counts from `radacct_hist` to identify anomalies.\n2. Search cron logs like:\n```bash\ngrep -B5 Modified /shared/abc/radius/log/000_radius_ops.20210506.log\n```\n3. If the file exists in `radacct_orig_files`, copy it to load:\n```bash\nhdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/... /ez/warehouse/radius.db/radacct_load/\n```\n4. Reprocess with:\n```bash\n/shared/abc/radius/bin/000_radius_ops.sh\n```",
      "category": "Data Management & Query Execution",
      "context": "# abc - BigStreamer - GI3 -  Radius : Procedure for Missing Data\\n\\n\\n<b>Description:</b>\\n\\n```\\nTicket/Question for Missing Data at Radius Flow\\n```\\n\\n\\n<b>Actions Taken:</b>\\n\\n\\n**Question** :  Does  **radius.radacct_orig_files** table contains missing data ?\\n\\n   **Compare files from SFTP Repository with files in Table**\\n\\n   e.g\\n\\n **Sftp Repo Contents :**\\n\\n```\\n-rw-r--r-- 1 intra intra 219749225 May  6 11:40 radacct_2021-05-04_00-00.csv.bz2\\n-rw-r--r-- 1 intra intra 219497773 May  6 11:40 radacct_2021-05-04_01-30.csv.bz2\\n-rw-r--r-- 1 intra intra 219166609 May  6 11:40 radacct_2021-05-04_03-00.csv.bz2\\n-rw-r--r-- 1 intra intra 219090980 May  6 11:40 radacct_2021-05-04_04-30.csv.bz2\\n-rw-r--r-- 1 intra intra 218865632 May  6 11:40 radacct_2021-05-04_06-00.csv.bz2\\n-rw-r--r-- 1 intra intra 219100909 May  6 11:41 radacct_2021-05-04_07-30.csv.bz2\\n-rw-r--r-- 1 intra intra 219262945 May  6 11:41 radacct_2021-05-04_09-00.csv.bz2\\n-rw-r--r-- 1 intra intra 219734952 May  6 11:41 radacct_2021-05-04_10-30.csv.bz2\\n-rw-r--r-- 1 intra intra 219753745 May  6 11:41 radacct_2021-05-04_12-00.csv.bz2\\n-rw-r--r-- 1 intra intra 219985878 May  6 11:41 radacct_2021-05-04_13-30.csv.bz2\\n-rw-r--r-- 1 intra intra 220428037 May  6 11:41 radacct_2021-05-04_15-00.csv.bz2\\n-rw-r--r-- 1 intra intra 220573605 May  6 11:42 radacct_2021-05-04_16-30.csv.bz2\\n-rw-r--r-- 1 intra intra 220440718 May  6 11:42 radacct_2021-05-04_18-00.csv.bz2\\n-rw-r--r-- 1 intra intra 220170325 May  6 11:42 radacct_2021-05-04_19-30.csv.bz2\\n-rw-r--r-- 1 intra intra 220153678 May  6 11:42 radacct_2021-05-04_21-00.csv.bz2\\n-rw-r--r-- 1 intra intra 220329041 May  6 11:42 radacct_2021-05-04_22-30.csv.bz2\\n```\\n\\n**radius.radacct_orig_files**\\n\\nCommand : show files in <table>\\n\\n```\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_00-00.csv.20210504_001006.utc | 823.99MB |           |\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_01-30.csv.20210504_021002.utc | 822.76MB |           |\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_03-00.csv.20210504_031002.utc | 821.74MB |           |\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_04-30.csv.20210504_051001.utc | 821.36MB |           |\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_06-00.csv.20210504_061002.utc | 821.05MB |           |\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_07-30.csv.20210504_081003.utc | 821.63MB |           |\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_09-00.csv.20210504_091002.utc | 822.62MB |           |\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_19-30.csv.20210504_221010.utc | 825.45MB |           |\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_21-00.csv.20210504_221010.utc | 825.52MB |           |\\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_22-30.csv.20210504_231007.utc | 825.74MB |           |\\n```\\n\\n---\\n\\n**Answer (Yes ) :**\\n \\nIf files exist at **radius.radacct_orig_files** \\n\\na. How to Detect Missing Files \\n\\n  1. Checking Hourly Data at radacct_hist table . \\n\\n\\n| hour| rows |\\n| ------ | ------ |\\n| ..| .. |\\n| 16 | 1489477 |\\n| 17             | 1441560 |\t\t\\n| **18**             | **732**     |\t\t\\n| **19**             | **739837**  |\t\\n| 20             | 1450106 |\t\\n| ..| .. |\\n\\nIn this case missing file is the one with timestamp after the last hour with less data : ( 1930 ) \\n\\n\\n  2. Checking the OPS cron log file \\n\\n    File : **/shared/abc/radius/log/000_radius_ops.20210506.log**\\n  \\n -   Normal Entry (1) ( file exists , impala table populated ) :\\n\\n ```\\n INFO: 2021-05-06 05:14:36 --> insert into radius.RADACCT_HIST completed. 2174145 rows\\n INFO: 2021-05-06 05:14:36 --> HDFS:Clean-up RAD___radacct_*.utc files\\n hdfs dfs -rm -skipTrash /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_04-30.csv.20210506_051001.utc 2>/dev/null\\n Deleted /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_04-30.csv.20210506_051001.utc\\n ```\\n \\n  -   Normal Entry (2) (No file exists , impala table not populated ) :\\n \\n ```\\n INFO: 2021-05-06 07:11:01 --> insert into radius.RADACCT_HIST completed. 0 rows\\n INFO: 2021-05-06 07:11:01 --> HDFS:Clean-up RAD___radacct_*.utc files\\n ```\\n \\n - Abnormal Entry : ( File exists , Impala table NOT Populated ) \\n \\n \\n ```\\n INFO: 2021-05-06 08:13:02 --> insert into radius.RADACCT_HIST\\n INFO: 2021-05-06 08:13:26 --> insert into radius.RADACCT_HIST completed. 0 rows\\n INFO: 2021-05-06 08:13:26 --> HDFS:Clean-up RAD___radacct_*.utc files\\n hdfs dfs -rm -skipTrash /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_07-30.csv.20210506_081001.utc 2>/dev/null\\n Deleted /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_07-30.csv.20210506_081001.utc\\n ```\\n \\n  - **completed. 0 rows**\\n  - File **RAD___radacct_2021-05-06_07-30.csv.20210506_081001.utc** should be copied \\n \\n b. Copy missing hdfs files from radacct_orig_files to radacct_load\\n \\n ```\\n   hdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_20xx-yy-zz_rr-mm.csv.yyyymmdd_hhmmss.utc /ez/warehouse/radius.db/radacct_load/\\n  ....\\n ```\\n \\n c. Execute post script :\\n \\n \\n ```\\n /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.`date '+\\\\%Y\\\\%m\\\\%d'`.manually.log\\n ```\\n \\n \\n ---\\n **Answer (No ) :**  \\n \\n If files do not exist at **radius.radacct_orig_files** , execute following steps : \\n \\n \\n 1. sftp Requested  Files from radius sftp server\\n \\n 2. mv files to\\n \\n   [Remfgh]\\n `local_spool_area=\\\"/shared/radius_repo/cdrs\\\"`\\n \\n 3. Modify file  **radius.trn** :\\n \\n Default Status :\\n \\n ```\\n file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat\\\"\\n #-- local executions --\\n #file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat.local\\\"\\n ```\\n \\n When local file is used :\\n \\n ```\\n  #file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat\\\"\\n  #-- local executions --\\n  file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat.local\\\"\\n ```\\n \\n 4.Check if entry @ **/shared/radius_repo/radius_date.dat.local** , exist\\n \\n e.g\\n \\n ```\\n [File]\\n latest_file=\\\"/shared/radius_repo/cdrs/radarchive_2019-06-12.csv.bz2\\\"\\n ```\\n \\n and it is older than the new files arrived .\\n If not , create a dummy file on /shared/radius_repo/cdrs/ with older date from new files , and update /shared/radius_repo/radius_date.dat.local accordingly\\n \\n 5.Then execute the respective commands :\\n \\n ```\\n -  /shared/abc/radius/DataParser/scripts/radius.pl  -l -d -D -o >> /shared/abc/radius/DataParser/scripts/log/radius_cron_manual.log  2>&1\\n - /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.manual.log 2>&1\\n ```\\n \\n \\n 6. Rollback trn file to initial mode :\\n \\n \\n ```\\n file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat\\\"\\n #-- local executions --\\n #file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat.local\\\"\\n ```\\n \\n ---\\n **Important :**\\n \\n **All above actions should not be performed during scheduled crontab job for Radius ( Usually xx:10 ~ xx:15 ) \\n Check respective cron logs before manual executions described above .**\\n \\n ---\\n **Useful SQL statements :**\\n \\n ```\\n select hour(acctupdatetime) acctupdatetime, count(*)\\n  cnt\\n from radius.radacct_orig_files where acctupdatetime like '2021-05-04%'\\n group by 1 order by 1;\\n ```\\n \\n ```\\n select hour(acctupdatetime) acctupdatetime, count(*)\\n  cnt\\n from radius.radacct_hist where par_dt='20210504'\\n group by 1 order by 1;\\n ```\\n \\n <b>Affected Systems:</b>\\n \\n abc Bigstreamer Radius\\n \\n <b>Action Points:</b>\\n \\n ```\\n Verufy  Procedure Execution  by using mentioned Sql Statements\\n ```",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is fully answerable using the provided context. The context outlines how to detect missing data using row counts and logs, and how to recover it from archived sources using HDFS commands and ingestion scripts.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question is highly relevant to telecom engineers maintaining data integrity in BigStreamer, particularly in the Radius flow where hourly data completeness is critical for accurate analysis and operations.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is specific and self-contained. It clearly asks about detection and recovery steps for a missing RADIUS file marked as processed, requiring no additional context to understand the problem."
    },
    {
      "question": "What configuration changes were made to fix missing data in the `open_weather_map.openweathermap_final` table in Streamsets?",
      "answer": "To address missing entries in `open_weather_map.openweathermap_final`, the following changes were made in Streamsets:\n- On the SFTP FTP Client tab: increased `Max Batch Size(records)` from `1000` to `100000`, and `Batch Wait Time(ms)` from `1000` to `30000`.\n- On Hadoop-FS 1 and 2: increased `Idle Timeout` from `${1 * MINUTES}` to `${5 * MINUTES}`.",
      "category": "Application Functionality & Flow",
      "context": "# abc - BigStreamer - IM1599907 - Streamsets : Missing Data - open_weather_map.openweathermap_final\\n\\n\\n        <b>Description:</b>\\n        \\n        ```\\n        Good morning,\\n        as of 6/24 open_weather_map.openweathermap_final has very few entries daily. Please check the stream.\\n        ```\\n        \\n        <b>Actions Taken:</b>\\n        \\n        1. Changed on the `SFTP FTP Client bullet(Tab)` `Max Batch Size(records)` from `1000` to `100000` and `Batch Wait Time(ms)` from `1000` to `30000`\\n        2. On the `Hadoop-FS 1` and `Hadoop-FS 2` bullets changed the `Idle Timeout` from `${1 * MINUTES}` to `${5 * MINUTES}`\\n        \\n        \\n        <b>Affected Systems:</b>\\n        \\n        abc Bigstreamer Streamsets\\n        \\n        <b>Action Points:</b>\\n        \\n        Reference from devs:\\n        https://metis.xyztel.com/obss/bigdata/abc/devops/devops-projects/-/issues/58#nfgh_44105",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The answer is directly and fully supported by the context, which explicitly lists the configuration changes that were applied to resolve the issue with missing data in the Streamsets pipeline.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The question is highly relevant for telecom engineers working with BigStreamer, as it concerns debugging and resolving data ingestion issues in a real-time ETL pipeline—a critical operational task.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear, unambiguous, and self-contained. It specifies the affected table and technology (Streamsets), making it understandable without needing additional context."
    },
    {
      "question": "How do you re-run the Huawei TV flow manually for a date with missing partitions?",
      "answer": "Missing data for 20210831 in `huawei_tv.rel_play_tv_hist`, `rel_play_tvod_hist`, `rel_play_vod_hist`, and `rel_vod_info_hist` can be restored by rerunning the loader script:\n```bash\n/shared/abc/huawei_tv/bin/huawei_tv_load.sh 20210831\n```\nBefore that, confirm data existence using:\n```sql\nSELECT count(*), par_dt FROM huawei_tv.rel_vod_info_hist WHERE par_dt BETWEEN '20210825' AND '20210905' GROUP BY par_dt;\n```",
      "category": "Application Functionality & Flow",
      "context": "# abc - IM1631218 - huawei tv  \\n\\n<b>Description:</b>\\n\\n```\\nGood morning, please load the tables huawei_tv.rel_play_tv_hist, huawei_tv.rel_play_tvod_hist, huawei_tv.rel_play_vod_hist for 31/8-1/9 and huawei_tv.rel_vod_info_hist for 1/9 with data.\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to Hue and go to `Editor` > `Impala`\\n2. Check for missing partitions as stated in description\\n```bash\\nselect count(*), par_dt \\nfrom huawei_tv.rel_vod_info_hist  \\nwhere par_dt between '20210825' and '20210905' \\ngroup by par_dt \\norder by par_dt;\\nResult:\\ncount(*)\tpar_dt\t\\n...\\n15026\t20210830\t\\n14728\t20210901\t\\n14748\t20210902\t\\n...\\n```\\n\\n3. Partition for `20210831` is actually missing but `20210901` is here and seems fine.\\n\\n4. Check logs of flow but first login to `un2`. And yes, huawei tv loads data for yesterday's partition from today's table #crazyright\\n\\n``` bash\\n$ su - intra\\n$ cd /shared/abc/huawei_tv/\\n$ less log/huawei_tv_load.20210901.log\\n...\\nSFTP get files from : ./export/20210901\\nConnected to 172.28.128.150.\\nsftp> get export/20210901/*.csv /data/1/huawei_tv_LZ/\\nFetching /export/20210901/EPG_SCHEDULE.csv to /data/1/huawei_tv_LZ/EPG_SCHEDULE.csv\\nFetching /export/20210901/REL_PLAY_TV.csv to /data/1/huawei_tv_LZ/REL_PLAY_TV.csv\\nFetching /export/20210901/REL_PLAY_TVOD.csv to /data/1/huawei_tv_LZ/REL_PLAY_TVOD.csv\\nFetching /export/20210901/REL_PLAY_VOD.csv to /data/1/huawei_tv_LZ/REL_PLAY_VOD.csv\\nFetching /export/20210901/REL_VOD_INFO.csv to /data/1/huawei_tv_LZ/REL_VOD_INFO.csv\\n\\n```\\n\\n5. Let's inspect those files\\n```bash\\n$ sftp bigdata@172.28.128.150:/export\\nsftp> cd 20210901\\nsftp> ls -l\\n-rw-r--r--    1 0        0          476541 Sep  1 05:13 EPG_SCHEDULE.csv\\n-rw-r--r--    1 0        0               0 Sep  1 05:11 REL_PLAY_TV.csv\\n-rw-r--r--    1 0        0               0 Sep  1 05:10 REL_PLAY_TVOD.csv\\n-rw-r--r--    1 0        0               0 Sep  1 05:11 REL_PLAY_VOD.csv\\n-rw-r--r--    1 0        0         3470903 Sep  1 05:11 REL_VOD_INFO.csv\\n-rw-r--r--    1 0        0        11414886 Sep  1 05:13 SubscriberID_STBMACAddress_Relationship.csv\\n```\\nAs it is obvious, there are empty files, so abc needs to reload data into the remfgh server and we need to run the script for the missing partition:\\n```bash\\n$ /shared/abc/huawei_tv/bin/huawei_tv_load.sh 20210831\\n```\\n\\n\\n<b>Affected Systems:</b>\\n\\nabc Bigstreamer\\n\\n<b>Action Points:</b>\\n\\nWe have informed abc about each case and wait for their response.\\nPartition `20210901` is fine but `20210831` needs manual action as nfghd in step 5.",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The question is directly and fully answerable using the context, which includes both the missing partition, the script to re-run, and the process for verifying data presence.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "The task is highly relevant to real-world operations within BigStreamer, addressing a common data engineering challenge—recovering from missed ingestion runs for specific days.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clearly phrased, includes all relevant information, and can be understood without relying on additional background."
    },
    {
      "question": "How do you recover the `radius.radarchive_hist` table when a file is missing or corrupted?",
      "answer": "1. Connect to the SFTP server and re-download the missing file (e.g., `radarchive_2021-08-31.csv.bz2`).\n2. Place it in `/shared/radius_repo/cdrs`.\n3. Adjust `radius.trn` to use the `.dat.local` file for custom load.\n4. Ensure a dummy file older than the target exists in `/shared/radius_repo/cdrs`.\n5. Run:\n```bash\n/shared/abc/radius/DataParser/scripts/radius.pl -l -d -D -o\n/shared/abc/radius/bin/000_radius_ops.sh\n```",
      "category": "Troubleshooting & Issue Resolution",
      "context": "abc\\n\\nBigStreamer\\n\\nSyslog(un2)\\n\\nIssue Number: IM1629405 \\n\\nTitle:  radius.radarchive_hist  missing data\\n\\nDescription: \\n\\nPlease check why no data has been loaded into the table for 08/31\\n\\nUrgency \t2 - High\\n\\n\\n<b>Actions Taken:</b>\\n\\n0. refresh table via impala-shell:\\n\\n[un-vip.bigdata.abc.gr:21000] > refresh  radius.radarchive_hist;\\n\\n[un-vip.bigdata.abc.gr:21000] > select count(*),par_dt from radius.radarchive_hist  where par_dt = '20210831' group by par_dt; \\n\\nIf 'refresh' does not resolve the issue, then proceed ass below:\\n\\n1. intra crontab in un2:\\n\\n10 * * * *  /shared/abc/radius/DataParser/scripts/radius.pl -d -D -o >> /shared/abc/radius/DataParser/scripts/log/radius_cron.log  2>&1;/shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.`date '+\\\\%Y\\\\%m\\\\%d'`.log 2>&1;/shared/abc/radius/bin/002_wfm_radius_traffic_day.sh >> /shared/abc/radius/log/002_wfm_radius_traffic_day.`date '+\\\\%Y\\\\%m\\\\%d'`.log 2>&1\\n\\n\\n2. Script #1 : /shared/abc/radius/DataParser/scripts/radius.pl\\n3. Log #1    : /shared/abc/radius/DataParser/scripts/log/radius_cron.log\\n\\n4. Script #2 : /shared/abc/radius/bin/000_radius_ops.sh \\n5. Log #2    : /shared/abc/radius/log/002_wfm_radius_traffic_day.20210831.log\\n\\n\\n6. check Sftp server :\\n7. as 'intra'\\n8. [intra@un2 ~]$ sftp intra@79.128.184.153\\n\\n9. check if file exists and size eg :\\n10. sftp> ls -l  radarchive_2021-08-31.csv.bz2\\n11. -rw-r--r--    0 0        0              14 Sep  1 04:32 radarchive_2021-08-31.csv.bz2\\n\\n12. transfer it locally :\\n13. sftp> get  radarchive_2021-08-31.csv.bz2\\n14. exit\\n\\n15. extract it :\\n16. [intra@un2 ~]$ bzip2 -d  radarchive_2021-08-31.csv.bz2\\n\\n16. check it :\\n18. [intra@un2 ~]$ ll  radarchive_2021-08-31.csv\\n19. -rw-r--r-- 1 intra intra 0 Sep  1 12:21 radarchive_2021-08-31.csv\\n\\n\\n\\nTO LOAD THE MISSING Data (upon re-creating the data by abc)\\n--------------------------------------------------------------------\\n\\n\\nThe following procedure describes loading radius data.\\n\\nThe process is common for both file types\\n\\n    radararchive_hist\\n    radacct_hist\\n\\nDepending on the type of missing data, we also load the corresponding files.\\n\\nFor radararchive_hist one archive is produced per day. \\n\\nWhile for radacct_hist more.\\n\\n echo \\\"ls -ltr rada*.csv.bz2\\\" | sftp prdts@79.128.184.153 | tail -n10\\n\\n   183791490 Jul 27 10:30 radacct_2019-07-27_10-30.csv.bz2\\n\\n   184011607 Jul 27 12:00 radacct_2019-07-27_12-00.csv.bz2\\n\\n   184124420 Jul 27 13:30 radacct_2019-07-27_13-30.csv.bz2\\n\\n   184099607 Jul 27 15:00 radacct_2019-07-27_15-00.csv.bz2\\n\\n   184046108 Jul 27 16:30 radacct_2019-07-27_16-30.csv.bz2\\n\\n   184108503 Jul 27 18:00 radacct_2019-07-27_18-00.csv.bz2\\n\\n   184117618 Jul 27 19:30 radacct_2019-07-27_19-30.csv.bz2\\n\\n   183999927 Jul 27 21:00 radacct_2019-07-27_21-00.csv.bz2\\n\\n   184176100 Jul 27 22:30 radacct_2019-07-27_22-30.csv.bz2\\n\\n   162087027 Jul 28 04:32 radarchive_2019-07-27.csv.bz2\\n\\n\\nMoP for reloading missing data\\n\\nThe Case\\n\\nMissing data: radarchive\\n\\nMissing date: 2019-07-27\\n\\n\\nThe following actions will be performed by UN2 as an intra user\\n\\n1) Check TRN file\\n\\n/shared/abc/radius/DataParser/scripts/transferlist/radius.trn\\n\\n[Remfgh]\\n\\n;SFTP Server Settings\\n\\naddress=\\\"79.128.184.153\\\"\\n\\nprotocol=\\\"SFTP\\\"\\n\\nport=\\\"22\\\"\\n\\nusername=\\\"prdts\\\"\\n\\nspool_area=\\\"/home/prdts\\\"\\n\\nmove_area=\\\"\\\"\\n\\nfile_type=\\\"rada*\\\"\\n\\nsuffix=\\\".csv.bz2\\\"\\n\\nload_suffix=\\\"LOADED\\\"\\n\\nsuffix_tmp=\\\".tmp\\\"\\n\\n3) Check if the missing files still exist on the remfgh server\\n\\n$ sftp prdts@79.128.184.153\\n\\nsftp> cd /home/prdts\\n\\nsftp> ls -ltr radarchive*2019-07-27.csv.bz2\\n\\n    ...\\n    Jul 27 04:33 radarchive_2019-07-26.csv.bz2\\n\\n    Jul 28 04:32 radarchive_2019-07-27.csv.bz2\\n    ...\\n\\n\\n\\n4) get the missing files\\n\\n$ sftp prdts@79.128.184.153\\n\\nsftp> lcd /shared/radius_repo/cdrs (@un2)\\n\\nsftp> cd /home/prdts\\n\\nsftp> get radarchive_2019-07-27.csv.bz2\\n\\n\\n5) Change TRN file\\n\\n    #file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat\\\" --Production Load\\n\\n     file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat.local\\\" --Customade Load\\n\\n\\nAttention:\\n\\nthe file /shared/radius_repo/radius_date.dat.local should contain the filename of a file with an older date than the one to be loaded.\\n\\nfor example:\\n$ cat /shared/radius_repo/radius_date.dat.local\\n\\n    [File]\\n    latest_file=\\\"/shared/radius_repo/cdrs/radarchive_2019-06-12.csv.bz2\\\"\\n\\nIf we want to load the data of the file radarchive_2019-07-27.csv.bz2 then \\nTHERE SHOULD BE A SIMILAR FILE WITH AN OLDER DATE IN THE SAME DIRECTORY\\n\\nFile_a) Jun 19 12:38 radarchive_2019-06-12.csv.bz2 <--older file or dummy file\\n\\nFile_b) Jul 30 10:38 radarchive_2019-07-27.csv.bz2 <--file to load\\nFile_a will simply be used as a reference point to load all files later than it.\\n\\n\\nIf we don't have such a file, we can alternatively create a dummy file.\\n\\nThe following command creates a dummy file based on the timestamp of the regular file by subtracting 1 hour\\n\\n$ touch -r radarchive_2019-07-27.csv.bz2 -d '-1 hour' radarchive_2019-07-26.csv.bz2\\n\\n    $ ls -ltr\\n    -rw-rw-r-- 1 intra intra         0 Jul 29 09:38 radarchive_2019-07-26.csv.bz2\\n    -rw-r--r-- 1 intra intra 162087027 Jul 30 10:38 radarchive_2019-07-27.csv.bz2\\n\\n\\n\\n6) Loading of missing data\\n\\nwe serially execute the following.\\n\\nWe change the LOAD date in the log files\\n\\n\\n    /shared/abc/radius/DataParser/scripts/radius.pl  -l -d -D -o > /shared/abc/radius/DataParser/scripts/log/MAN.radius_cron.20190727.log  2>&1\\n\\n\\n    /shared/abc/radius/bin/000_radius_ops.sh > /shared/abc/radius/log/MAN.000_radius_ops.20190727.log 2>&1\\n\\n\\n7) check partitions\\n\\n\\n    $ bee -e \\\"show partitions radius.radarchive_hist;\\\"\\n\\n    $ bee -e \\\"show partitions radius.radacct_hist;\\\"\\n\\n    +------------------+\\n\\n    |    partition     |\\n\\n    +------------------+\\n\\n    | par_dt=20190726  |\\n\\n    | par_dt=20190727  | <-- Loaded\\n\\n    | par_dt=20190728  |\\n\\n    | par_dt=20190729  |\\n\\n    +------------------+\\n\\n\\n8) Rollback Changes in TRN file\\n\\nrevert the changes of step 5)\\n\\n     file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat\\\" --Production Load\\n\\n    #file_latest_timestamp=\\\"/shared/radius_repo/radius_date.dat.local\\\" --Custom Load\\n\\n\\n\\n20. In the above case, the file was of zero size and reported to abc.",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The answer is fully grounded in the context, which describes in detail the steps required to reload missing or corrupted files into the `radius.radarchive_hist` table, including recovery from the SFTP server, configuration adjustments, and manual job execution.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "This question is highly relevant for telecom engineers working with BigStreamer, especially those responsible for maintaining data integrity in historical RADIUS datasets. It directly addresses operational recovery procedures.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clear, concise, and unambiguous. It specifies the exact table and issue, allowing it to be understood and answered independently of the context."
    },
    {
      "question": "How can corrupted statistics in Impala tables like `sai.sms_raw` and `brond.td_dslam_week` be fixed?",
      "answer": "Use Impala shell to recompute or refresh statistics:\n```sql\ncompute stats sai.sms_raw;\nrefresh sai.sms_raw;\n```\nIf compute fails, use refresh instead. Avoid running compute stats on live-updated partitions, as they receive data every 5 minutes. For production-quality statistics, a dedicated project should be scoped.",
      "category": "Troubleshooting & Issue Resolution",
      "context": "<b>IM1630642</b>\\n\\n<b>Title :\tmissing or corrupted statistics </b>\\n\\n<b>Description:</b>\\n\\n```\\nGood evening,\\nPlease for your actions\\n\\nWARNING: The following tables have pfghntially corrupt table statistics. Drop and re-compute statistics to resolve this problem. -> \\n\\nsai.voice_quality_hist\\n\\nsai.sms_raw, \\n\\nsai.voice_raw\\n\\nbrond.brond_retrains_hist,\\n\\nbrond.td_dslam_week,\\n\\nookla.ookla_android,\\n\\nookla.ookla_ios,\\n\\nookla.ookla_stnet\\n\\ntemip.temip_impala_terminated_alarms,\\n\\ntemip.temip_kudu_terminated_alarms\\n```\\n\\n\\n<b>Actions Taken:</b>\\n\\n<b>Check tables hdfs status , un2 as intra :</b>\\n\\n hdfs dfs -du -h -s /ez/warehouse/sai.db/voice_quality_hist\\n\\n hdfs dfs -du -h -s /ez/warehouse/brond.db/brond_retrains_hist\\n\\n hdfs dfs -du -h -s /ez/warehouse/brond.db/td_dslam_week\\n\\n hdfs dfs -du -h -s /ez/warehouse/ookla.db/ookla_android\\n\\n hdfs dfs -du -h -s /ez/warehouse/ookla.db/ookla_ios\\n\\n hdfs dfs -du -h -s /ez/warehouse/ookla.db/ookla_stnet\\n \\n\\n\\n<b>Check current stats, from un2 as intra via impala shell:</b>\\n\\n show table stats  sai.voice_quality_hist               ;\\n\\n show table stats  sai.sms_raw                          ;\\n\\n show table stats  sai.voice_raw                        ;\\n\\n show table stats  brond.brond_retrains_hist            ;\\n\\n show table stats  brond.td_dslam_week                  ;\\n\\n show table stats  ookla.ookla_android                  ;\\n\\n show table stats  ookla.ookla_ios                      ;\\n\\n show table stats  ookla.ookla_stnet                    ;\\n\\n show table stats  temip.temip_impala_terminated_alarms ;\\n\\n show table stats  temip.temip_kudu_terminated_alarms   ;\\n\\n\t \\n\\n\\n<b>Recompute stats, from un2 as intra via impala shell:</b>\\n\\n compute stats   sai.voice_quality_hist               ;\\n\\n compute stats   sai.sms_raw                          ;\\n\\n compute stats   sai.voice_raw                        ;\\n\\n compute stats   brond.brond_retrains_hist            ;\\n\\n compute stats   brond.td_dslam_week                  ;\\n\\n compute stats   ookla.ookla_android                  ;\\n\\n compute stats   ookla.ookla_ios                      ;\\n\\n compute stats   ookla.ookla_stnet                    ;\\n\\n compute stats   temip.temip_impala_terminated_alarms ;\\n\\n compute stats   temip.temip_kudu_terminated_alarms   ;\\n\\n<b>ΝΟΤΕ:</b>\\nIf there is a failure in any aof the tables, run 'refresh' on this table.\\n\\n\\n\\n<b>Refresh stats, from un2 as intra via impala shell:</b>\\n\\n refresh   sai.voice_quality_hist               ;\\n\\n refresh   sai.sms_raw, sai.voice_raw           ;\\n\\n refresh   brond.brond_retrains_hist            ;\\n\\n refresh   brond.td_dslam_week                  ;\\n\\n refresh   ookla.ookla_android                  ;\\n\\n refresh   ookla.ookla_ios                      ;\\n\\n refresh   ookla.ookla_stnet                    ;\\n\\n refresh   temip.temip_impala_terminated_alarms ;\\n\\n refresh   temip.temip_kudu_terminated_alarms   ;\\n\\n\\n\\n---------------- Below comment from Dionysia in tiket ------------------------------\\n------------------------------------------------------------------------------------\\n\\n\\nAfter investigating the tables (sai.voice_quality_hist, sai.sms_raw, sai.voice_raw) we saw that we calculate statistics every morning for the previous day's data. These tables seem to fill up throughout the day, so the stats don't appear to be up to date.\\nThe statistics refer to data in partitions of previous days.\\n\\n- In terms of performance, it is not possible to run compute statistics on partitions that continuously receive data during the day.\\nThere are streams that load data every 5 minutes.\\nDoing so would significantly delay data processing.\\n\\n- Obviously, the queries that are executed include the current partition and that is why the Warning for missing statistics is displayed.\\nAttached you will find an example for sai.sms_raw for different par_dt, in which it appears that for historical data no warning is displayed.\\n\\nIf statistics are needed for specific tables, we should consider it as a project and not under Support. We should consider how the performance of the cluster is affected and make the right design, so that we can agree on the frequency that the statistics should be calculated.\\n\\n\\n<b>Title :\tfurther actions have also been made </b>",
      "question_groundedness_score": "5",
      "judge_feedback_question_groundedness": "The context includes precise details about the corrupted statistics, the affected tables, and the corrective actions (compute or refresh stats), fully supporting the answer.",
      "question_relevance_score": "5",
      "judge_feedback_question_relevance": "Corrupted or outdated statistics directly affect query performance in Impala. Fixing them is critical in BigStreamer operations, making the question highly relevant.",
      "question_standalone_score": "5",
      "judge_feedback_question_standalone": "The question is clearly formulated, naming specific Impala tables and the nature of the problem, and does not require external context to understand."
    }
  ]
}