{
    "qa_pairs": [
        {
            "question": "What is the correct procedure to resolve a failed EXPORT job in DWH_IBank caused by a duplicate key in the loan payment data?",
            "answer": "To resolve the EXPORT job failure due to a duplicate key, first check for duplicate entries in `prod_trlog_ibank_analytical.dwh_details_loan_payment_stg` using Impala Shell. Then, create backup and temp tables, remove the duplicate entry by ID and timestamp, and overwrite the original table. Finally, rerun the export script:\\n```bash\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\\n```",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# mno - BigStreamer - IM2070630 - Failed batch Job on Grafana\\n\\n<b>Description:</b>\\n\\n```bash\\nToday 26/1/2023 the following batch job failure appeared on Grafana\\n\\nApplication: DWH_IBank\\nJob Name: EXPORT\\nComponent: LOAN_PAYMENT\\nStatus: Failed\\nDescription: Code:1\\nThanks\\n```\\n\\n<b>Actions Taken:</b>\\n\\n<b>Root Cause Analysis:</b>\\n\\nAnalysis was performed in collaboration with @lmn and @iaravant\\n\\n1. Check MapReduce logs from YARN UI - App Name: PROD_IBank_DWH_EXPORT_LoanPaymentDetails_*\\n\\n   ![IM2070630_yarn_app](.media/IM2070630_yarn_app.png)\\n\\n   ![IM2070630_yarn_mapreduce](.media/IM2070630_yarn_mapreduce.png)\\n\\n   There was a duplicate entry in prod_trlog_ibank_analytical.dwh_details_loan_payment_stg.\\n   Duplicate Key Value: 0E86AF89-F15C-4B78-8925-08ED8D237805\\n\\n2. Check the tables from Impala Shell with PRODUSER\\n\\n   ```bash\\n   [PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\\n   ```\\n\\n   Check for duplicates in dwh_details_loan_payment_stg and dwh_details_loan_payment with id=0E86AF89-F15C-4B78-8925-08ED8D237805\\n\\n   ![IM2070630_details_duplicates](.media/IM2070630_details_duplicates.PNG)\\n\\n   Check service_audit specifically for id=0E86AF89-F15C-4B78-8925-08ED8D237805\\n\\n   ![IM2070630_service_audit](.media/IM2070630_service_audit.PNG)\\n\\n   Check service_audit for duplicates\\n\\n   ![IM2070630_service_audit_monthly](.media/IM2070630_service_audit_monthly.PNG)\\n\\n   Service_audit contained two entries with the same id=0E86AF89-F15C-4B78-8925-08ED8D237805 and different timestamps.\\n\\n<b>Resolution:</b>\\n\\nSolution provided by @fgh and @adrint\\n\\n```bash\\n[PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\\n```\\n\\n```bash\\n# == DWH LoanPayment ==\\n\\n# Create table with original data\\ncreate table prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig like prod_trlog_ibank_analytical.dwh_details_loan_payment;\\n\\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt=20230125;\\n\\n# Create table and insert only required data\\ncreate table prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp like prod_trlog_ibank_analytical.dwh_details_loan_payment;\\n\\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt = 20230125 and id != '0E86AF89-F15C-4B78-8925-08ED8D237805';\\n\\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt = 20230125 and id = '0E86AF89-F15C-4B78-8925-08ED8D237805' and tr_timestamp = '202301025 09:38:13.072489000' limit 1;\\n\\n# Overwrite normal table with correct data\\ninsert overwrite prod_trlog_ibank_analytical.dwh_details_loan_payment partition(par_dt) select * from prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp where par_dt = 20230125;\\n```\\n\\n```bash\\n# Run Export procedure \\n/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\\n```\\n\\n```bash\\n[PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\\n```\\n\\n```bash\\n# Drop temporary table\\ndrop prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp purge\\n\\n# TODO Check data and drop backup table with initial orginal data\\ndrop prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig purge\\n```\\n\\n<b>Our Ticket Response:</b>\\n\\n```bash\\nGood evening,\\n\\nAfter investigation, it was determined that there is a problem with two different transactions that occurred on 25/01/2023 which were declared with the same id.\\n\\nThe external SQL server LoanPaymentDetails restricts the id field to be UNIQUE, hence the expected error displayed by sqoop-export.\\n\\nSQLException: Violation of PRIMARY KEY constraint 'PK_LoanPaymentDetails'.\\nCannot insert duplicate key in object 'srcib.LoanPaymentDetails'.\\nThe duplicate key value is (0e86af89-f15c-4b78-8925-08ed8d237805)\\n\\nThe id of the problematic transaction is 0E86AF89-F15C-4B78-8925-08ED8D237805 and the first transaction has a timestamp of 09:38:13.072489000 while the second one has a timestamp of 2023-01-25 2023-01-25 09:38:13.476066000.\\n\\nWe suggest deleting one of the two transactions so that we can proceed with the export of the loan payment data for the remaining transactions. The information for both transactions remains in the big data environment\\nso it can be retrieved if needed later.\\n\\nPlease let us know if you agree to proceed with the proposed action.\\n\\nThank you\\n```\\n\\n```bash\\nAfter communication via email, we proceeded to reconstruct the table, keeping only the record with timestamp 09:38:13.072489000 and reran the job. Please, investigate this on your part and take the necessary actions so that duplicate records are not sent to us. Please confirm that the data is in the SQLServer so that we can close the ticket.\\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is highly specific and directly answerable using the provided context, which details the exact issue and resolution steps for a failed EXPORT job due to a duplicate key.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The question is extremely relevant for telecom engineers using the BigStreamer platform, as it addresses a common issue in data export jobs that could disrupt analytics workflows.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear and can be understood on its own without needing the context, as it specifies the problem (failed EXPORT job) and the cause (duplicate key in loan payment data)."
          },
          {
            "question": "A batch job fails with Code 6 while trying to EXPORT loan payment data. What could be the cause and how should it be resolved?",
            "answer": "Code 6 indicates a timeout in the sqoop-eval stage due to a SQLServer connection issue. The job did not proceed to sqoop-export. The solution is to rerun the job at a less busy time to avoid timeout:\\n```bash\\n# Example rerun\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\\n```",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# mno - BigStreamer - IM2074270 - Failed Batch Job on Grafana\\n\\n<b>Description:</b>\\n\\n```bash\\nGood evening,\\n\\nThe following failed batch job appeared in the grafana system:\\n\\napplication : DWH_IBank\\njob_name : EXPORT\\ncomponent : LOAN_PAYMENT\\ndate : 30-01-2023\\nstatus : FAILED\\ndescription : code 6\\nhost : -\\n```\\n\\n</b>Actions Taken:</b>\\n\\n1. Check Loan Payment - Export Status from Grafana\\n\\n   ```bash\\n   LOCAL MONITOR -> Batch Jobs DR -> DW Jobs\\n   ```\\n\\n   The script excited with Code 6, which means that it timed-out, waiting for the Sqoop MapReduce job to be in running state.\\n\\n2. Check Impala Query Status\\n\\n   ```bash\\n   Cloudera Manager -> Impala -> Queries -> statement RLIKE '.*details_loan_payment.*' -> Search\\n   ```\\n\\n   ![IM2074270_impala_queries](.media/IM2074270_impala_queries.png)\\n\\n   Impala query ran successfully.\\n\\n3. Check MapReduce job from YARN\\n\\n   ![IM2074270_yarn_apps](.media/IM2074270_yarn_apps.png)\\n\\n   There wasn't any job for Loan Payment,so it had never been submitted.\\n\\n4. Check export logs\\n\\n    ```bash\\n    #from dr1edge01\\n    less /var/log/datawarehouse-ibank/PRODUSER/sched_export.log\\n    ```\\n\\n   ![IM2074270_export_logs](.media/IM2074270_export_logs.png)\\n\\n   There was a Connection Timed Out error from sqoop-eval which indicates a problem with the SQLServer.\\n\\n5. Check export script\\n\\n    ```bash\\n    #from dr1edge01\\n    less /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_to_dwh.sh\\n    ```\\n\\n   ![IM2074270_export_script](.media/IM2074270_export_script.png)\\n\\n   As we can see in the image above, sqoop-export runs after sqoop-eval. In our case sqoop-export did not run because sqoop-eval exited with error.\\n\\n6. Inform Customer and ask for a rerun\\n\\n<b>Our Ticket Response:</b>\\n\\n```bash\\nGood evening,\\n\\nfrom the analysis of the logs we see that sqoop was not submitted because the evaluation for SQL Server was run first, which crashed with a Network I/O exception. (Connection Timed Out)\\n\\nPlease rerun the job.\\n\\nWe also see that the DWH started today at 12. Because at this time we have an increased chance of the SQL Server evaluation crashing as the traffic on it is increased, could you inform us why the execution of the DWH was delayed?\\n\\nThank you\\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is directly answerable using the provided context, which clearly explains the cause (SQLServer timeout) and resolution (rerun the job).",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The question is highly relevant for telecom engineers using BigStreamer, as it addresses a critical issue (batch job failure) that could impact data export processes.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear and self-contained, specifying the error (Code 6) and the context (EXPORT loan payment data) without needing additional context."
          },
          {
            "question": "How can you resolve 'OutOfMemoryError' issues on the Spark History Server in Cloudera?",
            "answer": "Inspect the logs on the node and confirm the Java heap size in Cloudera UI. If it is too low (e.g., 512M), increase it to a higher value like 2G to match the configuration of similar roles on other nodes. Then restart the role.",
            "category": "Infrastructure & Deployment",
            "context": "# mno - BigStreamer - IM2098517 - Health issue on dr1edge01\\n\\n<b>Description:</b>\\n\\n```\\nGood morning,\\n\\nThe following health issue has occurred in the cloudera manager system:\\n\\nSpark on yarn - History Server (dr1node03) - Unexpected Exits\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to Cloudera for DR Site\\n2. We inspected logs for this role for the time that the problem arose: `Cloudera > Diagnostics > Logs` and chose `Service: Spark on Yarn` and `Role: History Server`. We could not identify the root cause by these logs\\n3. ssh to dr1node03 as root, went to `/var/run/process` , and inspected logs from the process that ran at the time of the problem and found out that the process with pid 51291 was killed while a `OutOfMemoryError` occured\\n   ![terminal_screenshot](.media/IM2098517/spark_on_yarn.png)\\n\\n4. We checked the  `java heap size` of the History Server through Cloudera UI configuration tab. It was set to 512M.\\n5. We checked the respective option for the PR Site and it was set to 2G\\n6. We set the `java heap size` of the History Server to 2G at the DR Site\\n7. We proceeded to restart of the role after communication with the customer\\n    \\n\\n\\n<b>Our Ticket Response:</b>\\n\\n```\\n07/03/23 16:28:55 Europe/Eastern (MASTROKOSTA MARIA):\\nWe have restarted the History Server after a phone call. There was no problem during the restart.\\n\\nPlease let us know if we can close the ticket.\\n\\nThank you\\n07/03/23 16:21:21 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe exit occurred due to an out of memory error. We have changed the java heap size of the History Server from 512MB to 2GB as in the PR. We will need to restart the role. There will be no outage.\\n\\nThank you\\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for diagnosing and resolving OutOfMemoryError in Spark History Server.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Highly relevant for telecom engineers using BigStreamer, as memory issues in Spark History Server can impact job monitoring and debugging capabilities.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely self-contained, clearly specifying the error (OutOfMemoryError) and component (Spark History Server in Cloudera)."
          },
          {
            "question": "What causes a YARN Node Manager failure and how can it be mitigated?",
            "answer": "A Node Manager failure on pr1node01 was due to excessive disk usage from krb5kdc logs. These logs had grown to 80GB. The mitigation included deleting old logs and changing log rotation policy to weekly, keeping only 7 logs instead of 12.",
            "category": "Infrastructure & Deployment",
            "context": "# mno - BigStreamer - IM2095156 - Alarm on PRDBA  Cloudera Manager\\n\\n</b>Description:</b>\\n\\n```\\nWe have a critical alarm in PRDBA Cloudera Manager -\\n\\nYARM -- Node Manager (pr1node03)\\nProcess Status\\n\\nThank you\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. Login to Cloudera UI for the PR Site\\n2. Cloudera > Yarn\\n3. Upon inspection we noticed that the alert was about pr1node01 (Node Manager) and not pr1node03 (JobHistory Server)\\n4. Ssh pr1node01 and inspect logs at /var/loh/hadoop-yarn. We could not find the root cause from logs\\n5. Restart the Node Manager role for the specific node. After the restart the alert disappeared.\\n6. During further investigation, from Cloudera UI we saw that prior to ```Process Status``` alert there was a ```NODE_MANAGER_LOG_FREE_SPACE``` alert\\n7. From pr1node01 as root `df -h /`. The usage of `/` was at 98% at that time\\n8. Upon inspection we noticed that the krb5kdc logs had increased over the last months peaking the monthly log file to ~80G.\\n9. We proceeded to the removal of krb5kdc log files for 2022.\\n10. As a permanent solution, we implemented changes to retention policy for krb5kdc logs. Specifically, we changed the rotation to weekly from monthly and the storage to 7 old logs from 12 logs files that it was prior the change.  This change was implemented at pr1node02, dr1node01 and dr1node02 as well.\\n\\n\\n    ![logrotate_krb5kdc](.media/IM2095156/IM2095156_logrotate_krb5kdc.PNG)\\n\\n<b>Our Ticket Response:</b>\\n\\n```\\n09/03/23 15:47:41 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nFollowing the investigation, we have changed the retention for krb5kdc logs. Specifically, we have set the rotation to be weekly instead of monthly and to keep 7 log files. Note that the monthly krb5kdc log file had reached 80G.\\n\\nPlease let us know if we can proceed with closing the ticket.\\n\\nThank you\\n\\n01/03/23 07:16:41 Europe/Eastern (MASTROKOSTA MARIA):\\nGood morning,\\n\\nThere was a malfunction in the yarn node manager since 3.42, resulting in the ibank and online streaming topologies falling as recorded in ticket SD2157107.\\n\\nWe proceeded to restart at 4:53 to get it back up. During the restart, the online merge batch crashed, which was resubmitted (related ticket SD2157111).\\n\\nAt this time, yarn and the flows are running normally.\\n\\nFrom the investigation it appears that the root partition on pr1node01 had filled up, which was caused by the local kdc logs. We have proceeded to clean the corresponding log files and are investigating changes to the retention of the logs to avoid future problems.\\n\\nPlease let us know if you consider the workaround acceptable.\\n\\nThank you\\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully answerable from the context, which provides a clear root cause (disk usage from logs) and detailed mitigation steps (log cleanup and rotation policy change).",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Extremely relevant for telecom engineers as YARN Node Manager failures can disrupt critical data processing jobs in BigStreamer.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is perfectly clear on its own, specifying both the component (YARN Node Manager) and what information is needed (causes and mitigation)."
          },
          {
            "question": "How should you run a MAN_DATE extract and export job when there is a duplicate key error?",
            "answer": "Use the `-f` flag to force truncation before running extract and export scripts to avoid primary key constraint violations:\\n```bash\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\\n```",
            "category": "Application Functionality & Flow",
            "context": "# mno - BigStreamer - IM2117067 - Grafana Batch Job failed\\n\\n<b>Description:</b>\\n\\nTitle: Batch Job Failed\\nBody:\\n```\\nHello,\\n\\nToday 29/03 in Grafana application a failed Batch Job appeared.\\n\\nApplication : DWH_IBank\\nJob_Name : Extract\\nComponent: MAN_DATE\\nDate: 28/03/23\\nStatus: Failed\\nDescription Code 1\\n```\\n\\nInformation regarding the extract, logs etc. available [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-extract) \\n\\nInformation regarding the export, logs etc. available [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-export) \\n\\n</b>Actions Taken:</b>\\n\\nWe tried to invoke the Extract MAN_DATE script but it failed multiple times. After investigating the logs of the executor using internal firefox and the logs of the DWH_Ibank_MAN_DATE script we saw the following :\\n\\n`Cannot insert dublicate key in object srcib.MandateDetails. The dublicate key value is (e5435435-4354254235-121nfdgd33)`\\n\\n\\nThat means that the table already has records in it, so we have to drop this records or invoke the script with the `-f` option that gives the ability to trancate the table and then insert the records. \\n\\n```\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\\n```\\n\\nAfter executing the extract script, we use the same `-f` to run the export script. The reason that we are running export is because in the above mentioned step we run the extract and trancated the table so the export is neccessary. \\n\\n```\\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\\n```\\n\\nThe issue has been resolved by executing the above two commands. The key in this case was to find in the logs that we have dublicate keys. ",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides specific commands and the '-f' flag solution for handling duplicate key errors in MAN_DATE jobs.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses a common issue in data processing workflows that could disrupt critical operations in BigStreamer.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely self-contained, clearly specifying both the operation (MAN_DATE extract/export) and the specific error condition (duplicate key)."
          },
          {
            "question": "How do you rerun a failed Merge Batch ingestion job for IBank and why is it done in patches?",
            "answer": "You rerun the job using the following commands split by time window to manage load:\\n```bash\\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 00:00:00\\\" \\\"2023-02-28 12:00:00\\\"\\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 12:00:00\\\" \\\"2023-02-28 18:00:00\\\"\\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 18:00:00\\\" \\\"2023-03-01 00:00:00\\\"\\n```\\nThis ensures that large record volumes are handled without overloading HBase or Impala.",
            "category": "Application Functionality & Flow",
            "context": "# mno - BigStreamer - IM2095966 - Failed Batch Job on Grafana\\n\\n<b>Description:</b>\\n\\n```\\nGood evening,\\n\\nThe following failed batch job appeared in the Grafana system:\\n\\napplication :  IBank_Ingestion\\njob_name : MergeBatch\\ncomponent : JOB\\ndate : 28-02-2023\\nstatus : FAILED\\ndescription :\\nhost : pr1edge01.mno.gr\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. We identified the failed step using the alarm name. Steps `MSSQL Sqoop Import (Migration)` and `Insert to Service Audit` had been executed successfully. We rerun the `Merge Batch` according to [this](../supportDocuments/applicationFlows/ibank.md#merge-batch).\\n2. The job had not completed at approximately 9.pm on 01/03/2023 we terminated the job after communication with the customer in order for the night flow to run without any problems. We scheduled to rerun the job in the following day after the completion of the daily MergeBatch.\\n3. On 02/03/2023 we reran the job in 3 patches \\n   ```bash\\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 00:00:00\\\" \\\"2023-02-28 12:00:00\\\"\\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 12:00:00\\\" \\\"2023-02-28 18:00:00\\\"\\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2023-02-28 18:00:00\\\" \\\"2023-03-01 00:00:00\\\"\\n   ```\\n4. The `Upsert to HBase` stage that synchronises the `Visible` table caused an Impala problem during which Impala stopped to process this job as well as other requests.\\n5. The problem is described below.\\n\\n<b>Our Ticket Response:</b>\\n\\n```\\n03/03/23 11:17:49 Europe/Eastern (POULAS GIORGOS):\\nAfter investigating yesterday's Impala issue, we found the following:\\n\\nDue to HBase quotas set in the PROD_IBANK namespace, we have limited the parallelism in the Impala query to run on an Impala daemon.\\n\\nThe daemon that ran the query to enrich the Service Audit Visible (pr1node04) encountered a problem as it did not have the resources required to process the large volume of records we had at the end of the month, while at the same time accepting requests from the REST APIs of the live streams.\\n\\nAs a result of the above, the queries from the live systems were not completing and accumulating, exhausting the available connections that Impala can accept. The malfunction of the live streams is also the problem you observed last night.\\n\\nWe propose as a workaround today after 9pm. disable quotas in the PROD_IBANK namespace and rerun the script without the single node limitation, so that the load is shared across all 9 available Impala daemons. We will then examine the alternatives for modifying the flow and re-enabling quotas.\\n\\nThere is no downtime required for the above actions.\\n\\nIf you need further information we can arrange a call.\\n\\nG. Poulas\\n03/03/23 00:53:26 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe service audit has been filled on the PR Site. The job that fills the visible table is pending as it was canceled in the context of ticket SD2159021.\\n\\nThank you\\n03/03/23 00:52:57 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe service audit has been filled on the PR Site. The job that fills the visible table is pending as it was canceled under ticket SD2159021.\\n\\nThank you\\n02/03/23 15:56:38 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe execution on both sites started after the scheduled execution of the Merge Batch for 01/03/2023, which has been completed without a problem. At this time, the DR has processed until 18:00, while the PR has processed the data until 12:00. The executions on both sites are being monitored so that they can be resubmitted in case of a problem.\\n\\nThank you\\n02/03/23 15:54:31 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe execution on both sites started after the scheduled Merge Batch execution on 01/03/2023, which has been completed without any problems. At the moment, the DR has processed until 18:00, while the PR has processed the data until 12:00. The executions on both sites are being monitored so that they can be resubmitted in case of a problem.\\n\\nThank you\\n01/03/23 21:11:19 Europe/Eastern (MASTROKOSTA MARIA):\\nFollowing our telephone communication, the job has been stopped and will be re-executed tomorrow in order to avoid problems with the evening streams.\\n\\nThank you\\n```\\n",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides specific commands for patched reruns and explains the resource management reasons behind this approach.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Extremely relevant for telecom engineers as it addresses a critical data ingestion process and resource optimization in BigStreamer operations.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the action (rerun failed job) and the specific aspect needing explanation (patch approach)."
          },
          {
            "question": "How can Impala concurrency issues related to HBase quotas be resolved for a job using 'Upsert to HBase'?",
            "answer": "Remove the line `set num_nodes = 1` from the script and disable HBase quotas in the PROD_IBANK namespace. This allows parallel execution across all 9 Impala daemons. Afterward, rerun the script:\\n```bash\\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh\\n```",
            "category": "Data Management & Query Execution",
            "context": "# mno - BigStreamer - IM2097021 - Multiple Health issues on PR Impala\\n\\n<b>Description:</b>\\n\\n```\\nToday 02/03/2023 & 23:30 the Following alarms appeared on Cloudera\\n\\n[Impala: Daemon (pr1node01)]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node02)]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node03)]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node04)]\\n[Pause Duration]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node05)]\\n[Impala Concurrent Client Connections]\\n------------\\n[Impala: Daemon (pr1node06)]\\n[Impala Concurrent Client Connections]\\n```\\n\\n</b>Actions Taken:</b>\\n\\n1. Login to Cloudera for PR Site \\n2. To identify the Impala query from `Upsert to HBase` we can see logs from the script at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log` as `PRODREST` user. We cite a screenshot that shows the query. Also, we see the url where we can monitor the query progress (paste this url on a firefox opened through terminal), as well as the coordinator.\\n   \\n   ![logs_screenshot](.media/upsert_to_hbase_logs_query.PNG)\\n   > Ndef: These are not logs from that specific script execution, just a sample to see where you can find the query information you need.\\n\\n3. From `Cloudera > Impala > Queries` we identified the query and noticed that it had stopped getting processed. In addition, we noticed that Impala had stopped processing other queries as well\\n4. We cancelled the query that ran for `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` execution. We can cancel the query in two ways.\\n   1. From `Cloudera > Impala > Queries` you can click `cancel` at the dropdown next to the query\\n   2. From the url that we monitor the query\\n5. We restarted Impala daemon role for pr1node01. This solved the problem with this specific node, however the service did not correspond\\n6. We restarted Impala daemon role for pr1node04 that was the coordinator for the query. This solved the problem and recovered the service functionality\\n7. Upon investigation, we concluded that the change to `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` that stops the parallel execution of the query by Impala daemons (set num_nodes = 1) was the cause of the problem\\n8. We scheduled to rerun the `Upsert to HBase` stage the following day after reverting the script to use all Impala daemons for parallel execution.\\n9.  On 03/03/2023 \\n   - we disabled HBase quotas for ` PROD_IBANK` namespace on PR Site according to [this](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/22263_mno_HBASE_TUNING.docx) MoP\\n   - we removed `set num_nodes = 1` from `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\\n   - reran the script\\n10. The script ran successfully\\n\\n<b>Action Points:</b>\\n\\n1. We opened [this](https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/67) issue to investigate and deploy a permanent fix for running `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` alongside HBase quotas.\\n\\n<b>Our Ticket Response:</b>\\n\\n```\\n03/03/23 00:49:29 Europe/Eastern (MASTROKOSTA MARIA):\\nGood evening,\\n\\nThe root cause is the same as ticket SD2158913. The job that populates the visible table was canceled after consultation with the customer as it was affecting the live streams.\\n\\nThe job will be scheduled to be rerun after consultation\\n\\nThank you\\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides specific technical solutions (removing node limitation and disabling quotas) and the exact script to rerun.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses critical performance issues in HBase/Impala integration that could impact data processing workflows.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely self-contained, clearly specifying both the technical context (Impala concurrency) and the specific operation ('Upsert to HBase')."
          },
          {
            "question": "How can Kerberos authentication issues be resolved on RHEL 8 for a new Way4Streams installation?",
            "answer": "1. Remove `sssd-kcm` and ensure krb5.conf has:\\n```conf\\ndefault_ccache_name = FILE:/tmp/krb5cc_%{uid}\\nallow_weak_crypto = true\\n```\\n2. If `klist -kte` shows DEPRECATED encryption, allow weak crypto as above.\\n3. Restart authentication processes or reinitiate with `kinit`.\\n\\nThis allows proper usage of keytabs even with weak encryption schemes.",
            "category": "Data Management & Query Execution",
            "context": "# mno - BigStreamer - way4streams-venia - Kerberos Authentication Errors on new Way4Streams installation\\n\\n<b>Description:</b>\\n\\n```text\\nReporting and investigation for this issue was performed in a teams call, since we did not have access to the server that Way4Streams was installed.\\n\\nThe error we were facing was something along the lines\\n\\n/way4/DEVUSER.keytab does not contain any keys for DEVUSER@BANK.CENTRAL.mno.GR\\n```\\n\\n<b>Actions Taken:</b>\\n\\n1. The new server hosting the application is RHEL 8 instead of Solaris. We tried to manually `kinit`\\n\\nFrom the server with `way4`\\n\\n``` bash\\nkinit DEVUSER@BANK.CENTRAL.mno.GR -kt /way4/DEVUSER.keytab\\n```\\n\\nOutput:\\n\\n```bash\\nTicket cache: KCM:1500\\nDefault principal: DEVUSER@BANK.CENTRAL.mno.GR\\n\\nValid starting       Expires              Service principal\\n15/03/2023 12:35:29  16/03/2023 12:35:29  krbtgt/BANK.CENTRAL.mno.GR@BANK.CENTRAL.mno.GR\\n\trenew until 22/03/2023 12:35:29\\n```\\n\\n2. **Anything** but `FILE` ticket caches is sure to create a problem.\\n\\nFrom the server with `root`:\\n\\n``` bash\\nvi /etc/krb5.conf\\n```\\n\\nChange the following under `libdefaults` section:\\n\\n``` conf\\ndefault_ccache_name = FILE:/tmp/krb5cc_%{uid}\\n```\\n\\nAlso, remove `sssd-kcm`\\n\\n```bash\\nyum remove sssd-kcm\\n```\\n\\n3. After that the klist output used a `FILE` cache, but the problem persisted.\\n\\nSince the OS problems were resolved we focused the keytab.\\n\\nFrom the server with `way4`\\n\\n``` bash\\nklist -kte /way4/DEVUSER.keytab\\n```\\n\\nOutput:\\n\\n```\\nKeytab name: FILE:/way4/DEVUSER.keytab\\nKVNO Timestamp           Principal\\n---- ------------------- ------------------------------------------------------\\n  0 01/01/1970 00:00:00 DEVUSER@BANK.CENTRAL.mno.GR (DEPRECATED:arc4-hmac) \\n```\\n\\nThat DEPRECATED flag is not a good sign. \\n\\n4. Searching for `rc4-hmac` and `OpenJDK11` we stumbled upon this link https://bugs.openjdk.org/browse/JDK-8262273\\n\\nFrom the server with `root`:\\n\\n``` bash\\nvi /etc/krb5.conf\\n```\\n\\nAdd the following under `libdefaults` section:\\n\\n``` conf\\nallow_weak_crypto = true\\n```\\n\\nThe issue was resolved!\\n\\n<b>Affected Systems:</b>\\n\\nWay4Streams QA (Not supported by jkl)\\n\\n<b>Action Points:</b>",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for resolving Kerberos authentication issues specific to RHEL 8 and Way4Streams.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses authentication issues that could prevent proper functioning of the Way4Streams application in BigStreamer.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the technical context (Kerberos authentication) and the specific environment (RHEL 8 for Way4Streams)."
          },
          {
            "question": "How do you rerun the 'Enrich SA from SA_old' job after a memory error and ensure it's properly monitored in Grafana?",
            "answer": "Check the logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log` for memory errors. If no data exists for the failed partition (check using Impala), rerun the script:\\n```bash\\n/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20230330\\n```\\nThen, update the monitoring database with:\\n```bash\\npsql -d monitoring\\nINSERT INTO prod.monitoring (...) VALUES (...);\\n```\\nFinally, verify the status from Grafana.",
            "category": "Application Functionality & Flow",
            "context": "# mno - BigStreamer - SD2180781 - Failed job at Grafana \\n\\n</b>Description:</b>\\n\\n```\\nGood evening.\\nThe following failed job appeared in Grafana:\\n\\nApplication: IBank_Migration\\nJob_name: Enrich SA from SA_old\\nComponment: JOB\\nDate: 30-03-2023\\nStatus: FAILED\\nHost: pr1edge01.mno.gr\\n```\\n\\n</b>Actions Taken:</b>\\n\\n1. We have to check first the logs from the failed job `Enrich SA from SA_old` as described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\\n\\nDetailed information from the above link\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\\n\\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n\\n\\n2. The error was the below from logs:\\n```\\nERROR: Memory limit exceeded: Failed to allocate row batch\\nEXCHANGE_NODE (id=5) could not allocate 1.00 MB without exceeding limit.\\nError occurred on backend pr1node05.mno.gr:22000\\nMemory left in process limit: 27.82 GB\\n\\n```\\n3. To verify the below error also checked from Cloudera > Impala > Queries\\n\\n![impala_query_error](.media/SD2180781/SD2180781_IMPALA_QUERY_ERROR.PNG)\\n\\n4. Now we have to go to `troubleshooting steps` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit) in order to check that no records are present in `prod_trlog_ibank.service_audit`\\n\\n  ``` bash\\n  # eg. 09-11-2019\\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \\\"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\\\"\\n  ```\\n\\n!Ndef : The par_dt is -1 from today\\n\\n5. No records exists on `par_dt` `20230330`\\n\\n6. Now we have to run the command `For the previous day:` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\\n\\nIf no records exist and no other process is up, you can ran the script again.\\n  - For the previous day:\\n\\n    ``` bash\\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\\n    ```\\n\\n  - For a specified date:\\n\\n    ``` bash\\n    # e.g. 09-11-2019\\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\\n    ```\\n\\n7. When the job finished `succesfully` we have to updated the monitoring postgres database in order for the entry `Enrich SA from SA_old` to appeared green/success in Grafana.\\n\\n```bash\\nssh Exxxx@pr1edge01.mno.gr\\nsudo -i -u postgres\\npsql -d monitoring\\nselect * from prod.monitoring where par_dt = 20230330;\\nINSERT INTO prod.monitoring (application, job_name,component,status,par_dt,start_time,end_time,description,params,host) VALUES ('IBank_Migration','Enrich SA from SA_old','JOB',0,20230330,'2023-03-31 03:18:30.000','2023-03-31 05:00:42.000','','','pr1edge01.mno.gr') ON CONFLICT (application, job_name,component,par_dt) DO UPDATE SET status=0, start_time='2023-03-31 03:18:30.000', end_time='2023-03-31 05:00:42.000',description='';\\n```\\n8. Check from Grafana that the job `Enrich SA from SA_old` is now `succeded`\\n9. After i execute the below steps mannualy from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md)\\n\\na. [Upsert to HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`\\n\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\\n\\n**Alerts**:\\n\\n- IBank_Migration Enrich hbase tables JOB\\n- IBank_Migration Enrich hbase tables Impala_insert\\n- IBank_Migration Enrich hbase tables Spark\\n\\n**Troubleshooting Steps**:\\n\\n- Use the script logs to identify the cause of the failure\\n\\n  Ndef: If job failed and the following error appears :`ERROR: RetriesExhaustedWithDetailsException: Failed <num> actions: CallTimeoutException: <num> times, servers with issues: [dr/pr]1node02.mno.gr`,  execute script again. The error has to do with HBase merging/spliting on a region server, but a detailed reason is unknown.\\n- The script uses upsert and can be safely run many times.\\n\\n\\n  - For the previous day:\\n\\n    ``` bash\\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\\n    ```\\n\\n  - For a specified date:\\n\\n    ``` bash\\n    # e.g. 09-11-2019\\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20191109  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\\n    ```\\n\\n\\nb. [Duplicates between Impala and Kudu/HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#duplicates-between-impala-and-kuduhbase)\\n\\n**User**: `PRODREST`\\n\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log`\\n\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\\n\\n**Alerts**:\\n\\n- **Not Monitored**\\n\\n**Troubleshooting Steps**:\\n\\n- Check `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log` for errors\\n- You can safely skip this step if not running for the previous day\\n- Sample execution:\\n\\n  ``` bash\\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_ibank.service_audit_stream prod_trlog_ibank.service_audit_old ibank >> /var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log 2>&1",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for rerunning the job after a memory error and updating Grafana monitoring.",
            "question_relevance_score": "5", 
            "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses a critical data processing job that impacts analytics in BigStreamer.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the job ('Enrich SA from SA_old') and the specific issue (memory error and monitoring)."
          },
          {
            "question": "What is the procedure to continue the merge batch and HBase upsert process for failed DWH flows due to a stalled job on February 29?",
            "answer": "Manually execute merge batch steps on PR site using instructions from the application flow documentation. Then use HDFS replication to copy data to DR and run the HBase upsert step using:\\n```bash\\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20240229\\n```\\nEnsure HBase markers and Kudu partitions are cleaned and reset properly using `alter table ... drop range partition` and `add range partition`.",
            "category": "Application Functionality & Flow",
            "context": "# mno - BigStreamer - IM2285747 - merge batch 29/2\\n\\n<b>Description:</b>\\n\\n```\\nGood evening,\\n\\ncould you please check the merge batch in DR/pr for 2/29. It seems to be still running. We have disabled it on 1/31 until this is finished.\\n\\nThanks,\\nThanos\\n```\\n\\n<b>Investigation:</b>\\n\\n**Ndef**: All log paths and query executions are found/executed from pr1edge01/dr1edge01.\\n\\n1. Login to Grafana and make sure that the failed step is the Merge Batch.\\n2. Login to DR/PR edge nodes and through the node's firefox check YARN at https://dr1node03:8090 and https://pr1node03:8090 for the PROD_Ibank merge batch job.\\n3. Check the stages tab for stages that have been completed for this job. At PR the `insert into` stages had completed after 8h. The RDD stages had failed and continued failing.\\n![Yarn UI](.media/IM2285747_1.png)\\n![Spark Stages](.media/IM2285747_2.png)\\n4. The 8 hour mark gives us some clues as to the failure. In essence after 8 hours kerberos tickets are dropped, leading to continuous authentication failures and timeouts as shown in the logs `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`.\\n5. Checked the merge batch logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log` and `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log` and indeed we found authorization errors.\\n6. Checked the size and count of the `service_audit_old` table through Impala and HDFS and it was found to be among the largest ever both in size (over 115 GB) and count (60 mil). The query used is `select  count(*) from prod_trlog_ibank.service_audit_old where par_dt=20240229`.\\n7. Inspecting the submition script we can see that for each individual sub-script the date is generated anew in the sub-script invocation. This means that since the merge in question spanned multiple dates, all steps started after the day's end wouldn't have completed succesfully.\\n8. Inspecting the range partitions through Impala with the query `show range partitions prod_trlog_ibank.service_audit_stream`, we can find multiple leftover range partitions since November. This added further computation time for the already large table.\\n\\nInvestigation for 05/03:\\n\\nFor this day, while the size and count were not unusually large the last step of the process, ie. the\\nhbase upsert took unusually long and couldn't complete on its own. The process seemed to get stuck on 1\\nsingular task as shown by the logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`, which leads us to conclude that a certain record was problematic because all tasks completed\\nnormally except a specific one that kept failling even on re-runs.\\n\\nInvestigation - Summary:\\n\\n- Greater than most end of the month dates size(over 115 GB) and count (60 mil)\\n- Multiple leftover range partitions\\n- Execution spanning multiple days, triggering the dynamic date issue with the submition script\\n- Stage execution surpassing the 8 hour mark, triggering a known kerberos authorization bug\\n- Problematic record for 05/03\\n\\n<b>Resolution:</b>\\n\\n1. Since the data was already loaded into PR for 29/2, the rest of the steps were executed manually on it\\n   according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#batch instructions for\\n   steps beyond the Merge Batch .\\n2. For all the remaining dates since 29/2 and because the cron jobs were stopped, each day was executed\\n   manually in full in each site, half of them (days) in PR and half of them (days) in DR following\\n   all the steps in the sub-steps guide from https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md .\\n3. For each job completed in one site it was replicated over to the other using HDFS replication, through\\n   the destination's Cloudera Manager for both `service_audit` and `service_audit_old` tables. **A similar\\n   procedure for table replication exists in [Table Replication](./20201218-IM1389913.md) but not for HDFS replication.**\\n   - /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit_old/par_dt=$date\\n   - /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit/par_dt=$date\\n4. Once replicated the HBase Upsert step was run on the destination site according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .\\n5. HBase markers were manually set for each job that didn't complete automatically according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .\\n6. Kudu range partitions were cleaned up manually for all previous days and 3 new partitions were created\\n   for the 3 previous days, in order for the cleanup script to continue functioning as nornal. More details\\n   on this can be found at the end of this document.\\n\\n\\nResolution for 05/03 HBase Upsert:\\n\\nFor this operation specifically more resources had to be allocated to the spark-submit job at `/opt/ingestion/PRODREST/ibank/spark/submit/visible_trn_hbase_daily_upsert/submitVisibleTrnToHbaseIndexesDailyUpsert_STABLE.sh`. After its completion resources were reverted back to normal:\\n```\\nIncrease node count to 12\\nDecrease core count to 1\\nIncrease tasks to 40\\n```\\n\\nKudu Range Partitions:\\n\\nRange partitions are created for the `service_audit_stream` table on the `u_timestamp` column. The commands to create and delete them\\ncan be found below for some example dates. Ndef that those partitions are in UTC time, so the time to\\ncreate/drop must be converted to local time, taking into account DST. For winter we are at GMT+2 so in order\\nto include a full day it must range for 22:00 of the previous to 22:00 of the current (where current is the day you want to delete).\\n\\n```sh\\n# Drop\\nalter table prod_trlog_ibank.service_audit_stream drop range partition '2023-02-26T22:00:00.000000Z' <= VALUES < '2023-02-27T22:00:00.000000Z' ;\\n\\n# Create\\nalter table prod_trlog_ibank.service_audit_stream add range partition '1970-01-01T00:00:00.000000Z' <= VALUES < '2023-02-28T22:00:00.000000Z' ;\\n```\\n\\nThe example above includes the first partition, which tracks from 1970 to the date in question. When deleting previous partitions, the first one\\nmust be recreated in order to include the time from 1970 to the first date, and then you can create the daily partitions. For example, lets say we have\\nthe following partitions:\\n\\n```\\n| 1970-01-01T00:00:00.000000Z <= VALUES < 2023-11-29T22:00:00.000000Z | # We want to drop this one in order to include a bigger range\\n| 2023-11-29T22:00:00.000000Z <= VALUES < 2023-11-30T22:00:00.000000Z | # We want to drop this one due to policy\\n| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\\n| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\\n| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\\n| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This must not be touched\\n| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |\\n| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |\\n| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |\\n```\\n\\nThe above will become:\\n\\n```\\n| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # This was created by including all dates from 1970 to this one\\n| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # This was dropped and recreated\\n| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # This was dropped and recreated\\n| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This was not touched\\n| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |\\n| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |\\n| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |\\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for manual execution of merge batch, HDFS replication, HBase upsert, and Kudu partition management.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Extremely relevant for telecom engineers as it addresses critical data warehouse flow failures that impact analytics operations in BigStreamer.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the failed process (merge batch and HBase upsert) and the specific date context (February 29)."
          },
          {
            "question": "How do you resolve a Kudu table creation error due to a 'Thrift SASL frame too long' error?",
            "answer": "Add this flag to all Kudu Masters under CM > Kudu > Safety Valve:\\n```bash\\n--hive_metastore_max_message_size_bytes=858993459\\n```\\nRestart the Kudu masters, then restart all tablet servers after stopping active ingestion flows by placing shutdown markers in HDFS. After tablets restart, retry the table creation query.",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# mno - BigStreamer - IM1908793 - Error on creating kudu table\\n\\n<b>Description:</b>\\n\\n```\\nGood morning.\\n\\nI am getting the following error while creating a temporary kudu table,\\n\\nERROR: ImpalaRuntimeException: Error creating Kudu table 'prod_trlog_card_analytical.opticash_dispencing_atm_tmp'\\n\\nCAUSED BY: NonRecoverableException: failed to wait for Hive Metastore notification log listener to catch up: failed to retrieve notification log events: failed to get Hive Metastore next notification: Thrift SASL frame is too long: 338.01M/100.00M\\n```\\n\\n<b>Actions Taken:</b>\\n\\n**Steps in order to investigate and make sure that the table is not created**\\n1. Login to `dr1edge01.mno.gr` with personal account and then to `dr1node01.mno.gr`\\n2. Move to the process folder:\\n```bash\\ncd /var/run/cloudera-scm-agent/process/\\n```\\n3. Find the latest process and go to that folder. In our case is 12200-kudu-KUDU_TSERVER. So move to that folder:\\n```bash\\ncd 12200-kudu-KUDU_TSERVER\\n```\\n4. Use the keytab you just found in that folder:\\n```bash\\nkinit -kt kudu.keytab kudu/`hostname`\\n```\\n5. Check kudu cluster health and specifically for `prod_trlog_card_analytical` database in order to check if the wanted table is created.\\n```bash\\nkudu cluster ksck dr1node04.mno.gr dr1node05.mno.gr dr1node06.mno.gr | grep -i prod_trlog_card_analytical\\n```\\n\\nAs you can see the `prod_trlog_card_analytical.opticash_dispencing_atm_tmp` table is not created.\\n\\n**Optional**: You can also verify that from impala-shell running the following commands:\\n- Login to `dr1edge01.mno.gr` with personal account\\n- impala-shell -i dr1edge01 -k --ssl\\n- `[dr1edge01.mno.gr:21000] default> use prod_trlog_card_analytical;`\\n- `[dr1edge01.mno.gr:21000] default> show tables;`\\n\\nAs you can see the `prod_trlog_card_analytical.opticash_dispencing_atm_tmp` table is not created.\\n\\n6. Login to CM DR with your pesonal account > Go to impala > Queries\\n7. In the search bar type the following in order to find the query:\\n`STATEMENT RLIKE '.*prod_trlog_card_analytical.opticash_dispencing_atm_tmp'.*` and click on the query details for investigation.\\n\\nWe found that the query they try to run is the following:\\n\\n```bash\\nCREATE TABLE IF NOT EXISTS prod_trog_card analytical.opticash ispencing_atm_tmp\\ncashp id, STRING NOT NULL\\n, transaction date STRING NOT NULL\\n,denom id STRING\\n, cassette STRING\\n, crncy id STRING\\n, open Bal BIGINT\\n, norm del BIGINT\\n, norm rtr BIGINT\\n, unpl_ del BIGINT\\n, unpl_tr BIGINT\\n, wthdrwls BIGINT\\n, pre_wdrw BIGINT\\n, deposits BIGINT\\n, clos_bal BIGINT\\n, bal_disp BIGINT\\n, bal_escr BIGINT\\n, bal_unav BIGINT\\n, opr_stat STRING\\n, excld_fl STRING\\nPRIMARY KEY (cashp_id, transaction date, denom_id, cassette)\\n) STORED AS KUDU\\n```\\n\\nWe try to rerun the above query and we get the following error:\\n```bash\\nERROR: ImpalaRuntimeException: Error creating Kudu table 'prod_trlog_card_analytical.opticash_dispencing_atm_tmp'\\nCAUSED BY: NonRecoverableException: failed to wait for Hive Metastore notification log listener to catch up: failed to retrieve notification log events: failed to get Hive Metastore next notification: Thrift SASL frame is too long: 338.01M/100.00M\\n```\\n\\n**Time to fix the problem**\\n\\nAs a first step, let's try to fix `Thrift SASL frame is too long: 338.01M/100.00M` error.\\n\\n1. Login to Cloudera Manager in DR site with your personal administrative account:\\n\\n`Kudu > Instances > Click on Master > Select Tab Configuration`\\n\\n2. In `Search` box write safety valvue and at `Master Advanced Configuration Snippet (Safety Valve for gflagfile)` add th following flag:\\n\\n```bash\\n--hive_metastore_max_message_size_bytes=858993459\\n```\\n\\n>**Important Ndef**:  The above step with flag must be set at all three masters\\n\\n3. Restart the three kudu masters (one at a time)\\n4. After rerunning the query the table is not still created but this time we get the following error: \\n```bash\\nSASL decode failed: SASL(-1): generic failure:\\nwO706 15:44:11.242372 109675 hms_notification_log_listener.cc:130] Hive Metastore notification log listener poll failed: Not authorized: failed to ret\\nrieve notification log events: failed to get Hive Metastore next notification: SASL decode failed: SASL(-1): generie failure:\\nw0706 15:44:35.127687 109673 hms_client.cc:345] Time spent get HMS notification events: real 8.885s user 0.000s sys 0.228s\\n```\\n\\n5. Restarting all Tablet Servers (dr1node01-10),one at a time, fixed the problem. \\n\\n**Before Restarting Tablets the following Flows must be stopped !!!**\\n```\\nPROD_IBANK_IngestStream_Visible\\nPROD_Online_IngestStream\\nPROD_IBank_IngestStream\\n```\\n\\nStop the flows:\\n\\n>Ndef: We used following command because flows were working fine. Otherwise, we you should kill the application.\\n\\n```bash\\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown _marker/\\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\\n```\\nWhen Tablets are all up and running make sure you start again the flows. \\n\\nVerify that Tablets and Kudu is up and running by checking graphs and CM UI (CM -> Kudu -> Charts Library)\\n\\nInformation about how to start flows can be found [here](http://https://metis.ghi.com/obss/oss/sysadmin-group/support/-/tree/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows \\\"here\\\")\\n\\nWe verified that the problem is fixed by running the querry and got Table has been created message.\\n\\n<b>Root Cause Analysis:</b>\\n\\nThis problem occurred due to dr1node07 disk replacement.\\n\\nPlease refer to *IM1893876* for more information.\\n\\nThe fact that kudu tablets were offline for more than 1 days resulted in networking issues between Tablets.\\n\\n\\n<b>Affected Systems:</b>\\n\\nDisaster Site\\n\\n\\n**Well Done!!**\\n\\n\\n\\n",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is perfectly answerable from the context, which provides detailed steps for resolving the specific Kudu table creation error including configuration changes and restart procedures.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Highly relevant for telecom engineers as it addresses a critical data storage issue that could impact analytics operations in BigStreamer.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely clear on its own, specifying both the error condition ('Thrift SASL frame too long') and the specific operation (Kudu table creation)."
          },
          {
            "question": "What actions can resolve an Impala memory allocation error during a service audit enrichment job?",
            "answer": "The error 'Memory limit exceeded' from an Impala EXCHANGE_NODE can be resolved by rerunning the job with:\n```bash\n/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20230330\n```\nIf rerun fails again, reduce query parallelism or split the job into smaller subsets.",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# mno - BigStreamer - SD2180781 - Failed job at Grafana \n\n</b>Description:</b>\n\n```\nGood evening.\nThe following failed job appeared in Grafana:\n\nApplication: IBank_Migration\nJob_name: Enrich SA from SA_old\nComponment: JOB\nDate: 30-03-2023\nStatus: FAILED\nHost: pr1edge01.mno.gr\n```\n\n</b>Actions Taken:</b>\n\n1. We have to check first the logs from the failed job `Enrich SA from SA_old` as described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\n\nDetailed information from the above link\n\n**User**: `PRODREST`\n\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\n\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n\n\n2. The error was the below from logs:\n```\nERROR: Memory limit exceeded: Failed to allocate row batch\nEXCHANGE_NODE (id=5) could not allocate 1.00 MB without exceeding limit.\nError occurred on backend pr1node05.mno.gr:22000\nMemory left in process limit: 27.82 GB\n\n```\n3. To verify the below error also checked from Cloudera > Impala > Queries\n\n![impala_query_error](.media/SD2180781/SD2180781_IMPALA_QUERY_ERROR.PNG)\n\n4. Now we have to go to `troubleshooting steps` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit) in order to check that no records are present in `prod_trlog_ibank.service_audit`\n\n  ``` bash\n  # eg. 09-11-2019\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\"\n  ```\n\n!Ndef : The par_dt is -1 from today\n\n5. No records exists on `par_dt` `20230330`\n\n6. Now we have to run the command `For the previous day:` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\n\n**User**: `PRODREST`\n\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\n\nIf no records exist and no other process is up, you can ran the script again.\n  - For the previous day:\n\n    ``` bash\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n    ```\n\n  - For a specified date:\n\n    ``` bash\n    # e.g. 09-11-2019\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n    ```\n\n7. When the job finished `succesfully` we have to updated the monitoring postgres database in order for the entry `Enrich SA from SA_old` to appeared green/success in Grafana.\n\n```bash\nssh Exxxx@pr1edge01.mno.gr\nsudo -i -u postgres\npsql -d monitoring\nselect * from prod.monitoring where par_dt = 20230330;\nINSERT INTO prod.monitoring (application, job_name,component,status,par_dt,start_time,end_time,description,params,host) VALUES ('IBank_Migration','Enrich SA from SA_old','JOB',0,20230330,'2023-03-31 03:18:30.000','2023-03-31 05:00:42.000','','','pr1edge01.mno.gr') ON CONFLICT (application, job_name,component,par_dt) DO UPDATE SET status=0, start_time='2023-03-31 03:18:30.000', end_time='2023-03-31 05:00:42.000',description='';\n```\n8. Check from Grafana that the job `Enrich SA from SA_old` is now `succeded`\n9. After i execute the below steps mannualy from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md)\n\na. [Upsert to HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\n\n**User**: `PRODREST`\n\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`\n\n**Script**: `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\n\n**Alerts**:\n\n- IBank_Migration Enrich hbase tables JOB\n- IBank_Migration Enrich hbase tables Impala_insert\n- IBank_Migration Enrich hbase tables Spark\n\n**Troubleshooting Steps**:\n\n- Use the script logs to identify the cause of the failure\n\n  Ndef: If job failed and the following error appears :`ERROR: RetriesExhaustedWithDetailsException: Failed <num> actions: CallTimeoutException: <num> times, servers with issues: [dr/pr]1node02.mno.gr`,  execute script again. The error has to do with HBase merging/spliting on a region server, but a detailed reason is unknown.\n- The script uses upsert and can be safely run many times.\n\n  - For the previous day:\n\n    ``` bash\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n    ```\n\n  - For a specified date:\n\n    ``` bash\n    # e.g. 09-11-2019\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20191109  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n    ```\n\n\nb. [Duplicates between Impala and Kudu/HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#duplicates-between-impala-and-kuduhbase)\n\n**User**: `PRODREST`\n\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log`\n\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\n\n**Alerts**:\n\n- **Not Monitored**\n\n**Troubleshooting Steps**:\n\n- Check `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_ibank.service_audit_stream prod_trlog_ibank.service_audit_old ibank >> /var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log 2>&1",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is explicitly answerable using the provided context, which includes detailed logs of the error message, specific paths to the script, and even example rerun commands. The answer aligns well with the troubleshooting steps outlined in the context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Resolving memory allocation errors in Impala during service audit jobs is a critical and realistic task for telecom engineers using BigStreamer. This is highly relevant in day-to-day troubleshooting of distributed data processing jobs.",
            "question_standalone_score": "4",
            "judge_feedback_question_standalone": "The question is mostly self-contained and understandable to someone familiar with Impala and service audit jobs. However, a reader unfamiliar with the BigStreamer platform or the specific job might find 'service audit enrichment job' slightly vague without additional context."
          },
          {
            "question": "How do you handle and recover from a failed disk replacement on a DataNode (dr1node02) in HDFS?",
            "answer": "Unmount affected partitions, run `bdadiskutility`, and if cache issues occur, clear preserved cache using:\n```bash\nMegaCli64 -DiscardPreservedCache -L7 -force -a0\n```\nRe-add virtual disk with:\n```bash\nMegaCli64 CfgLdAdd r0[ENCLOSURE:slot] a0\n```\nAfter configuration, remount disks, verify with `lsblk`, and restart the DataNode service.",
            "category": "Infrastructure & Deployment",
            "context": "# mno - BigStreamer - SD2389640 - hdfs - Data Directory Status\n\n<b>Description:</b>\n\n```\nGood morning,\n\nThe following alert has appeared in Cloudera Manager (DR):\nDataNode (dr1node02)\nData Directory Status\n\nThank you.\n```\n\n<b>Actions Taken:</b>\n\nThere are references from the similar issue [20220620-SD1951890.md](20220620-SD1951890.md).\n\nAfter investigation we saw that the problem occurred due to disk issue on dr1node02.\n\nWe communicated with Oracle and disk replacement was scheduled.\n\n> Ndef that disk replacement perfomerd online so there was no downtime.\n\nWe followed the steps as described at [20220620-SD1951890.md](20220620-SD1951890.md) and [sync_mysql.md](sync_mysql.md), which include the following:\n\n1. Stopping the processes that specifically run at the disk slots `s1` and `s7` of the server `dr1node02`. On our case was the hdfs datanode and some yarn applications . We identified them with:\n\n2. Stopping the mysql slaves using the command:\n```\nmysql -u root -p\nSHOW SLAVE STATUS\\G;\n```\n\n3. Ensuring that the no processes are running at the partitions with the following commands:\n```bash\nlsof /u02\n```\n\n```bash\nlsof /u08\n```\n\n4. Unmounting the two partitions, so the disks can be replaced.\n\n```bash\numount <mountpoint>\n```\n\n5. Once the disks have been replaced we ran the following command for both partitions:\n\n```bash\nbdadiskutility /u02\n```\n\n6. After running the command, we got the following error:\n```\nVirtual Drive <VIRTUAL_DRIVE_NUMBER> is incorrectly mapped.\n<TIMESTAMP> : Error executing 'MegaCli64 CfgLdAdd r0[<ENCLOSURE>:<SLOT>] a0'\n<TIMESTAMP> : Error code is 84 . Response is <<\nAdapter 0: Configure Adapter Failed\n\nFW error description:\nThe current operation is not allowed because the controller has data in cache for offline or missing virtual disks.\n\nExit Code: 0x54>>\nFound a disk with a Firmware State of Unconfigured(good).\nSuccessfully cleared the cache for the logical drive.\nSuccessfully added the disk to its own RAID(0) volume.\n```\n\n7. After communicating with Oracle Support [SR 3-36895603206 : Wrong disk status after replacement](https://support.oracle.com/epmos/faces/SrDetail?_afrLoop=206254157461870&srNumber=3-36895603206&queryModeName=Technical&needSrDetailRefresh=true&_afrvwxowMode=0&_adf.ctrl-state=iwvcvrye_184), we ran the following commands to solve the issue:\n\n- `For s1 # The disk slot 1 of the server that corresponds to mount point /u02`\n- `For s7 # The disk slot 7 of the server that corresponds to mount point /u08`\n\n- Validated if there is a cache pinned for any device, running command:\n\n```bash\nMegaCli64 -GetPreservedCacheList -a0 \n```\n\nIf the old disk has pinned the cache, the command will return something like:\n\n```\nAdapter #0\n \nVirtual Drive(Target ID 07): Missing.\n \nExit Code: 0x00\n```\n- In this case, the disk in slot 7 had the pinned cache and had to clear.\n\nRemove the pinned cache by running command:\n\n```bash\n#MegaCli64 -DiscardPreservedCache -L7 -force -a0 <<<< where -LX should be replaced by the Target ID number reported in previous step.\n```\nGet the `ENCLOSURE_NUMBER`\n```bash\nMegaCli64 LdPdInfo a0 | more\n```\n- Added the virtual disk back\n\n```bash\nMegaCli64 CfgLdAdd r0[ENCLOSURE_NUMBER:slot] a0\n```\nOn our case was:\n\nFor `s1`\n```bash\nMegaCli64 CfgLdAdd r0[252:1] a0\n```\n\nFor `s7`\n\n```bash\nMegaCli64 CfgLdAdd r0[252:7] a0\n```\n\nStarted configuring the disk at `slot1`\n\n```bash\nbdadiskutility -f /u02\n```\n\nWait until the mirroring is finished and after that.\n\nStarted configuring the disk at `slot7`\n\n```bash\nbdadiskutility -f /u08\n```\n\n- Checks:\n\nFor `s1`:\n\n```bash\nparted /dev/disk/by-hba-slot/s1 -s unit chs print\niscsi # Check that all disks appeared\nlsblk # Check that all disks appeared\n```\n\nFor `s7`:\n\n```bash\nparted /dev/disk/by-hba-slot/s7 -s unit chs print\niscsi # Check that all disks appeared\nlsblk # Check that all disks appeared\n```\n\n8. We proceed with the start of the `datanode` role of `dr1node02`",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question directly relates to the failure described in the context, which provides a detailed log of the issue, root cause, and the recovery process, including all relevant commands. The answer is fully supported by the context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Handling hardware-level failures like disk replacements is critical for engineers managing HDFS and BigStreamer infrastructure. This question reflects a realistic and high-priority operational issue that engineers may face.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is self-contained, specifying both the failure type (disk replacement) and the component involved (DataNode in HDFS, dr1node02). No additional context is required to understand what the user is asking."
          },
          {
            "question": "What steps should be taken when all Cloudera services appear unhealthy due to NFS issues?",
            "answer": "Check logs like `/var/log/messages` and `cloudera-scm-agent.log` for NFS timeouts. Confirm NFS availability. Once the issue is resolved by the customer (e.g., freeing up space), Cloudera services should return to healthy state automatically.",
            "category": "Infrastructure & Deployment",
            "context": "# mno - BigStreamer - IM2241809 - Every Host on PR1 and DR1 are in critical state\n\n<b>Description:</b>\n\n```\nGood evening,\n\nAll Hosts in Dr1 and PR1 are in critical state\n\nPlease take action.\n```\n\n<b>Actions Taken:</b>\n\n**Steps in order to investigate and make sure that the table is not created**\n1. Login to `PR` and `DR` cloudera manager in order to check the health of each cluster. The status was unhealthy for all services on both clusters.\n2. Login to `Grafana` in order to check that applications running. All the applications were running without errors.\n3. ssh to `pr1edge01.mno.gr` with personal account\n4. sudo to root\n5. Move to the log folder:\n```bash\ncd /var/log\n```\n6. Check messages file\n```bash\nless messages\n```\nThe output was:\n![image](.media/IM2241809/pr1edge01_messages.png)\n\n7. From the above output we saw that at `22:13:02 pr1edge01_kernel: nfs: server 999.999.999.999 not responding`.\n8. Now lets check the `agent logs` of an internal node.\n9. ssh to `pr1node03.mno.gr` with personal account\n10. sudo to root\n11. Move to the log folder:\n```bash\ncd /var/log/cloudera-scm-agent\n```\n12. Check `cloudera-scm-agent.log` file\n```bash\nless cloudera-scm-agent.log\n```\nThe output was:\n![image](.media/IM2241809/pr1node03_agent_logs.png)\n\n13. Due to unavaliability of `nfs storage`(responisibility of the customer to maintain), `Host Monitor` service of Cloudera management services had `timeout` errors because couldn't collect metrics from each filesystem of the nodes.\n14. Customer informed that `nfs storage` caused the issue on both clusters and the unhealthy state of all services was not real because `Host Monitor` was not able to collect metrics in order to be appeared on `CM`. Also all flows ran without errors during the issue.\n15. Customer informed us that the `nfs` storage was full and after their actions it's ok. We checked the `CM` and all the services now is healthy.\n\n<b>Root Cause Analysis:</b>\n\nThis problem occurred due to `nfs` unavaliability.\n\n<b>Our Ticket Response:</b>\n\n```\nGood evening,\n\nThe issue was caused by the nfs storage used on the nodes of both clusters becoming full. This resulted in the host monitor of the cloudera management services timeouting as it was unable to collect metrics for each filesystem of the nodes.\n\nRelevant screenshots are attached showing the above causes of the issue.\n\nThroughout the issue, the flows were up and running as seen in grafana as it was a malfunction of the management services resulting in the incorrect image of all services in Cloudera Manager PR & DR respectively.\n\nAfter space was freed up on the nfs, both clusters returned to good health.\n\nThank you.\n```\n\n<b>Affected Systems:</b>\n\nDisaster/Primary Site\n\n\n**Well Done!!**",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context directly supports the question. It describes a scenario where all Cloudera services appear unhealthy due to NFS-related issues and provides detailed troubleshooting steps including log inspection and root cause analysis. The answer is fully derivable from this context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This is a highly relevant operational question for telecom engineers using BigStreamer. Understanding how NFS storage issues can affect Cloudera service health is critical for effective monitoring and incident resolution.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear and self-contained. It specifies the issue (all Cloudera services unhealthy) and the suspected cause (NFS issues), allowing it to be understood without needing to refer to the context."
          },
          {
            "question": "What caused the Merge Batch job failure on 2023-05-31, and how was it resolved?",
            "answer": "The failure was due to increased response sizes for service `/CAMPAIGNMANAGEMENT/GETCAMPAIGNS`, doubling average JSON size post 2023-05-12. The job was split into time chunks and run over three days. After completion, `distcp` was used for replication and upsert to HBase executed.",
            "category": "Data Management & Query Execution",
            "context": "# mno - BigStreamer - IM2158906 - Failed job at Grafana\n\n<b>Description:</b>\n\n```\nGood evening\nFollowing ticket SD2221480, the failed job [IBank_Ingestion]-[MergeBatch]-[JOB] appeared again but for d1edge01.mno.gr\n\nThank you.\n```\n\n<b>Actions Taken:</b>\nAfter communicating with the customer, we proceeded to manually kill the job as it was running for over 14 hours and was affecting live production flows.\n\nProceeding to investigate the issue, we saw the following:\n\n1. Going to Cloudera Manager => Yarn => Applications =>\n\n``\nname RLIKE '.*PROD_IBank_MergeBatch' and application_duration > 3h\n``\n\nWe notice that the Merge batch for `20230531` was running `14 hours` without finishing.\n\n2. As a second step, let's check the number of records in `impala` and the space occupied in `hdfs` by each `par_dt` from `20230509` to `20230530`\n\nThe number of records in `impala` as an example for the `par_dt` `20230511` and `20230512`:\n\nWhat we observe above is that while the `par_dt` `20230511` and `20230512` they have no difference in the number of records, they are twice as large. Where did this come from and what impact does it have? We will analyze it in the next steps.\n\n3. Let's see how this increase came about:\n\nAnalyzing the sum of length for response_text_data for each service, we notice that from `12/05/2023` onwards the service_name `'/CAMPAIGNMANAGEMENT/GETCAMPAIGNS'` takes up much more space as shown below compared to previous days.\n\nUntil 11/05/2023 the average avg(length(response_text_data)) of prod_trlog_ibank.service_audit is stable at ~12K while from 12/05/2023 we see it approximately doubling.\n\nThis resulted in the `Merge Batch` not finishing as since the size of each `json` has doubled it takes much longer to execute the spark job.\n\n4. How did we handle it to get it running?\n\nAs described [here] in the subchapter `If the problem is with resources (out-of-memory errors):` we ran the `Merge Batch` in separate chunks of the day.\n\nThe process took 3 days to complete as each chunk of the day took ~9 hours.\n\n5. After it was completed on one site, `distcp` was performed for the `prod_trlog_ibank.service_audit` and `prod_trlog_ibank.service_audit_old` tables with `par_dt` `20230530` on the other site. Finally, on the other site, after `distcp` was completed, we ran [upsert-to-hbase-migration]\n\n6. Finally, after all the steps were completed on both sites, the developers manually deleted the old data in `kudu`.\n\nAfter all the above was completed, we should proceed with the consultation with the bank, to execute the `DWH` flows for the days that did not run due to the above issue. The `DWH` flow was removed, so we did not need to take any action.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context thoroughly explains the root cause of the job failure (increased JSON size for a specific service), the timeline (from May 12 onwards), the exact service involved, the intermediate steps (splitting the job), and the resolution steps including HBase upsert and `distcp`. It fully supports the answer provided.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The question addresses a concrete failure scenario encountered in production, involving performance issues, Spark job execution, and resolution procedures involving distributed systems. This type of issue is highly relevant for engineers operating BigStreamer in telecom environments.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is specific and fully understandable on its own. It clearly refers to a known job failure on a specific date and asks both about the cause and resolution, without vague or context-dependent language."
          },
          {
            "question": "How can you reduce the risk of Kudu scan timeouts during Spark MergeBatch jobs?",
            "answer": "Ensure the correct number of partitions is used. In the case of the Online MergeBatch job, 468 partitions caused timeout errors. It was fixed by adjusting to 180 partitions for `prod_trlog_online.service_audit_stream` and deleting unnecessary Kudu disk data.",
            "category": "Data Management & Query Execution",
            "context": "# mno - BigStreamer - IM2193241 - Failed job in Grafana\n\n</b>Description:</b>\n\n```text\nGood morning...\n\nThe following failed job appeared in Grafana today 26/07:\n\nApplication: Online_Ingestion\nJob_name: MergeBatch\nComponment: JOB\nDate: 25-07-2023\nHost: pr1edge01.mno.gr\n\nPlease take action.\n```\n\n<b>Actions Taken:</b>\n\n1. Re-run the failed step as described [here](../supportDocuments/applicationFlows/online.md#batch)\n2. The flow completed successfully, we proceeded with the investigation\n\n   Logs from the application:\n\n   ```\n   Caused by: org.apache.kudu.client.NonRecoverableException: cannot complete before timeout: ScanRequest(scannerId=\"22c757bfcf674a05a08f14c316e745e9\", tablet=c42b07f18435403297fee37add478c0b, attempt=1, KuduRpc(method=Scan, tablet=c42b07f18435403297fee37add478c0b, attempt=1, TimeoutTracker(timeout=30000, elapsed=30004), Trace Summary(0 ms): Sent(1), Received(0), Delayed(0), MasterRefresh(0), AuthRefresh(0), Truncated: false \n   ```\n\n   Spark UI:\n\n   ![Spark UI](.media/IM2193241_1.png)\n\n3. Stage 0 should have 180 partitions not 468\n\n   ![Spark UI normal](.media/IM2193241_2.png)\n\n4. Informed development team to correct the number of partitions for `prod_trlog_online.service_audit_stream`. This deleted unnecessary data from Kudu's disks and next run (see 3) did not have any failed tasks.\n\n<b>Affected Systems:</b>\n\nmno Primary Site\n\n<b>Action Points:</b>",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context clearly describes the cause of the failure (Kudu scan timeout), presents evidence from logs and Spark UI, and shows that reducing the number of partitions and cleaning up disk space resolved the issue. This supports the answer completely.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This is highly relevant to engineers operating Spark jobs on BigStreamer, as it deals with partition tuning, performance bottlenecks, and resolution of critical runtime errors  all typical challenges in real production environments.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is fully self-contained, specifying the system (Kudu), issue (scan timeouts), and job type (Spark MergeBatch) without referring to prior context or using vague language."
          },
          {
            "question": "How is the Radius flow triggered, and what are the key processing steps for radacct data?",
            "answer": "The Radius flow is triggered every 1h and 30 minutes via a coordinator. Files are fetched from the input SFTP server and stored temporarily. Then they are decompressed, loaded into Hive (staging tables), and enriched using reference data. The final enriched CSV files are sent to the Trustcenter SFTP server, and finally inserted into Kudu and Impala tables (`radius.radacct`).",
            "category": "Application Functionality & Flow",
            "context": "# Radius\n\n## Main Flow\n\n``` mermaid\n    graph TD\n    subgraph AA[Startup]\n      direction TB\n      AA1(\"Cleanup HDFS Folder:\" /ez/warehouse/radius.db/tmp)-->AA2(\"Create Local Folder:\" ./sftp_files)-->AA3(\"Create Local Folder:\" ./exported_files)-->AA4(\"Housekeep Trustcenter SFTP server\")\n    end\n    subgraph AB[Load]\n      AB1(\"SFTP Server\") -->|Excluding:<br>Non-valid filenames<br>Files with suffix .LOADED<br>Files older than `days_back` according to filename | AB2(\"Temporary Directory on Nodemanager: ./sftp_files\")\n      AB1 -->|Add suffix .LOADED to<br>filenames in HDFS file:<br>/user/radius/unrenamed_files|AB1\n      AB2 -->|Decompress| AB3(\"Temporary Directory on Nodemanager: ./sftp_files\")\n    end\n    subgraph AC[radarchive]\n      AC1(\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\") -->|Hive LOAD|AC2(\"Hive: radius.radarchive_stg\")\n      AC2 -->|Refresh/Impala Insert|AC3(\"Impala: radius.radarchive\")\n      AC2 -->|Refresh/Impala Upsert<br>Reduced fields|AC4(\"Kudu: radius.radreference\")\n    end\n    subgraph AD[radacct]\n      subgraph ADA[File Export]\n        direction TB\n        AD1(\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\") -->|Hive LOAD|AD2(\"Hive: radius.radacct_stg\")\n        AD6(\"npce.fixed_super_repo<br>Responsibility: abc\") -->AD8\n        AD7(\"demo.dummy_radius_dslams<br>Responsibility: abc\") -->AD8\n        AD2 --> AD8(\"Join\")\n        AD8 -->|Refresh/Impala Select|AD9(\"Temporary Directory on Nodemanager:<br> ./exported_files/radacct_enriched_yearMonthDay_startHour-endHour.csv\")\n        AD9 -->|SFTP Put|AD10(\"Trustcenter SFTP\")\n      end\n      subgraph ADB[Populate Table]\n        direction TB\n        AD3(\"Kudu: radius.radreference\") -->AD4\n        AD11(\"Hive: radius.radacct_stg\") -->AD4(\"Join on username\")\n        AD4 -->|Refresh/Impala Insert|AD5(\"Impala: radius.radacct\")\n      end\n      AD10 -.->|On successful SFTP PUT<br>Populate the table| AD3\n    end\n    subgraph AE[Finish]\n      AE1(\"SFTP Server: Add suffix .LOADED to file\")\n    end\n    AA4 --> AB1\n    AB3 -->|Filename: radarchive_yearMonthDay_startHour-endHour.csv|AC1\n    AB3 -->|Filename: radacct_yearMonthDay_startHour-endHour.csv|AD1\n    AC4 -->|On Success|AE1\n    AD5 -->|On Success|AE1\n```\n\n- **User**: `radius`  \n- **Coordinator**: `Radius_Load_Coordinator`  \n- **Workflow**: `Radius_Load_Workflow`\n- **HDFS path**: `/user/radius`\n- **Runs**: `every 1h and 30mins`\n- **Config file**: `HDFS: /user/radius/config/settings_prod.ini`\n- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n- **Input SFTP Server**:\n  - **Host**: `999.999.999.999`\n  - **Port**: `22`\n  - **User**: `prdts`\n  - **Remdef Files Folder**: `/home/prdts/transfer`\n  - **Port Forward**:\n    - **Host**: `un-vip.bigdata.abc.gr`\n    - **Port**: `2222`\n- **Trustcenter SFTP Server**:\n  - **Host**: `unc2.bigdata.abc.gr`\n  - **Port**: `22`\n  - **User**: `trustcenterftp`\n  - **Remdef Files Folder**: `/rd`\n\n[...additional context truncated for brevity...]",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context provides a detailed mermaid diagram and textual description of the flow, including timing, trigger mechanism (coordinator), and specific steps for handling radacct files. The answer is directly grounded in the details from the context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Understanding how and when the Radius flow runs and how radacct data is processed is critical for telecom engineers managing ingestion pipelines and troubleshooting job execution issues. This is highly relevant to their operations.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear, precise, and fully understandable without any external context. It explicitly asks about the trigger mechanism and steps for a specific data type, making it fully standalone."
          },
          {
            "question": "What is the role of the `210_refData_Load.sh` and `220_refData_Daily_Snapshot.sh` scripts in the Reference Data Flow?",
            "answer": "`210_refData_Load.sh` loads reference CSV files (e.g., cells_20230530.csv.gz) into Hive load tables. It must be executed with the reference type and date:\n```bash\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20230530\n```\nThen `220_refData_Daily_Snapshot.sh` creates a daily snapshot from the most recent partition into snapshot tables for consumption.",
            "category": "Application Functionality & Flow",
            "context": "# Reference Data Flow\n\n## Installation info\n\n### Data Source File\n- Local FileSystem Directories\n  - host :`un2.bigdata.abc.gr (999.999.999.999)`\n  - user : `vantagerd`\n  - spool area : `/data/1/vantage_ref-data/REF-DATA/` link points to `/shared/vantage_ref-data/REF-DATA`\n  - file_types : `<refType>_<refDate>.csv.gz`  \n<refType>: `cells, crm, devices, services` (i.e. cells_20230530.csv.gz)\n  - load_suffix : `<YYYYMMDD>.LOADED` *(i.e. cells_20230530.csv.cells_20230531.LOADED)*\n\n- HDFS Directories\n\t- hdfs landingzone : `/ez/landingzone/REFDATA`\n\n### Scripts-Logs Locations\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\n- user : `intra`\n- script path : `/shared/abc/refdata/bin`\n- script files: \n\t- `210_refData_Load.sh`\n\t- `220_refData_Daily_Snapshot.sh`\n\n- log path : `/shared/abc/refdata/log`\n- log files: \n\t- `210_refData_Load.<YYYYMM>.log`\n\t- `220_refData_Daily_Snapshot.<YYYYMM>.log`\n\n### Crontab Scheduling\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\n- user : `intra`  \n\truns at : Daily at 00:05\n\t`5 0 * * * /shared/abc/refdata/bin/210_refData_Load.sh CELLS $(date '+%Y%m%d' -d \"yesterday\")`\n\t\nNdef1: The entry above loads reference data for CELLS.  \nNdef2: For each reference type a new entry of the `210_refData_Load.sh` is required passing the following parameters  \n<reference Type> : `cells, crm, devices, services`  \n<reference Date> : `yesterday` is the default value  \n\n### Hive Tables\n- Target Database: `refdata`\n- Target Tables: \n\t1. `rd_cells_load`\n\t1. `rd_services_load`\n\t1. `rd_crm_load`\n\t1. `rf_devices_load`\n\n## Data process\n### High Level Overview\n\nSteps 1-3:\nabc is responsible for the preparation/creation of the Reference Data flat files.\nThese files are stored into a specific directory in `UN2` node using the SFTP-PUT method as user `vantagerd`\n\nSteps 4-5:\nScript `210_refData_Load.sh` is responsible to read, parse and load the contents of reference files into HIVE tables (aka LOAD tables).\nThese tables keep the data of all completed loads. That is, they contain all the historicity of the reference data.\nThe data of each load is stored in a separate partition identified by the date of the loading (i.e. par_dt=20230530)\n\nSteps 6-7:\nScript `220_refData_Daily_Snapshot.sh` reads the most recently added data from the LOAD table and store them as a snapshot into a separate table (aka snapshot tables).\nThe data of these tables are used for the enrichment of various fact data (i.e. Traffica (SAI))\n\n## Manually Run\n`210_refData_Load.sh` script is responsible for the loading of a reference file.\n\nTo run the script two arguments are required  \n`/shared/abc/refdata/bin/210_refData_Load.sh <refType>  <refDate>`  \n\n1st: **<refType>**, the Reference Type\n- CELLS\n- CRM\n- DEVICES\n- SERVICES\n\n2nd: **<refDate>**, the date that the flat file contains in its filename\n\ni.e.\n- cells_20220207.csv.gz\n- cells_20220208.csv.gz\n- cells_20220209.csv.gz\n- services_20220207.csv.gz\n- devices_20220208.csv.gz\n- crm_20220209.csv.gz\n\nOnce the loads completed, the flat files renamed to <CURRENT_YYYYMMDD>.LOADED\n\n## Troubleshooting\nCurrently, the Reference Data flow does not support the `Monitoring` services.\nAn email will be sent by the system with the point of failure.\n\n- Check the log files for errors/exceptions\n- In case of failure follow the instructions described in **Manually Run**\n\n### Common errors\n- Reference data file is empty or the contents of the file is not the expected.  \nIf this is the case, update abc that the file is invalid and ask them to send a new one.\n- Other factors not related to the specific flow (impala/hive availability, Kerberos authentication)\n\n## Data Check\n- Check final tables for new partitions\n- Check the amount of data in final tables",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context explicitly describes the role of both scripts, including what they load, how they store partitions, and their usage examples. The question is fully and unambiguously answerable from the context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Understanding the role of these scripts is essential for managing reference data ingestion and ensuring data integrity across analytics processes. This is a common and practical task for BigStreamer engineers.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear, self-contained, and understandable without any external information. It names both scripts and asks for their role within the Reference Data Flow, which is unambiguous."
          },
          {
            "question": "How should you troubleshoot the `AppEmptyQueryException` alert caused by missing CPU/MEM interface data in the IPVPN flow?",
            "answer": "Check the logs:\n```bash\ntail -f /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20231012.log\n```\nLook for messages like 'No file found.. sleep and try again..'. Also confirm no files were created on the NNM server by checking:\n```bash\nssh custompoller@nnmprd01\ntail -f /home/custompoller/export_metrics/log/transfer-new-files-cut.20231012.log\n```",
            "category": "Troubleshooting & Issue Resolution",
            "context": "[[[_TOC_]]\n# Introduction\nThis support document outlines the steps that compose the IPVPN-SM replacement process that computes Key Performance Indicators (KPIs) regarding the network quality of abc. The ETL process involves a series of scripts and a Spring Boot application that work together to retrieve, transform, and load data from Impala tables into a remdef server.\n\nThere are three major KPI categories, *Components*, *SLA* and *Interfaces*, which consist of the following subcategories:\n|KPI category|Metrics involved| Relevant Impala tables|\n|---|---|---|\n|Components|CPU, Memory (MEM)|bigcust.nnm_ipvpn_componentmetrics_hist,<br>  nnmnps.nms_node |\n|Interfaces|Interfaces (IF)|bigcust.perf_interfacemetrics_ipvpn_hist,<br>  bigcust.pe_interfaces,<br>  nnmnps.nms_node|\n|SLA|Availability (AV), Quality of Service (QOS)|bigcust.nnmcp_ipvpn_slametrics_hist,<br> bigcust.customer_pl,<br> bigcust.customer_sla_config_ipvpn,<br> bigcust.sla_configurations|\n...\n(Truncated for brevity - full technical documentation continues with details on application behavior, logs, troubleshooting, and manual override instructions.)",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context provides detailed information on the IPVPN-SM application's flow, failure handling, exception types including AppEmptyQueryException, and log paths. It even mentions specific log files and what to look for, which makes the answer fully grounded in the context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question targets a common operational issue in a production BigStreamer pipeline (missing data due to AppEmptyQueryException in the IPVPN flow). Troubleshooting this directly affects service reliability and data completeness, which is highly relevant for telecom engineers.",
            "question_standalone_score": "4",
            "judge_feedback_question_standalone": "The question is mostly self-contained, but it slightly depends on internal knowledge of what 'AppEmptyQueryException' and 'IPVPN flow' refer to. While likely known by engineers working in this context, these could be obscure to an outsider, preventing a perfect 5."
          },
          {
            "question": "How can you resolve a failed Prometheus import flow if a partition was partially inserted?",
            "answer": "Use Impala to drop the faulty partition:\n```sql\nALTER TABLE prometheus.dwh22 DROP IF EXISTS PARTITION (par_dt='20220401');\n```\nThen, rerun the Oozie workflow `Prometheus-Import-Workflow` for the failed date via Hue interface.",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# Prometheus\n\n## Useful Links\n\n[Infrastructure](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/Infastructure)  \n[Monitoring](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/home#monitoring)  \n\n\n## Oozie workflow\n\n``` mermaid\n  graph TD\n    A[Oracle DB table DWSRC.DWH22  <br> via Port Forward 999.999.999.999:6634] -->|Sqoop Import| B[HDFS Staging Directory]\n    B -->|Hive Load| C[Hive: prometheus.dwh22]\n    C -->|Impala Refresh| D[Impala: prometheus.dwh22]\n```\n\nRuns every day at `06:30 AM UTC`\n\n**User**: `prometheus`  \n**Coordinator**: `Prometheus-Coordinator`  \n**Workflow**: `Prometheus-Import-Workflow`  \n**Source Database**:  \n\n- **Host**: `999.999.999.999`  \n- **Port**: `1521`  \n- **SID**: `A7`\n- **User**: `bigstreamer`  \n\n**Target Table**: `prometheus.dwh22`  \n**HDFS Installation Directory**: `/user/prometheus/flows`  \n**HDFS Staging Directory**: `/ez/warehouse/prometheus.db/tmp_sqoop_jobs/`\n\n**Alerts**:\n\n- Mail with subject: `Prometheus Flow failed`\n\n**Troubleshooting Steps**:\n\n- Check messages written to Monitoring App\n\n    - Check monitoring app for successful executions:  \n        - From `un2` with personal account:\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'`\n \n    - Check monitoring app for failed executions:  \n  \n        - From `un2` with personal account:\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=FAILED&operativePartition=<timestamp e.g.:20220518>'`\n    - Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n    - Grafana link: `https://unc1.bigdata.abc.gr:3000/d/PcKYyfTVz/prometheus-dashboard?orgId=1&from=now-2d&to=now`\n\n- Check if partition is loaded:\n\n  From `Hue` as `prometheus` in `Impala Editor`:\n\n  ``` sql\n  SHOW PARTITIONS prometheus.dwh22;\n  SELECT COUNT(*) FROM prometheus.dwh22 WHERE par_dt='<par_dt>';\n  ```\n\n- Check logs for failed steps:  \n\n  From `Hue` as `prometheus` in `Workflows`:\n\n  - Search for `Prometheus-Import-Workflow` and filter for failed\n  - Go to logs and check both stdout and stderr\n\n- In case a partition has partially been inserted into the final table `prometheus.dwh22` and the error that caused the failure has been resolved:\n\n  From `Hue` as `prometheus` in `Impala Editor`:\n\n    ``` sql\n    ALTER TABLE prometheus.dwh22 DROP IF EXISTS PARTITION (par_dt='<par_dt>');\n    ```\n\n  - For the previous day:\n\n    From `Hue` as `prometheus` in `Workflows`:\n\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n\n  - For the previous day:\n\n    From `Hue` as `prometheus` in `Workflows`:\n\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n\n  - For partitions older than yesterday:\n\n    From `Hue` as `prometheus` in `File Browser`:\n\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and set `days_back` to the number of days back needed to reach the partition\n\n    From `Hue` as `prometheus` in `Workflows`:\n\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n\n    From `Hue` as `prometheus` in `File Browser`:\n\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and restore `days_back` to `1`",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context explicitly explains that in case of a partially inserted partition, one should drop the partition and rerun the Prometheus import workflow from Hue. The provided answer matches this instruction exactly, making the question fully answerable from context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Handling failed Prometheus import flows is a routine and critical task for data engineers working with BigStreamer. This scenario (partial partition insert) is realistic and operationally important, so the question is highly relevant.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clearly worded and fully understandable on its own. It mentions a specific exception scenario and names the flow, making it self-contained and unambiguous."
          },
          {
            "question": "What steps are involved in exporting TeMIP alarms from Kudu to Impala?",
            "answer": "The `TeMIP_kudu_2_Impala_CO` Oozie coordinator runs daily at 06:00. It transfers older records from:\n- `temip.temip_kudu_terminated_alarms` to `temip.temip_impala_terminated_alarms`\n- `temip.temip_kudu_historic_events` to `temip.temip_impala_historic_events`\nMonitor the workflow `TeMIP_kudu_2_Impala_WF` through Hue and check logs for errors.",
            "category": "Infrastructure & Deployment",
            "context": "# TeMIP\n\n## Overview\n\nThe `abc TeMIP alarms live feed to BigStreamer` application is a Java application hosted on a Wildfly application server. The objective of the application is to receive and store (in near real time) the TeMIP alarms (from specific TeMIP Operation Contexts) into the BigStreamer ecosystem. The `Apache Kudu` storage engine was selected in order to achieve near real time CRUD operations (Create, Read, Update, Delete). The `Apache Impala` is used for extended data retention (6 months). The `Apache Oozie` scheduler  is used in order to automatically run the necessary scripts.\n\n...\n\n### Move Kudu to Impala\n\nThe `Move Kudu to Impala` flow consists of a coordinator called `TeMIP_kudu_2_Impala_CO` which executes once a day and is responsible for moving the alarms from kudu to the equivalent impala table.\n\n``` mermaid\n flowchart TD\n  A[OOZIE Job] \n  Z[Kudu: temip.temip_kudu_terminated_alarms]\n  K[Kudu: temip.temip_kudu_historic_events]\n  A --> Z\n  A --> K\n  Z --> |Move older alarms to impala|H[Impala: temip.temip_impala_terminated_alarms]\n  K --> |Move older events to impala|L[Impala: temip.temip_impala_historic_events]\n```\n\n- **Oozie Coordinator**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_kudu_2_Impala_CO`\n    - **Execution:** `everyday at 06:00 local time`\n    - **Approximate Duration:** `15 minutes`\n    - **Workflow:** `TeMIP_kudu_2_Impala_WF`\n      - **Script:** `hdfs:/user/temip/temip_kudu_to_impala.sh`\n      - **Logs:** `Through Oozie Job in HUE`\n\n**Alerts:**\n\n- **Not Monitored**\n\n**Troubleshooting Steps:**\n\n1. Check that workflow `TeMIP_kudu_2_Impala_WF` runs successfully. This can be done through accessing `Hue` with `temip` user and selecting `Jobs`. Then filter the jobs with `user:temip` and look for a `job` with the below properties:\n    - **Name:**  `oozie:launcher:T=shell:W=temip_kudu_to_impala:A=shell-661a:*`\n    - **Type:** `Oozie Launcher`\n    - **Execution Time:** `06:00 AM`\n\n    Select the desired shell execution, view its logs and search for any ERRORS.\n1. After the root cause of the problem has been identified, re-rerun the failed execution. This can be done through accessing `Hue` with `temip` user.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context clearly describes that the TeMIP_kudu_2_Impala_CO coordinator handles moving data from Kudu to Impala, and provides the exact source and destination tables involved. It also confirms the daily schedule and mentions workflow and log monitoring through Hue. The answer aligns precisely with this information.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Understanding how to export TeMIP alarms from Kudu to Impala is a critical task for engineers maintaining data pipelines in BigStreamer, especially for long-term storage and querying. This operation is part of daily ETL processes, making the question highly relevant.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear and fully self-contained. It specifies what is being exported (TeMIP alarms), from where (Kudu), to where (Impala), and asks for the steps involved, without needing additional context."
          },
          {
            "question": "What is the process for replacing hardware on a BigStreamer node and contacting Dell support?",
            "answer": "1. SSH into the node and run:\n```bash\nsudo -i\nipmitool lan print | grep -i 'IP Address'\n```\n2. Access IDRAC via browser, get the service tag.\n3. Call Dell support and provide the tag.\n4. Collect TSR logs from IDRAC and send them to Dell.\n5. Inform abc before any IDRAC operation.",
            "category": "Infrastructure & Deployment",
            "context": "# abc - BigStreamer - How to open a ticket to DELL\n\n<b>Description:</b>\n\n```\nBelow is a step-by-step description of the process from opening a ticket to collecting TSR logs from IDRAC.\n```\n\n<b>Actions Taken:</b>\n\n1. ssh with your personal account on the issue node.\n2. sudo -i;ipmitool lan print | grep -i 'IP Address' # To find the Managment IP. Otherwise e.g grep nodew /etc/hosts \n```\nIf the ipmitool package did not exist just install it. yum install ipmitool;\n```\n3. connect via vnc. Open firefox and type the `IP Address` from step 2\n4. From `Server-->Overview-->Server Information` copy the `Service Tag number`\n5. Call Dell support `2108129800`. They need the `Service Tag number` from step 4\n6. An engineer will create a case and sent you all the necessary steps. If not the link to collect the TSR logs is `https://www.dell.com/support/kbdoc/el-gr/000126803/export-a-supportassist-collection-via-idrac7-and-idrac8`\n7. Inform `abc` before any action on the IDRAC.\n8. Download localy the TSR gz file. ssh on the node with vnc (e.g un4). The downloaded files stored under `/home/cloudera/Downloads/` and the format is `TSRdate_service_tag.zip`\n9. Send the zip file/files to DELL and wait for their response.\n\nDone!",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The answer is fully grounded in the context, which clearly outlines the entire process of hardware replacement and Dell support interactionfrom SSH access, identifying the IDRAC IP, locating the service tag, to contacting Dell and handling TSR logs.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This is a highly relevant operational question for telecom engineers managing BigStreamer infrastructure. Hardware failures are critical, and knowing how to interact with Dell support is essential.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely self-contained and unambiguous. It clearly asks for the hardware replacement and support process on a BigStreamer node without needing external information."
          },
          {
            "question": "How can you verify if Trafficas flow is paused and resume it?",
            "answer": "Check status:\n```bash\ncurl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disabled\"\n```\nIf the output is true, resume with:\n```bash\ncurl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\"\n```",
            "category": "Data Management & Query Execution",
            "context": "# Traffica Flow\n\n## Useful links\n\n- [Wiki](https://metis.ghi.com/obss/bigdata/abc/etl/traffica/traffica-devops/-/wikis/home)\n- [Infrastructure](https://metis.ghi.com/obss/bigdata/abc/etl/traffica/traffica-devops/-/wikis/Infrastructure)\n\n... [truncated for brevity]",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context clearly outlines the HTTP endpoints used to check the paused status and resume the Traffica flow, making the question fully answerable with no ambiguity.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Verifying and resuming paused flows is a critical operational task for telecom engineers ensuring data pipelines like Traffica remain functional. This is highly relevant to BigStreamer system operations.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely self-contained and specific. It clearly asks about verifying and resuming a paused flow, requiring no additional context to understand."
          },
          {
            "question": "How are LTE data files for TrustCenter generated and validated?",
            "answer": "The Oozie coordinator `Location_Mobility_2Hour_CO` executes `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` every 2 hours. It extracts data from `eea.eea_hour`, creates files like `LM_02_lte_yyyyMMdd_000xx.txt`, and uploads them to `/lm` via SFTP. Validation includes checking `/shared/abc/location_mobility/logging/LM_02_lte_reconciliation.log` for the correct file entries and line counts.",
            "category": "Data Management & Query Execution",
            "context": "# TrustCenter Flows\n\n[Full detailed context provided with exact filenames, scripts, paths, and log file usage]",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context provides all relevant details for how LTE data files are generated and validated, including scripts used, scheduling, filenames, data source, and validation log paths. The answer is fully grounded in this context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question is highly relevant for telecom engineers using BigStreamer, especially those responsible for ensuring timely data delivery to TrustCenter and verifying data integrity.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely self-contained and unambiguous. It clearly asks for the process of file generation and validation for LTE data without needing any additional context."
          },
          {
            "question": "How do you increase Java heap space for Streamsets and validate the changes?",
            "answer": "1. Go to Cloudera Manager > Streamsets > Configuration > Java Options and set:\n```bash\n-Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow\n```\n2. Remove old configs from 'Data Collector Advanced Configuration Snippet' if present.\n3. Restart Streamsets.\n4. Validate with:\n```bash\nps -ef | grep -i streamsets | grep -i xmx\njmap -heap <pid>\n```",
            "category": "Infrastructure & Deployment",
            "context": "# Streamsets - Java Heap Space\n\n        Streamsets Java Heap Memory was increased due to [this](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/102#ndef_95081) issue.\n\n        Actions Taken:\n\n        1. Configure Java Options from CLoudera Manager\n\n           ```bash\n           cluster -> Streamsets -> Configuration -> Java options: `-Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow`\n           ```\n\n        2. Remove old configuration\n\n           ```bash\n           cluster-> Streamsets -> Configuration -> Data Collector Advanced Configuration Snippet (Safety Valve) for sdc-env.sh\n           ```\n\n           ```bash\n           #Remove the following line, if exists\n           export SDC_JAVA_OPTS=\\\"-Xmx16384m -Xms16384m -server -XX:-OmitStackTraceInFastThrow ${SDC_JAVA_OPTS}\\\"\n           ```\n\n        3. Restart Streamsets\n\n           ```bash\n           cluster -> Streamsets -> Restart\n           ```\n\n        4. Check Streamsets Process Options\n\n           ```bash\n           [root@un2 ~]# ps -ef | grep -i streamsets | grep -i xmx\n           ```\n\n        5. Check Max Java Heap Space for Streamsets via jconsole or jmap\n\n           **jconsole**\n\n           ```bash\n           /usr/java/latest/bin/jconsole\n           ```\n\n           Select `com.streamsets.pipeline.BootstrapMain`, `Connect` and check the metrics.\n\n           **jmap**\n\n           ```bash\n           jmap -heap <pid>\n           ```\n\n           Sample output and detailed memory info follows.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully answerable using the provided context. The steps to increase Java heap space in Streamsets and validate those changes using both `ps` and `jmap` are described in detail, including example outputs and where to configure settings in Cloudera Manager.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question is highly relevant for BigStreamer engineers since managing and optimizing Streamsets memory is critical for stable pipeline execution, especially in high-volume telecom environments.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is self-contained and unambiguous. It clearly asks for two specific tasks: increasing Java heap space and validating the configuration, without relying on external context."
          },
          {
            "question": "How can you repair OpenLDAP replication between kerb1 and kerb2 if the Manager password has changed?",
            "answer": "1. Prepare `replication_config.ldif` with updated credentials.\n2. Apply config using:\n```bash\nldapmodify -H ldaps://kerb1.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\nldapmodify -H ldaps://kerb2.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\n```\n3. Test replication by creating a test user on kerb1 and verifying it appears on kerb2 via ldapsearch.",
            "category": "Infrastructure & Deployment",
            "context": "# How to fix openldap replication\n\nThe broken replication between `kerb1`/`kerb2` could happened in case any of the below cases appeared/happened.\n\n- Case 1: You changed the `Manager` password of openldap instance\n- Case 2: Replication broken for any other reason (power outage etc) between `kerb1` and `kerb2`\n\nFor every ldasearch the password of `Manager` is [here](KnowledgeBase/prodsyspasswd.kdbx)\n\n# For Case 1 follow the below steps:\n\nLogin into kerb1 node as root\n\n```bash\nssh kerb1\nsudo -i\n```\nBackup:\n\n```bash\nslapcat -n 0 -l config.ldif\nslapcat -n 2 -l data.ldif\n```\nCreate ldif file replication fix\n\n```bash\nvi replication_config.ldif\n\ndn: olcDatabase={0}config,cn=config\nchangetype:modify\nreplace: olcSyncrepl\nolcSyncrepl: rid=001\n  provider=ldaps://kerb1.bigdata.abc.gr/\n  binddn=\"cn=config\"\n  bindmethod=simple\n  credentials=\"new password\"\n  searchbase=\"cn=config\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nolcSyncrepl: rid=002\n  provider=ldaps://kerb2.bigdata.abc.gr/\n  binddn=\"cn=config\"\n  bindmethod=simple\n  credentials=\"new password\"\n  searchbase=\"cn=config\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\n\nadd: olcMirrorMode\nolcMirrorMode: TRUE\n\ndn: olcDatabase={2}bdb,cn=config\nchangetype:modify\nreplace: olcSyncrepl\nolcSyncrepl: rid=003\n  provider=ldaps://kerb1.bigdata.abc.gr/\n  binddn=\"cn=Manager,dc=bigdata,dc=abc,dc=gr\"\n  bindmethod=simple\n  credentials=`new password`\n  searchbase=\"dc=bigdata,dc=abc,dc=gr\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nolcSyncrepl: rid=004\n  provider=ldaps://kerb2.bigdata.abc.gr/\n  binddn=\"cn=Manager,dc=bigdata,dc=abc,dc=gr\"\n  bindmethod=simple\n  credentials=`new password`\n  searchbase=\"dc=bigdata,dc=abc,dc=gr\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\n\nadd: olcMirrorMode\nolcMirrorMode: TRUE\n```\n\nFix the replication:\n\n```bash\nldapmodify  -H ldaps://kerb1.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\nldapmodify  -H ldaps://kerb2.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\n```\n\nChecks:\n\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:\n\nLogin into admin node as root:\n\nOpen firefox\n```bash\nfirefox\n```\nphpldapadmin link: (https://kerb1.bigdata.abc.gr/phpldapadmin/)\n\n# Steps to create an ldap user\n\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\n2. Expand tree `people`\n3. Click `create a new entry here`\n4. The type of account will be `User account`\n5. Fill all the empty boxes. \n6. The user will be `no login` and the group/gid `disabled`\n7. Create object\n\nAfter succesfully creation of `testuser` check if exist on `kerb2` from the dropdown menu or via ldapsearch\n\n```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=testuser'\n```\n\nIf user exist then replication fixed. Just delete the `testuser`.\n\n# Steps to delete an ldap user\n\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\n2. Expand tree `people`\n3. Check the new user and from the right bar click `delete this entry`",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context provides detailed steps for updating replication credentials in OpenLDAP and validating that replication is restored, fully answering the question.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question addresses a real-world failure scenario (Manager password change) in LDAP replication which is essential for authentication infrastructure in BigStreamer systems.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question clearly presents the problem and environment (OpenLDAP, replication between kerb1 and kerb2, Manager password change), making it fully self-contained and unambiguous."
          },
          {
            "question": "How can you check whether anonymization and retention scripts ran successfully?",
            "answer": "1. For retention:\n```bash\ngrep \"Script Status\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.*.log | tail -n1\n```\nCheck `Status=0`. If not, grep for errors:\n```bash\negrep -i '(error|problem|except|fail)' /shared/abc/cdo/log/Retention/*<SnapshotID>*.log\n```\n2. For anonymization:\n```bash\ngrep \"Script Status\" /shared/abc/cdo/log/100.Anonymize_Data_Main.*.log | tail -n1\negrep '(:ERROR|with errors)' /shared/abc/cdo/log/Anonymize/*<RunID>*\n```",
            "category": "Data Management & Query Execution",
            "context": "#check Retention\nlogin @un2 as intra\n1st level:\n$ grep \"Script Status\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.202012.log | tail -n1\n\n.. Script Status ==> Scr:203.Retention_Dynamic_Drop_DDL.sh, Dt:2020-12-18 08:13:12, Status:0, Snapshot:1608267602, RunID:1608271202, ExpRows:3327, Secs:790, 00:13:10\n\nif Status != 0 we have a problem\n\n---\n\n2nd level:\nwe get the Snapshot ID from the above (e.g. Snapshot:1608267602)\n\n$ egrep -i '(error|problem|except|fail)' /shared/abc/cdo/log/Retention/*1608267602*.log\n\nif it comes out < 10 it doesn't particularly worry us.\nIf it comes out a lot it's not good.\n\n#Anonymization\n$ grep \"Script Status\" /shared/abc/cdo/log/100.Anonymize_Data_Main.202012.log | tail -n1\nex: Script Status ==> Scr:100.Anonymize_Data_Main.sh, Dt:2020-12-17 21:01:03, Status:, RunID:1608228002, Secs:3661, 01:01:01\n\nwe take RunID from the above (.. RunID:1608228002)\n$ egrep '(:ERROR|with errors)' /shared/abc/cdo/log/Anonymize/*1608228002*.log | less\n\n> 0 we have a problem",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context explicitly describes both retention and anonymization verification procedures, including the exact commands and what to look for in the output. The answer is entirely grounded in the provided material.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Script verification for anonymization and retention is critical for compliance, data integrity, and operational reliability in telecom data platforms like BigStreamer. This is highly relevant to daily tasks of engineers.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clearly worded and conveys a specific intent  checking the success of two types of scripts  without depending on external context."
          },
          {
            "question": "How do you deduplicate rows in the `energy_efficiency.cell` table for a specific date range?",
            "answer": "1. Backup existing data:\n```sql\nCREATE TABLE energy_efficiency.cell_bak LIKE energy_efficiency.cell;\nINSERT INTO TABLE energy_efficiency.cell_bak PARTITION (par_dt) SELECT * FROM energy_efficiency.cell;\n```\n2. Overwrite with distinct records:\n```sql\nINSERT OVERWRITE TABLE energy_efficiency.cell PARTITION (par_dt)\nSELECT DISTINCT * FROM energy_efficiency.cell WHERE par_dt BETWEEN '20211210' AND '20211215';\n```",
            "category": "Data Management & Query Execution",
            "context": "# Streamsets - Energy Efficiency\n\n## Access\n\nStreamsets Login Page: https://999.999.999.999:18636/\n\nFiles:\n\nFrom un2 with sdc user:\n\n```bash\nsftp bigd@999.999.999.999\ncd /ossrc\n```\n\n## Check for Duplicates\n\nExecute the following from Impala\n\n```bash\nselect count(*), par_dt from energy_efficiency.cell where par_dt>'202111201' group by par_dt order by par_dt desc;\n```\n\n```bash\nselect count(*) from (select distinct * from energy_efficiency.cell where par_dt='20211210') a;\n```\n\n## Solve Duplicates\n\nExecute the following from Impala\n\nBackup table:\n```bash\nCREATE TABLE  energy_efficiency.cell LIKE energy_efficiency.cell;\nINSERT INTO TABLE energy_efficiency.cell_bak PARTITION (par_dt) SELECT * FROM energy_efficiency.cell;\n```\n\nModify table:\n```bash\nINSERT OVERWRITE TABLE energy_efficiency.cell partition (par_dt)\n\tSELECT DISTINCT * FROM energy_efficiency.cell\n\tWHERE par_dt between '20211210' and '20211215';\n```\n\nDrop Backup table:\n```bash\nDROP TABLE energy_efficiency.cell;\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context contains detailed, explicit SQL commands and procedures for deduplicating rows in the specified table and date range. The answer is entirely based on the provided context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Data deduplication in production tables like `energy_efficiency.cell` is a highly relevant operational task for engineers managing data pipelines in telecom environments.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is fully understandable on its own. It clearly specifies the table and operation (deduplication over a date range) without requiring prior context."
          },
          {
            "question": "What are the steps to create a keytab file for a user in NYMA?",
            "answer": "1. SSH to kerb1 and run:\n```bash\nkadmin.local\nlistprincs <user>@CNE.abc.GR\naddprinc <user>@CNE.abc.GR\nktadd -k /tmp/<user>.keytab -norandkey <user>@CNE.abc.GR\n```\n2. Copy the keytab to `un2`:\n```bash\nscp -p /tmp/<user>.keytab un2:/tmp\n```\n3. On `un2`, move it to the correct user folder:\n```bash\ncp /tmp/<user>.keytab /home/users/skokkoris/\nchown skokkoris. /home/users/skokkoris/<user>.keytab\n```",
            "category": "Application Functionality & Flow",
            "context": "# How to create a keytab in NYMA\n\nLogin into kerb1 node as root\n\n```bash\nssh kerb1\nsudo -i\n```\n\nUse command-line interface to the Kerberos administration system\n\n```bash\nkadmin.local\n```\n\nCheck if there is a principal for the corresponding username\n\n```bash\nlistprincs <username>@CNE.abc.GR\n```\n\nCreate a principal if there is not one\n\n```bash\naddprinc <username>CNE.abc.GR\n```\n\nCreate the keytab\n\n```bash\nktadd -k /tmp/<username>.keytab -norandkey <username>@CNE.abc.GR\n```\n\nCopy the keytab file to un2 node\n\n```bash\nscp -p /tmp/<username>.keytab un2:/tmp\n```\n\nLogin into un2, place keytab file under /home/users/skokkoris/ and change ownership into skokkoris\n\n```bash\nssh un2\nsudo -i\ncp -p /tmp/<username>.keytab /home/users/skokkoris/\nchown skokkoris. /home/users/skokkoris/<username>.keytab\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The steps in the answer are directly and completely derived from the context, which outlines the procedure for creating and deploying a keytab file for a user in NYMA.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Creating keytab files is a common and critical task in authentication management, especially in secured telecom infrastructure environments like BigStreamer.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is fully understandable on its own. It clearly specifies what is needed (keytab creation), for whom (a user), and in which system (NYMA)."
          },
          {
            "question": "How do you set up a Kubernetes service account for RAN.AI access with kubeconfig?",
            "answer": "1. Create `service_account.yml` and `role_binding.yml` and apply:\n```bash\nkubectl apply -f service_account.yml\nkubectl apply -f role_binding.yml\n```\n2. If k8s version >1.24, create a secret:\n```yaml\nkind: Secret\nmetadata:\n  name: <secret>\n  annotations:\n    kubernetes.io/service-account.name: <account>\n```\n3. Get kubeconfig with:\n```bash\nkubectl view-serviceaccount-kubeconfig -n <namespace> <account> > ~/.kube/config\n```",
            "category": "Application Functionality & Flow",
            "context": "# Kubernetes User Environment Setup\n\n## Tools\n\nThe main tool that needs to be installed is **kubectl**, instructions for which can be found\n[here](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/). Additionally after\ninstallation, completion can be enabled by executing:\n```bash\nmkdir -p /etc/bash_completion.d\nkubectl completion bash | sudo tee /etc/bash_completion.d/kubectl\n```\nIf possible a nice to have plugin for setting up kubeconfigs can be found at\n[kubectl-view-serviceaccount-kubeconfig-plugin](https://github.com/superbrothers/kubectl-view-serviceaccount-kubeconfig-plugin/releases). Simply place the binary under `/usr/local/bin/`.\n\nAdditionally in order to install **helm**, follow the (instructions](https://helm.sh/docs/intro/install/)\nand set up completion by executing the following:\n```bash\nhelm completion bash | sudo tee /etc/bash_completion.d/helm\n```\n\n## Service Account\n\nCreate the following YAML files, that contain the definition for the service account and its\nrole binding:\n- **`service_account.yml`**\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: <Account Name>\n  namespace: <RAN.AI Namespace>\n```\n- **`role_binding.yml`**\n```yaml\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: <Binding Name>\n  namespace: <RAN.AI Namespace>\nsubjects:\n- kind: ServiceAccount\n  name: <Account Name>\n  namespace: <RAN.AI Namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit\n```\n\nApply both in sequence:\n```bash\nkubectl apply -f service_account.yml\nkubectl apply -f role_binding.yml\n```\n\n### User Secret\n\nFor Kubernetes versions over 1.24 when creating a service account it's secret is not automatically created\nand mounted, so in that case create the following secret:\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <Account Secret Name>\n  namespace: <RAN.AI Namespace>\n  annotations:\n    kubernetes.io/service-account.name: <Account Name>\ntype: kubernetes.io/service-account-token\n```\n\nExecute the following to get the new users config. If the plugin is not available the config must be constructed manually:\n```\nkubectl view-serviceaccount-kubeconfig -n <RAN.AI Namespace> <Account Name> > <User's Home>/.kube/config\n```\n\nFor reference the config looks like this:\n```yaml\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: ...\n    server: https://<Server>:<Port>\n  name: <Cluster Name>\ncontexts:\n- context:\n    cluster: <Cluster Name>\n    namespace: <RAN.AI Namespace>\n    user: <Account Name>\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: <Account Name>\n  user:\n    token: ...\n```\nIn the above segment `certificate-authority-data` and `token` can be obtained by executing:\n```bash\n# Find the Account's Secret Token name\nkubectl get secrets -n <RAN.AI Namespace>\n\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.ca\\.crt}'\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.token}' | base64 --decode\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The steps in the answer are directly supported by the context, including detailed YAML configuration and commands for both creating service accounts and retrieving kubeconfig credentials.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Setting up Kubernetes service accounts with kubeconfig is a key task in managing secure access to BigStreamer environments like RAN.AI, making this highly relevant to telecom engineers.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear, specific, and understandable without any need for external information. It states the goal (setup service account with kubeconfig) and the scope (for RAN.AI)."
          },
          {
            "question": "How do you manually execute the Cube Indicators pipeline from the terminal?",
            "answer": "1. SSH into `un1.bigdata.abc.gr` as `intra` and go to:\n```bash\ncd projects/cube_ind\n```\n2. Fetch latest script:\n```bash\nkinit -kt /home/intra/intra.keytab intra\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\n```\n3. Edit and run `run_cube.sh` with the correct date:\n```bash\nvim run_cube.sh\n./run_cube.sh\n```",
            "category": "Troubleshooting & Issue Resolution",
            "context": "### Execute Cube Indicators via Terminal\n\n1. Connect to `un1.bigdata.abc.gr` and change user to `intra` using sudo.\n\n2. Change directory \n```\ncd projects/cube_ind\n```\n\n3. Remove old pyspark script\n```\nrm Indicators_Spark_Job.py\n```\n\n4. Get new one from HDFS\n```\nkinit -kt /home/intra/intra.keytab intra\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\n```\n\n6. Edit submit script to change execution date. Execution date equals the missing date + 2. E.g data for 20210101 will be loaded on 20210103.\n```\nvim run_cube.sh\n...verhead=4096 Indicators_Spark_Job.py <date> brond.dsl_s...\n```\n\n7. Run the submit script \n```\n./run_cube.sh\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context provides step-by-step instructions for manually executing the Cube Indicators pipeline, matching the answer exactly. All commands and actions are covered.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Running the Cube Indicators pipeline is a core BigStreamer task, relevant for engineers ensuring data pipelines are processed correctly. It directly supports real-world troubleshooting and maintenance.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is specific and clear on its own. It mentions the exact task (manually executing the Cube Indicators pipeline) and the interface (terminal), making it self-contained."
          },
          {
            "question": "What should you check if the `Coord_Cube_Spark_Indicators` job fails?",
            "answer": "Check dependencies:\n- `brond.fixed_radio_matches_unq_inp` via `101_fixed_radius.sh`\n- `radius.radacct_hist` via `radius.pl`\n- `brond.brond_retrains_hist` via `brond_retrains.pl`\nEnsure all source tables are populated before triggering `Coord_Cube_Spark_Indicators`.",
            "category": "Troubleshooting & Issue Resolution",
            "context": "### Cube Indicators Pipeline\n\n* Coordinator `1011_Fixed_brond_customers`  populates table `brond.fixed_brond_customers_daily`\n* Coordinator `Coord_post_BROND_FIXED_CUSTOMERS` populates tables `brond.fixed_brond_customers_daily_unq` & `brond.fixed_customers_brond_latest`\n* Coordinator `Coord_Cube_Spark_Indicators` for `par_date=date -2 days` populates table `brond.cube_indicators`  and its dependencies are:\n  * `brond.fixed_radio_matches_unq_inp` populated by script `un2:/shared/abc/brond/bin/101_fixed_radius.sh`\n  * `brond.fixed_brond_customers_daily_unq`\n  * `radius.radacct_hist` populated by script `un2:/shared/abc/radius/DataParser/scripts/radius.pl`\n  * `brond.brond_retrains_hist` populated by script `un2:/shared/abc/brond/DataParser/scripts/brond_retrains.pl`\n  * `brond.dsl_stats_week_xdsl_hist` populated by Coordinator `coord_brond_load_dsl_daily_stats`",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully answerable using the provided context. The dependencies of the `Coord_Cube_Spark_Indicators` job are clearly listed, along with the scripts responsible for populating them.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This is a highly relevant question for telecom engineers working with BigStreamer, especially those monitoring pipeline execution and debugging job failures.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear and specific. It refers to a well-defined job and asks what checks are necessary if it fails, requiring no prior context to be understood."
          },
          {
            "question": "How can you diagnose and prevent duplicate job submissions in CDSW for scheduled workflows like 'Set_Point_Automation'?",
            "answer": "1. Log in to CDSW Admin panel and inspect the usage and job history.\n2. SSH into `mncdsw1.bigdata.abc.gr` and run:\n```bash\nkubectl get pods\nkubectl logs <cron-pod-name> | grep job=<jobId>\n```\n3. Verify if the CDSW scheduler is submitting the same job twice.\n4. Confirm no duplicated entries in the scheduler DB (access via `psql` inside the db pod) and ensure cron configurations do not overlap.\n5. Adjust the scheduling configuration in the `crons` table if needed.",
            "category": "Application Functionality & Flow",
            "context": "# abc - BigStreamer - IM1900072 - Execution schedule job\n\n<b>Description:</b>\n\n```\nGood morning,\nIt was observed that there is a problem with the jobs in cloudera data science, specifically the Energy Bills project and the Set_Point_Automation job at the time and date that it is scheduled to run, it was observed that it starts and runs 2 times at the same time.\nThis is easy to understand from the job history and the cases are the ones I have attached with a screenshot.\nThank you and for any clarification I am at your disposal\n```\n\n<b>Actions Taken:</b>\n\n1. Login to CDSW on https://mncdsw1.bigdata.abc.gr with personal account.\n2. From the Admin tab select usage.\n3. Confirm duplicate submitted jobs.\n4. SSH to `mncdsw1.bigdata.abc.gr` with personal account and change to `root` with sudo\n5. Find the scheduler pod\n``` bash\nkubectl get pods\n... <pod list> ...\n```\n6. Confirm that jobs were submitted by the CDSW scheduler\n``` bash\nkubectl logs cron-7d5f5656c7-ws77j | grep job=624 \n# Job ID can be obtained from the URL when inspecting the job from the Web GUI\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context contains clear and detailed steps related to diagnosing duplicate job submissions in CDSW, including logging into pods, checking logs, and inspecting the scheduler database. The question is fully answerable based on this information.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This is highly relevant to telecom engineers working with BigStreamer, particularly those dealing with automation pipelines in CDSW. Identifying and resolving duplicate job submissions is a critical operational task.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is fully self-contained and understandable. It clearly defines the context of 'duplicate job submissions' and specifies the environment ('CDSW' and the job name), requiring no additional information to interpret."
          },
          {
            "question": "Why does the CDSW job 'pollaploi' from the Energy Bills project fail frequently, and how should it be configured?",
            "answer": "The job uses excessive cluster resources: 100 Spark executors  4GB = 400GB RAM and 200 vcores. Since the cluster has 1TB RAM, this can cause OOM errors. Reduce executors or apply dynamic allocation more efficiently:\n```python\n.config(\"spark.executor.instances\", \"20\")\n.config(\"spark.executor.memory\", \"2g\")\n```",
            "category": "Application Functionality & Flow",
            "context": "# abc - BigStreamer - IM2172470 - abc (492) BigStreamer - CDH - energy_efficiency.pollaploi\n\n<b>Description:</b>\n\n```\nGood morning,\nwe have a problem with the pollaploi job located in the Energy Bills project in impala\nthe frequency of it failing has increased quite a bit\n```\n\n<b>Actions Taken:</b>\n\nAfter communication with customer we undestand that the issue occurs for job at workbench and not for flow. So:\n\n1. Login to Cloudera Data Science Workbench with your personal account (https://mncdsw1.bigdata.abc.gr/)\n\n2. Click on the left **Sessions** tab and then on **Scope** select **All Projects** and click on **Energy Bills** Project and find **Pollaploi** job.\n\n3. Go on **History** tab and you will see that there are a lot of pollaploi jobs with status Failure\n\n4. Click on one job with status Failure and then go to **See job details** and then click on **Script: Energy_Bills_Automation/Energy_Bills_Automation.py**\n\n5. When investigated the script we saw below snippet of spark configuration:\n\n```bash\nspark = SparkSession.builder\\\n.master(\"yarn\")\\\n.config(\"spark.submit.deployMode\", \"client\")\\\n.config(\"spark.eventLog.enabled\", \"true\")\\\n.config(\"spark.executor.instances\", \"100\")\\\n.config(\"spark.executor.cores\", \"2\")\\\n.config(\"spark.executor.memory\", \"4g\")\\\n.config(\"spark.rpc.message.maxSize\", \"1024\")\\\n.config(\"spark.executor.memoryOverhead\", \"800\")\\\n.config(\"spark.driver.memory\", \"4g\")\\\n.config(\"spark.driver.memoryOverhead\", \"800\")\\\n.config(\"spark.spark.driver.maxResultSize\", \"4g\")\\\n.config(\"spark.executor.dynamicAllocation.initialExecutors\", \"4\")\\\n.config(\"spark.executor.dynamicAllocation.minExecutors\", \"4\")\\\n.config(\"spark.executor.dynamicAllocation.maxExecutors\", \"4\")\\\n.config(\"spark.sql.broadcastTimeout\", \"1000\")\\\n.config(\"spark.kryoserializer.buffer.max\", \"1024m\")\\\n.getOrCreate()\n```\n\nSo, there are 100 instances * 2 cores = 200 vcores\nand 100 instances * 4G ram = 400GB ram\n\nThe cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.\n\n<b>Our Ticket Response:</b>\n\n```bash\nGood evening,\n\nUpon investigation we noticed that the job you mentioned fails with an out-of-memory error.\n\nAdditionally, according to the spark configuration snippet below in your job:\n\n```\n.master(\"yarn\")\\\n.config(\"spark.submit.deployMode\", \"client\")\\\n.config(\"spark.eventLog.enabled\", \"true\")\\\n.config(\"spark.executor.instances\", \"100\")\\\n.config(\"spark.executor.cores\", \"2\")\\\n.config(\"spark.executor.memory\", \"4g\")\\\n```\n\nWe see that you have given 100 instances * 2 cores = 200 vcores and 100 instances * 4Gram = 400GB ram\n\nThe cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.\n\nTherefore, the problem concerns the specific job. For this reason, jobs should be optimized according to the guidance given in an earlier communication for a similar issue, taking into account the configuration that has already been done in the cluster.\n\nIf you don't need anything else please if we can proceed to close the ticket.\n\nThanks\n```\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer CDSW",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context clearly explains the root cause of the job failures (OOM due to excessive Spark resource allocation) and includes the relevant Spark configuration. The answer is fully grounded in the provided context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This is highly relevant to telecom engineers using BigStreamer in production, particularly those managing CDSW workflows and Spark jobs. Preventing job failure is a critical operational concern.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is fully self-contained. It names the job, the platform (CDSW), and the associated project, and clearly describes the failure behavior and the information being requested."
          },
          {
            "question": "What causes the error 'Engine exited with status 34' on jobs in CDSW and how is it resolved?",
            "answer": "The error is caused by the CNI plugin failing to assign IPs due to a stalled weave pod. Restart the Docker daemon indirectly using supervisord:\n```bash\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart <CDSW_DOCKER_ID>\n```",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# abc - IM1923742 - Job's problem\n\n<b>Description:</b>\n\n```\nGood morning,\nit has been observed that jobs show the problem Engine exited with status 34.\nsome of them are:\n Set_Point_Automation job in the Set Point Automation project (error today 22/7)\n Cabins Live Measurements job in the Energy Bills project (error yesterday 21/7)\n Flows_update_all_counters_12:00_no_par_dt job in the Monitoring Flows project (error yesterday 7/15)\n```\n\n<b>Actions Taken:</b>\n\n1. Connect with your personal LDAP account in 'https://mncdsw1.bigdata.abc.gr/'\n2. Go to last tab (admin).\n3. Select `Activity` tab.\n4. Inspect the Jobs in question.\n\nThe jobs are in `FAILED` status. The logs for the failed applications are missing.\n\n5. Troubleshoot from the command line:\n\nFrom `mncdsw1` as root (use personal account and then sudo):\n\n```bash\nkubectl get pods -w -A # Wait a pod to fail (namespace should be like default-user-XXX)\n\n# After a while, a pod has failed, describe it\n\nkubectl describe pod -n default-user-XXX XXXXXXXX\n```\n\n``` logs\nEvents\nWarning  FailedCreatePodSandBox  10s  kubelet, wrkcdsw4.bigdata.abc.gr  Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"...\" network for pod \"XXXXXXXX\": networkPlugin cni failed to set up pod \"XXXXXXXX_default\" network: unable to allocate IP address: Post http://127.0.0.1:6784/ip/....: dial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container \"....\" network for pod \"XXXXXXXX\": networkPlugin cni failed to teardown pod \"XXXXXXXX_default\" network: Delete http://127.0.0.1:6784/ip/....: dial tcp 127.0.0.1:6784: connect: connection refused]\n```\n\nThis error points us to the CNI plugin\n\nCheck the logs for the weave pods\n\n```bash\nkubectl logs -n kube-system weave-net-XXXXX\n# Weave pod in wrkcdsw4 has stopped logging events\n```\n\nThe pod was not responding and could not be deleted.\n\n7. Restart the docker daemon to restart all containers on `wrkcdsw4`\n\n_At the time of the issue, CDSW had stale configuration that required full restart (outage) which was not desirable_\n\nTo avoid applying the settings, restart the service with the same configuration by triggering a restart by `supervisord` deployed as part of the Cloudera agent\n\nFrom wrkcdsw4 as root (use personal account and then sudo):\n\n```bash\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf status | grep DOCKER\n# Sample\n# 145071-cdsw-CDSW_DOCKER          RUNNING   pid 39353, uptime 29 days, 0:40:20\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart 145071-cdsw-CDSW_DOCKER\n```\n\n8. Check that the node is operational after the restart\n\nFrom `mncdsw1` as root (use personal account and then sudo):\n\n```bash\ncdsw status # You might have to wait a few minutes\n```\n\n9. Inform the customer about the problem\n\n```text\nA component of CDSW on worker node 4 encountered a problem resulting in jobs running on that node not being able to start. The function of the component has been restored and the jobs are now running normally.\n\nPlease let us know if you need anything else or if we can proceed to close the ticket.\n```\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer CDSW",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context explicitly identifies the root cause (CNI plugin failure due to a non-responsive weave pod) and provides the exact resolution procedure involving a supervised Docker restart. The answer is fully grounded in this detailed troubleshooting flow.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question addresses a specific and recurring operational issue in BigStreamers CDSW environment that affects job execution across multiple projects. Diagnosing and resolving such failures is a critical real-world task for telecom engineers.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is fully understandable without needing the context. It names the error, the platform (CDSW), and the use case (scheduled jobs), and clearly seeks both the cause and the resolution."
          },
          {
            "question": "How do you fix RStudio Connect when LDAP authentication fails after a password reset for bind user `t1-svc-cnebind`?",
            "answer": "1. SSH into unrstudio1 and verify LDAP bind user credentials:\n```bash\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D \\\"t1-svc-cnebind\\\" -W -b \\\"dc=groupnet,dc=gr\\\"\n```\n2. If it fails, update the password in:\n```bash\nvi /etc/rstudio-connect/rstudio-connect.gcfg\n```\n3. Restart the service:\n```bash\nsystemctl restart rstudio-connect\n```",
            "category": "Troubleshooting & Issue Resolution",
            "context": "## Contents:\n\n<b>Description:</b>\n\n`RStudio user applications not working We would like to inform you that the applications of the user kmpoletis are not running showing the following error. This morning we reset the user's password because it had expired. It can connect normally with the credentials. Also let me know that the applications of the other users (Charisis, Ploskas) are running normally.`\n\n<b>Actions Taken:</b>\n\n- Ssh to **unrstudio1**\n\n- Do an ldap search using `t1-svc-cnebind` as the following in order to check if `t1-svc-cnebind` password is still valid:\n\n```\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D \\\"t1-svc-cnebind\\\" -W -b \\\"dc=groupnet,dc=gr\\\" '(sAMAccountName=t1-svc-cnebind)'\n```\n\n-  In the above case, if you get an error that you can't connect with the ldap server, that means it has an expired `t1-svc-cnebind` password, so you will need to replace it with a new one.\n\n- Check the configuration file below for the current ldap t1-svc-cnebind password & replace it with new **(Responsible for that abc Admin)**:\n```\nssh unrstudio1\nvi /etc/rstudio-connect/rstudio-connect.gcfg\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context fully describes the troubleshooting procedure for LDAP authentication failure due to an expired bind user password and outlines the exact steps included in the answer. The answer is thus completely grounded in the provided context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This issue affects user authentication and application availability on RStudio Connect, which is part of the BigStreamer ecosystem. Ensuring continuous access is essential, making this highly relevant to telecom engineers.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clearly phrased and provides all necessary details (issue, system, user role), making it fully understandable without needing the context."
          },
          {
            "question": "What steps do you take when CDSW refuses to start and reports errors like 'Failed to run CDSW Nodes Check'?",
            "answer": "1. Check health logs:\n```bash\nless /var/log/cdsw/cdsw_health.log\n```\n2. Investigate for control plane pod issues or ETCD probe failures.\n3. Confirm disk I/O isn't maxed out (`dm-7`) using:\n```bash\nlsblk | grep cdsw\n```\n4. Identify if Persistent Volumes are bound correctly with `kubectl get pv`.\n5. Remediate based on impalad host resource usage.",
            "category": "Infrastructure & Deployment",
            "context": "# abc - BigStreamer - IM2073052 - Application not working\n\n<b>Description:</b>\n\n```bash\nGood morning,\n\nIn CDSW we get error \"Unexpected Error. An unexpected error occurred\" when connecting. We saw that the node mncdsw1.bigdata.abc.gr was down. We did a restrart, just that, and it now appears to be in good health status.\n\nHowever, we still get the same error.\n\nIn CDSW status it has the following message\n\nFailed to run CDSW Nodes Check. * Failed to run CDSW system pods check. * Failed to run CDSW application pods check. * Failed to run CDSW services check. * Failed to run CDSW secrets check. * Failed to run CDSW persistent volumes check. * Failed to run...\n\nPlease for your checks.\n\nThanks\n```\n\n<b>Actions Taken:</b>\n\n1. Restart CDSW\n\n   The customer had already restarted CDSW, so we tried it once more in order to live monitor it.\n\n   ```bash\n   Cloudera Manager -> CDSW -> Restart\n   ```\n\n2. Check status\n\n   We followed the logs until CDSW was available again.\n\n   ```bash\n   #from mncdsw1\n   cdsw status\n   ...\n   Cloudera Data Science Workbench is ready!\n   ```\n\n   Since CDSW was up and running, we continued with root cause analysis.\n\n3. Check logs\n\n   ```bash\n   less /var/log/cdsw/cdsw_health.log\n   ```\n\n   Firstly, we noticed an abnormal behavior with some of the control plane pods:\n\n   ```bash\n   2023-01-29 05:50:53,868 ERROR cdsw.status:Pods not ready in cluster kube-system ['component/kube-controller-manager', 'component/kube-scheduler'].\n   ```\n\n   And after that, CDSW lost connection with apiserver pod completely:\n\n   ```bash\n   2023-01-29 05:51:42,392 WARNING urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError(...)': /api/v1/nodes\n   2023-01-29 05:51:43,371 ERROR cdsw.status:Failed to run CDSW Nodes Check.\n   ```\n\n4. Check node resources\n\n   From Cloudera Manager we saw that CPU and Memory were not increased but Disk I/O reached 100%.\n\n   From the image above we noticed that the issue occurred on dm-7.\n\n   ```bash\n   lsblk | grep cdsw-var\n   ```\n\n   Persistent Volumes were also checked:\n\n   ```bash\n   kubectl get pv\n   ```\n\n   Volumes are correctly bound and mounted under /var/lib/cdsw.\n\n5. Check kubelet logs\n\n   Kubelet logs pointed to liveness probe failures in etcd:\n\n   ```bash\n   Liveness probe for \"etcd...\" failed: HTTP probe failed with statuscode: 503\n   ```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context explicitly describes the issue ('Failed to run CDSW Nodes Check') and all the troubleshooting steps provided in the answer, making the answer fully grounded.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The question addresses a high-impact system-level failure in CDSW, which is integral to BigStreamer environments. It's highly relevant to telecom engineers responsible for system availability and deployment.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is precise, clearly describes the scenario (CDSW failing to start with a specific error), and can be understood independently of the context."
          },
          {
            "question": "How do you recover a Cloudera host like `mn3` that lost contact with Cloudera Manager due to CPU errors?",
            "answer": "1. Access iDRAC (e.g., https://10.255.242.85/), check server logs for hardware faults.\n2. Cold reboot the server via iDRAC.\n3. SSH into the node and verify agent:\n```bash\nsystemctl status cloudera-scm-agent\n```\n4. Confirm host is healthy again from Cloudera Manager.",
            "category": "Infrastructure & Deployment",
            "context": "# abc - BigStreamer - IM2104114 -  mn3 bad health\n\n<b>Description:</b>\n\n```\nGood morning,\n\nThe mn3 node is in bad health with the message This host has been out of contact with the Cloudera Manager Server for too long. This host is not in contact with the Host Monitor.\n\nPlease for your checks.\n```\n\n<b>Actions Taken:</b>\n\n1. From `admin` tried to reach mn3 but with no response:\n\n```bash\nping mn3\n```\n\n2. Since there is no response from `mn3` time to reboot server.\n\nLogin to `admin` from ad hoc connection and then type from command line `firefox`.\n\nSelect `default profile` and go to idrac for mn3 `https://10.255.242.85/` with the credentials you can find [here](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/prodsyspasswd.kdbx).\n\n3. From left Column click on `Server` and then go to `Logs`.\n\nAt that moment we see the following error for CPU:\n```bash\nMon Mar 13 2023 08:44:27 CPU 1 machine check error detected.\nMon Mar 13 2023 08:44:27 CPU 2 machine check error detected.\nMon Mar 13 2023 08:44:27 CPU 1 machine check error detected.\n```\n\n4. Time to reboot server:\n\nFrom `https://10.255.242.85/` go to Server and then click on `Power Cycle System (cold boot)`\n\n5. In order to check that server is up and running open java console, otherwise check logs from left Column click on `Server` and then go to `Logs`\n\n6. Login to Cloudera Manager with your personal account and then to `Hosts` -> `All hosts` and search for mn3.\n\nVerify that it is back again to cluster \n\n7. Login to `admin` and then ssh to `mn3` in order to verify that cloudera agent is up and running.\n\n```bash\nsystemctl status cloudera-scm-agent status\n```\n\n<b>Investigation:</b>\n\n1. Investigated `/var/log/messages` and we saw bellow info:\n\n```bash\nMar 13 09:10:47 mn3.bigdata.abc.gr systemd-logind[1628]: New session 6590707 of user nagios.\n...\n```\n\nAs we see at `09:10:47` mn3 lost contact from Cloudera Manager\n\n2. Investigated below logs between time that problem occurred but we didn't find any useful information\n\n```bash\njournalctl -S \"2023-03-13 08:00:00\" -U \"2023-03-13 10:50:00\"\nless /var/log/dmesg\nless /var/log/cloudera-scm-agent/cloudera-scm-agent.log\n```\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer Master/Management Services\n\n<b>Our Ticket Response:</b>\n\n```\nGood morning,\n\nAfter investigation we notice that there was a problem with the CPU due to some process and the node had to be rebooted in order to recover.\n\nWe will monitor the node and in case the problem appears again we will proceed with a ticket to Dell.\n\nIf you don't need anything else please let us know if we can proceed to close the ticket.\n\nThank you\n```\n",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The steps to recover the node from CPU errors and loss of contact with Cloudera Manager are directly detailed in the context. The answer mirrors these steps accurately.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Recovering a critical node like `mn3` is highly relevant to system stability and operations in BigStreamer environments. This task is essential for telecom engineers handling platform reliability.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear, provides necessary details (node name, error type, and Cloudera Manager), and can be fully understood without referencing the context."
          },
          {
            "question": "How can you confirm that Fraport branch probes are properly loaded into `bigcust.nnmcp_ipvpn_slametrics_hist`?",
            "answer": "1. Connect with Impala:\n```bash\nimpala-shell -i un-vip.bigdata.abc.gr -k --ssl\n```\n2. Run:\n```sql\nSELECT DISTINCT qa_probe_name FROM bigcust.nnmcp_ipvpn_slametrics_hist WHERE par_dt='20230202' AND customer='fraport';\n```\n3. If names are present, verify device types:\n```sql\n... AND device_type IN ('ce', 'cpe', 'nte', ...);\n```",
            "category": "Data Management & Query Execution",
            "context": "# abc - IM2076207 -  registration  Fraport\n\n<b>Description:</b>\n\n```\nGood evening,\n\nWe've noticed that we're not getting metrics at the following new points in the Fraport client (there don't seem to be any records at all)\n...\nPlease for your checks.\n\nThanks\n```\n\n<b>Actions Taken:</b>\n\n1. As seen from [here](../supportDocuments/applicationFlows/ip_vpn.md) branch metrics are exported by querying `bigcust.nnmcp_ipvpn_slametrics_hist`...\n\n...\n\n5. Inform the customer about the problem\n\n``` text\nGood evening,\n\nWe see that for the new points you mention there are SLA metrics in the respective tables. These points are excluded when generating the CSV as the probe name does not match what is expected...\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully answerable using only the context, which contains detailed Impala queries targeting the `bigcust.nnmcp_ipvpn_slametrics_hist` table, along with clear explanations about device types and probe name formats.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The question is highly relevant, as confirming the correct ingestion of network SLA metrics into BigStreamer is a core operational task for telecom engineers managing customer data pipelines.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear, unambiguous, and includes all the necessary information for understanding, including the specific table and probe types it refers to."
          },
          {
            "question": "What caused the Location Mobility flow to fail on 2023-04-19 and how was it resolved?",
            "answer": "1. Initial failure was due to unreachable impalad: `sn102.bigdata.abc.gr:22000`, caused by a memory-hogging query.\n2. Later failures related to a missing HDFS file triggered a 'Disk I/O error'.\n3. After dev team corrected the reference config table, the flow resumed. No restart needed.\nUse Impala to verify availability of partitioned files and adjust load info if required.",
            "category": "Data Management & Query Execution",
            "context": "### abc - BigStreamer - IM2131290 - Location Mobility Failure\n\n<b>Description:</b>\n\n```...```\n\n<b>Root Cause Analysis:</b>\n\nFirst thing that we have checked were the comments on the [md]... After checking the logs of the flow we saw that the flow was running successfully after it had failed for half day. The reason that it was able to run was the retention that had already taken place the day that we received the ticket.\n\nWe were able to identify the issue that caused the problem. After checking the logs of the host we saw that the host didn't have enough memory at the time.\n\n![sn102_memory](.media/sn102_memory.JPG) because of the user `ranai-geo` that had run an impala query \n\n![query](.media/query.JPG) that took all the resources of the impala daemon on sn102.\n\n![query_details](.media/q_details.JPG)\n\n<b>Actions Taken:</b>\n\nNo restart needed of the flow. There were some adjustments from dev team that took place.\n\nDev Resolution:\n\n```\nIt was necessary to make some corrections in the configuration table of the refdata.mediation_loc_mobility_load_info flow to synchronize the data sets that will be exported.\nAdditionally, due to the following exception a refresh table was added before the select in npce.eea_hour.\nERROR: Disk I/O error on sn111.bigdata.abc.gr:22000: Failed to open HDFS file hdfs://nameservice1/ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\nError(2): No such file or directory\nRoot cause: RemfghException: File does not exist: /ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The answer is entirely supported by the context, which includes a detailed timeline, error messages, root cause diagnosis, and the exact steps that were taken for resolution.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This is highly relevant to engineers maintaining BigStreamer, especially those involved in troubleshooting data pipelines and understanding resource-related job failures.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is fully self-contained and clear, specifying both the system (Location Mobility flow) and the timeframe (2023-04-19) with a clear request for cause and resolution."
          },
          {
            "question": "What steps should be followed if the `RA_Dsession` or `RA_Dtraffic` files are not exported?",
            "answer": "1. Check logs under `/shared/abc/location_mobility/log/`.\n2. Run Impala queries:\n```bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh --query=\"SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201023';\"\n```\n3. If `max(par_dt)` is already past export date, files were exported. To re-export, run:\n```bash\ncd /shared/abc/location_mobility/run\n./export_ra_bs_01.sh -t 20201115\n./export_ra_bs_02.sh -t 20201115\n```",
            "category": "Application Functionality & Flow",
            "context": "# abc - BigStreamer - IM1333238 - abc BigStreamer IT files (RA Dsession / RA Dtraffic)\n\n<b>Description:</b>\n\n```\nRA_Dsession/RA_Dtraffic are not exported\n```\n\n<b>Actions Taken:</b>\n\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `mtuser` with sudo\n2. Inspect logs of *RA* flow\n``` bash\ncd /shared/abc/location_mobility/log/\nless ra_export_bs_01.oozie.20201026.log\nless ra_export_bs_02.oozie.20201026.log\n```\n3. Check if max partition of source tables is greater or equal than the export date.\nIn `ra_export_bs_01.oozie.20201026.log`:\n``` bash\nQuery: SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201024'\n...\n[2020/10/26 09:00:22] - INFO: max_date=NULL and export_date=20201024\n```\nIn `ra_export_bs_02.oozie.20201026.log`:\n``` bash\nQuery: SELECT MAX(par_dt) FROM device_traffic WHERE par_dt >= '20201024'\n...\n[2020/10/26 09:00:22] - INFO: max_date=NULL and export_date=20201024\n```\nThe above messages show that data from the source tables have been exported to files already.\n\n4. Validate the results of the query:\nFor `RA_Dsession`:\n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201023';\"\n...\n+-------------+\n| max(par_dt) |\n+-------------+\n| 20201023    |\n+-------------+\nFetched 1 row(s) in 1.38s\n```\nFor `RA_Dtraffic`:\n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"SELECT MAX(par_dt) FROM device_traffic WHERE par_dt >= '20201023';\"\n...\n+-------------+\n| max(par_dt) |\n+-------------+\n| 20201023    |\n+-------------+\nFetched 1 row(s) in 1.38s\n```\n5. Files will be exported at the next execution if the source tables contain new entries. Due to the size of the exported files runs only for the previous day `par_dt`.\n\nIf the customer requests to generate the files for the missing days:\n\n``` bash\ncd /shared/abc/location_mobility/run\n./export_ra_bs_01.sh -t 20201115 # Run for specific date\n./export_ra_bs_02.sh -t 20201115 # Run for specific date\n```\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer Backend\n\n<b>Action Points:</b>\n\nN/A",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The answer is fully supported by the context, including specific log locations, Impala queries, and shell commands for re-exporting the data. It directly reflects the troubleshooting steps and logic provided.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question is highly relevant for telecom engineers dealing with daily exports of telecom-related session and traffic data in BigStreamer. It concerns production-critical data pipelines.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is fully self-contained and understandable without any external context. It clearly specifies which files are in question and asks about troubleshooting and remediation steps."
          },
          {
            "question": "How do you restore the `osix.sip` topology if data loading has stopped?",
            "answer": "1. SSH into `unosix1`, switch to `osix`, and kinit:\n```bash\nsudo -iu osix\nkinit -kt osix.keytab osix\n```\n2. Check YARN and listeners:\n```bash\nyarn application -list | grep OSIX-SIP-NORM\n```\nIf not running, restart from:\n```bash\n/home/users/osix/topologies/binary-input-impala-output/sip_norm/\n./submit_sip_norm.sh\n```\n3. Confirm with Impala:\n```sql\nSELECT count(*), par_dt FROM osix.sip WHERE par_dt > '20201124' GROUP BY par_dt;\n```",
            "category": "Application Functionality & Flow",
            "context": "# abc - BigStreamer - IM1367129 - osix.sip 25/11/2020\n\n<b>Description:</b>\n\n```\nGood evening,\n\nWe notice that no data is being loaded from 25/11/2020 at 07:00 on osix.sip.\n```\n\n<b>Actions Taken:</b>\n\n1. ssh unosix1 with your personal account\n2. sudo -iu osix\n3. kinit -kt osix.keytab osix\n4. yarn application -list | grep OSIX-SIP-NORM\n5. In our case the topology was down and the kudu script didn't resubmit it.\n6. Check if `coord_OsixStreaming_SIP_MonitorResubmit` is running.\n7. Check if listeners is healthy and data inserted `http://172.25.37.251/dashboard/#osix_listeners`\n8. The rate for `listen_sip_core` should be between 12K and 22K messages. If there is an extreme problem e.g. the rate is 0, consider restarting the problematic listener\n9. Check the logs of monitor script `hdfs dfs -ls /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM` and `hdfs dfs -cat /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM/date_monitor_sip_norm.log`\n10. Start again the topology `unosix1:/home/users/osix/topologies/binary-input-impala-output/sip_norm/` and execute `./submit_sip_norm.sh` until the state appeared `RUNNING`\n11. yarn application -list | grep OSIX-SIP-NORM\n12. Connect to impala-shell or Hue and execute `SELECT count(*), par_dt FROM osix.sip WHERE par_dt>'20201124' group by par_dt;` to check if the data inserted on the table.\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer  \n\n<b>Nfgh:<b>\n\nRecommended Mop for help `18316_abc_Generic_MOP_CDH_5_16_2_Upgrade`",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully answerable from the context, which contains detailed step-by-step actions for verifying and restarting the topology and validating data ingestion.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Restoring a streaming topology like `osix.sip` is a critical real-world task for telecom engineers using BigStreamer. The issue directly affects data availability and service quality.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear, specific, and self-contained. It provides all necessary information to be understood without referencing additional materials."
          },
          {
            "question": "What caused the Spark job failure in the Geolocation LTE pipeline and how should it be handled?",
            "answer": "Follow the documentation at `http://10.124.161.38/trac/hadoop/wiki/dev/project/abc/geolocation_viavi`. Check for older fail folders under `/ez/warehouse/geolocation.db/geo_<tech>_fail/`. Remove those older than 2 days. Failures are rare and no additional handling is usually necessary.",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# abc - BigStreamer - GI9 - abc Bigstreamer Geolocation mail for Spark job failure\n\n<b>Description:</b>\n\n```\nThe following mail arrived:\n\nGeolocation ALERT:[WARN] - Phase #4b, Spark job GeoViavi-LTE failed (1605552391)\n\nS550.Geo_Tech_Load_Data.sh\n2020-11-16 20:54:05 --> Phase #4\nSpark job GeoViavi-LTE failed with return_code 1.\nApplicationID:application_1599948124043_370289.\nLoadID:1605552391.\nStart time of job :1605552487.\n\nThis is an automated e-mail.\nPlease do not reply.\n```\n\n<b>Actions Taken:</b>\n\nThe [full documentation](http://10.124.161.38/trac/hadoop/wiki/dev/project/abc/geolocation_viavi) provided by the developers' team\n\nFollowed the `Failure Handling Manual Mechanism` section of the guide above\n\nIf there are other dates in the `/ez/warehouse/geolocation.db/geo_<technology>_fail/` HDFS folder that are older than two days, they can be deleted. Retention for the geolocation tables is 2 days for the eponymus and 1 day for the anonymous table.\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer Geolocation\n\n<b>Action Points:</b>\n\nNone, failures of that kind are rare and not worth the extra effort.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context clearly explains the cause of the Spark job failure and provides instructions on how to handle it. The answer aligns directly with the documented resolution steps.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This is a highly relevant question for telecom engineers managing BigStreamer. It concerns the stability of geolocation data pipelines, which are important for network monitoring and analytics.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear and specific without needing any additional context. It defines both the system (Geolocation LTE pipeline) and the issue (Spark job failure), making it fully standalone."
          },
          {
            "question": "How do you handle stale metadata issues or corrupted Parquet files in `osix.sip`?",
            "answer": "1. Run Impala:\n```sql\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='REGISTER');\n```\n2. If error persists, check file:\n```bash\nparquet-tools meta <file>\n```\n3. If not a valid Parquet file, move it:\n```bash\nhdfs dfs -mv <bad_file> /ez/landingzone/tmp/osix_sip/other\n```\n4. Refresh metadata again.",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# abc - BigStreamer - IM1364500 - abc BigStreamer oozie job hive2script failed / stale metadata\n\n<b>Description:</b>\n\n```\nImpala queries finish with error for table osix.sip and partition 20201123\nMessage:\nERROR processing query/statement. Error Code: 0, SQL state: File 'hdfs://nameservice1/ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/par_method=REGISTER/part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000' has an invalid version number: .??6\nThis could be due to stale metadata. Try running \"refresh osix.sip\".\n```\n\n<b>Actions Taken:</b>\n\n1. Checked that same query results in error using Hive.\n2. Checked that the problem occurs only with par_hr=08 partition. \n```\nbash\nselect distinct sip.callinguser \nas callinguser \nfrom OSIX.sip where par_dt='20201123' \nAND par_hr != '08' \nAND sip.callingUser IS NOT NULL;\n...\nFetched X rows in X seconds.\n```\n2. Inspected logs of Osix SIP application for that time. Login in `unosix1.bigdata.abc.gr`, switch user to `osix` and kinit first.\n```\nbash\n$ sudo su - osix\n$ cd\n$ kinit -kt osix.keytab osix\n$ yarn logs -applicationId application_1599948124043_405502\n```\n3. As `sn87.bigdata.abc.gr` was running a Spark executor of this application the time it was forced to shutdown, inspected if there are any corrupt files in the table. Login to any datanode first.\n```bash\n$ cd /var/run/cloudera-scm-agent/process/ \n$ ls -lahtr | grep -i hdfs\n$ cd <last directory>\n$ kinit -kt hdfs.keytab hdfs/`hostname -f`\n$ hdfs fsck /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08 -includeSnapshots\n...\nStatus healthy\n```\n4. Inspected format of written files. After communication with the dev team the batch id was retrieved so only a few files were checked. Login to un2.  \n```\n$ hdfs dfs -copyToLocal /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/part-*-17ead666-d5cb-437e-a849-c08ef825bec4.* .\n$ parquet-tools meta part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000\nfile:/home/users/u15/part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000 is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [46, -19, -49, 54]\n```\n4. Some of the files didn't have a correct parquet format so we removed them from the table.\n```bash\n$ hdfs dfs -mv hdfs dfs -mv  /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/par_method=OTHER/part-00005-17ead666-d5cb-437e-a849-c08ef825bec4.c000 /ez/landingzone/tmp/osix_sip/other\n...\n```\n\n5. Refresh the table and check that problem is fixed.\n```\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='REGISTER');\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='OTHER');\nselect count(*) from OSIX.sip where par_dt='20201123' ;\n```\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer Backend\n\n<b>Action Points:</b>\n\nN/A",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context explicitly outlines both the metadata issue and the steps taken to identify and resolve corrupted Parquet files in `osix.sip`. The answer reflects these steps clearly.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Stale metadata and file corruption in HDFS are high-priority issues for telecom engineers using BigStreamer. Resolving them is essential for maintaining data pipeline reliability.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is fully self-contained and specific. It clearly defines both the system (`osix.sip`) and the problem (stale metadata or corrupted files), making the query comprehensible without further context."
          },
          {
            "question": "How can you completely disable root SSH access on all nodes while ensuring scripts work with a substitute user?",
            "answer": "1. Create `backup_user`, add it to AllowedGroups, and configure rsync scripts:\n```bash\nHost wrkcdsw1.bigdata.abc.gr\n    User backup_user\n    IdentityFile /root/backup_user_id/id_backup_user_rsa\n```\n2. Disable root login:\n```bash\nsalt '*' cmd.script salt://prepare_sshd_config.sh\nsalt '*' cmd.script salt://disable_root_login.sh\nsalt '*' cmd.run 'service sshd reload'\n```",
            "category": "Infrastructure & Deployment",
            "context": "# abc - BigStreamer - GI7 - abc BigStreamer Configure root ssh login\n\n<b>Description:</b>\n\n```\nSecurity requirements from abc mandates that we should change the ssh configuration in all servers in order not to permit root ssh login\n```\n\n<b>Actions Taken:</b>\n\n1. Login to `admin.bigdata.abc.gr` with personal account and change to root with sudo\n2. Inspect the status of sshd on all nodes\n\n``` bash\nsalt '*' cmd.run 'grep \\\"^PermitRootLogin\\\" /etc/ssh/sshd_config'\n```\n\n3. Inspect cronjobs of root user on all nodes\n\n``` bash\nsalt '*' cmd.run 'cronjob -l'\n```\n\n4. After collecting the scripts executed by root user, we checked the scripts that contain the words `ssh`, `scp` and `rsync`\n\n``` bash\nfor i in {'/root/disk_balance_with_check.sh' ... '/home/intra/scripts/hue_workflows_all.sh' '/root/hive_logs_retention.sh'}\ndo\nsalt \\\"*\\\" cmd.run \\\"if [ -f $i ]; then grep -e ssh -e scp -e rsync $i; fi\\\" | grep -B1 -e ssh -e scp -e rsync\ndone\n```\n\n5. User `backup_user` was created by abc admins to substitute the root login in the scripts above. From the above investigation we determined that `backup_user` needs to be able to login to nodes `un1.bigdata.abc.gr` and `wrkmncdsw1.bigdata.abc.gr`\n\n```bash\n# Set ACLs and add to AllowedGroups in sshd_config on each node\n```\n\n6. Created saltstack state that installs private key for passwordless ssh under `/root/backup_user_id/id_backup_user_rsa`\n7. Updated relevant scripts to use `backup_user`\n8. Applied scripts to normalize sshd config and disable root login across all nodes:\n\n```bash\nsalt '*' cmd.script salt://prepare_sshd_config.sh\nsalt '*' cmd.script salt://disable_root_login.sh\nsalt '*' cmd.run 'service sshd reload'\n```\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer OS",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The answer is fully grounded in the context, which outlines the specific steps, salt scripts, user setup, file permissions, and SSH configuration changes required to disable root access while enabling `backup_user` as a substitute.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Managing SSH access across BigStreamer infrastructure is highly relevant for compliance and operational security. Ensuring automated scripts function post-change is critical in telecom environments.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is completely self-contained. It clearly defines the goal (disable root access) and the constraint (ensure script continuity), making it easy to understand without needing extra context."
          },
          {
            "question": "What is the proper procedure for updating the BIOS and iDRAC firmware on a Dell PowerEdge C6320 server via CLI?",
            "answer": "1. Download `.BIN` files for both BIOS and iDRAC.\n2. Upload to `/tmp/` on target server.\n3. Run:\n```bash\nchmod +x iDRAC*.BIN && ./iDRAC*.BIN\nchmod +x BIOS*.BIN && ./BIOS*.BIN\n```\n4. If iDRAC update fails, reset:\n```bash\nipmitool -U root -P c0sm0t31 mc reset cold\n```",
            "category": "Infrastructure & Deployment",
            "context": "# abc - BigStreamer - IM1363402 - abc BigStreamer - HW\n\n<b>Description:</b>\n\n```\nWe see that sn87 has a problem with the CPU (attached). It has gone out of cluster. Please for your actions and communication with DELL.\n```\n\n<b>Actions Taken:</b>\n\n1. Check Idrac logs for the description error `Overview-->Server-->Logs`\n2. Export the lifecycle logs `Overview-->Server-->Troubleshooting-->Support Assist-->Export Collection` and save the TSR*.zip\n3. Open a case on DELL SUPPORT(2108129800). Dell need the service tag from `Overview` of Idrac\n4. Send them the TSR*.zip\n5. In this case necessary was the update of BIOS & Lifecycle Controller of Idrac\n6. Dell send us the right update files based on our servers `PowerEdge C6320`\n7. Updated the BIOS base on the link `https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Updating%20BIOS%20on%20Dell%2013G%20PowerEdge%20Servers.pdf`\n(to update the BIOS via OS-CLI, see APPENDIX below )\n8. Updated the Lifecycle Controller base on the link `https://dl.dell.com/topicspdf/idrac_2-75_rn_en-us.pdf`\n(to update the Lifecycle Controller via OS-CLI, see APPENDIX below )\n9. After the update of both versions the host was up with the roles stopeed for 1 day.\n10. After 1 day send the lifecycle logs like `Step 2` and forward the zip file to Dell.\n11. If any error exist start the roles.\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer HW\n\n<b>References:</b>\nAPPENDIX  - Procedure for BIOS & F/W upgrade for C6320 with CLI (via OS shell)...",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The answer is fully grounded in the context. The context describes the BIOS and iDRAC update procedures in detail, including exact command sequences and error handling via `ipmitool`, which the assistant answer summarizes accurately.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This task is directly related to hardware maintenance for nodes running BigStreamer. BIOS and iDRAC updates are critical for system stability, especially after hardware faults that take nodes out of the cluster.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear and self-contained. It provides enough detail (hardware, goal, method) to be understandable and actionable without requiring prior context."
          },
          {
            "question": "How can you investigate missing log data from a specific syslog server?",
            "answer": "1. SSH into `un2` as root.\n2. Check `rsyslog.conf` and logrotate:\n```bash\ncat /etc/rsyslog.conf\ncat /etc/logrotate.conf\n```\n3. Verify message sources and delivery servers configured.\n4. Review system logs under `/var/log/messages` or journald.",
            "category": "Data Management & Query Execution",
            "context": "abc\n\nBigStreamer\n\nSyslog(un2)\n\nIssue Number: IM1299104 \nTitle: Missing logs\n\n<b>Description:</b>\n\nGood evening, the abc syslog administrators have noticed much lower than expected and irregular log reception times from server 172.25.37.236 for the period 7/23-27.\nWe would like to investigate the cause and if possible find the missing logs.\n\nThank you very much for your immediate actions.\n\nKeywords: logs\nOwner: u27\nDate: 20200929\nStatus: Open\n\n<b>Actions Taken:</b>\n\n1. ssh un2 as root\n2. cat /etc/rsyslog.conf | more\n3. check the servers that messages transfered. ( only abc servers on this file)\n4. cat /etc/logrotate.conf | more ( check the rotate of messages)",
            "question_groundedness_score": "4",
            "judge_feedback_question_groundedness": "The question is mostly answerable from the context, which includes an investigation checklist involving rsyslog and logrotate. However, the answer goes slightly beyond by mentioning journald explicitly, which is not present in the context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question is highly relevant for engineers operating in BigStreamer, where proper log collection is essential for monitoring, debugging, and auditing distributed systems.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear, self-contained, and provides enough detail for someone to understand and address the issue without needing additional context."
          },
          {
            "question": "Why are exported CPU and Memory metrics files missing or delayed, and how can you reduce export latency?",
            "answer": "Delays are due to Impala query lag and incorrect timestamp rounding by Flume Morphline.\n1. Update Morphline rounding:\n```bash\n{ extractTimeBucket { field : file, bucket:5, rounding:mathematical } }\n```\n2. Restart Flume agent.\n3. Investigate query performance and table schema for optimization:\n```sql\nSELECT count(*) FROM bigcust.nnm_ipvpn_componentmetrics_hist WHERE par_dt='20201027';\n```",
            "category": "Data Management & Query Execution",
            "context": "# abc - BigStreamer - IM1333238 - abc BigStreamer SM-MISSING DATA\n\n<b>Description:</b>\n\n```\nFiles CPU_LOAD and MEMORY_USAGE are exported in the wrong order or not at all\nMissing files for 4:25 and 5:05 a.m.\n```\n\n<b>Actions Taken:</b>\n\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `ipvpn` with sudo\n2. Inspect logs of *export component files* flow\n...\n10. Investigate delays in export\n...\n12. Find processes with heavy disk i/o\n...\n\n<b>Action Points:</b>\n\n# abc - BigStreamer - IM1336999 - abc BigStreamer SM-MISSING DATA\n\n<b>Description:</b>\n\n```\nSince 27/10/2020 12:40 pm 3 files have not been registered to EEM due to delays. Normal offset is 8 minutes e.g. metrics for 13:05 have to be transferred to the exchange directory before 13:13.\n```\n\n...\n\n<b>Action Points:</b>\n\n1. Investigate empty response of Impala query for 13:05 even after refreshing the table\n2. Investigate slightly increased response times of Impala queries and ways to reduce them.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully answerable based on the provided context. The logs, Morphline configuration, and Impala query behavior are all described in detail, allowing the answer to directly address causes and solutions.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question is highly relevant for telecom engineers dealing with metric ingestion, processing, and file exports in BigStreamer. It focuses on troubleshooting data delays and optimizing pipeline performance.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is self-contained, with no vague references. It clearly describes the issue (missing or delayed metric files) and asks for root cause and mitigation, which makes it fully understandable without external context."
          },
          {
            "question": "How can you manually re-ingest missing data in the `refdata.rd_cells_load` table when a partition has fewer rows than expected?",
            "answer": "1. Identify the affected partition by checking:\n```bash\nimpala-shell -q 'show partitions refdata.rd_cells_load;'\n```\n2. Verify file size with:\n```bash\nimpala> refresh refdata.rd_cells_load;\nimpala> show files in refdata.rd_cells_load partition (par_dt>='20201110');\n```\n3. Copy data from a correct partition (e.g., from 20201111 to 20201110):\n```bash\nhdfs dfs -cp /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/\n```\n4. Rename old file:\n```bash\nhdfs dfs -mv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/.cells_20201110.csv\n```\n5. Re-check row counts:\n```sql\nSELECT par_dt, count(*) FROM refdata.rd_cells_load WHERE par_dt >= '20201109' GROUP BY par_dt ORDER BY 1;\n```",
            "category": "Application Functionality & Flow",
            "context": "# abc - BigStreamer - IM1353607  - abc BigStreamer (refdata.rd_cells)\n\n<b>Description:</b>\n\n```\nPlease load the data for 11/11 and for 10/11\n```\n\n<b>Actions Taken:</b>\n\n1. Check the size of current partition from Impala-Shell\n...\n6. We execute the query bellow to check if the partitions \"20201111\" & \"20201110\" have the same number of Rows.\n...\nexample output\n```\npar_dt   | cnt   \n---------+-------\n20201109 | 105576\n20201110 | 105325\n20201111 | 105325\n```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully answerable using only the context, which clearly describes the issue with the specific partition, and the steps taken to re-ingest data from a neighboring partition.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The question is highly relevant to telecom engineers working on BigStreamer, as it addresses data integrity and recovery procedures within the platform's data ingestion workflows.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is specific, unambiguous, and self-contained. It clearly describes the problem and the table involved, without relying on prior context."
          },
          {
            "question": "What should be checked if the `energy_efficiency.pollaploi` table hasn't updated as expected?",
            "answer": "1. Check the SFTP source for new files:\n```bash\nsftp bigd@172.16.166.30\ncd energypm\nls -ltr\n```\n2. Inspect the Oozie workflow `energy_efficiency_load_pollaploi` via Hue.\n3. Review logs:\n```bash\nless /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log\nless /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.next_date.log\n```\n4. Validate row counts:\n```bash\nwc -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/*.txt\nimpala-shell -q 'SELECT COUNT(*) FROM energy_efficiency.pollaploi;'\n```",
            "category": "Application Functionality & Flow",
            "context": "# abc - IM1382364 - Energy efficiency  update\n\n<b>Description:</b>\n\n```\nPlease let us know whenever the pollaploi table in schema energy efficiency is updated. \nAlso to investigate why an update has not been made based on the latest file.\n```\n\n<b>Actions Taken:</b>\n\n1. ssh un2 with your personal account\n2. sudo -iu intra\n3. sftp `bigd@172.16.166.30`\n4. cd energypm\n5. ls -ltr\n6. Open HUE dashboard and search for `energy_efficiency_load_pollaploi` Workflow\n7. Check if workflow failed.\n8. ssh `un2` with your personal account.\n9. sudo -i\n10. less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log` and less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.next_date.log`. The next date should returned no changes.\n11. At un2 `wc -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/2020_10_pollaploi.txt`\n12. Connect to impala-shell and execute `select count(*) from energy_efficiency.pollaploi`\n13. The count should be the same on `11 & 12`\n14. Check on Impala Queries UI if the queries ran without exception `STATEMENT RLIKE '.*energy_efficiency_load_pollaploi.*'`\n\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context provides a clear step-by-step investigation log that maps directly to the checks outlined in the answer. The answer is fully grounded in the available context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Monitoring and troubleshooting table updates is a core part of maintaining data integrity and freshness in BigStreamer, especially in telecom-related energy efficiency workflows.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is self-contained and clearly identifies the table, symptom (lack of update), and required investigative goal. It can be understood without any external reference."
          },
          {
            "question": "How do you restore `radius.radacct_hist` when it failed to ingest rows despite having correct data in `radacct_orig_files`?",
            "answer": "1. Locate affected files in:\n```bash\nhdfs dfs -ls /ez/warehouse/radius.db/radacct_orig_files | grep 20210209_07\n```\n2. Copy to load table:\n```bash\nhdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-09_07-30.csv.* /ez/warehouse/radius.db/radacct_load/\n```\n3. Rerun ingestion:\n```bash\n/shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.20210211.log.manual 2>&1\n```",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# abc - IM1443515 - radius.radacct_hist\n\n<b>Description:</b>\n\n```\nGood morning,\n\nPlease investigate the three-day reduction in registrations at 06:00 and 07:00 (02/08 - 02/10) and inform us about it.\n```\n\n<b>Actions Taken:</b>\n- Login to `un2.bigdata.abc.gr` with personal account and change to `intra` with sudo.\n- Compare the count of the inserted data between the radius.radacct_hist and the original files radius.radacct_orig_files\n...\n- The ingested files have been backed up in the radius.radacct_orig_files. Find the appropriate files for the given dates/hours\n...\n- Copy the files to the load table\n...\n- Run the radius procedure again\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer\n\n<b>Action Points:</b>\n\nResolution of Cloudera Issue 752877 - Hive Metastore innodb lock await time out which is the root cause of this issue.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context provides detailed logs, HDFS file listings, and exact actions taken to identify the discrepancy between source and target tables. The question is fully answerable based on this information.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The question targets a critical issue in data pipeline reliability within BigStreamer, involving ingestion recovery, which is highly relevant to the day-to-day work of telecom data engineers.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question clearly names the tables involved and the scenario (data present but not ingested), making it understandable and actionable without needing the context."
          },
          {
            "question": "What steps should you follow when CDSW jobs are stuck in 'Scheduled' due to resource exhaustion?",
            "answer": "1. Login to CDSW Admin UI and check the Activity tab for CPU and memory usage.\n2. Remove all pending or stuck pods:\n```bash\nkubectl get pods | grep Pending | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\nkubectl get pods | grep 'Init:0/1' | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n```",
            "category": "Troubleshooting & Issue Resolution",
            "context": "# abc - IM1476278 - CDSW Not enough CPU/GPU/Memory \n\n<b>Description:</b>\n\n```\nGood morning, \n\nFor 2 days now all jobs in CDSW fail (stay in status schedule) with the message\n\"Unschedulable: No host in the cluster currently has enough CPU, memory and GPU to run the engine\"\n```\n\n<b>Actions Taken:</b>\n\n1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'\n2. Go to last tab(admin).\n3. Select `Activity` tab.\n4. Check `CPU` and `Memory` graphs.\n5. In our case all scheduled jobs stucked at `Scheduling` due to a job that ran `800 times at 14/03 07:00`\n \n- Delete all PENDING pods to free resources\n\n```bash\n[root@mncdsw1 ~]# kubectl get pods\n[root@mncdsw1 ~]# kubectl get pods | grep Pending | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n[root@mncdsw1 ~]# kubectl get pods | grep \"Init:0/1\" | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n[root@mncdsw1 ~]# kubectl get pods\n```\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer CDSW\n\n<b>Action Points:</b>\n\nIf the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a place can be found. However, it will not be killed for excessive CPU usage.\n\nAs an action point we could `monitoring compute & memory resource usage` from the Pod status via our monitoring tool.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully supported by the provided context, which includes the specific problem (jobs stuck due to lack of resources), diagnostics (UI and kubectl), and exact remediation steps.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This issue is critical for maintaining the operational functionality of CDSW within BigStreamer. Resource exhaustion and pod scheduling problems are highly relevant for engineers working in this environment.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clearly phrased and understandable without requiring additional context. It specifies the component (CDSW), the issue (jobs stuck in 'Scheduled'), and the suspected cause (resource exhaustion)."
          },
          {
            "question": "How do you recover BigStreamer after a NameNode failure affecting HDFS, HBase, and Oozie?",
            "answer": "1. Access Cloudera Manager: https://172.25.37.232:7183\n2. Verify that one NameNode is active. Restart if both are in standby.\n3. After restart, check health of HBase, Oozie, Impala.\n4. Use Kibana to examine system metrics during outage:\n```url\nhttp://10.20.9.82:5601/app/kibana\n```",
            "category": "Infrastructure & Deployment",
            "context": "# abc - IM1391585 -   BigStreamer\n\n<b>Description:</b>\n\n```\nGood morning,\n\nPlease check immediately if BigStreamer is working properly.\n\nWe have received many alerts over the weekend and today regarding HDFS, for various nodes as well as for services (eg oozie). We also noticed problems with geolocation streams and loc mob files.\n\nThanks,\nSK\n```\n\n<b>Actions Taken:</b>\n\n1. Connect with personal creds `https://172.25.37.232:7183` Cloudera Manager\n2. Namenodes were both on standby mode which caused bad health on HDFS,HBASE,OOZIE,IMPALA. After nn1 restarted nn2 became the Active and nn1 the Standby namenode. All the other services was stable after this manual action except HBASE which restarted.\n3. Since all services were stable check HUE `https://172.25.37.236:8888/oozie/list_oozie_workflows/` to ensure that all workflows running.\n4. The specific timeline which namenodes crashed the load,cpu,network,hdfs_read/write,nodes health,,namenodes health,impala queries if something heavy executed,yarn applications if something heavy executed `http://10.20.9.82:5601/app/kibana`\n5. Opened a case on Cloudera with namenodes diagnostics.\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer\n\n<b>Action Points:</b>\n\nMonitor the status/health of services and inform with mail/alert when a service/role is down.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully grounded in the context, which details the NameNode failure, its impact on HDFS, HBase, and Oozie, and the recovery steps taken using Cloudera Manager and Kibana.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The issue is highly relevant for telecom engineers managing BigStreamer, as NameNode failures can disrupt core services. Recovery procedures are critical in such scenarios.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is well-formed and can be fully understood without the context. It specifies the system (BigStreamer), the failure (NameNode), the impact (HDFS, HBase, Oozie), and the desired outcome (recovery steps)."
          },
          {
            "question": "How do you reprocess RADIUS files for missing data during system downtime?",
            "answer": "1. Manually fetch the missing files via SFTP and place under `/shared/radius_repo/cdrs`\n2. Edit `radius.trn` to use `.dat.local`:\n```bash\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n```\n3. Run:\n```bash\n/shared/abc/radius/DataParser/scripts/radius.pl -l -d -D -o >> radius_cron_manual.log\n/shared/abc/radius/bin/000_radius_ops.sh >> radius_ops.manual.log\n```\n4. Validate HDFS ingest:\n```bash\nhdfs dfs -ls /ez/warehouse/radius.db/radacct_hist/par_dt=20201220\n```",
            "category": "Infrastructure & Deployment",
            "context": "# abc - IM1391612 - Data loading\n\n<b>Description:</b>\n\n```\nGood morning,\n\nPlease immediately load all the data for all the hours and for all the flows (sai,vantage,radius etc) for the period of time the system was not working and run all the aggregated tables. We should also be sent a summary table with the relevant information so that we know if and in which streams there are data deficiencies.\n```\n\n<b>Actions Taken:</b>\n\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `intra` with sudo.\n2. Check files that have not been ingested. As you can see, there is a gap for radacct 20201220 and for radarchive between 3:00 - 16:30\n...\n3. Change directory and transfer missing files from sftp.\n...\n4. Check that /shared/radius_repo/radius_date.dat.local points to an older file\n...\n5. Change configuration file of ingestion script.\n...\n6. Execute ingestion scripts when making sure it is not executed at the moment. When scripts have finished, revert changes in the .trn file.\n...\n7. Check that files have been loaded to hist tables.\n...\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer\n\n<b>Action Points:</b>\n\nSection `Radius` in https://edn2.bigdata.intranet.gr/abc/BigStreamer/cluster_monitoring/blob/master/future_steps/flows_applications.md",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is directly grounded in the context, which describes the exact problem (RADIUS data gaps during downtime), the file locations, ingestion script usage, and verification steps.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Reprocessing RADIUS data is critical for ensuring completeness and correctness in BigStreamers metrics pipelines, making this highly relevant for telecom engineers.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is self-contained and clearly articulates the scenario (reprocessing RADIUS data during downtime), with no vague references or reliance on prior turns."
          },
          {
            "question": "What should be done if an Agama schema table has missing data for a specific day?",
            "answer": "1. Locate the ingestion script `/shared/abc/agama/bin/<table>.sh` on `un2`.\n2. Edit the script and hardcode the date values:\n```bash\n##dt_sftp=2021/07/07\n##dt=20210707\n```\n3. Rerun the script manually.\n4. After execution, validate data in Impala:\n```sql\nSELECT COUNT(*), par_dt FROM agama.<table> WHERE par_dt >= '20210402' GROUP BY par_dt;\n```\n5. Revert the script to use dynamic values again.",
            "category": "Data Management & Query Execution",
            "context": "# abc - IM1500475 - agama schema missing data 02/04\n\n<b>Description:</b>\n\n```\nGood evening,\n\nPlease investigate if primary data has been created for 02/04/2021 and if so upload it.\n\nThank you\n```\n\n<b>Actions Taken:</b>\n\n1. ssh with you personal ldap account at `un2` from `admin`\n2. sudo -iu intra\n3. cronta -l | grep agama\n4. Check logs of the script that data is missing.\n5. Also check if the files exist at `sftp_server:directory`\n6. If files exist modify script for the <table> un2:/shared/abc/agama/bin/`table`.sh\n7. comment `yest_sftp`,`yest`,`dt_sftp`,`dt`. Uncomment the static `dt_sftp`,`dt` (e.g `##dt_sftp=2021/07/07 ##dt=20210707`)\n8. Run the script and when is finished connnect to impala-sheel `select count(*),par_dt from agama.table where par_dt >= '2021xxxx' group by 2;`\n9. Edit again the script on un2 with the default values to run tomorrow\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context clearly describes the exact steps taken to resolve missing data in an Agama schema table, including script modification and validation. The answer is fully supported by the context.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This is a highly relevant operational task in BigStreamer, ensuring data consistency in ingestion pipelines, which is essential for analytics and downstream processing.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clearly formulated and self-contained, specifying the table, the issue (missing data), and the timeframe, making it easy to understand without additional context."
          },
          {
            "question": "How do you detect and restore a missing hourly RADIUS file that was incorrectly marked as processed?",
            "answer": "1. Use hourly row counts from `radacct_hist` to identify anomalies.\n2. Search cron logs like:\n```bash\ngrep -B5 Modified /shared/abc/radius/log/000_radius_ops.20210506.log\n```\n3. If the file exists in `radacct_orig_files`, copy it to load:\n```bash\nhdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/... /ez/warehouse/radius.db/radacct_load/\n```\n4. Reprocess with:\n```bash\n/shared/abc/radius/bin/000_radius_ops.sh\n```",
            "category": "Data Management & Query Execution",
            "context": "# abc - BigStreamer - GI3 -  Radius : Procedure for Missing Data\n\n<b>Description:</b>\n\n```\nTicket/Question for Missing Data at Radius Flow\n```\n\n<b>Actions Taken:</b>\n\n**Question** :  Does  **radius.radacct_orig_files** table contains missing data ?\n\n   **Compare files from SFTP Repository with files in Table**\n\n   e.g\n\n **Sftp Repo Contents :**\n\n```\n-rw-r--r-- 1 intra intra 219749225 May  6 11:40 radacct_2021-05-04_00-00.csv.bz2\n...\n-rw-r--r-- 1 intra intra 220329041 May  6 11:42 radacct_2021-05-04_22-30.csv.bz2\n```\n\n**radius.radacct_orig_files**\n\nCommand : show files in <table>\n\n```\nhdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_00-00.csv.20210504_001006.utc\n...\nhdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_22-30.csv.20210504_231007.utc\n```\n\n...\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer Radius\n\n<b>Action Points:</b>\n\n```Verify Procedure Execution by using mentioned SQL Statements```",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is fully answerable using the provided context. The context outlines how to detect missing data using row counts and logs, and how to recover it from archived sources using HDFS commands and ingestion scripts.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question is highly relevant to telecom engineers maintaining data integrity in BigStreamer, particularly in the Radius flow where hourly data completeness is critical for accurate analysis and operations.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is specific and self-contained. It clearly asks about detection and recovery steps for a missing RADIUS file marked as processed, requiring no additional context to understand the problem."
          },
          {
            "question": "What configuration changes were made to fix missing data in the `open_weather_map.openweathermap_final` table in Streamsets?",
            "answer": "To address missing entries in `open_weather_map.openweathermap_final`, the following changes were made in Streamsets:\n- On the SFTP FTP Client tab: increased `Max Batch Size(records)` from `1000` to `100000`, and `Batch Wait Time(ms)` from `1000` to `30000`.\n- On Hadoop-FS 1 and 2: increased `Idle Timeout` from `${1 * MINUTES}` to `${5 * MINUTES}`.",
            "category": "Application Functionality & Flow",
            "context": "# abc - BigStreamer - IM1599907 - Streamsets : Missing Data - open_weather_map.openweathermap_final\n\n<b>Description:</b>\n\n```\nGood morning,\nas of 6/24 open_weather_map.openweathermap_final has very few entries daily. Please check the stream.\n```\n\n<b>Actions Taken:</b>\n\n1. Changed on the `SFTP FTP Client bullet(Tab)` `Max Batch Size(records)` from `1000` to `100000` and `Batch Wait Time(ms)` from `1000` to `30000`\n2. On the `Hadoop-FS 1` and `Hadoop-FS 2` bullets changed the `Idle Timeout` from `${1 * MINUTES}` to `${5 * MINUTES}`\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer Streamsets\n\n<b>Action Points:</b>\n\nReference from devs:\nhttps://metis.xyztel.com/obss/bigdata/abc/devops/devops-projects/-/issues/58#nfgh_44105",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The answer is directly and fully supported by the context, which explicitly lists the configuration changes that were applied to resolve the issue with missing data in the Streamsets pipeline.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The question is highly relevant for telecom engineers working with BigStreamer, as it concerns debugging and resolving data ingestion issues in a real-time ETL pipelinea critical operational task.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear, unambiguous, and self-contained. It specifies the affected table and technology (Streamsets), making it understandable without needing additional context."
          },
          {
            "question": "How do you re-run the Huawei TV flow manually for a date with missing partitions?",
            "answer": "Missing data for 20210831 in `huawei_tv.rel_play_tv_hist`, `rel_play_tvod_hist`, `rel_play_vod_hist`, and `rel_vod_info_hist` can be restored by rerunning the loader script:\n```bash\n/shared/abc/huawei_tv/bin/huawei_tv_load.sh 20210831\n```\nBefore that, confirm data existence using:\n```sql\nSELECT count(*), par_dt FROM huawei_tv.rel_vod_info_hist WHERE par_dt BETWEEN '20210825' AND '20210905' GROUP BY par_dt;\n```",
            "category": "Application Functionality & Flow",
            "context": "# abc - IM1631218 - huawei tv  \n\n<b>Description:</b>\n\n```\nGood morning, please load the tables huawei_tv.rel_play_tv_hist, huawei_tv.rel_play_tvod_hist, huawei_tv.rel_play_vod_hist for 31/8-1/9 and huawei_tv.rel_vod_info_hist for 1/9 with data.\n```\n\n<b>Actions Taken:</b>\n\n1. Login to Hue and go to `Editor` > `Impala`\n2. Check for missing partitions as stated in description\n```bash\nselect count(*), par_dt \nfrom huawei_tv.rel_vod_info_hist  \nwhere par_dt between '20210825' and '20210905' \ngroup by par_dt \norder by par_dt;\n```\nResult:\ncount(*)\tpar_dt\t\n...\n15026\t20210830\t\n14728\t20210901\t\n14748\t20210902\t\n...\n\n3. Partition for `20210831` is actually missing but `20210901` is here and seems fine.\n\n4. Check logs of flow but first login to `un2`. And yes, huawei tv loads data for yesterday's partition from today's table #crazyright\n\n``` bash\n$ su - intra\n$ cd /shared/abc/huawei_tv/\n$ less log/huawei_tv_load.20210901.log\n...\nSFTP get files from : ./export/20210901\nConnected to 172.28.128.150.\nsftp> get export/20210901/*.csv /data/1/huawei_tv_LZ/\nFetching /export/20210901/EPG_SCHEDULE.csv to /data/1/huawei_tv_LZ/EPG_SCHEDULE.csv\nFetching /export/20210901/REL_PLAY_TV.csv to /data/1/huawei_tv_LZ/REL_PLAY_TV.csv\nFetching /export/20210901/REL_PLAY_TVOD.csv to /data/1/huawei_tv_LZ/REL_PLAY_TVOD.csv\nFetching /export/20210901/REL_PLAY_VOD.csv to /data/1/huawei_tv_LZ/REL_PLAY_VOD.csv\nFetching /export/20210901/REL_VOD_INFO.csv to /data/1/huawei_tv_LZ/REL_VOD_INFO.csv\n\n```\n\n5. Let's inspect those files\n```bash\n$ sftp bigdata@172.28.128.150:/export\nsftp> cd 20210901\nsftp> ls -l\n-rw-r--r--    1 0        0          476541 Sep  1 05:13 EPG_SCHEDULE.csv\n-rw-r--r--    1 0        0               0 Sep  1 05:11 REL_PLAY_TV.csv\n-rw-r--r--    1 0        0               0 Sep  1 05:10 REL_PLAY_TVOD.csv\n-rw-r--r--    1 0        0               0 Sep  1 05:11 REL_PLAY_VOD.csv\n-rw-r--r--    1 0        0         3470903 Sep  1 05:11 REL_VOD_INFO.csv\n-rw-r--r--    1 0        0        11414886 Sep  1 05:13 SubscriberID_STBMACAddress_Relationship.csv\n```\nAs it is obvious, there are empty files, so abc needs to reload data into the remfgh server and we need to run the script for the missing partition:\n```bash\n$ /shared/abc/huawei_tv/bin/huawei_tv_load.sh 20210831\n```\n\n<b>Affected Systems:</b>\n\nabc Bigstreamer\n\n<b>Action Points:</b>\n\nWe have informed abc about each case and wait for their response.\nPartition `20210901` is fine but `20210831` needs manual action as nfghd in step 5.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The question is directly and fully answerable using the context, which includes both the missing partition, the script to re-run, and the process for verifying data presence.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "The task is highly relevant to real-world operations within BigStreamer, addressing a common data engineering challengerecovering from missed ingestion runs for specific days.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clearly phrased, includes all relevant information, and can be understood without relying on additional background."
          },
          {
            "question": "How do you recover the `radius.radarchive_hist` table when a file is missing or corrupted?",
            "answer": "1. Connect to the SFTP server and re-download the missing file (e.g., `radarchive_2021-08-31.csv.bz2`).\n2. Place it in `/shared/radius_repo/cdrs`.\n3. Adjust `radius.trn` to use the `.dat.local` file for custom load.\n4. Ensure a dummy file older than the target exists in `/shared/radius_repo/cdrs`.\n5. Run:\n```bash\n/shared/abc/radius/DataParser/scripts/radius.pl -l -d -D -o\n/shared/abc/radius/bin/000_radius_ops.sh\n```",
            "category": "Troubleshooting & Issue Resolution",
            "context": "abc\n\nBigStreamer\n\nSyslog(un2)\n\nIssue Number: IM1629405 \n\nTitle:  radius.radarchive_hist  missing data\n\nDescription: \n\nPlease check why no data has been loaded into the table for 08/31\n\nUrgency \t2 - High\n\n<b>Actions Taken:</b>\n\n0. refresh table via impala-shell:\n\n[un-vip.bigdata.abc.gr:21000] > refresh  radius.radarchive_hist;\n\n[un-vip.bigdata.abc.gr:21000] > select count(*),par_dt from radius.radarchive_hist  where par_dt = '20210831' group by par_dt; \n\nIf 'refresh' does not resolve the issue, then proceed as below:\n\n1. intra crontab in un2:\n\n10 * * * *  /shared/abc/radius/DataParser/scripts/radius.pl -d -D -o >> /shared/abc/radius/DataParser/scripts/log/radius_cron.log  2>&1;/shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.`date '+\\%Y\\%m\\%d'`.log 2>&1;/shared/abc/radius/bin/002_wfm_radius_traffic_day.sh >> /shared/abc/radius/log/002_wfm_radius_traffic_day.`date '+\\%Y\\%m\\%d'`.log 2>&1\n\n2. Script #1 : /shared/abc/radius/DataParser/scripts/radius.pl\n3. Log #1    : /shared/abc/radius/DataParser/scripts/log/radius_cron.log\n\n4. Script #2 : /shared/abc/radius/bin/000_radius_ops.sh \n5. Log #2    : /shared/abc/radius/log/002_wfm_radius_traffic_day.20210831.log\n\n6. check Sftp server :\n7. as 'intra'\n8. [intra@un2 ~]$ sftp intra@79.128.184.153\n\n9. check if file exists and size eg :\n10. sftp> ls -l  radarchive_2021-08-31.csv.bz2\n11. -rw-r--r--    0 0        0              14 Sep  1 04:32 radarchive_2021-08-31.csv.bz2\n\n12. transfer it locally :\n13. sftp> get  radarchive_2021-08-31.csv.bz2\n14. exit\n\n15. extract it :\n16. [intra@un2 ~]$ bzip2 -d  radarchive_2021-08-31.csv.bz2\n\n16. check it :\n18. [intra@un2 ~]$ ll  radarchive_2021-08-31.csv\n19. -rw-r--r-- 1 intra intra 0 Sep  1 12:21 radarchive_2021-08-31.csv\n\nTO LOAD THE MISSING Data (upon re-creating the data by abc)\n--------------------------------------------------------------------\n\n[...] Extensive MoP instructions on editing TRN file, dummy file creation, manual execution, validation, and rollback steps.\n\n20. In the above case, the file was of zero size and reported to abc.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The answer is fully grounded in the context, which describes in detail the steps required to reload missing or corrupted files into the `radius.radarchive_hist` table, including recovery from the SFTP server, configuration adjustments, and manual job execution.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "This question is highly relevant for telecom engineers working with BigStreamer, especially those responsible for maintaining data integrity in historical RADIUS datasets. It directly addresses operational recovery procedures.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clear, concise, and unambiguous. It specifies the exact table and issue, allowing it to be understood and answered independently of the context."
          },
          {
            "question": "How can corrupted statistics in Impala tables like `sai.sms_raw` and `brond.td_dslam_week` be fixed?",
            "answer": "Use Impala shell to recompute or refresh statistics:\n```sql\ncompute stats sai.sms_raw;\nrefresh sai.sms_raw;\n```\nIf compute fails, use refresh instead. Avoid running compute stats on live-updated partitions, as they receive data every 5 minutes. For production-quality statistics, a dedicated project should be scoped.",
            "category": "Troubleshooting & Issue Resolution",
            "context": "<b>IM1630642</b>\n\n<b>Title :\tmissing or corrupted statistics </b>\n\n<b>Description:</b>\n\n```\nGood evening,\nPlease for your actions\n\nWARNING: The following tables have potentially corrupt table statistics. Drop and re-compute statistics to resolve this problem. -> \n\nsai.voice_quality_hist\nsai.sms_raw, \nsai.voice_raw\nbrond.brond_retrains_hist,\nbrond.td_dslam_week,\nookla.ookla_android,\nookla.ookla_ios,\nookla.ookla_stnet\ntemip.temip_impala_terminated_alarms,\ntemip.temip_kudu_terminated_alarms\n```\n\n<b>Actions Taken:</b>\n\n<b>Check tables hdfs status , un2 as intra :</b>\n\n hdfs dfs -du -h -s /ez/warehouse/sai.db/voice_quality_hist\n hdfs dfs -du -h -s /ez/warehouse/brond.db/brond_retrains_hist\n hdfs dfs -du -h -s /ez/warehouse/brond.db/td_dslam_week\n hdfs dfs -du -h -s /ez/warehouse/ookla.db/ookla_android\n hdfs dfs -du -h -s /ez/warehouse/ookla.db/ookla_ios\n hdfs dfs -du -h -s /ez/warehouse/ookla.db/ookla_stnet\n\n<b>Check current stats, from un2 as intra via impala shell:</b>\n\n show table stats  sai.voice_quality_hist;\n show table stats  sai.sms_raw;\n show table stats  sai.voice_raw;\n show table stats  brond.brond_retrains_hist;\n show table stats  brond.td_dslam_week;\n show table stats  ookla.ookla_android;\n show table stats  ookla.ookla_ios;\n show table stats  ookla.ookla_stnet;\n show table stats  temip.temip_impala_terminated_alarms;\n show table stats  temip.temip_kudu_terminated_alarms;\n\n<b>Recompute stats, from un2 as intra via impala shell:</b>\n\n compute stats   sai.voice_quality_hist;\n compute stats   sai.sms_raw;\n compute stats   sai.voice_raw;\n compute stats   brond.brond_retrains_hist;\n compute stats   brond.td_dslam_week;\n compute stats   ookla.ookla_android;\n compute stats   ookla.ookla_ios;\n compute stats   ookla.ookla_stnet;\n compute stats   temip.temip_impala_terminated_alarms;\n compute stats   temip.temip_kudu_terminated_alarms;\n\n<b>:</b>\nIf there is a failure in any of the tables, run 'refresh' on this table.\n\n<b>Refresh stats, from un2 as intra via impala shell:</b>\n\n refresh   sai.voice_quality_hist;\n refresh   sai.sms_raw, sai.voice_raw;\n refresh   brond.brond_retrains_hist;\n refresh   brond.td_dslam_week;\n refresh   ookla.ookla_android;\n refresh   ookla.ookla_ios;\n refresh   ookla.ookla_stnet;\n refresh   temip.temip_impala_terminated_alarms;\n refresh   temip.temip_kudu_terminated_alarms;\n\n---------------- Below comment from Dionysia in ticket ------------------------------\n\nAfter investigating the tables (sai.voice_quality_hist, sai.sms_raw, sai.voice_raw) we saw that we calculate statistics every morning for the previous day's data. These tables seem to fill up throughout the day, so the stats don't appear to be up to date. The statistics refer to data in partitions of previous days.\n\n- In terms of performance, it is not possible to run compute statistics on partitions that continuously receive data during the day.\nThere are streams that load data every 5 minutes. Doing so would significantly delay data processing.\n\n- Obviously, the queries that are executed include the current partition and that is why the Warning for missing statistics is displayed.\n\nIf statistics are needed for specific tables, we should consider it as a project and not under Support. We should consider how the performance of the cluster is affected and make the right design, so that we can agree on the frequency that the statistics should be calculated.",
            "question_groundedness_score": "5",
            "judge_feedback_question_groundedness": "The context includes precise details about the corrupted statistics, the affected tables, and the corrective actions (compute or refresh stats), fully supporting the answer.",
            "question_relevance_score": "5",
            "judge_feedback_question_relevance": "Corrupted or outdated statistics directly affect query performance in Impala. Fixing them is critical in BigStreamer operations, making the question highly relevant.",
            "question_standalone_score": "5",
            "judge_feedback_question_standalone": "The question is clearly formulated, naming specific Impala tables and the nature of the problem, and does not require external context to understand."
          }          
    ]
}