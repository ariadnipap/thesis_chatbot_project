{
    "0": {
        "page_content": "---\ntitle: Prometheus Oracle to Hive ETL Flow\nsystem: BigStreamer\ncomponent: Prometheus\njob_name: Prometheus-Import-Workflow\nsource_system: Oracle\nsource_tables:\n  - DWSRC.DWH22\ndestination_system: Hive\ndestination_tables:\n  - prometheus.dwh22\nschedule: daily at 06:30 UTC\ncoordinator: Prometheus-Coordinator\nworkflow: Prometheus-Import-Workflow\nscript_path: /user/prometheus/flows\nmonitoring_table: monitoring.jobstatus\nowner: prometheus\ntags:\n  - Prometheus\n  - Oracle to Hive\n  - ETL\n  - BigStreamer\n  - Monitoring\n  - Oozie\n  - Impala\n  - Workflow Troubleshooting\n  - Partition Drop\n  - Grafana\n---\n# Prometheus\nThis document describes the Prometheus ETL flow that extracts data from Oracle table DWSRC.DWH22 into the Hive table prometheus.dwh22 using a daily Oozie workflow. It includes scheduling details, partition management, and troubleshooting guidelines in case of failures.\n## Useful Links\nReferences to internal documentation related to infrastructure and monitoring of the Prometheus ETL flow.\n[Infrastructure](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/Infastructure)  \n[Monitoring](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/home#monitoring)  \n## Oozie workflow / ETL Flow: Oracle to Hive\nDescription of the Oracle-to-Hive import flow, configuration details, and how to monitor and troubleshoot job execution.\n``` mermaid\n  graph TD\n    A[Oracle DB table DWSRC.DWH22  <br> via Port Forward 999.999.999.999:6634] -->|Sqoop Import| B[HDFS Staging Directory]\n    B -->|Hive Load| C[Hive: prometheus.dwh22]\n    C -->|Impala Refresh| D[Impala: prometheus.dwh22]\n```\nRuns every day at `06:30 AM UTC`\n**User**: `prometheus`  \n**Coordinator**: `Prometheus-Coordinator`  \n**Workflow**: `Prometheus-Import-Workflow`  \n**Source Database**:  \n- **Host**: `999.999.999.999`  \n- **Port**: `1521`  \n- **SID**: `A7`\n- **User**: `bigstreamer`  \n**Target Table**: `prometheus.dwh22`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "prometheus.md - Part 1"
        }
    },
    "1": {
        "page_content": "**Source Database**:  \n- **Host**: `999.999.999.999`  \n- **Port**: `1521`  \n- **SID**: `A7`\n- **User**: `bigstreamer`  \n**Target Table**: `prometheus.dwh22`  \n**HDFS Installation Directory**: `/user/prometheus/flows`  \n**HDFS Staging Directory**: `/ez/warehouse/prometheus.db/tmp_sqoop_jobs/`\n**Alerts**:\n- Mail with subject: `Prometheus Flow failed`\n**Troubleshooting Steps**:\n- Check messages written to Monitoring App\n    - Check monitoring app for successful executions:  \n        - From `un2` with personal account:\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'`\n    - Check monitoring app for failed executions:  \n        - From `un2` with personal account:\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=FAILED&operativePartition=<timestamp e.g.:20220518>'`\n    - Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n    - Grafana link: `https://unc1.bigdata.abc.gr:3000/d/PcKYyfTVz/prometheus-dashboard?orgId=1&from=now-2d&to=now`\n- Check if partition is loaded:\n  From `Hue` as `prometheus` in `Impala Editor`:\n  ``` sql\n  SHOW PARTITIONS prometheus.dwh22;\n  SELECT COUNT(*) FROM prometheus.dwh22 WHERE par_dt='<par_dt>';\n  ```\n- Check logs for failed steps:  \n  From `Hue` as `prometheus` in `Workflows`:\n  - Search for `Prometheus-Import-Workflow` and filter for failed\n  - Go to logs and check both stdout and stderr\n- In case a partition has partially been inserted into the final table `prometheus.dwh22` and the error that caused the failure has been resolved:\n  From `Hue` as `prometheus` in `Impala Editor`:\n    ``` sql\n    ALTER TABLE prometheus.dwh22 DROP IF EXISTS PARTITION (par_dt='<par_dt>');\n    ```\n  - For the previous day:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "prometheus.md - Part 2"
        }
    },
    "2": {
        "page_content": "From `Hue` as `prometheus` in `Impala Editor`:\n    ``` sql\n    ALTER TABLE prometheus.dwh22 DROP IF EXISTS PARTITION (par_dt='<par_dt>');\n    ```\n  - For the previous day:\n    From `Hue` as `prometheus` in `Workflows`:\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n  - For the previous day:\n    From `Hue` as `prometheus` in `Workflows`:\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n  - For partitions older than yesterday:\n    From `Hue` as `prometheus` in `File Browser`:\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and set `days_back` to the number of days back needed to reach the partition\n    From `Hue` as `prometheus` in `Workflows`:\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n    From `Hue` as `prometheus` in `File Browser`:\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and restore `days_back` to `1`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "prometheus.md - Part 3"
        }
    },
    "3": {
        "page_content": "---\ntitle: Brond ADSL/VDSL Flow - Daily xDSL Statistics Ingestion Pipeline\ndescription: Ingestion flow for ADSL and VDSL metrics via SFTP to Hive using Oozie scheduling. Includes FTP retrieval, parsing, staging, HDFS upload, Hive table overwrite, monitoring, and error resolution for brond_adsl_stats_daily and brond_vdsl_stats_daily tables.\njob_name: BROND_ADSL_STATS / BROND_VDSL_STATS\ncomponent: MAIN\nsystem: BigStreamer\nhost: un-vip.bigdata.abc.gr\nowner: brond\ncoordinator: Brond_Load_xDSL_Coord_NEW\nworkflow: Brond_Load_xDSL_WF_NEW\ntarget_tables:\n  - brond.brond_adsl_stats_daily\n  - brond.brond_vdsl_stats_daily\nstaging_tables:\n  - brond.brond_adsl_stats_daily_stg\n  - brond.brond_vdsl_stats_daily_stg\nhdfs_landingzone: /ez/warehouse/brond.db/landing_zone/brond_dsl_stats\nschedule: [04:00, 05:00, 06:00, 10:00 UTC]\nload_type: daily\nretry_policy: manual rerun supported via HUE\nlast_updated: 2025-05-01\nkeywords:\n  - adsl\n  - vdsl\n  - bigstreamer\n  - brond\n  - stats\n  - SFTP\n  - HDFS\n  - Hive\n  - Oozie\n  - impala\n  - beeline\n  - daily partitioning\n  - monitoring\n  - jobstatus\n  - load overwrite\n  - parsing\n  - staging\n  - automation\n  - file suffix loaded\n  - xDSL metrics\n  - manual triggering\n  - file landing\n  - kerberos\n  - workflow rerun\n  - log tracing\n  - partition validation\n  - brond_dsl_stats\n---\n# Brond ADSL/VDSL Flow\n## Installation info\nThis section provides configuration details about the source systems, local and HDFS directories, scripts, logging paths, and job scheduling for the ADSL/VDSL load pipeline.\n### Data Source File\nInformation about the raw data files retrieved via SFTP, including naming patterns and filesystem structure.\n- Source system: FTP Server  \n  - host :`999.999.999.999`\n  - port :`22`\n  - protocol :`SFTP`\n  - user : `bigd`\n  - spool area : `/ADSL_Brond_DWH`\n  - file_type : `DWH_ADSL*.csv.gz` and `DWH_VDSL*.csv.gz`\n  - load_suffix : `LOADED`\n- Local FileSystem Directories\n\t- node : `un-vip.bigdata.abc.gr (999.999.999.999)`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 1"
        }
    },
    "4": {
        "page_content": "- file_type : `DWH_ADSL*.csv.gz` and `DWH_VDSL*.csv.gz`\n  - load_suffix : `LOADED`\n- Local FileSystem Directories\n\t- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n\t- landing_zone : `/data/1/brond_dsl_stats_LZ`\n\t- archive_dir= : `/data/1/brond_dsl_stats_LZ/archives`\n\t- work_dir= : `/shared/abc/brond_dsl_stats/repo`\n- HDFS Directories\n\t- hdfs_Bin : `/user/brond`\n\t- hdfs_dir : `/ez/warehouse/brond.db/landing_zone/brond_dsl_stats`\n\t- hdfs_pending_dir : `/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/not_loaded`\n\t- hdfs_stats_dir : `/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/stats`\n### Scripts-Configuration Location\nLocations of the scripts and .trn configuration files used to manage the flow.\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- scripts path : `/shared/abc/brond_dsl_stats/DataParser/scripts`\n-\tconfigurations path : `/shared/abc/brond_dsl_stats/DataParser/scripts/transferlist/*.trn` (i.e. brond_retrains.trn)\n### Logs Location\nLog file path and format used to trace each data loading run.\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- path : `/shared/abc/brond_dsl_stats/DataParser/scripts/log`\n- log file: `002.Brond_xDSL_Load.<YYYYMMDD>.log`\n### Oozie Scheduling\nDetails about the Oozie coordinator, workflow, and execution commands.\n- user : `brond`\n- Coordinator :`Brond_Load_xDSL_Coord_NEW`  \n\truns at : `04:00, 05:00, 06:00, 10:00 UTC`\n- Workflow : `Brond_Load_xDSL_WF_NEW`  \n- Main script : `HDFS:/user/brond/000.Brond_xDSL_Oozie_Main.sh`\n- SSH Identity file : `HDFS:/user/brond/id_rsa`\nNdef: **Main Script** runs `oozie_brond_xdsl.sh` located on `un-vip.bigdata.abc.gr` using **ssh** as user **brond**  \n`$ ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa brond@un-vip.bigdata.abc.gr \"/shared/abc/brond_dsl_stats/DataParser/scripts/oozie_brond_xdsl.sh\"`\n### Hive Tables\nStaging and final target tables used for storing the cleaned ADSL/VDSL metrics.\n- Target Database: `brond`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 2"
        }
    },
    "5": {
        "page_content": "### Hive Tables\nStaging and final target tables used for storing the cleaned ADSL/VDSL metrics.\n- Target Database: `brond`\n- Staging Tables: `brond.brond_adsl_stats_daily_stg, brond.brond_vdsl_stats_daily_stg`\n- Target Tables: `brond.brond_adsl_stats_daily, brond.brond_vdsl_stats_daily`\n### Beeline-Impala Shell commands\nUseful commands to interact with Hive and Impala for loading and querying data.\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/default;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala-shell: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k`\n## Data process\nStep-by-step breakdown of how raw files are transferred, parsed, staged, and loaded into Hive tables.\n1. sftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```bash\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond_DWH\n-rw-r--r--    0 507      500      35399779 Nov 27 06:19 ADSL_Brond_DWH/DWH_ADSL.327_2022_11_27.csv.gz.LOADED\n-rw-r--r--    0 507      500      35440542 Nov 28 06:57 ADSL_Brond_DWH/DWH_ADSL.328_2022_11_28.csv.gz.LOADED\n-rw-r--r--    0 507      500      35360378 Nov 29 06:20 ADSL_Brond_DWH/DWH_ADSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      35415258 Nov 30 06:48 ADSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz\n-rw-r--r--    0 507      500      150757798 Nov 27 05:33 ADSL_Brond_DWH/DWH_VDSL.327_2022_11_27.csv.gz.LOADED\n-rw-r--r--    0 507      500      150728306 Nov 28 06:26 ADSL_Brond_DWH/DWH_VDSL.328_2022_11_28.csv.gz.LOADED\n-rw-r--r--    0 507      500      150589497 Nov 29 05:34 ADSL_Brond_DWH/DWH_VDSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      150823890 Nov 30 06:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_30.csv.gz\n```\n2. rename the raw file(s) in remdef SFTP server by adding the suffix .LOADED",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 3"
        }
    },
    "6": {
        "page_content": "-rw-r--r--    0 507      500      150823890 Nov 30 06:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_30.csv.gz\n```\n2. rename the raw file(s) in remdef SFTP server by adding the suffix .LOADED\n`echo \"rename /ADSL_Brond_DWH/ADSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz /ADSL_Brond_DWH/ADSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz.LOADED\" | sftp -oport=22 bigd@999.999.999.999`\n`echo \"rename /ADSL_Brond_DWH/VDSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz /ADSL_Brond_DWH/VDSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz.LOADED\" | sftp -oport=22 bigd@999.999.999.999`\n3. parsing raw files in `/data/1/brond_dsl_stats_LZ`\n- removes the headers (1st line)\n- removes double-qudefs chars\n- defines the PAR_DT value from the filename (i.e. DWH_ADSL.330_2022_11_30.csv.gz convert to 20221130)\n- add the prefix `HDFS___` to raw file\n- add the suffix `<load time>` to raw file  \nLoad time format:`<YYYYMMDD_HHMISS>`  \ni.e. `HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz`\n4. put raw files into HDFS landingzone\n```bash\nhdfs dfs -put /data/1/brond_dsl_stats_LZ/HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz /ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz`\nhdfs dfs -put /data/1/brond_dsl_stats_LZ/HDFS___DWH_VDSL.330_2022_11_30.csv.gz.20221201_060005.gz /ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_VDSL.330_2022_11_30.csv.gz.20221201_060005.gz`\n```\n5. clean-up any copy of the raw files from local filesystem  \n`/data/1/brond_dsl_stats_LZ`  \n`/shared/abc/brond_dsl_stats/repo`\n6. load HDFS files into hive staging tables  \n`brond.brond_adsl_stats_daily_stg` and `brond.brond_vdsl_stats_daily_stg`  \n```bash\nbeeline -e \"LOAD DATA INPATH '/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz' OVERWRITE INTO TABLE brond.brond_adsl_stats_daily_stg PARTITION (par_dt='20221130')\"",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 4"
        }
    },
    "7": {
        "page_content": "beeline -e \"LOAD DATA INPATH '/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_VDSL.330_2022_11_30.csv.gz.20221201_060005.gz' OVERWRITE INTO TABLE brond.brond_vdsl_stats_daily_stg PARTITION (par_dt='20221130')\"\n```\n> Ndef: Once the load completed, the staging tables should contain no data.*\t\n7. update hive tables with filtered columns  \nscript: `/shared/abc/brond_dsl_stats/DataParser/scripts/003.Brond_xDSL_Post.sh`\n- `brond_adsl_stats_daily`\n```sql\n\t\tset hive.exec.dynamic.partition.mode=nonstrict;\n\t\tinsert overwrite table brond.brond_adsl_stats_daily partition (par_dt) \n\t\tselect \n\t\t\tserv_tel,\n\t\t\tserv_siid,\n\t\t\tne_name,\n\t\t\tne_port,\n\t\t\tcard_tehn,\n\t\t\tcard_type,\n\t\t\tinv_port,\n\t\t\tmeasure_date,\n\t\t\tlast_change,\n\t\t\taif_adm,\n\t\t\taif_oper,\n\t\t\topmod_annex,\n\t\t\tup_sign_attn,\n\t\t\tup_snr,\n\t\t\tup_crt_rate,\n\t\t\tup_max_rate,\n\t\t\tdn_sign_attn,\n\t\t\tdn_snr,\n\t\t\tdn_crt_rate,\n\t\t\tdn_max_rate,\n\t\t\tprf_name,\n\t\t\tradius,\n\t\t\tsproto,\n\t\t\tpar_dt\n\t\tfrom brond.brond_adsl_stats_daily_stg \n\t\twhere 1=1\n\t\t;\n\t\t```\n- `brond_vdsl_stats_daily`\n```sql\n\t\tset hive.exec.dynamic.partition.mode=nonstrict;\n\t\tinsert overwrite table brond.brond_vdsl_stats_daily partition (par_dt) \n\t\tselect \n\t\t\tserv_tel,\n\t\t\tserv_siid,\n\t\t\tne_name,\n\t\t\tne_port,\n\t\t\tcard_tehn,\n\t\t\tcard_type,\n\t\t\tinv_port,\n\t\t\tmeasure_date,\n\t\t\tcustid,\n\t\t\tlast_change,\n\t\t\taif_adm,\n\t\t\taif_oper,\n\t\t\topmod_annex,\n\t\t\tup_sign_attn,\n\t\t\tup_snr,\n\t\t\tup_max_rate,\n\t\t\tup_crt_rate,\n\t\t\tdn_sign_attn,\n\t\t\tdn_snr,\n\t\t\tdn_max_rate,\n\t\t\tdn_crt_rate,\n\t\t\tprf_name,\n\t\t\tradius,\n\t\t\tsproto,\n\t\t\tpar_dt\n\t\tfrom brond.brond_vdsl_stats_daily_stg \n\t\twhere 1=1\n\t\t;\n```\nIn the event where multiple files are transfered (files refer to the same data), we proceed with overwriting data in brond table. Deletion of those multiple files is abc's responsibility.\n## Monitoring\nInstructions for checking job execution status, component-level logs, and tracking metadata.\n### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 5"
        }
    },
    "8": {
        "page_content": "### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \nConnection command: `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n### Monitoring Message list\nFor each type of load (ADSL or VDSL) the following set of messages will be recorded in the Monitoring database.\n```\nid    | execution_id | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n------+--------------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n15675 | 1659931204   | BROND       | BROND_ADSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:00:04 | 2022-08-08 07:01:38 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n15677 | 1659931204   | BROND       | BROND_ADSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:00:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n15679 | 1659931204   | BROND       | BROND_ADSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n15681 | 1659931204   | BROND       | BROND_ADSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n15683 | 1659931204   | BROND       | BROND_ADSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 6"
        }
    },
    "9": {
        "page_content": "15685 | 1659931204   | BROND       | BROND_ADSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr\n15687 | 1659931204   | BROND       | BROND_ADSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:00:26 |                     |                        | brond | un2.bigdata.abc.gr\n15689 | 1659931204   | BROND       | BROND_ADSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:01:38 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 7"
        }
    },
    "10": {
        "page_content": "id    | execution_id | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n------+--------------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n15691 | 1659931204   | BROND       | BROND_VDSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:01:38 | 2022-08-08 07:03:51 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n15693 | 1659931204   | BROND       | BROND_VDSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:01:40 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n15695 | 1659931204   | BROND       | BROND_VDSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n15697 | 1659931204   | BROND       | BROND_VDSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n15699 | 1659931204   | BROND       | BROND_VDSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr\n15701 | 1659931204   | BROND       | BROND_VDSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 8"
        }
    },
    "11": {
        "page_content": "15703 | 1659931204   | BROND       | BROND_VDSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:02:05 |                     |                        | brond | un2.bigdata.abc.gr\n15705 | 1659931204   | BROND       | BROND_VDSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:03:51 |                     |                        | brond | un2.bigdata.abc.gr\n```\n### Monitoring Component list\nDescriptions of each logical step in the ADSL/VDSL processing pipeline as represented in monitoring.\n|Component | Description \n|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED| \n|GET RAW XDSL FILES|sftp-get the raw files from the remdef server.<br />i.e.<br />DWH_ADSL.197_2022_07_18.csv.gz<br />DWH_VDSL.197_2022_07_18.csv.gz\n|RENAME FILES @SFTP SERVER| Rename the raw files in remdef server by adding the suffix .LOADED<br />i.e.<br />DWH_ADSL.197_2022_07_18.csv.gz.LOADED<br />DWH_VDSL.197_2022_07_18.csv.gz.LOADED\n|PARSING FILES| removes any control chars (if any) from the raw files\n|LOAD HDFS LANDINGZONE|PUT the parsing files into HDFS\n|CLEAN-UP THE INPUT FILES|Clean-up any copy of the raw files from the filesystem\n|LOAD HDFS FILES INTO HIVE STAGING TABLES| Load raw data (files) into the staging tables<br />`brond.brond_adsl_stats_daily_stg`<br />`brond.brond_vdsl_stats_daily_stg`\n|UPDATE HIVE TABLES WITH FILTERED COLUMNS| Update final tables with the necessary columns only.<br />`brond.brond_adsl_stats_daily`<br />`brond.brond_vdsl_stats_daily`\n### Monitoring database Queries\nSample SQL queries to check logs and monitoring status for the latest executions.\n- List messages of the last load  \n`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n```sql\nselect \n  execution_id, id, application, job, component, operative_partition,  \n  status, system_ts, system_ts_end, message, user,host",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 9"
        }
    },
    "12": {
        "page_content": "```sql\nselect \n  execution_id, id, application, job, component, operative_partition,  \n  status, system_ts, system_ts_end, message, user,host   \nfrom jobstatus a where upper(job) like 'BROND__DSL%'   \nand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'BROND__DSL%');\nexecution_id | id    | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n-------------+-------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n1659931204   | 15675 | BROND       | BROND_ADSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:00:04 | 2022-08-08 07:01:38 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n1659931204   | 15677 | BROND       | BROND_ADSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:00:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n1659931204   | 15679 | BROND       | BROND_ADSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15681 | BROND       | BROND_ADSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15683 | BROND       | BROND_ADSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 10"
        }
    },
    "13": {
        "page_content": "1659931204   | 15685 | BROND       | BROND_ADSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15687 | BROND       | BROND_ADSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:00:26 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15689 | BROND       | BROND_ADSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:01:38 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15691 | BROND       | BROND_VDSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:01:38 | 2022-08-08 07:03:51 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n1659931204   | 15693 | BROND       | BROND_VDSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:01:40 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n1659931204   | 15695 | BROND       | BROND_VDSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15697 | BROND       | BROND_VDSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15699 | BROND       | BROND_VDSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 11"
        }
    },
    "14": {
        "page_content": "1659931204   | 15701 | BROND       | BROND_VDSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15703 | BROND       | BROND_VDSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:02:05 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15705 | BROND       | BROND_VDSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:03:51 |                     |                        | brond | un2.bigdata.abc.gr\n-------------+-------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n```\n### Monitoring Health-Check\n- Check Monitoring status.  \n```bash\t\n$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\t\n{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n```  \n- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\nCommon issues, log inspection tips, and diagnostic commands for resolving errors in the flow.\n- An email will be sent by the system with the point of failure.\n- Check the log file for errors/exceptions  \n`egrep -i 'error|fail|exception|problem' /shared/abc/brond_dsl_stats/DataParser/scripts/log/002.Brond_xDSL_Load.YYYYMMDD.log`\n- List Failed Monitoring messages of the last load\n```bash\n/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 12"
        }
    },
    "15": {
        "page_content": "select * from jobstatus where upper(job) like 'BROND__DSL' \n\tand status='FAILED'\n\tand operative_partition=(select max(operative_partition) from jobstatus where upper(job) like 'BROND__DSL' and operative_partition regexp '[0-9]{8}')\n\torder by id;\n\texecution_id | id    | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n\t-------------+-------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n\t1659946615   | 15825 | BROND       | BROND_ADSL_STATS | MAIN                                     | 20220808            | FAILED  | 2022-08-08 11:16:55 | 2022-08-08 11:16:55 | No raw files found     | brond | un2.bigdata.abc.gr\n\t1659946615   | 15827 | BROND       | BROND_ADSL_STATS | GET RAW XDSL FILES                       | 20220808            | FAILED  | 2022-08-08 11:16:55 |                     | No raw files found     | brond | un2.bigdata.abc.gr\n\t1659946615   | 15829 | BROND       | BROND_VDSL_STATS | MAIN                                     | 20220808            | FAILED  | 2022-08-08 11:16:56 | 2022-08-08 11:16:56 | No raw files found     | brond | un2.bigdata.abc.gr\n\t1659946615   | 15831 | BROND       | BROND_VDSL_STATS | GET RAW XDSL FILES                       | 20220808            | FAILED  | 2022-08-08 11:16:56 |                     | No raw files found     | brond | un2.bigdata.abc.gr\n\t```\n### Common errors  \n- `No raw files found`, there are no raw files available for loading at remdef server.  \nA WARNING message will be sent to abc by email.\nNo actions required from OBSS. Responsible abc. \n- Other factors not related to the specific flow\n- impala/hive availability\n- Kerberos authentication (A.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 13"
        }
    },
    "16": {
        "page_content": "No actions required from OBSS. Responsible abc. \n- Other factors not related to the specific flow\n- impala/hive availability\n- Kerberos authentication (A.  \n> Ndef: The flow checks if the ticket is still active before any HDFS action.  \nIn case of expiration the flow performs a `kinit` command*\n## Manually triggering the workflow\nProcedure to rerun the Oozie workflow manually in case files are uploaded after the scheduled runs.\nThere are cases where abc might upload new files after the scheduled workflow timing, and request these files to be\nprocessed in the same day. This can only be done by manually triggering the workflow. Before doing so, you need to make some checks first:\n### Check workflow logs\n1. Login to https://999.999.999.999:8888/hue/accounts/login?next=/hue using the brond account\n2. Go to \"Jobs\" > \"Workflows\"\nIf all workflow executions (\"Brond_Load_xDSL_WF_NEW\") were successful, you can proceed by checking that the file(s)\nabc added, were copied after the scheduled timings of the workflow\n### Check added files\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```bash\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond\n-rw-r--r-- 0 507 500 21925249 Nov 29 06:22 ADSL_Brond/Counter_Collection_24H.329_2022_11_29.csv.gz.LOADED\n-rw-r--r-- 0 507 500 22107252 Nov 29 13:52 ADSL_Brond/Counter_Collection_24H.330_2022_11_29.csv.gz\n```\nThe second file was indeed added after the scheduled time and has not been picked up by the workflow.\n### Trigger workflow\nYou can now proceed to manually trigger the workflow:\n1. Go to HUE and select \"Jobs\"\n2. Go to \"Workflow\" and select \"Brond_Load_xDSL_WF_NEW\"\n3. In the next screen, select \"Rerun\"\n4. Wait for the workflow to successfully end\n5. If no errors occur, proceed with:\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```bash\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 14"
        }
    },
    "17": {
        "page_content": "5. If no errors occur, proceed with:\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```bash\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond\n-rw-r--r-- 0 507 500 21925249 Nov 29 06:22 ADSL_Brond/Counter_Collection_24H.329_2022_11_29.csv.gz.LOADED\n-rw-r--r-- 0 507 500 22107252 Nov 29 13:52 ADSL_Brond/Counter_Collection_24H.330_2022_11_29.csv.gz.LOADED\n```\n## Data Check\nValidation steps to confirm data has been properly loaded and partitioned in final Hive tables.\n- **Check final tables for new partitions**:\n- Impala-shell: \n```bash\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 15"
        }
    },
    "18": {
        "page_content": "refresh brond.brond_adsl_stats_daily;  \n\tshow partitions brond.brond_adsl_stats_daily;  \n\t\n\tpar_dt   | #Rows  | #Files | Size     | Bytes Cached | Cache Replication | Format | Incremental stats | Location                                                                        \n\t---------+--------+--------+----------+--------------+-------------------+--------+-------------------+---------------------------------------------------------------------------------\n\t20221130 | 629397 |      1 | 155.09MB | NOT CACHED   | NOT CACHED        | TEXT   | false             | hdfs://nameservice1/ez/warehouse/brond.db/brond_adsl_stats_daily/par_dt=20220808\n\tTotal    |     -1 |      1 | 155.09MB | 0B           |                   |        |                   |                                                                                 \n\n\n\trefresh brond.brond_vdsl_stats_daily;\n\tshow partitions brond.brond_vdsl_stats_daily\n\t\n\tpar_dt   | #Rows   | #Files | Size     | Bytes Cached | Cache Replication | Format | Incremental stats | Location                                                                        \n\t---------+---------+--------+----------+--------------+-------------------+--------+-------------------+---------------------------------------------------------------------------------\n\t20221130 | 2157413 |      1 | 588.26MB | NOT CACHED   | NOT CACHED        | TEXT   | false             | hdfs://nameservice1/ez/warehouse/brond.db/brond_vdsl_stats_daily/par_dt=20220808\n\tTotal    |      -1 |      1 | 588.26MB | 0B           |                   |        |                   |                                                                                 \n```\n- **Check the amount of data in final tables**:\n- Impala-shell: \n\t```bash\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\n\tSELECT par_dt, count(*) as cnt from brond.brond_adsl_stats_daily group by par_dt order by 1;\n\tpar_dt   | cnt   \n\t---------+-------\n\t20221130 | 629397",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 16"
        }
    },
    "19": {
        "page_content": "SELECT par_dt, count(*) as cnt from brond.brond_vdsl_stats_daily group by par_dt order by 1;\n\tpar_dt   | cnt    \n\t---------+--------\n\t20221130 | 2157413\n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_xDSL_Stats_Flow.md - Part 17"
        }
    },
    "20": {
        "page_content": "---\ntitle: def_NETWORK_MAP ETL Flow (OneTicket)\nsystem: BigStreamer\ncomponent: OneTicket\njob_name: Oracle_to_Hive_OneTicket_Load\nsource_system: Oracle\nsource_tables:\n  - def_NETWORK_MAP.ACTIVITY\n  - def_NETWORK_MAP.AFFECTED_CUSTOMERS\n  - def_NETWORK_MAP.AFFECTED_OCT_WTT\n  - def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n  - def_NETWORK_MAP.OPEN_MW\n  - def_NETWORK_MAP.OPEN_NTT\n  - def_NETWORK_MAP.OPEN_OCT\n  - def_NETWORK_MAP.OPEN_WTT\ndestination_system: Hive\ndestination_tables:\n  - def_network_map.activity\n  - def_network_map.affected_customers\n  - def_network_map.affected_oct_wtt\n  - def_network_map.defective_netw_element\n  - def_network_map.open_mw\n  - def_network_map.open_ntt\n  - def_network_map.open_oct\n  - def_network_map.open_wtt\nschedule: every 5 minutes\ncoordinator: def_NETWORK_MAP_Coordinator\nworkflow: def_NETWORK_MAP_Workflow\nscript_path: HDFS:/user/def_network_maps/100.OneTicket_Main.sh\nmonitoring_table: monitoring.jobstatus\nowner: def_network_maps\ntags:\n  - OneTicket\n  - Oracle to Hive ETL\n  - def_NETWORK_MAP\n  - BigStreamer\n  - Monitoring\n  - Oozie\n  - HDFS\n  - Impala\n  - Beeline\n  - Troubleshooting\n  - Log Analysis\n---\n# def_NETWORK_MAP Flow (OneTicket)\nThis document describes the ETL process that exports operational data from Oracle to Hive every 5 minutes using the OneTicket flow. It covers installation details, process phases, monitoring mechanisms, and troubleshooting steps. The data is primarily used for network defect tracking and service impact analysis.\n## Installation & Setup\nConfiguration paths, database sources, and execution environment for the OneTicket ETL process.\n### Data Source Tables\n- Source system: Oracle Database 11g Enterprise Edition (999.999.999.999.0)  \n\t- Server:`999.999.999.999:1521`\n\t- Port Forward:`999.999.999.999:1521`\n\t- User:`def_network_map`\n\t- SID:`defsblf_rw`\n\t- Oracle Tables: \n\t\t- def_NETWORK_MAP.ACTIVITY\n\t\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 1"
        }
    },
    "21": {
        "page_content": "- SID:`defsblf_rw`\n\t- Oracle Tables: \n\t\t- def_NETWORK_MAP.ACTIVITY\n\t\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t\t- def_NETWORK_MAP.OPEN_MW\n\t\t- def_NETWORK_MAP.OPEN_NTT\n\t\t- def_NETWORK_MAP.OPEN_OCT\n\t\t- def_NETWORK_MAP.OPEN_WTT\n\t\t- def_NETWORK_MAP.EXPORT_CTL\n### Scripts-Configuration Location in HDFS\n- user : `def_network_maps`\n- scripts path : `HDFS:/user/def_network_maps/`\n-\tconfiguration path : `HDFS:/user/def_network_maps/`\n\t- `oneTicket_env.sh`, The environment of the flow\n\t- `OraExpData.tables`, List of tables which are going to be exported/imported from Oracle to Hive\n\t- `monitoring.config`, The `Monitoring` connection details\n\t- `oraclecmd.config`, The Oracle connection details\n\t- `oneticket.keystore`, The Oracle password file\n- Temp dir : `HDFS:/ez/landingzone/tmp/oneTicket`\n### Export Data Location\n- node : Dynamically defined by the Oozie service  \n\ti.e. `sn95.bigdata.abc.gr`\n- Directory : Dynamically defined by the Oozie service\n\ti.e. `/data/2/yarn/nm/usercache/def_network_maps/appcache/application_1668434520231_277391/container_e276_1668434520231_277391_01_000001`\n### Logs Location\n- user : `def_network_maps`\n- logs path : `/user/def_network_maps/log`\n- log files: \n\t- `101.OneTicket_OraMetaData.<YYYYMM>.log`\n\t- `102.OneTicket_OraData_CTRL.<YYYYMM>.log`\n\t- `103.OneTicket_OraData_Export_Import.<TABLE_NAME>.<UNIX-TIME>.log`\n\t- `104.OneTicket_OraData_Import_Hive.<UNIX-TIME>.log`\n\t`<UNIX-TIME>` is the timestamp of the load in unix-epoch format  \n\t`<TABLE_NAME>` list of values:  \n\t- def_NETWORK_MAP.ACTIVITY\n\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t- def_NETWORK_MAP.OPEN_MW\n\t- def_NETWORK_MAP.OPEN_NTT\n\t- def_NETWORK_MAP.OPEN_OCT\n\t- def_NETWORK_MAP.OPEN_WTT\n\ti.e.  \n\t```\n\t$ hdfs dfs -ls -t -r /user/def_network_maps/log\n\t\n\t101.OneTicket_OraMetaData.202302.log\n\t102.OneTicket_OraData_CTRL.202302.log",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 2"
        }
    },
    "22": {
        "page_content": "- def_NETWORK_MAP.OPEN_OCT\n\t- def_NETWORK_MAP.OPEN_WTT\n\ti.e.  \n\t```\n\t$ hdfs dfs -ls -t -r /user/def_network_maps/log\n\t\n\t101.OneTicket_OraMetaData.202302.log\n\t102.OneTicket_OraData_CTRL.202302.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.ACTIVITY.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.AFFECTED_CUSTOMERS.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.AFFECTED_OCT_WTT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_MW.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_NTT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_OCT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_WTT.1675939511.log\n\t104.OneTicket_OraData_Import_Hive.1675939511.log\t\n\t```\n### Oozie Scheduling\n- user : def_network_maps\n- Coordinator :`def_NETWORK_MAP_Coordinator`  \n\truns at : every 5 minutes on a Daily basis  \n\t\t`0,5,10,15,20,25,30,35,40,45,50,55 * * * *` \n- Workflow : `def_NETWORK_MAP_Workflow`  \n\tBash script : `HDFS:/user/def_network_maps/100.OneTicket_Main.sh`\n### Hive Tables\n- Target Database: `def_network_map`\n- Target Tables: \n\t- def_NETWORK_MAP.ACTIVITY\n\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t- def_NETWORK_MAP.OPEN_MW\n\t- def_NETWORK_MAP.OPEN_NTT\n\t- def_NETWORK_MAP.OPEN_OCT\n\t- def_NETWORK_MAP.OPEN_WTT\n### Database CLI commands\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/def_network_map;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr -d def_network_map --ssl -k`\n- Oracle*: `sqlplus -s def_network_map/<PASSWORD>@999.999.999.999:1521/defsblf_rw`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 3"
        }
    },
    "23": {
        "page_content": "- Impala: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr -d def_network_map --ssl -k`\n- Oracle*: `sqlplus -s def_network_map/<PASSWORD>@999.999.999.999:1521/defsblf_rw`\n- MySql*: `mysql -u monitoring -p -h 999.999.999.999 monitoring`\n*\\*The passwords for the Oracle and MySql databases can be found [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)*\n## Data Process: Oracle to Hive ETL Steps\nStep-by-step breakdown of how data is exported from Oracle and ingested into Hive tables.\n### In General\nThe flow consist of two basic procedures and one control Oracle table.  \n\t- the **Export** procedure, which is running at the remdef Oracle server (Responsible def/abc),  \n\t- the **Import** procedure, which is running at the BigStreamer cluster,  \n\t- the `def_NETWORK_MAP.EXPORT_CTL` table, used to synchronize the **Export** procedure with the **Import** procedure.  \nThe data in `def_NETWORK_MAP.EXPORT_CTL` is similar to the following\nConnect to Oracle (see [Database CLI commands](#database-cli-commands))  \n```\nselect * from def_NETWORK_MAP.EXPORT_CTL;  \nEXPORT_SEQUENCE | TARGET                 | EXPORT_START_DT     | EXPORT_END_DT       | ROW_COUNT | IMPORT_START_DT     | IMPORT_END_DT      \n----------------+------------------------+---------------------+---------------------+-----------+---------------------+--------------------\n              0 | TOTAL                  | 2022-11-15 16:50:01 | 2022-11-15 17:07:09 |           | 2022-11-22 09:28:24 | 2022-11-22 09:29:12\n              5 | ACTIVITY               | 2022-11-15 16:51:27 | 2022-11-15 17:04:36 |     73211 |                     |                    \n              6 | AFFECTED_CUSTOMERS     | 2022-11-15 17:04:36 | 2022-11-15 17:04:54 |     14438 |                     |                    \n              7 | OPEN_OCT               | 2022-11-15 17:04:54 | 2022-11-15 17:07:05 |     58338 |                     |",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 4"
        }
    },
    "24": {
        "page_content": "7 | OPEN_OCT               | 2022-11-15 17:04:54 | 2022-11-15 17:07:05 |     58338 |                     |                    \n              8 | OPEN_WTT               | 2022-11-15 17:07:05 | 2022-11-15 17:07:09 |      3690 |                     |                    \n              1 | OPEN_MW                | 2022-11-15 17:10:05 | 2022-11-15 17:10:42 |       249 |                     |                    \n              2 | OPEN_NTT               | 2022-11-15 17:10:42 | 2022-11-15 17:11:03 |      6957 |                     |                    \n              3 | AFFECTED_OCT_WTT       | 2022-11-15 17:11:03 | 2022-11-15 17:11:20 |      1782 |                     |                    \n              4 | DEFECTIVE_NETW_ELEMENT | 2022-11-15 17:11:20 | 2022-11-15 17:11:21 |      6236 |                     |                    \n```\n  Ndef: We are interesting for the 1st row only `EXPORT_SEQUENCE=0`  \n### Phased\n1. The **Export** procedure is implemented by def/abc.  \n\tIt is responsible to prepare the data in Oracle tables (see Oracle Table list in [Data Source Tables](#data-source-tables))  \n\tOnce completed, it updates the `def_NETWORK_MAP.EXPORT_CTL.EXPORT_START_DT` column with the current system's timestamp.   \n\tNdef: It is not known how often the **Export** procedure runs.\n2.  The **Import** procedure is implemented by jkl.  \n\tIt checks periodically if the value of `def_NETWORK_MAP.EXPORT_CTL.EXPORT_START_DT` has been updated.  \n\t- In case of new value, the procedure exports the data from the Oracle tables  \n\t`./oracle_cmd.sh \"select * from <table>\" > ./<table>.exp`  \n\t- stores them into HDFS  \n\t`hdfs dfs -moveFromLocal ./<table>.exp .`\n\t- and, consequently, load them into the corresponding [Hive Tables](#hive-tables).  \n\t*Connect to Beeline (see [Database CLI commands](#database-cli-commands))*\n\t`Beeline <connection> -e \"load data inpath './<table>.exp' overwrite into table <table>;\"`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 5"
        }
    },
    "25": {
        "page_content": "*Connect to Beeline (see [Database CLI commands](#database-cli-commands))*\n\t`Beeline <connection> -e \"load data inpath './<table>.exp' overwrite into table <table>;\"`\n\t- Once the Import procedure completed, the `IMPORT_START_DT` column will be updated with the current system's timestamp.   \n\t*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*\n\t`update def_NETWORK_MAP.EXPORT_CTL.EXPORT_START_DT`\n\t- Compute table statistics using impala-shell\t \n\t*Connect to Impala (see [Database CLI commands](#database-cli-commands))*\n\t`compute incremental stats brond.brond_retrains_hist;`\n3. In case the `EXPORT_START_DT` value isn't updated, the **Import** procedure exists doing nothing.\n## Monitoring\nDescribes how load jobs are tracked in the monitoring.jobstatus table and validated via logs and queries.\n### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \nConnection command: `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n### Monitoring Message list \u2192 Monitoring Messages in MySQL\nFor each load for each TABLE the following set of messages will be recorded in the Monitoring database.\n```sql\nexecution_id | id     | application | job             | component                 | operative_partition | status  | system_ts           | system_ts_end       | param0 | message                                                              | user             | host                    \n-------------+--------+-------------+-----------------+---------------------------+---------------------+---------+---------------------+---------------------+--------+----------------------------------------------------------------------+------------------+-------------------------",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 6"
        }
    },
    "26": {
        "page_content": "1670509202   | 402171 | ONETICKET   | def_NETWORK_MAP | MAIN                      | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 | 2022-12-08 16:22:05 |        | Procedure Started                                                    | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402173 | ONETICKET   | def_NETWORK_MAP | CHECK_FOR_AVAILABLE_DATA  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 |                     |        | New data found: 2022-12-08 15:50:04                                  | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402207 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-<TABLE-NAME>  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     | 6987   | Oracle export def_NETWORK_MAP.OPEN_NTT data. Rows:6987               | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402209 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-<TABLE-NAME> | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402211 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-<TABLE-NAME>    | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     |        | Move def_NETWORK_MAP.OPEN_NTT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402241 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-<TABLE-NAME>    | 20221208_1620       | SUCCESS | 2022-12-08 16:21:32 |                     |        | Load def_NETWORK_MAP.OPEN_NTT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402247 | ONETICKET   | def_NETWORK_MAP | COMPUTE_STATS             | 20221208_1620       | SUCCESS | 2022-12-08 16:22:04 |                     |        | Compute statistics                                                   | def_network_maps | un-vip.bigdata.abc.gr\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 7"
        }
    },
    "27": {
        "page_content": "```\n### Monitoring Component list\n|Component | Description \n|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED| \n|CHECK_FOR_AVAILABLE_DATA| Check the Oracle table EXPORT_CTL if there are new data to export\n|EXPORT_DATA-\\<TABLE-NAME\\>| Exports data from Oracle to `/shared/abc/oneTicket/exp`\n|DATA_PARSING-\\<TABLE-NAME\\>| Change column separator and remove the string \"null\"\n|HDFS_MOVE-\\<TABLE-NAME\\>| Move export file from local file system to HDFS `/ez/landingzone/tmp/oneTicket`\n|LOAD_DATA-\\<TABLE-NAME\\>| Load export file from HDFS `/ez/landingzone/tmp/oneTicket` into the HIVE table (i.e. `def_NETWORK_MAP.OPEN_NTT`)\n|COMPUTE_STATS| performs compute statistics on HIVE tables using impala-shell\n### Monitoring database Queries \u2192 Sample Monitoring DB Queries\n- List messages of the last load  \n`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n```sql\n  select \n    execution_id, id, application, job, component, operative_partition,  \n    status, system_ts, system_ts_end, param0, message, user,  host\n  from jobstatus a where 1=1\n  and upper(application)='ONETICKET' and upper(job) like 'def_NETWORK_MAP'   \n  and execution_id=(\n    select max(execution_id) execution_id from jobstatus where 1=1\n    and upper(application)='ONETICKET' and upper(job) like 'def_NETWORK_MAP'   \n    and lower(message) not like '%no new export found%' \n    and component='CHECK_FOR_AVAILABLE_DATA'\n    and system_ts>=date(now())-30)\n  order by id\n  ;",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 8"
        }
    },
    "28": {
        "page_content": "execution_id | id     | application | job             | component                                           | operative_partition | status  | system_ts           | system_ts_end       | param0 | message                                                              | user             | host                    \n  -------------+--------+-------------+-----------------+-----------------------------------------------------+---------------------+---------+---------------------+---------------------+--------+----------------------------------------------------------------------+------------------+-------------------------\n  1670509202   | 402171 | ONETICKET   | def_NETWORK_MAP | MAIN                                                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 | 2022-12-08 16:22:05 |        | Procedure Started                                                    | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402173 | ONETICKET   | def_NETWORK_MAP | CHECK_FOR_AVAILABLE_DATA                            | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 |                     |        | New data found: 2022-12-08 15:50:04                                  | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402183 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.AFFECTED_OCT_WTT        | 20221208_1620       | SUCCESS | 2022-12-08 16:20:12 |                     | 1867   | Oracle export def_NETWORK_MAP.AFFECTED_OCT_WTT data. Rows:1867       | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402185 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.AFFECTED_OCT_WTT       | 20221208_1620       | SUCCESS | 2022-12-08 16:20:12 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 9"
        }
    },
    "29": {
        "page_content": "1670509202   | 402187 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.AFFECTED_CUSTOMERS      | 20221208_1620       | SUCCESS | 2022-12-08 16:20:15 |                     | 17397  | Oracle export def_NETWORK_MAP.AFFECTED_CUSTOMERS data. Rows:17397    | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402189 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.AFFECTED_OCT_WTT          | 20221208_1620       | SUCCESS | 2022-12-08 16:20:15 |                     |        | Move def_NETWORK_MAP.AFFECTED_OCT_WTT data in HDFS                   | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402191 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.AFFECTED_CUSTOMERS     | 20221208_1620       | SUCCESS | 2022-12-08 16:20:15 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402193 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_MW                 | 20221208_1620       | SUCCESS | 2022-12-08 16:20:17 |                     | 238    | Oracle export def_NETWORK_MAP.OPEN_MW data. Rows:238                 | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402195 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_MW                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:17 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402197 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:18 |                     | 6035   | Oracle export def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT data. Rows:6035 | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 10"
        }
    },
    "30": {
        "page_content": "1670509202   | 402199 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT | 20221208_1620       | SUCCESS | 2022-12-08 16:20:18 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402201 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.AFFECTED_CUSTOMERS        | 20221208_1620       | SUCCESS | 2022-12-08 16:20:18 |                     |        | Move def_NETWORK_MAP.AFFECTED_CUSTOMERS data in HDFS                 | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402203 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_MW                   | 20221208_1620       | SUCCESS | 2022-12-08 16:20:21 |                     |        | Move def_NETWORK_MAP.OPEN_MW data in HDFS                            | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402205 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT    | 20221208_1620       | SUCCESS | 2022-12-08 16:20:21 |                     |        | Move def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT data in HDFS             | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402207 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_NTT                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     | 6987   | Oracle export def_NETWORK_MAP.OPEN_NTT data. Rows:6987               | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402209 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_NTT               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 11"
        }
    },
    "31": {
        "page_content": "1670509202   | 402211 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_NTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     |        | Move def_NETWORK_MAP.OPEN_NTT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402213 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_WTT                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     | 3621   | Oracle export def_NETWORK_MAP.OPEN_WTT data. Rows:3621               | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402215 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_WTT               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402217 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_WTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:30 |                     |        | Move def_NETWORK_MAP.OPEN_WTT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402219 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.ACTIVITY                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:36 |                     | 74433  | Oracle export def_NETWORK_MAP.ACTIVITY data. Rows:74433              | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402221 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.ACTIVITY               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:37 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 12"
        }
    },
    "32": {
        "page_content": "1670509202   | 402223 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.ACTIVITY                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:40 |                     |        | Move def_NETWORK_MAP.ACTIVITY data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402225 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_OCT                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:40 |                     | 60164  | Oracle export def_NETWORK_MAP.OPEN_OCT data. Rows:60164              | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402227 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_OCT               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:40 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402229 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_OCT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:43 |                     |        | Move def_NETWORK_MAP.OPEN_OCT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402231 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.ACTIVITY                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:57 |                     |        | Load def_NETWORK_MAP.ACTIVITY data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402233 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.AFFECTED_CUSTOMERS        | 20221208_1620       | SUCCESS | 2022-12-08 16:21:04 |                     |        | Load def_NETWORK_MAP.AFFECTED_CUSTOMERS data into Hive               | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 13"
        }
    },
    "33": {
        "page_content": "1670509202   | 402235 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.AFFECTED_OCT_WTT          | 20221208_1620       | SUCCESS | 2022-12-08 16:21:11 |                     |        | Load def_NETWORK_MAP.AFFECTED_OCT_WTT data into Hive                 | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402237 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT    | 20221208_1620       | SUCCESS | 2022-12-08 16:21:18 |                     |        | Load def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT data into Hive           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402239 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_MW                   | 20221208_1620       | SUCCESS | 2022-12-08 16:21:25 |                     |        | Load def_NETWORK_MAP.OPEN_MW data into Hive                          | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402241 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_NTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:21:32 |                     |        | Load def_NETWORK_MAP.OPEN_NTT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402243 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_OCT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:21:39 |                     |        | Load def_NETWORK_MAP.OPEN_OCT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402245 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_WTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:21:46 |                     |        | Load def_NETWORK_MAP.OPEN_WTT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 14"
        }
    },
    "34": {
        "page_content": "1670509202   | 402247 | ONETICKET   | def_NETWORK_MAP | COMPUTE_STATS                                       | 20221208_1620       | SUCCESS | 2022-12-08 16:22:04 |                     |        | Compute statistics                                                   | def_network_maps | un-vip.bigdata.abc.gr\n  ```\n### Monitoring Health-Check\n- Check Monitoring status.  \n```bash  \n$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'  \n{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n```\n- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\nActions to follow in case of failure, based on alert messages and log investigation.\nAn email will be sent by the system with the point of failure.  \ni.e.\n<pre>\nFrom: abc_bigd@abc.gr  \nSubject: ONETICKET - def_NETWORK_MAP: FAILED  \nLoad <b>def_NETWORK_MAP.ACTIVITY</b> data into Hive (1673849411)  \n<b>Exec_id:1673849411</b>  \nThis is an automated e-mail.  \nPlease do not reply.  \n</pre>\n**Actions**  \n1. Write down the values of the `Table name` and `Exec_id` described in the alert email  \ni.e. \n- Table name: `def_NETWORK_MAP.ACTIVITY`\n- Exec_id:`1673849411`\n2. Copy from HDFS the folowing log files which contains the specific `Table name` and `Exec_id` in its filename.\n- 103.OneTicket_OraData_Export_Import.\\<Table name\\>.\\<Exec_id\\>.log\n- 104.OneTicket_OraData_Import_Hive.\\<Exec_id\\>.log\n<pre>\nhdfs dfs -get /user/def_network_maps/hdfs dfs -get 103.OneTicket_OraData_Export_Import.<b>def_NETWORK_MAP.ACTIVITY.1673849411</b>.log\nhdfs dfs -get /user/def_network_maps/hdfs dfs -get 104.OneTicket_OraData_Import_Hive.<b>1673849411</b>.log\n</pre>\n3. Searches for Exception messages in log files",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 15"
        }
    },
    "35": {
        "page_content": "hdfs dfs -get /user/def_network_maps/hdfs dfs -get 104.OneTicket_OraData_Import_Hive.<b>1673849411</b>.log\n</pre>\n3. Searches for Exception messages in log files  \n`egrep '(Exception:|Coused by)' 10[1-4].OneTicket_OraData*.log`  \ni.e.\n<pre>\n$ egrep '(Exception:|Coused by)' 10[1-4].OneTicket_OraData*.log\n104.OneTicket_OraData_Import_Hive.1673849411.log:javax.security.sasl.SaslException: GSS initiate failed\n104.OneTicket_OraData_Import_Hive.1673849411.log:Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find\n</pre>\n### Common errors  \n  - impala/hive availability\n  - Kerberos authentication\n  *Ndef: The flow checks if the ticket is still active before any HDFS action.  \n  In case of expiration the flow performs a `kinit` command*\n## Data Check\nOptional validation queries for verifying data completeness and load success.\nThe data checks below are provided for informational purposes only.  \nIf any of them returns wrong data, then no actions need to be taken from the support team.  \nThe flow runs periodically over the day and every time overwrites the data.   \n### Check Load Status.\nif the difference between `EXPORT_START_DT` and `IMPORT_START_DT` is greater than 2 hours it is considered as a problem in loading procedure.  \n*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*  \n\t<pre>\n\tselect \n\t  EXPORT_START_DT, IMPORT_START_DT,\n\t  case when 24*(EXPORT_START_DT-IMPORT_START_DT)>2 then 'ERROR' else 'OK' end Load_Status\n\tfrom EXPORT_CTL where EXPORT_SEQUENCE=0;\n\t</pre>\n\t<pre>\n\tEXPORT_START_DT     | IMPORT_START_DT     | LOAD_STATUS\n\t--------------------+---------------------+------------\n\t<b>2022-12-02 10:46:11 | 2022-12-02 07:48:26 | ERROR      </b>#in case of load issue\n\t\n\tEXPORT_START_DT     | IMPORT_START_DT     | LOAD_STATUS\n\t--------------------+---------------------+------------\n\t2022-12-02 10:46:11 | 2022-12-02 10:48:26 | OK         #under normal circumstances\n\t</pre>",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 16"
        }
    },
    "36": {
        "page_content": "--------------------+---------------------+------------\n\t2022-12-02 10:46:11 | 2022-12-02 10:48:26 | OK         #under normal circumstances\n\t</pre>\n### Check data in Hive-Impala tables\n*Connect to Impala (see [Database CLI commands](#database-cli-commands))*  \n```sql\nselect * from (\n  select distinct  'activity' tbl, upd_ts from def_network_map.activity union all\n  select distinct  'affected_customers', upd_ts from def_network_map.affected_customers union all\n  select distinct  'affected_oct_wtt', upd_ts from def_network_map.affected_oct_wtt union all\n  select distinct  'defective_netw_element', upd_ts from def_network_map.defective_netw_element union all\n  select distinct  'open_mw', upd_ts from def_network_map.open_mw union all\n  select distinct  'open_ntt', upd_ts from def_network_map.open_ntt union all\n  select distinct  'open_oct', upd_ts from def_network_map.open_oct union all\n  select distinct  'open_wtt', upd_ts from def_network_map.open_wtt\n)a order by tbl\n;",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 17"
        }
    },
    "37": {
        "page_content": "+------------------------+---------------------+\n| tbl                    | upd_ts              |\n+------------------------+---------------------+\n| activity               | 2022-12-16 10:50:18 |\n| affected_customers     | 2022-12-16 10:50:18 |\n| affected_oct_wtt       | 2022-12-16 10:50:18 |\n| defective_netw_element | 2022-12-16 10:50:18 |\n| open_mw                | 2022-12-16 10:50:18 |\n| open_ntt               | 2022-12-16 10:50:18 |\n| open_oct               | 2022-12-16 10:50:18 |\n| open_wtt               | 2022-12-16 10:50:18 |\n+------------------------+---------------------+\nFetched 8 row(s) in 6.10s\n```\n`upd_ts` should have the same value *(+- 10 seconds)* as the one in `IMPORT_START_DT` from Oracle table `EXPORT_CTL`  \ni.e.  \n*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*  \n`select IMPORT_START_DT from EXPORT_CTL where EXPORT_SEQUENCE=0;`\n```sql\nIMPORT_START_DT     \n--------------------\n2022-12-16 10:50:19\n```\n*Conclusion: Hive/Impala tables contain the correct data according to the control table*\n```\n     upd_ts     = 2022-12-16 10:50:18 #Hive/Impala tables\nIMPORT_START_DT = 2022-12-16 10:50:19 #Oracle Control Table\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "NETWORK_MAP_Support_Notes.md - Part 18"
        }
    },
    "38": {
        "page_content": "---\ntitle: Monitoring Application  \nsystem: BigStreamer  \ncomponent: Monitoring  \njob_name: monitoring-app  \nhost_nodes:\n  - un5.bigdata.abc.gr\n  - un6.bigdata.abc.gr\nvip_host: un-vip.bigdata.abc.gr  \nport: 12800  \ncontainer_name: monitoring-app-{version}  \nconfig_path: /opt/monitoring_app/monitoring_config  \nlog_paths:\n  - /opt/monitoring-app/logs/monitoring-api.log\n  - /opt/monitoring-app/logs/tomcat/access_log.log\ndatabase:\n  host: db-vip.bigdata.abc.gr:3306\n  user: monitoring\n  schema: monitoring\n  table: jobstatus\nmetrics_sink:\n  system: Graphite\n  host: un-vip.bigdata.abc.gr\n  port: 2004\nowner: monitoring  \ntags:\n  - monitoring\n  - jobstatus\n  - Graphite\n  - Docker\n  - container\n  - IPVPN\n  - BigStreamer\n  - MySQL\n  - logging\n  - health-check\n  - HAProxy\n  - load-balancer\n  - deployment\n  - cronjobs\n  - metrics\n---\n# Monitoring application\n## Overview\nThe purpose of application is to monitor all streamming and batch jobs through HTTP calls. Requests are written to database ``monitoring`` and table ``jobstatus``. Metrics about application performance are written to Graphite.\n## Deployment Details\n### App Deployment & Access\nDeployed on nodes **un5**, **un6**  \n- Host: `un5.bigdata.abc.gr`, `un6.bigdata.abc.gr`\n- Port: 12800\nReached via HAProxy  \n- Host: **un-vip.bigdata.abc.gr**\n- Port: 12800\n### Configuration Files\n `/opt/monitoring_app/monitoring_config`\n### Application Logs\nThese log files capture runtime and access events of the monitoring application. Use them for debugging service failures, connection issues, or API call errors.\n- application logs: `/opt/monitoring-app/logs/monitoring-api.log`\n  - older logs: `/opt/monitoring-app/logs/2022-11`\n- access logs:  `/opt/monitoring-app/logs/tomcat/access_log.log`\n### Monitoring Database (MySQL)\n- Host: db-vip.bigdata.abc.gr:3306\n- User: monitoring\n- Schema: monitoring\n- Table: jobstatus\n### Metrics Export to Graphite\n- Host: un-vip.bigdata.abc.gr\n- Port: 2004\n## Operations & Maintenance Procedures",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "manage-monitoring-app.md - Part 1"
        }
    },
    "39": {
        "page_content": "- User: monitoring\n- Schema: monitoring\n- Table: jobstatus\n### Metrics Export to Graphite\n- Host: un-vip.bigdata.abc.gr\n- Port: 2004\n## Operations & Maintenance Procedures\n### Check Container Status\nDescribes how to verify if the Docker container running the monitoring app is active or stopped, and how to restart it when necessary.\n#### Container\n| Description| Command |\n| ----------- | ----------- |\n|check container is running| `sudo docker ps --filter=\"name=monitoring-app-{version}\" --filter=\"status=running\"`|\n|check container is stopped| `sudo docker ps --filter=\"name=monitoring-app-{version}\" --filter=\"status=exited\"`|\n|check container status| `sudo docker ps --filter=\"name=monitoring-app-{version}\"`|\n|stop monitoring-app| `sudo docker stop monitoring-app-{version}`|\n|start monitoring-app|`sudo docker start monitoring-app-{version}`|\n|run monitoring-app if container is killed & removed|[use start app script](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/Deployment-PROD#run-start-up-script)|\n### Stop & Start Monitoring App\n#### Connect to nodes\n1. SSH to un2   \n`ssh root@un2`\n2. SSH to un5/un6 nodes via personal sudoer user.  \n(superuser privileges to perform Docker operations)\n   1. `ssh <user>@<un5/un6>`\n   1. `sudo su`\n#### Stop current container\n1. Get container name with docker ps as described above\n2. Stop container  \n`docker stop monitoring-app-{version}`\n#### Start stopped container\nTo start the container using the latest version (the latest container version is the one that was stopped with the previous command) use,\n`docker start monitoring-app-{version}`\n### Application API Endpoints\n- Using un5 IP\n| Description | Command |\n| ----------- | ----------- |\n| Check app is running | `curl --location --request GET 'http://999.999.999.999:12800/monitoring/app/status'` |\n| Check Load Balancer is enabled | `curl --location --request GET 'http://999.999.999.999:12800/monitoring/app/lb/check' `|",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "manage-monitoring-app.md - Part 2"
        }
    },
    "40": {
        "page_content": "| Check Load Balancer is enabled | `curl --location --request GET 'http://999.999.999.999:12800/monitoring/app/lb/check' `|\n| Enable Load Balancer | `curl --location --request PUT 'http://999.999.999.999:12800/monitoring/app/lb/enable'` |\n| Disable Load Balancer  | `curl --location --request PUT 'http://999.999.999.999:12800/monitoring/app/lb/disable'`|\n### Troubleshooting Steps\n- Check [logs](#logs) to identify the problem \n### Deployment Guide\n- [Deployment guide](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/Deployment-PROD)",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "manage-monitoring-app.md - Part 3"
        }
    },
    "41": {
        "page_content": "---\ntitle: IPVPN-SM Replacement ETL & App  \ndescription: Spring Boot-based application and ETL scripts that extract KPIs from BigStreamer and export them to the SQM server every 5 minutes. Replaces legacy IPVPN-SLA pipeline components for CPU, Memory, QoS, Availability, and Interface metrics.  \nsystem: BigStreamer  \ncomponent: IPVPN-SM  \njob_name: Export_IPVPN_KPIs_to_SQM  \nowner: ipvpn  \nsource_tables:\n  - bigcust.nnm_ipvpn_componentmetrics_hist\n  - bigcust.perf_interfacemetrics_ipvpn_hist\n  - bigcust.nnmcp_ipvpn_slametrics_hist\n  - nnmnps.nms_node\n  - bigcust.customer_pl\n  - bigcust.customer_sla_config_ipvpn\n  - bigcust.sla_configurations\ndestination_system: SQM Server  \ndestination_endpoint: /inventory/measurements  \napi_ingestion_endpoint: /ingest-query  \nschedule:\n  frequency: every 5 minutes  \n  retry_policy: up to 5 retries per interval  \ndeployment:\n  application_host_group:\n    - un1.bigdata.abc.gr\n    - un2.bigdata.abc.gr\n  haproxy_vip: un-vip.bigdata.abc.gr\n  haproxy_port: 13001\n  http_port: 13000\n  jmx_port: 13800\n  scripts_path: /shared/abc/ip_vpn/sm-replacement/scripts\n  config_path: /shared/abc/ip_vpn/sm-app/deployment/config\n  logs_path: /shared/abc/ip_vpn/sm-app/deployment/logs\nauthentication:\n  kerberos_keytab: /home/users/ipvpn/ipvpn.keytab\n  jaas_config: /shared/abc/ip_vpn/sm-app/deployment/config/jaas.conf\n  token_auth_keystore: /shared/abc/ip_vpn/sm-app/deployment/config/credentials.keystore\nmonitoring:\n  grafana_dashboard: https://unc1.bigdata.abc.gr:3000/d/HiuaKlU4z/ipvpn-sm-replacement-monitoring?orgId=1\n  mysql_monitoring_db_host: 999.999.999.999\n  mysql_user: monitoring\n  mysql_database: monitoring\nfailure_handling:\n  retries: 5\n  logging_paths:\n    - /shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*\n    - /shared/abc/ip_vpn/sm-app/deployment/logs/sm-server-requests.log\n    - /shared/abc/ip_vpn/sm-app/deployment/logs/application.log\n    - /shared/abc/ip_vpn/sm-app/deployment/logs/tomcat/access_log.log\nscripts:\n  orchestrators:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 1"
        }
    },
    "42": {
        "page_content": "- /shared/abc/ip_vpn/sm-app/deployment/logs/application.log\n    - /shared/abc/ip_vpn/sm-app/deployment/logs/tomcat/access_log.log\nscripts:\n  orchestrators:\n    - initiate_export_CPU.sh\n    - initiate_export_MEM.sh\n    - initiate_export_QOS.sh\n    - initiate_export_AV.sh\n    - initiate_export_IF.sh\n  helpers:\n    - compute_metrics_via_sm_app.sh\n    - query_sm.sh\n    - sm-replacement-call-repeater.sh\ntags:\n  - ipvpn\n  - ipvpn-sm\n  - sqm\n  - bigstreamer\n  - kpi\n  - sla\n  - availability\n  - qos\n  - memory\n  - cpu\n  - interface\n  - impala\n  - spring boot\n  - haproxy\n  - kerberos\n  - token authentication\n  - export\n  - metrics\n  - monitoring\n  - failure handling\n  - retry\n  - ingestion\n  - flume\n  - hive\n  - bash scripts\n  - automation\n  - crontab\n  - 5-minute jobs\n  - grafana\n  - mysql\n---\n# Introduction\nOverview of the replacement flow that calculates and exports KPIs (CPU, Memory, QoS, etc.) from BigStreamer to SQM via IPVPN-SM app.\nThis support document outlines the steps that compose the IPVPN-SM replacement process that computes Key Performance Indicators (KPIs) regarding the network quality of abc. The ETL process involves a series of scripts and a Spring Boot application that work together to retrieve, transform, and load data from Impala tables into a remdef server.\nThere are three major KPI categories, *Components*, *SLA* and *Interfaces*, which consist of the following subcategories:\n|KPI category|Metrics involved| Relevant Impala tables|\n|---|---|---|\n|Components|CPU, Memory (MEM)|bigcust.nnm_ipvpn_componentmetrics_hist,<br>  nnmnps.nms_node |\n|Interfaces|Interfaces (IF)|bigcust.perf_interfacemetrics_ipvpn_hist,<br>  bigcust.pe_interfaces,<br>  nnmnps.nms_node|\n|SLA|Availability (AV), Quality of Service (QOS)|bigcust.nnmcp_ipvpn_slametrics_hist,<br> bigcust.customer_pl,<br> bigcust.customer_sla_config_ipvpn,<br> bigcust.sla_configurations|\n# Application Flow\nStep-by-step description of how the ETL pipeline is executed using scripts and Spring Boot app.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 2"
        }
    },
    "43": {
        "page_content": "# Application Flow\nStep-by-step description of how the ETL pipeline is executed using scripts and Spring Boot app.\n## Scripts & IPVPN-SM App operations\nThe metrics computation of each of the three categories (sla, components, interfaces) is triggered by the `un2:/shared/abc/ip_vpn/run/initiate_export_*.sh` script that runs as part of the IPVPN-SLA [cronjob](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/blob/master/PROD/crontab/crontab_per_user.txt), scheduled to run every 5 minutes, and the computation refers to a full five-minute interval.\nThe ETL process follows the following flow:\n### IPVPN-SLA\n- [IPVPN-SLA Support Doc](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ip_vpn.md)\n### IPVPN-SM\n- [IPVPN-SM Dev Wiki](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/home)\n3. The `export_*_kpis.sh` calls the IPVPN-SM App to execute the Impala queries. The curl request to the IPVPN-SM App is executed asynchronously, allowing the IPVPN-SM pipeline to continue with other scheduled 5 min intervals without waiting for the query results.\n4. The IPVPN-SM App executes the Impala queries, applies transformations on the result set based on the agreed schema specifications, and sends the results to the SQM server.\n## IPVPN-SM Application\nThe IPVPN-SM Application executes the Impala queries specified in the `/shared/abc/ip_vpn/sm-app/deployment/config/field-mappings.yml`, applies transformations on the result set based on the agreed [schema specifications](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Data-Transmission-Schema) , and sends the results to the SQM server.\n### IPVPN-SM Endpoint: /ingest-query\nThe endpoint for the IPVPN-SM application is `/ingest-query`. This endpoint should be called with a POST request.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 3"
        }
    },
    "44": {
        "page_content": "### IPVPN-SM Endpoint: /ingest-query\nThe endpoint for the IPVPN-SM application is `/ingest-query`. This endpoint should be called with a POST request.\nDetails on the request body fields and examples [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Application-details#request-body-1)\n### Check application status\n```bash\ncurl --location --request GET 'un-vip:13001/ipvpn-sm/app/status'\n```\n## Application Flow Diagram\n```mermaid\nflowchart TD;\n  A(\"/shared/abc/ip_vpn/run/initiate_export_*.sh <br> ipvpn@un2\") --> B(\"/shared/abc/ip_vpn/run/export_*_kpis.sh <br> ipvpn@un2\")\n  B -- 1. Calls Sync ----> C(\"/shared/abc/ip_vpn/sm-replacement/scripts/refresh_*_kpis.sh <br> ipvpn@un2\")\n  B -- 2. Calls Async ---> I(\"/shared/abc/ip_vpn/sm-replacement/scripts/compute_metrics_via_sm_app.sh <br> ipvpn@un2\")--> J(\"/shared/abc/ip_vpn/sm-replacement/scripts/query_sm.sh <br> ipvpn@un2\")\n  J -- /ingest-query request--->IPVPN-SM_Replacement_Application\n  subgraph IPVPN-SM_Replacement_Application[\"root@un1/un2\"]\n\t E(Impala Queries)\n\t F(Data with agreed schema)\n  end\n  E -- data transformation --> F\n  F -- POST --> G[SQM Server]\n  style A fill:#F8F8F8\n  style B fill:#F8F8F8\n  style IPVPN-SM_Replacement_Application fill:#DCDCDC\n  style I fill:#DCDCDC\n  style C fill:#DCDCDC\n  style E fill:#DCDCDC\n  style F fill:#DCDCDC\n  style J fill:#DCDCDC\n  style G fill:darkgray\n```\n# Infrastructure\nStep-by-step description of how the ETL pipeline is executed using scripts and Spring Boot app.\nThe ETL pipeline infrastructure includes the following components:\n## Application\n- un1/un2 files\n\t- **Deployment path**: `/shared/abc/ip_vpn/sm-app/deployment`\n\t- **Configuration**: `/shared/abc/ip_vpn/sm-app/deployment/config`\n\t\t- [application.yml](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/blob/main/PROD/deployment/config/application.yml)\n\t- **Logs**: `/shared/abc/ip_vpn/sm-app/deployment/logs`\n- Data sources:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 4"
        }
    },
    "45": {
        "page_content": "- **Logs**: `/shared/abc/ip_vpn/sm-app/deployment/logs`\n- Data sources:\n\t- **Impala**:  `un-vip.bigdata.abc.gr:21050`\n\t- **hive database**: `bigcust`\n- Authentication:\n  The authentication to Impala is done with Kerberos. The files used are:\n    - `/etc/krb5.conf`\n    - `/shared/abc/ip_vpn/sm-app/deployment/config/jaas.conf`\n    - `/home/users/ipvpn/ipvpn.keytab`\n- IPVPN-SM app:\n  The application uses HAProxy, a load balancer that distributes incoming requests to the deployed application nodes.\n  - **HAProxy**:\n  Host: `un-vip` ,\n  Port: `13001`\n  - **Application**:\n    Hosts: `un1/un2`,\n    Active Ports:\n      - HTTP port : `13000`\n      - JMX port : `13800`\n- External SQM server:\n\t- **host**: `gw-prod-sqm.aksprd.abc.gr`\n\t- **port**: `443`\n\t- **endpoint**: `/inventory/measurements`\n## Bash scripts\n| Script | Description | Location |\n|--------|-------------|----------|\n| `initiate_export_*.sh` | Triggers category-specific KPI exports | `/shared/abc/ip_vpn/run/` |\n| `compute_metrics_via_sm_app.sh` | Prepares & posts payload to SM App | `/shared/abc/ip_vpn/sm-replacement/scripts/` |\n| `query_sm.sh` | Constructs the payload for SM App | Same as above |\n| `sm-replacement-call-repeater.sh` | Manual runner across intervals | Same as above |\nList of helper scripts used by the SLA cronjob and their role in preparing and triggering requests.\nIn addition to the IPVPN-SM application, IPVPN-SM utilizes certain bash scripts. Their primary purpose is to generate the appropriate input data that is subsequently passed as the body parameter in the POST requests to the IPVPN-SM application.\nThey are invoked by the IPVPN-SLA `export_*_kpis.sh` [scripts](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/blob/master/PROD/nnmnps/bin/export_component_kpis.sh).\n- **Node**: `un2`\n- **User**: `ipvpn`\n- **Scripts**: `/shared/abc/ip_vpn/sm-replacement/scripts`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 5"
        }
    },
    "46": {
        "page_content": "- **Node**: `un2`\n- **User**: `ipvpn`\n- **Scripts**: `/shared/abc/ip_vpn/sm-replacement/scripts`\n- **IPVPN-SM App host & port configured in file** : `/shared/abc/ip_vpn/sm-replacement/scripts/config.properties`\n- **Logs**: `/shared/abc/ip_vpn/sm-replacement/log`\nThe specific order in which they are called is the following:\n```mermaid\ngraph LR\nA[\"export_*_kpis.sh<br><i>IPVPN-SLA</i>\"] -->B[compute_metrics_via_sm_app.sh]\nB -->C[query_sm.sh]\nC --/ingest-query-->D(SM Application)\nstyle A fill:darkgray\nstyle B fill:#ECECEC\nstyle C fill:#ECECEC\n```\n## Deployment Instructions\nStop/Start guide on [abc_BigStreamer_Apps_MoP_Restart.docx](https://ghi.sharepoint.com/:w:/t/abcBigData/EZsK97GxFyVHkeBoFTFjuRYBtPF7lK86cTA44wMhayNlNA?e=Gjh0ZO)  \nCheck project wiki [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Application-details#deployment-instructions)\n## SQM Token Authentication\nThe authentication on the SQM server is done with the HTTP Token Auth in IPVPN-SM application. It requires providing the keystore file, where the token is stored.\n# Failure Handling\nHow to debug failed requests, interpret logs, and what to check first when something breaks.\n## Logs\n### Failure Log Locations\n#### Script Logs on un2\nThe scripts which refresh the Impala tables and which call the IPVPN-SM application write logs under the directories:\n1. The `refresh_*_.sh script` Logs\n`/shared/abc/ip_vpn/sm-replacement/log/refresh_*_kpis`\n2. The `compute_metrics_via_sm_app.sh` Logs\n`/shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*`\nThe asterisk is used to dendef the type of the particular category.\n#### Application Logs on un1/un2\nIn case of failure, the logs related to the IPVPN-SM app are the following:\n1. **Access Logs**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/tomcat/access_log.log`\n2. **Application Logs**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/application.log`\n3. **Requests sent to SQM Server**",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 6"
        }
    },
    "47": {
        "page_content": "`/shared/abc/ip_vpn/sm-app/deployment/logs/tomcat/access_log.log`\n2. **Application Logs**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/application.log`\n3. **Requests sent to SQM Server**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/sm-server-requests.log`\n#### IPVPN-SLA Logs on `un2`\nAside from the above logs, the operation of the IPVPN-SLA scripts is also monitored by the original implementation. Details about the logging of IPVPN-SLA can be found [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ip_vpn.md)\n### Auto-retry mechanism\nDuring the schedules runs, if the call to the IPVPN-SM app fails, the call to the application is repeated. If the application fails on all 5 attemps, the script exits. On each failed attempt, an email alert is sent via the monitoring app. This operation is done inside the `un2:/shared/abc/ip_vpn/sm-replacement/scripts/compute_metrics_via_sm_app.sh` script.\n# Support\nWhere to check system health (Grafana, MySQL), common exceptions and their causes, and support scripts.\n## Check request status via Monitoring\n### Visual Dashboards (Grafana)\nCheck grafana dashboard for failed requests:\n`https://unc1.bigdata.abc.gr:3000/d/HiuaKlU4z/ipvpn-sm-replacement-monitoring?orgId=1&refresh=5s`\n### Monitoring DataBase (MySQL)\nIPVPN-SM App uses Monitoring App in order to record statistics regarding the status of each metric computation request. Querying monitoring DB allows us to investigate pdefntial failed requests.\n1. Connect to monitoring DB\n`mysql -umonitoring -p -h 999.999.999.999`\n2. `use monitoring;`\n#### See failed requests in the past 24h excluding EmptyQuery Exceptions\nOften persistent EmptyQuery alerts demand further investigation in the CustomPoller component of IPVPN-SLA application and the associated metrics cannot be retrieved from the Impala tables.   \nTo identify failed metrics in the past 24h due to errors other than EmptyQuery exceptions, we can use the following query\n```sql",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 7"
        }
    },
    "48": {
        "page_content": "To identify failed metrics in the past 24h due to errors other than EmptyQuery exceptions, we can use the following query\n```sql\nmysql> select distinct x.job, x.5min_interval_ts, x.sent_to_SQM, y.request_repetitions, x.exception from ( select d.job, DATE_FORMAT( STR_TO_DATE( operative_partition, '%Y%m%d%H%i%s' ), '%Y-%m-%d %H:%i' ) as 5min_interval_ts, c.operative_partition, c.status AS 'sent_to_S\nQM', DATE_FORMAT( metric_ts_start, '%Y-%m-%d %H:%i:%s' ) as metric_ts_start, DATE_FORMAT( c.system_ts, '%Y-%m-%d %H:%i:%s' ) as metric_ts_end, SUBSTRING_INDEX( c.message, 'com.jkl.bigstreamer.abc.ipvpn.sm.exception.', -1 ) as exception from ( SELECT job, compone\nnt as start_comp, status as started, operative_partition as start_oppart, system_ts as metric_ts_start FROM jobstatus WHERE application = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_START' and message = 'n_attempt=0' order by system_ts desc,\njob ) d inner join ( select b.* from ( SELECT MAX(system_ts) as max_ts FROM jobstatus WHERE application = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_END' group by operative_partition, job ) a inner join ( SELECT * FROM jobstatus WHERE applic\nation = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_END' ) b on a.max_ts = b.system_ts ) c on d.job = c.job and d.start_oppart = c.operative_partition order by 5min_interval_ts desc ) x left join ( select count(*)-1 as 'request_repetitions',\njob, operative_partition from jobstatus where application = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_START' group by operative_partition, job ) y on x.operative_partition = y.operative_partition and x.job = y.job where sent_to_SQM=\"FAILED\"\n and SUBSTRING_INDEX( exception, ':', 1 )!=\"AppEmptyQueryException\" order by 5min_interval_ts desc;",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 8"
        }
    },
    "49": {
        "page_content": "+-----+------------------+-------------+---------------------+-------------------+\n| job | 5min_interval_ts | sent_to_SQM | request_repetitions | exception         |\n+-----+------------------+-------------+---------------------+-------------------+\n| AV  | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| CPU | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| MEM | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| IF  | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| QOS | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| AV  | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| CPU | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| IF  | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| MEM | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| QOS | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n+-----+------------------+-------------+---------------------+-------------------+",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 9"
        }
    },
    "50": {
        "page_content": "```\nThese are the requests that should be manually handled following the actions described next.\n#### Inspect specific metric request\nIn case of alerts, we should first check the status of the particular requests on the monitoring DB.\nUsing the metric category and the reference interval associated with a request to IPVPN-SM App we can query the Monitoring DB\nby passing the corresponding job and operative_partition values.\n- `select application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\"IPVPN-SM\" and job=\"<category>\" and operative_partition=\"<YYYYMMDDhhmmss>\" order by system_ts;`\nFor example. if we want to check the status of a request for QOS metric category and `2023-05-10 06:55:00` reference interval:\n```sql\nmysql> select application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\"IPVPN-SM\" and job=\"QOS\" and operative_partition=\"20230510065500\" order by system_ts;\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\n| application | job | component  | operative_partition | status  | message                                                                    | system_ts               |\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\n| IPVPN-SM    | QOS | MAIN_START | 20230510065500      | SUCCESS | n_attempt=0                                                                | 2023-05-10 07:04:51.000 |\n| IPVPN-SM    | QOS | MAIN_END   | 20230510065500      | FAILED  | com.jkl.bigstreamer.abc.ipvpn.sm.exception.AppEmptyQueryException | 2023-05-10 07:05:02.000 |\n| IPVPN-SM    | QOS | MAIN_START | 20230510065500      | SUCCESS | n_attempt=1                                                                | 2023-05-10 07:05:22.000 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 10"
        }
    },
    "51": {
        "page_content": "| IPVPN-SM    | QOS | MAIN_START | 20230510065500      | SUCCESS | n_attempt=1                                                                | 2023-05-10 07:05:22.000 |\n| IPVPN-SM    | QOS | MAIN_END   | 20230510065500      | SUCCESS |                                                                            | 2023-05-10 07:05:35.000 |\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\n4 rows in set (0.01 sec)\n```\nAccording to the above results, we conclude that the computation of the QOS metrics for the selected 5min interval failed on the first attempt with AppEmptyQueryException error. However the request was successful on the second attempt.\n#### Inspect overall requests' status\nIf we want to check if any of the requests has failed, for instance in the past 24h\n```\nselect application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\"IPVPN-SM\" and component=\"MAIN_END\" and status=\"FAILED\" and system_ts >= now() - INTERVAL 1 DAY;\n```\n##  Pdefntial Error Cases\n### AppEmptyQueryException (Impala Query Returned No Data)\nThis exception occurs when the executed Impala query fails. The underlying reason behind this message can be one the following:\n1. The query did not retrieve any records for the `specified 5min interval`. For category **IF**, this type of failure of a 5min interval may cause the failure of `the following 5min interval` as well. This occurs because an aggregation between two consecutive intervals is performed. \n2. Inability to open one of the HDFS files that constitute a table associated with the particular Impala query.\n3. Unreachable Impala deamon.\n4. Inability to allocate Memory (OutOfMemoryError: unable to create new native thread).\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 11"
        }
    },
    "52": {
        "page_content": "4. Inability to allocate Memory (OutOfMemoryError: unable to create new native thread).\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\nIf the `bigcust` tables are empty, manually adding metrics won't be effective. In such cases, it's crucial to conduct a detailed investigation to pinpoint the specific step within the IPVPN-SLA process where the failure occurred. Such an investigation is documented [here](../procedures/ipvpn_sm_AppEmptyQuery_resolution_MoP.md)\n### SMSystemException (SQM Ingestion Failure)\nThe above exception is solely due to SQM server's inability to ingest the information sent by IPVPN-SM App.\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n### SMValidationException (Schema Mismatch)\nThis occurs in the case when the schema of the data sent does not comply with the agreed schema.\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n### AppQueryIngestionException (Transformation Failure)\nThis can occur due to inability to apply the particular transformation on the retrieved data.\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n### SMAuthException (Token Auth Failure)\nThis occurs if the token used as basic HTTP authentication to the SQM endpoint is rejected.\n**Support Actions:**\n1. [Update keystore with the new credentials](#update-http-auth-token)\n2. [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n## Actions\n### Update HTTP Auth Token\n1. Create keystore\n```\nkeytool -importpass -alias sm_token -storetype JCEKS -validity 365000 -keystore credentials.keystore\n```\n2. Enter <keystore_pw>\n2. Save the given credentials in the format: `username:password`\n#### Instructions to update the file if given new credentials\n1. Update keystore\n`keytool -keypasswd -keystore credentials.keystore -alias sm_token`\n2. Enter <keystore_pw>\n3. Enter new credentials in the format: `username:password`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 12"
        }
    },
    "53": {
        "page_content": "1. Update keystore\n`keytool -keypasswd -keystore credentials.keystore -alias sm_token`\n2. Enter <keystore_pw>\n3. Enter new credentials in the format: `username:password`\n### Manual Retry: sm-replacement-call-repeater.sh {#manual-call}\nIn case we want to bypass the scripts running on un2 we can post a request directly on the IPVPN-SM application, either by performing a curl request specifying the [request body](#request-body) details or by running the script `/shared/abc/ip_vpn/sm-replacement/scripts/sm-replacement-call-repeater.sh` providing the arguments:\n- `category`, one the valid category types {AV,QOS,CPU,MEM,IF}\n- `reference-start-time` in the format yyyyMMddHHmm\n- `reference-end-time` in the format yyyyMMddHHmm (inclusive)\nThe script will successively call the `refresh_*_.sh` and  `compute_metrics_via_sm_app.sh` scripts for the given category and the period specified by the end and start arguments.\n1. In case we want perform the execution for just **one 5m interval**, we provide the same argument for `reference-start-time` and `reference-end-time` fields. E.g. for 5m period of 18/12/2022 10:10:\n```\n./sm-replacement-call-repeater.sh CPU 202212181010 202212181010\n```\n2. If we want to perform requests for **5 multiple consecutive** 5min intervals E.g. for 5m periods of 18/12/2022 10:10 to 10:30 (inclusive):\n```\n./sm-replacement-call-repeater.sh CPU 202212181010 202212181030\n```\nThe logs of this operation are appended to the log files in `/shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ipvpn_sm_replacement.md - Part 13"
        }
    },
    "54": {
        "page_content": "---\ntitle: Streamsets Pipeline Operations\nsystem: BigStreamer\ncomponent: Streamsets\ntool: StreamSets Data Collector\nlocation: un2.bigdata.abc.gr\nuser: sdc\nmonitoring: mysql.jobstatus\nhdfs_paths:\n  - /ez/landingzone/StreamSets/aums\n  - /ez/landingzone/StreamSets/eems\n  - /ez/landingzone/StreamSets/energy_efficiency\n  - /ez/landingzone/StreamSets/nemo\n  - /ez/landingzone/StreamSets/open_weather_map\nlog_path: /shared/sdc/log/sdc.log\nstreamsets_ui: https://un2.bigdata.abc.gr:18636\nhue_ui: https://un-vip.bigdata.abc.gr:8888\nretention_policy: varies_by_pipeline\nschedule: varies_by_pipeline\ndescription: |\n  Streamsets flows automate ingestion of CSV/ZIP files from SFTP sources,\n  transform the data, and load it into Hive and Impala tables partitioned by date.\n  Each pipeline corresponds to a data feed from a remote system such as AUMS, EEMS, Energy Efficiency, Nemo, or OpenWeatherMap.\n  - streamsets\n  - hdfs ingestion\n  - data pipeline\n  - aums\n  - eems\n  - nemo\n  - energy efficiency\n  - sftp to hive\n  - sdc\n  - data engineering\n  - troubleshooting\n  - monitoring\n---\n# Streamsets\n**Utility Node / Server:** `un2.bigdata.abc.gr`  \n**User:** `sdc`  \n**[Password](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)**   \n**Logs:** `/shared/sdc/log/sdc.log`  \n**Log Retention:** `10 days`  \n**Configuration:** `/shared/sdc/configuration/pipelines.properties`  \n**Streamsets:** `https://un2.bigdata.abc.gr:18636`  \n**Hue:** `https://un-vip.bigdata.abc.gr:8888`\n## Streamsets Flows\nStreamsets pipelines automate the ingestion of remote SFTP data files into Hive/Impala. Each flow handles decompression, transformation, and loading based on naming conventions and partitioned timestamps.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 1"
        }
    },
    "55": {
        "page_content": "`Streamsets Flows` are used for getting files from sftp remdef resources, processing them, storing them into HDFS directories and loading the file data into Hive and Impala tables. The tables are partitioned based on the file name which contain a timestamp (e.g. \\*\\_20181121123916.csv -> par_dt='20181121'). \n``` mermaid\n  graph TD\n    C[Remdef Sftp Server]\n    A[SFTP] --> |Transform and Place Files|B[HDFS]\n    B --> |Transform and Run Query|D[Hive]\n    style C fill:#5d6d7e\n```\n### AUMS\n| Pipelines | Status |\n| --------- | ------ |\n| AUMS Data File Feed | Running |\n| AUMS Metadata File Feed | Running |\n#### AUMS Data File Feed\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`   \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/aums/`  \n**SFTP Compressed File:** `aems_data_*.zip` containing `data_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/aums/archive_data` \n**Hive Database:** `aums`  \n**Hive Table Name:** `archive_data`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `AUMS Data File Feed`\n#### AUMS Metadata File Feed\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`    \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/aums/`  \n**SFTP Compressed File:** `aems_data_*.zip` containing `metadata_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/aums/archive_metadata` \n**Hive Database:** `aums`  \n**Hive Table Name:** `archive_metadata`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `AUMS Metadata File Feed`\n### EEMS\n| Pipelines | Status |\n| --------- | ------ |\n| EEMS Data File Feed | Running |\n| EEMS Metadata File Feed | Running |\n#### EEMS Data File Feed\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`   \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/aums_eems/`  \n**SFTP Compressed File:** `aems_data_*.zip` containing `data_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/aums/eems_archive_data/` \n**Hive Database:** `aums`  \n**Hive Table Name:** `eems_archive_data`  \n**Hive Retention:** `2 years`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 2"
        }
    },
    "56": {
        "page_content": "**HDFS Path:** `/ez/landingzone/StreamSets/aums/eems_archive_data/` \n**Hive Database:** `aums`  \n**Hive Table Name:** `eems_archive_data`  \n**Hive Retention:** `2 years`\n**Logs `grep` keyword**: `EEMS Data File Feed`\n#### EEMS Metadata File Feed\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/aums_eems/`  \n**SFTP Compressed File:** `aems_data_*.zip` containing `metadata_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/aums/eems_archive_metadata/` \n**Hive Database:** `aums`  \n**Hive Table Name:** `eems_archive_metadata`  \n**Hive Retention:** `2 years`\n**Logs `grep` keyword**: `EEMS Metadata File Feed`\n### Energy-Efficiency\n| Pipelines | Status |\n| --------- | ------ |\n| energy_efficiency enodeb_auxpiu | Running |\n| energy_efficiency enode_boards | Running |\n| energy_efficiency enodeb_vswr | Running |\n| energy_efficiency nodeb_auxpiu | Running |\n| energy_efficiency nodeb_boards | Running |\n| energy_efficiency nodeb_vswr | Running |\n| energy_efficiency tcu_temperatures | Running | \n| energy_efficiency cells | Running |\n| energy_efficiency Huawei_potp_sdh_hour | _Stopped_ |\n| energy_efficiency Huawei_potp_wdm_hour | _Stopped_ |\n| energy_efficiency baseband FAN TEST | Running |\n| energy_efficiency baseband RET TEST | Running |\n| energy_efficiency baseband SFP TEST | Running |\n| energy_efficiency baseband TEMP SERIAL TEST | Running |\n| energy_efficiency baseband VSWR TEST | Running |\n| energy_efficiency basebandsouth FAN TEST | Running |\n| energy_efficiency basebandsouth RET TEST | Running |\n| energy_efficiency basebandsouth SFP TEST | Running |\n| energy_efficiency basebandsouth TEMP SERIAL TEST | Running |\n| energy_efficiency basebandsouth VSWR TEST | Running |\n#### Energy Efficiency enodeb_auxpiu\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `eNodeB_AuxPIU_*.csv`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 3"
        }
    },
    "57": {
        "page_content": "#### Energy Efficiency enodeb_auxpiu\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `eNodeB_AuxPIU_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/enodeb_auxpiu/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `enodeb_auxpiu`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enodeb_auxpiu`\n#### Energy Efficiency enode_boards\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `eNodeB_boards_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/enodeb_board/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `enodeb_board`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enode_boards`\n#### Energy Efficiency enodeb_vswr\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `eNodeB_VSWR_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/enodeb_vswr` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `enodeb_vswr`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enodeb_vswr`\n#### Energy Efficiency nodeb_auxpiu\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `NodeB_AuxPIU_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/nodeb_auxpiu/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `nodeb_auxpiu`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enodeb_auxpiu`\n#### Energy Efficiency nodeb_boards\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `NodeB_boards_*.csv`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 4"
        }
    },
    "58": {
        "page_content": "#### Energy Efficiency nodeb_boards\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `NodeB_boards_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/nodeb_board/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `nodeb_board`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enode_boards`\n#### Energy Efficiency nodeb_vswr\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `NodeB_VSWR_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/nodeb_vswr/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `nodeb_vswr`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency nodeb_vswr`\n#### Energy Efficiency tcu_temperatures\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `TCU_tempratures_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/tcu_temperatures/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `tcu_temperatures`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency tcu_temperatures`\n#### Energy Efficiency cells\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `NodeB_Cells_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/cell/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `cell`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency cells`\n#### Energy Efficiency Huawei_potp_sdh_hour\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `none`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 5"
        }
    },
    "59": {
        "page_content": "#### Energy Efficiency Huawei_potp_sdh_hour\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `none`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/huawei_potp_sdh_hour/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `huawei_potp_sdh_hour`  \n**Hive Retention:** `none`\n#### Energy Efficiency Huawei_potp_wdm_hour\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `none`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/huawei_potp_wdm_hour/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `huawei_potp_wdm_hour`  \n**Hive Retention:** `none`\n#### Energy Efficiency baseband FAN TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_FAN_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_fan/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_fan`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband FAN TEST`\n#### Energy Efficiency baseband RET TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_RET_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_ret/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_ret`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband RET TEST`\n#### Energy Efficiency baseband SFP TEST \n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_SFP_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_sfp/`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 6"
        }
    },
    "60": {
        "page_content": "**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_SFP_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_sfp/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_sfp`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband SFP TEST`\n#### Energy Efficiency baseband TEMP SERIAL TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_TEMP_SERIAL_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_temp_serial/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_temp_serial`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth TEMP SERIAL TEST`\n#### Energy Efficiency baseband VSWR TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_VSWR_*.csv`\n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_vswr/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_vswr`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth VSWR TEST`\n#### Energy Efficiency basebandsouth FAN TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_FAN_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandsouth_fan/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_fan`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth FAN TEST`\n#### Energy Efficiency basebandsouth RET TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 7"
        }
    },
    "61": {
        "page_content": "#### Energy Efficiency basebandsouth RET TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_RET_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandsouth_ret/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_ret`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband RET TEST`\n#### Energy Efficiency basebandsouth SFP TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_SFP_*.csv`\n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandsouth_sfp/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_sfp`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth SFP TEST`\n#### Energy Efficiency basebandsouth TEMP SERIAL TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_TEMP_SERIAL_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandsouth_temp_serial/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_temp_serial`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband TEMP SERIAL TEST`\n#### Energy Efficiency basebandsouth VSWR TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_VSWR_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/' basebandsouth_vswr'/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_vswr`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth VSWR TEST`\n### Nemo\n| Pipelines | Status |\n| --------- | ------ |",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 8"
        }
    },
    "62": {
        "page_content": "**Hive Table Name:** `basebandsouth_vswr`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth VSWR TEST`\n### Nemo\n| Pipelines | Status |\n| --------- | ------ |\n| Nemo Network Connectivity | Running |\n| Nemo Video | _Stopped_ |\n| Nemo Voice | Running |\n| Nemo Signal Coverage | Running |\n| Nemo Datahttp | Running |\n| Nemo Web | _Stopped_ |\n| Nemo Data Session v2 | Running | \n| Nemo Streaming Session | Running |\n| Nemo Call Session | Running |\n#### Nemo Network Connectivity\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `netcon__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/network_connectivity_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `network_connectivity_details_investigation`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Network Connectivity`\n#### Nemo Video\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `video__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/video_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `video_details_investigation`  \n**Hive Retention:** `60 partitions`\n#### Nemo Voice\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `voice__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/voice_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `voice_details_investigation`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Voice`\n#### Nemo Signal Coverage\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `cov__1_*.gz`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 9"
        }
    },
    "63": {
        "page_content": "**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `cov__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/signal_coverage_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `signal_coverage_details_investigation`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Signal Coverage`\n#### Nemo Datahttp\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `datahttp__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/datahttp_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `datahttp_details_investigation`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Datahttp`\n#### Nemo Web\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `web__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/web_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `web_details_investigation`  \n**Hive Retention:** `60 partitions`\n#### Nemo Data Session v2\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/Handy_Files/`  \n**SFTP File:** `DATA_*.csv`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/data_session/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `data_session`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Data Session v2`\n#### Nemo Streaming Session\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/Handy_Files/`  \n**SFTP File:** `STREAMING_*.csv`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/streaming_session/`\n**Hive Database:** `nemo`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 10"
        }
    },
    "64": {
        "page_content": "**SFTP Path:** `/data/ftp/sftp/uploads/Handy_Files/`  \n**SFTP File:** `STREAMING_*.csv`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/streaming_session/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `streaming_session`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Streaming Session`\n#### Nemo Call Session\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/Handy_Files/`  \n**SFTP File:** `CALL_*.csv`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/call_session/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `call_session`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Call Session`\n### Open Weather Map\n| Pipelines | Status |\n| --------- | ------ |\n| open_weather_map_pipeline | Running |\n#### Open weather map pipelin\n**SFTP User:** `ipvpn`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/shared/vantage_ref-data/REF-DATA/OpenWeatherMap/`  \n**SFTP File:** `OpenWeatherMap_*`\n**HDFS Paths:**\n- `/ez/landingzone/StreamSets/open_weather_map/openweathermap_final/{pardt}/{weather}`  \n- `/ez/landingzone/StreamSets/open_weather_map/openweathermap_forecast/{pardt}/{weather}`\n**Hive Database:** `open_weather_map`  \n**Hive Table Names:**\n- `openweathermap_forecast`  \n- `openweathermap_final`    \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `open_weather_map_pipeline`\n## Monitoring\nMonitoring is done via a MySQL jobstatus database, and pipeline status is also tracked in Streamsets UI. Only successful loads are recorded; failed executions generate alerts via email.\n_Connection Details_\n**Database Type:** `mysql`   \n**Host:** `db-vip.bigdata.abc.gr:3306`  \n**DB Name:** `monitoring`  \n**DB User:** `monitoring`  \n**DB Password:** `https://metis.ghi.com/obss/bigdata/abc/devops/devops-projects/-/blob/master/System_Users/abc_dev.kdbx`  \n**Table:** `jobstatus`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 11"
        }
    },
    "65": {
        "page_content": "**DB User:** `monitoring`  \n**DB Password:** `https://metis.ghi.com/obss/bigdata/abc/devops/devops-projects/-/blob/master/System_Users/abc_dev.kdbx`  \n**Table:** `jobstatus`  \n**Connection command:** `/usr/bin/mysql -u monitoring -p -h db-vip.bigdata.abc.gr:3306 monitoring`\n_General details_\n**Requests:**\n- **Add:** Monitoring `add http requests` for **only** `SUCCESS` status. (FAILED status is not handled)\n- **Email:** If the pipeline `fails` to execute at any stage, an email alert is sent through the Streamsets UI.  \n**operativePartition:** is created from the filename `*_YYYYMMDD\\*.csv`\n### EEMS\n#### EEMS Data File Feed\n##### Components\n| Component | Status | Description |\n| ------ | ------ | ------ |\n| SFTP_HDFS | SUCCESS | Sftp get the raw files from the remdef server, process them and put the parsing files into HDFS landingzone |\n| MAIN | SUCCESS | Indicates the status of the whole load |\nFor `FAILED` components an `email` is sent through `Streamsets UI`.\n##### Records\nFor each execution the following set of messages will be recorded in the Monitoring database.\n| id | execution_id | application | job | component | operative_partition | status | system_ts | \n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n| 813749 | 7db009eb-e2b7-4379-8c00-393ac732b66e | EEMS | EEMS_DATA_FILE_FEED | SFTP_HDFS | 20230104 | SUCCESS | 2023-01-05T01:20:23.000Z |\n| 813750 | 7db009eb-e2b7-4379-8c00-393ac732b66e | EEMS | EEMS_DATA_FILE_FEED | MAIN | 20230104 | SUCCESS | 2023-01-05T01:20:28.000Z |\n##### Database Queries\n###### MySQL: List details of the last load\n```\nselect \nexecution_id, id, application, job, component, operative_partition,  \nstatus, system_ts, system_ts_end, message   \nfrom jobstatus a where upper(job) like 'EEMS_DATA_FILE_FEED%'   \nand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'EEMS_DATA_FILE_FEED%');\n```\n###### Application: List details of specific load\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 12"
        }
    },
    "66": {
        "page_content": "and execution_id=(select max(execution_id) from jobstatus where upper(job) like 'EEMS_DATA_FILE_FEED%');\n```\n###### Application: List details of specific load\n```\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=EEMS&job=EEMS_DATA_FILE_FEED&component=SFTP_HDFS&operativePartition=20230104'\n```\n#### EEMS Metadata File Feed\n##### Components\n| Component | Status | Description |\n| ------ | ------ | ------ |\n| SFTP_HDFS | SUCCESS | Sftp get the raw files from the remdef server, process them and put the parsing files into HDFS landingzone |\n| MAIN | SUCCESS | Indicates the status of the whole load |\nFor `FAILED` components an `email` is sent through `Streamsets UI`.\n##### Records\nFor each pipeline execution the following set of messages will be recorded in the Monitoring database.\n| id | execution_id | application | job | component | operative_partition | status | system_ts | \n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n| 808931 | 7bfb8fda-1573-46d0-be7f-a9f297538042 | EEMS | EEMS_METADATA_FILE_FEED | SFTP_HDFS | 20230104 | SUCCESS | 2023-01-04T17:28:03.000Z |\n| 808932 | 7bfb8fda-1573-46d0-be7f-a9f297538042 | EEMS | EEMS_METADATA_FILE_FEED | MAIN | 20230104 | SUCCESS | 2023-01-04T17:28:07.000Z |\n##### Database Queries\n###### MySQL: List details of the last load\n```\nselect \nexecution_id, id, application, job, component, operative_partition,  \nstatus, system_ts, system_ts_end, message   \nfrom jobstatus a where upper(job) like 'EEMS_METADATA_FILE_FEED%'   \nand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'EEMS_METADATA_FILE_FEED%');\n```\n###### Application: List details of specific load\n```\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=EEMS&job=EEMS_METADATA_FILE_FEED&component=SFTP_HDFS&operativePartition=20230104'\n```\n## Troubleshooting",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 13"
        }
    },
    "67": {
        "page_content": "```\n## Troubleshooting\nIf a pipeline fails, this section provides step-by-step instructions to diagnose the issue using Streamsets UI, system logs, and Hive partition checks.\nFollowing troubleshooting steps apply to all pipelines.\n**Step 1:** Log in to [Stremsets](https://un2.bigdata.abc.gr:18636/) with `sdc` user.\n1. Check that the specific pipeline has the status `RUNNING`. If it has any other status continue the investigation.   \n1. Open a pipeline. Check the generic summary (click anywhere in the board and select summary) to get an overview through dashboards on the data processed and errors. Also check if any job of the pipeline has errors (click on an action box and select tab Errors). Streamset will provide a specific ERROR message.\n**Step 2:** Look at the `logs`.\n**TO VIEW THE LOG FILE FOR A SPECIFIC PIPELINE:** `cat sdc.log | grep -i '<pipeline-grep-keyword>'`\n1. Through the `Stramsets UI`, `logs` can be viewed by pressing the paper icon (second) on the top right corner of the selected pipeline environment.\n1. Open `logs` and apply some filters in order to retrieve the information related to the specific pipeline. Logs can be found at `/shared/sdc/log/sdc.log`. To search logs use `grep` or open log file with `less sdc.log` and search `'/'`.\n    1. Grep for `Started reading file` to see when a new file is parsed successfully:\n        ```\n        cat sdc.log | grep -i 'Started reading file'\n        ```\n        > 2022-03-22 14:00:03,419 [user:\\*sdc] [pipeline:energy_efficiency basebandsouth RET TEST/energyeffd112ecef-f20d-45ff-bef4-b88f2117e3d7] [runner:] [thread:ProductionPipelineRunnable-energyeffd112ecef-f20d-45ff-bef4-b88f2117e3d7-energy_efficiency basebandsouth RET TEST] INFO  RemdefDownloadSource - **Started reading file**: /`basebandsouth_RET_20220322-092713.csv`\n    1. Grep `Error while attempting to parse file` for error while parsing files:\n        ```\n        cat sdc.log | grep -i 'Error while attempting to parse file'\n        ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 14"
        }
    },
    "68": {
        "page_content": "1. Grep `Error while attempting to parse file` for error while parsing files:\n        ```\n        cat sdc.log | grep -i 'Error while attempting to parse file'\n        ```\n        > ERROR RemdefDownloadSource - **Error while attempting to parse file**: /`baseband_TEMP_SERIAL_20220322-081534.csv`\n        > java.io.IOException: (line 3331) invalid char between encapsulated token and delimiter\n    1. Grep `A JVM error occurred while running the pipeline` for JVM errors:\n        ```\n        cat sdc.log | grep -i 'A JVM error occurred while running the pipeline'\n        ```\n        > ERROR ProductionPipelineRunnable - A JVM error occurred while running the pipeline, java.lang.OutOfMemoryError: Java heap     space java.lang.OutOfMemoryError: Java heap space\n    1. Grep `ERROR` for any errors that might occur.\n    1. Following `WARN` message with exceptions does not affect the insertion of data:\n        ```\n        2022-03-31 00:20:14,058 [user:*sdc] [pipeline:AUMS Metadata File Feed/AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2] [runner:0] [thread:ProductionPipelineRunnable-AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2-AUMS Metadata File Feed] WARN  UserGroupInformation - PriviledgedActionException as:sdc/un2.bigdata.abc.gr@CNE.abc.GR (auth:KERBEROS) cause:java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed)\n        2022-03-31 00:20:14,058 [user:*sdc] [pipeline:AUMS Metadata File Feed/AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2] [runner:0] [thread:ProductionPipelineRunnable-AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2-AUMS Metadata File Feed] INFO  HiveConfigBean - Connection to Hive become stale, reconnecting.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 15"
        }
    },
    "69": {
        "page_content": "2022-03-31 00:20:14,058 [user:*sdc] [pipeline:AUMS Metadata File Feed/AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2] [runner:0] [thread:ProductionPipelineRunnable-AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2-AUMS Metadata File Feed] WARN  TIOStreamTransport - Error closing output stream.\n        java.net.SocketException: Socket is closed\n        ......\n        2022-03-31 00:20:14,059 [user:*sdc] [pipeline:AUMS Metadata File Feed/AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2] [runner:0] [thread:ProductionPipelineRunnable-AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2-AUMS Metadata File Feed] INFO  HiveConfigBean - Error closing stale connection Error while cleaning up the server resources\n        java.sql.SQLException: Error while cleaning up the server resources\n        ......\n        Caused by: org.apache.thrift.transport.TTransportException: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe (Write failed)\n        ......\n        Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe (Write failed)\n        ......\n        Caused by: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe (Write failed)\n        ......\n        Caused by: java.net.SocketException: Broken pipe (Write failed)\n        ......\n        ```\n**Step 3:** Log in to [Hue](https://un-vip.bigdata.abc.gr:8888/hue/editor/?type=impala) with `intra` user, and check the status of loaded partitions of the tables which correspond to the pipeline.\n1. Run query `show partitions <database>.<table>`. \n1. If the table has no partitions or no stats you can use following query to check the partitions under investigation:  \n`select count(*), par_dt from <database>.<table> where par_dt > '<partition>' group by par_dt order by par_dt desc;`\n  - Ndef: Execute `REFRESH <table_name>` if Hive and Impala tables have different data.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 16"
        }
    },
    "70": {
        "page_content": "- Ndef: Execute `REFRESH <table_name>` if Hive and Impala tables have different data.\n**Step 4:** Check if there are files in sftp remdef directory, which haven't been processed and loaded into hive and impala tables. This is accomplished through comparing the file in the remdef directory and the partitions found in the hdfs directory.\n1. Access and view SFTP files in remdef directory\n    1. Login to `un2` and change to `sdc` user. \n    1. From there execute `sftp <sftp-user>@<sftp-server>:<sftp-path>`.\n    1. Run `ls -ltr` to view the latest files in the remdef directory.\n    1. Check that the files have the correct credential permissions and rights, `sftp user` and at least `-rw-r-----` permission. \n1. Access and view partitions in hdfs directory\n    1. Login to `un2` and change to `sdc` user.\n    1. From there execute `hdfs dfs -ls <hdfs-path>`.\n    1. Make sure partitions are created in the correct hdfs path.\nFinally, check for each file if there is an equivalent partition created. The partition's format is `YYYYMMDD` and it derives from the file name.\n**Step 5:** If the errors has been resolved and the pipeline status is (`EDITED` or `STOPPED`), start the pipeline and wait to see if the errors have been indeed fixed and no other errors have occurred due to the latest changes.\n---\n---\n### Common Problems and Ways to Fix them\nThere are some issues that occur quite often while using `Streamsets` flows. If any of the issues mentioned below happen, firstly follow the general troubleshooting steps to identify the problem that occurred and then follow the steps which correspond to fixing the issue. \n---\n#### Check file(s) has been loaded correctly\nLogin to `un2.bigdata.abc.gr` and change to `sdc` user.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 17"
        }
    },
    "71": {
        "page_content": "---\n#### Check file(s) has been loaded correctly\nLogin to `un2.bigdata.abc.gr` and change to `sdc` user. \n**Step 1:** Execute `sftp <sftp-user>@<sftp-server>:<sftp-path>`, run `ls -ltr` to view the latest files in the remdef directory and check that the files have the correct credential permissions and rights. The user must be the `sftp user` and permissions be at least `-rw-r----- `.\nExample:\n```\n-rw-r-----    1 nbi      nbi         87987 Jan  4 11:57 STREAMING_W52_2022.csv\n-rw-r-----    1 nbi      nbi       1795960 Jan  4 11:57 DATA_SESSIONS_W52_2022.csv\n-rw-r-----    1 nbi      nbi        284724 Dec 22 11:56 CALL_SESSIONS_W50_2022.csv\n```\n**Step 2:** Get the file(s) with \"YYYYMMDD-HHMMss\" as the datetime of the file you want to check its integrity from the sftp remdef directory by running `get <filaname> <local-path>`. The file will be copied in `un2` at the location `<local-path>`. A usefull local path is `/tmp/streamsets`. If does not exist just create it `mkdir /tmp/streamsets`\n**Step 3:** Execute `wc -l <filename>` to count the number of lines the file has.\n**Step 4:** Compare the number of lines with the equivalent value of records in the corresponding partition by running (in [Hue](https://999.999.999.999:8888/hue/editor/?type=impala) or with secimp) \n```\nselect count(*), par_dt from <database>.<table> where par_dt='<partition>' group by par_dt;\n```\nCommand `refresh tables` might be needed.\n**Step 5:** If the `returned result of records of the impala query` is equal to the `returned result of 'wc -l \\*\\_YYYYMMDD-HHMMss.csv' -1`, then the flow was executed correctly for the csv file for the examined date.\n**Step 6:** Clear the local directory from the unnecessary fetched data.\n---\n#### Manually inserting missing data in Hive and Impala\n---\nTo manually insert missing data in Hive and Impala there are two ways.\nA. Manually get and put the files with the missing data from the Streamsets pipeline remdef directory **(Suggested Way)**\n  For each file:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 18"
        }
    },
    "72": {
        "page_content": "A. Manually get and put the files with the missing data from the Streamsets pipeline remdef directory **(Suggested Way)**\n  For each file:\n  1. From `un2.bigdata.abc.gr` with user `sdc` execute `sftp <sftp-user>@<sftp-server>:<sftp-path>`.\n  1. From the sftp remdef directory, fetch locally the missing data by running `get <filename>.csv/zip <local-path>`. The file will be copied in `un2.bigdata.abc.gr` at `<local-path>`. A usefull local path is `/tmp/streamsets`. If does not exist just create it `mkdir /tmp/streamsets`\n  1. From the sftp remdef directory, put the recently fetched data in the directory again with a different name by executing `put <local-path>/<filename>.csv/zip <filename>_tmp.csv/zip`. This will result with the new file having a different name and timestamp (last modified) so the pipeline can see it and process it. \n  1. When the Streamset pipeline has finished processing the data, remove the `<filename>_tmp.csv/zip` file from the remdef sftp directory with the sftp command `rm <filename>_tmp.csv/zip`. `!IMPORTANT`\n  1. Clear the local directory from the unnecessary fetched data.\nB. Configure the offset of the Streamset\n  1. Select the wanted pipeline and `Stop` it\n  1. Select from the top right toolbar of the pipeline the `...` option and press `Reset Origin`\n  1. Select the component `SFTP FTP Client 1`\n  1. Go to Configuration panel and select the `SFTP/FTP/FTPS` and take the value found in the `File Name Pattern` field\n  1. Change the property `File Name Pattern` with the exact file name you want the stream to start processing. This sets the `offset` to the name of the file you set in the above field.\n  1. `Wait` for the file to be processed\n  1. `Stop` the pipeline again\n  1. `Reset` the `File Name Pattern` to its previous value\n  1. `Start` the pipeline",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 19"
        }
    },
    "73": {
        "page_content": "1. `Wait` for the file to be processed\n  1. `Stop` the pipeline again\n  1. `Reset` the `File Name Pattern` to its previous value\n  1. `Start` the pipeline  \n  This will make the Streamset pipeline to process only the specific files. For more information about offset and origin press [here](https://metis.ghi.com/obss/bigdata/documentation/-/wikis/dev/frameworks/streamsets#offset-through-streamset-gui).\n---\n#### Manually correct faulty data in Hive and Impala\n---\n**Step 1:** Check the faulty partitions by following the procedure found [here](#check-files-has-been-loaded-correctly).\n**Step 2:** In `Hue` as `intra`, delete existing wrong partitions that overlap with the required interval from kudu table and/or from impala table. \n  - If it is in kudu (10 most recent days are in kudu), do: `ALTER table <database>.<table> DROP IF EXISTS RANGE PARTITION 'v1'<= values < 'v2';`, where v1 and v2 the range of partitions. \n  - If it is in impala, do: `ALTER table <database>.<table> DROP IF EXISTS PARTITION (par_dt='v1');`, where v1 the wanted partition.\n**Step 3:** Follow the instruction [here](#manually-inserting-data-missing-in-hive-and-impala) to load the recently deleted data.\n### Exceptions and Possible Root Causes\n1.  `CONTAINER_0001 - net.schmizz.sshj.connection.ConnectionException: Stream closed`  \n    - SFTP Server side issue which results to missing data.\n1. `CONTAINER_0001 - net.schmizz.sshj.sftp.SFTPException: Permission denied`  \n    - Files are put in sftp directory with wrong user and file permissions and later changed to the correct ones\n    - Password and user were changed at the SFTP server but not updated in streamsets\n    - SFTP Server side issue\n1. `A JVM error occurred while running the pipeline, java.lang.OutOfMemoryError: Java heap space`\n    - SFTP Server read file issue. Logs will have \"Broken transport; encoutered EOF\" errors. This could happen as a result of issues with SFTP Server which causes Java heap space errors.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 20"
        }
    },
    "74": {
        "page_content": "- SFTP Server read file issue. Logs will have \"Broken transport; encoutered EOF\" errors. This could happen as a result of issues with SFTP Server which causes Java heap space errors.\n1. `TTransportException: java.net.SocketException: Connection closed by remdef host`\n```\n2023-01-12 11:50:21,208 [user:*sdc] [pipeline:EEMS Data File Feed/EEMSData7adbe2c9-4c70-425b-a475-fc766cd02ada] [runner:0] [thread:ProductionPipelineRunnable-EEMSData7adbe2c9-4c70-425b-a475-fc766cd02ada-EEMS Data File Feed] INFO\u00a0 HiveConfigBean - Error closing stale connection Error while cleaning up the server resources\njava.sql.SQLException: Error while cleaning up the server resources\n...\nCaused by: org.apache.thrift.transport.TTransportException: java.net.SocketException: Connection closed by remdef host\n2023-01-15 15:00:09,403 [user:sdc] [pipeline:energy_efficiency baseband VSWR TEST/energyeffa1e8ea2e-6e09-4529-889e-740157783fd8] [runner:0] [thread:ProductionPipelineRunnable-energyeffa1e8ea2e-6e09-4529-889e-740157783fd8-energy_efficienc\ny baseband VSWR TEST] WARN\u00a0 UserGroupInformation - PriviledgedActionException as:sdc/un2.bigdata.abc.gr@CNE.abc.GR (auth:KERBEROS) cause:java.sql.SQLException: [Simba][ImpalaJDBCDriver](500593) Communication link failure. Failed\nto connect to server. Reason: java.net.SocketException: Broken pipe (Write failed).\n2023-01-15 15:00:09,403 [user:sdc] [pipeline:energy_efficiency baseband VSWR TEST/energyeffa1e8ea2e-6e09-4529-889e-740157783fd8] [runner:0] [thread:ProductionPipelineRunnable-energyeffa1e8ea2e-6e09-4529-889e-740157783fd8-energy_efficienc\ny baseband VSWR TEST] INFO\u00a0 HiveConfigBean - Connection to Hive become stale, reconnecting.\n2023-01-15 15:00:09,408 [user:sdc] [pipeline:energy_efficiency baseband VSWR TEST/energyeffa1e8ea2e-6e09-4529-889e-740157783fd8] [runner:0] [thread:ProductionPipelineRunnable-energyeffa1e8ea2e-6e09-4529-889e-740157783fd8-energy_efficienc",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 21"
        }
    },
    "75": {
        "page_content": "y baseband VSWR TEST] INFO\u00a0 HiveConfigBean - Error closing stale connection [Simba][JDBC](10060) Connection has been closed.\njava.sql.SQLNonTransientConnectionException: [Simba][JDBC](10060) Connection has been closed.\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "streamsets.md - Part 22"
        }
    },
    "76": {
        "page_content": "---\ntitle: CSI-Redis Flow \u2013 Daily Export of CSI Metrics to Redis\ndescription: Spark-based ETL pipeline for collecting, aggregating, and exporting CSI metrics from HDFS to Redis via Oozie, with daily scheduling, monitoring integration, and Redis SFTP delivery and load execution.\njob_name: CSI_REDIS\ncomponent: MAIN\nsystem: BigStreamer\nhost: un-vip.bigdata.abc.gr\ntarget_system: Redis\ntarget_vm: 999.999.999.999\ntarget_port: 2223\ntarget_script: /home/bigstreamer/bin/102.CSI_Redis_Load_Data.sh\ncoordinator: Redis-CSI_Coordinator\nworkflow: Redis-CSI_Workflow\nowner: rediscsi\nexecution_schedule: 20:00 UTC daily\ndata_source_paths:\n  - /ez/warehouse/npce.db/yak_cells/*\n  - /ez/warehouse/csi.db/csi_cell_dashboard_primary_dly/*\n  - /ez/warehouse/csi.db/csi_cell_daily_v3/*\nexport_hdfs_path: /user/rediscsi/docx-data/csi/parquet/\nlogs_hdfs_path: /user/rediscsi/log\nmonitoring_db: monitoring\nmonitoring_host: 999.999.999.999\nmonitoring_table: jobstatus\nspark_jobs:\n  - AggregateRdCells\n  - AggregateCsiPrimary\n  - CSIAveragePerCellId\n  - AverageCsi\n  - PLMNCsiCellDistri\n  - TopWorstCsiCellTableAndMap\n  - CSIPerLocTimeCharts\n  - TopWorstDeltaCsiCellTableAndMap\nload_type: daily\ndelivery_target: Redis\nlast_updated: 2025-05-01\nkeywords:\n  - bigstreamer\n  - redis\n  - csi\n  - spark\n  - parquet\n  - metrics\n  - oozie\n  - hdfs\n  - sftp\n  - ssh\n  - jobstatus\n  - monitoring\n  - kafka\n  - impala\n  - hive\n  - beeline\n  - aggregation\n  - json configs\n  - top worst csi\n  - network metrics\n  - data delivery\n  - telecom\n  - plmn\n  - dashboards\n  - error tracing\n  - logs\n  - execution_id\n  - cxi-etl\n  - yaml\n  - tar.gz\n---\n# CSI-Redis Flow\n## Installation info\nSetup configuration for the CSI-Redis Flow, including input data paths, working directories, job scheduling, and tools.\n### Data Source\nDetails about the source data files, their HDFS locations, and relevant working directories used in the pipeline.\n- Source system: HDFS  \n  - user : `rediscsi`\n  - Parquet files:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 1"
        }
    },
    "77": {
        "page_content": "### Data Source\nDetails about the source data files, their HDFS locations, and relevant working directories used in the pipeline.\n- Source system: HDFS  \n  - user : `rediscsi`\n  - Parquet files:  \n\t\t- `/ez/warehouse/npce.db/yak_cells/*`  \n\t\t- `/ez/warehouse/csi.db/csi_cell_dashboard_primary_dly/*`  \n\t\t- `/ez/warehouse/csi.db/csi_cell_daily_v3/*`\n- Local FileSystem Directories\n  - user : `rediscsi`\n\t- exec node : defined by Oozie\n\t- work dir : defined by Oozie\n\t- export dir: `/csiRedis_exp_data`\n- HDFS Directories\n\t- Export dir : `/user/rediscsi/docx-data/csi/parquet/`\n\t- Status dir : `/user/rediscsi/docx-data/metatdata/checkpoints`\n#### Scripts-Configuration Location\nPaths for locating scripts and configuration files used in the CSI-Redis flow.\n- node : `HDFS`\n- user : `rediscsi`\n- scripts path : `hdfs:/user/rediscsi`\n-\tconfigurations path : `hdfs:/user/rediscsi`\n#### Logs Location\nInformation about where log files are stored and how they are named for each flow execution.\n- node : `HDFS`\n- user : `rediscsi`\n- path : `/user/rediscsi/log`\n- log file: `csiRedis.<partition data>.<execution ID>.tar.gz`  \n\t*i.e. `csiRedis.20230420.20230420_230010.tar.gz`*\n#### Oozie Scheduling\nDetails about the Oozie coordinator, workflow, script execution, and schedule for the CSI-Redis flow.\n- user : rediscsi\n- Coordinator :`Redis-CSI_Coordinator`  \n- Workflow : `Redis-CSI_Workflow`  \n- Shell : `/user/rediscsi/100.CSI_Main.sh`\n- runs at : `20:00 UTC Daily`\n#### Database CLI commands\nCommand-line tools for accessing Hive, Impala, and MySQL used in various validation and debugging steps.\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/def_network_map;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr -d def_network_map --ssl -k`\n- MySql*: `mysql -u monitoring -p -h 999.999.999.999 monitoring`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 2"
        }
    },
    "78": {
        "page_content": "- Impala: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr -d def_network_map --ssl -k`\n- MySql*: `mysql -u monitoring -p -h 999.999.999.999 monitoring`\n*\\*The password for the MySql database can be found [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)*\n### Data target\nRedis target information including VM details and script used to load processed data into Redis.\n- Redis VM:`999.999.999.999`\n- Port Forward:`un-vip.bigdata.abc.gr:2223`\n- user: `bigstreamer`\n- scripts path: `/home/bigstreamer/bin`\n-\tLoad Script: `102.CSI_Redis_Load_Data.sh`\n## Data process\nStep-by-step breakdown of the data processing flow from HDFS extraction to Redis loading, including Spark job execution and file management.\n### Set HDFS Export Path\nReplaces placeholders in configuration files with the actual export path for the current execution.\nDefines the export path in HDFS and updates the json configuration files.  \nReplaces the key-word `HDFS_PATH_YYYYMMDD` with the `/user/rediscsi/docx-data/csi/parquet/<execution ID>`  \ni.e. `/user/rediscsi/docx-data/csi/parquet/20230401_102030`  \n### Data Preparation\nExecutes Spark jobs that generate the intermediate data needed for aggregation.\nExecute the Data preparation Spark jobs\n- AggregateRdCells  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.aggregator.AggregateRdCells cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./aggregate_rd_cells_full.json`\n- AggregateCsiPrimary  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.aggregator.AggregateCsiPrimary cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar aggregate_csi_primary_inc.json`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 3"
        }
    },
    "79": {
        "page_content": "**IMPORTANT: if any of the above Spark jobs fails then the procedure stops.**\n### Data Aggregation\nExecutes Spark jobs for metric calculations, averages, and top/worst CSI indicators.\nExecute the Data Aggregation Spark jobs\n- CSIAveragePerCellId  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.csiarea.CSIAveragePerCellId ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./csi_average_per_cell_id_metrics_predef_all.json`\n- AverageCsi  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.avgcsi.AverageCsi ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./avg_csi_metrics_predef_all.json`\n- PLMNCsiCellDistri  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.plmncsicelldistribution.PLMNCsiCellDistri ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./plmn_csi_cell_distri_metrics_predef_all.json`\n- TopWorstCsiCellTableAndMap  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.topworstcsi.TopWorstCsiCellTableAndMap ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./top_worst_csi_metrics_predef_all.json`\n- CSIPerLocTimeCharts",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 4"
        }
    },
    "80": {
        "page_content": "- CSIPerLocTimeCharts  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.csibyloc.CSIPerLocTimeChartsToMongo ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./avg_csi_by_loc_metrics_predef_all_mongo.json`\n- TopWorstDeltaCsiCellTableAndMap  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.topworstdeltacsi.TopWorstDeltaCsiCellTableAndMap ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./top_worst_delta_csi_metrics_inc.json`\n**Ndef: The Spark jobs above create the export file in HDFS under `/user/rediscsi/docx-data/csi/parquet/<execution ID>`**\n### Get export files from HDFS\nRetrieves the processed export files from HDFS to local storage.\nCopy the export files from HDFS to the slave node's local filesystem  \nThe working slave node is defined by Oozie\n`hdfs dfs -get /user/rediscsi/docx-data/csi/parquet/<execution ID>/* ./csiRedis_exp_data/<execution ID>`  \ni.e. `hdfs dfs -get /user/rediscsi/docx-data/csi/parquet/20230401_102030/* ./csiRedis_exp_data/20230401_102030`\n### Archive export files\nArchives export files into a single `.tar.gz` for transfer.\ncreates a compressed tar file which contains all the log files\n`tar cvfz ./csiRedis_exp_data/<execution ID>/redisCSI.<execution ID>.tar.gz ./csiRedis_exp_data/<execution ID>`  \ni.e. `tar cvfz ./csiRedis_exp_data/<execution ID>/redisCSI.20230401_102030.tar.gz ./csiRedis_exp_data/20230401_102030`\n### Transfer Archived file to Redis VM\nTransfers the archived export data to the Redis VM using SFTP.\nTranfers the Archived file to Redis VM using `SFTP PUT`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 5"
        }
    },
    "81": {
        "page_content": "### Transfer Archived file to Redis VM\nTransfers the archived export data to the Redis VM using SFTP.\nTranfers the Archived file to Redis VM using `SFTP PUT`  \n`echo \"put ./csiRedis_exp_data/<execution ID>/redisCSI.<execution ID>.tar.gz <redis_LZ>\" | sftp -o \"StrictHostKeyChecking no\" -i ./id_rsa -P$<redis_Port> $<redis_User>@$<redis_Node>`  \ni.e. `echo \"put ./csiRedis_exp_data/20230401_102030/redisCSI.20230401_102030.tar.gz ./CSI_LZ\" | sftp -o \"StrictHostKeyChecking no\" -i ./id_rsa -P2223 bigstreamer@un-vip.bigdata.abc.gr`\n### Load Data to Redis DB\nRuns the load script on the Redis VM to ingest the data into the Redis database.\nExtracts the parquet files from the Archived file and load them into the Redis database  \nExecute the load script `Redis VM:/home/bigstreamer/bin/102.CSI_Redis_Load_Data.sh` remdefly.\n`ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa -P2223 bigstreamer@un-vip.bigdata.abc.gr \"/home/bigstreamer/bin/102.CSI_Redis_Load_Data.sh`\n## Monitoring\nDetails on monitoring connections, execution message logs, and tracked job components.\n### Monitoring connection details\nDatabase credentials and paths for querying execution logs.\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \n### Monitoring Message list\nExample log messages that indicate successful execution of each flow component.\nFor each load the following set of messages will be recorded in the Monitoring database.\n```sql\n+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+\n| execution_id    | component                       | job              | operative_partition | status  | system_ts               |\n+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 6"
        }
    },
    "82": {
        "page_content": "+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+\n| 20230420_230010 | MAIN_START                      | JOB_BEGIN        | 20230420            | SUCCESS | 2023-04-20 23:00:10.000 |\n| 20230420_230010 | UPDATE_HDFS_EXPORT_PATH         | PRE_TASK         | 20230420            | SUCCESS | 2023-04-20 23:00:10.000 |\n| 20230420_230010 | AGGREGATERDCELLS                | DATA_PREPARATION | 20230420            | SUCCESS | 2023-04-20 23:02:14.000 |\n| 20230420_230010 | AGGREGATECSIPRIMARY             | DATA_PREPARATION | 20230420            | SUCCESS | 2023-04-20 23:06:30.000 |\n| 20230420_230010 | COREKPIANDCSIBYLEVEL            | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:09:29.000 |\n| 20230420_230010 | CSIAVERAGEPERCELLID             | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:12:14.000 |\n| 20230420_230010 | AVERAGECSI                      | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:15:49.000 |\n| 20230420_230010 | PLMNCSICELLDISTRI               | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:17:17.000 |\n| 20230420_230010 | TOPWORSTCSICELLTABLEANDMAP      | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:24:42.000 |\n| 20230420_230010 | CSIPERLOCTIMECHARTSTOMONGO      | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:27:58.000 |\n| 20230420_230010 | TOPWORSTDELTACSICELLTABLEANDMAP | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:29:34.000 |\n| 20230420_230010 | GET_EXP_FILES_FROM_HDFS         | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:29:55.000 |\n| 20230420_230010 | TAR_EXP_FILES                   | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:30:09.000 |\n| 20230420_230010 | SFTP_PUT_EXP_FILE_TO_REDIS      | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:30:13.000 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 7"
        }
    },
    "83": {
        "page_content": "| 20230420_230010 | SFTP_PUT_EXP_FILE_TO_REDIS      | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:30:13.000 |\n| 20230420_230010 | LOAD_DATA_TO_REDIS_DB           | LOAD_REDIS       | 20230420            | SUCCESS | 2023-04-20 23:33:52.000 |\n| 20230420_230010 | MAIN_END                        | JOB_END          | 20230420            | SUCCESS | 2023-04-20 23:36:01.000 |\n+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+\n```\n### Monitoring Component list\nDescriptions of job components recorded in the monitoring logs and what each component does.\n```sql\n+---------------------------------+------------------+--------------------------------------------------------------------------------------------------\n| Component                       | Job              | Description\n+---------------------------------+------------------+--------------------------------------------------------------------------------------------------\n| MAIN_START                      | JOB_BEGIN        | Procedure Started\n| UPDATE_HDFS_EXPORT_PATH         | PRE_TASK         | Set the HDFS path in Json Configuration files\n| AGGREGATERDCELLS                | DATA_PREPARATION | Data preparation: `spark-submit` using `aggregate_rd_cells_full.json` config file\n| AGGREGATECSIPRIMARY             | DATA_PREPARATION | Data preparation: `spark-submit` using `aggregate_csi_primary_inc.json` config file\n| COREKPIANDCSIBYLEVEL            | DATA_AGGREGATION | Data aggregation: `spark-submit` using `core_kpi_and_csi_by_level_metrics_predef_all` config file\n| CSIAVERAGEPERCELLID             | DATA_AGGREGATION | Data aggregation: `spark-submit` using `csi_average_per_cell_id_metrics_predef_all` config file\n| AVERAGECSI                      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `avg_csi_metrics_predef_all` config file",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 8"
        }
    },
    "84": {
        "page_content": "| AVERAGECSI                      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `avg_csi_metrics_predef_all` config file\n| PLMNCSICELLDISTRI               | DATA_AGGREGATION | Data aggregation: `spark-submit` using `plmn_csi_cell_distri_metrics_predef_all` config file\n| TOPWORSTCSICELLTABLEANDMAP      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `top_worst_csi_metrics_predef_all` config file\n| CSIPERLOCTIMECHARTSTOMONGO      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `avg_csi_by_loc_metrics_predef_all_mongo` config file\n| TOPWORSTDELTACSICELLTABLEANDMAP | DATA_AGGREGATION | Data aggregation: `spark-submit` using `top_worst_delta_csi_metrics_inc` config file\n| GET_EXP_FILES_FROM_HDFS         | POST_TASK        | hdfs copyToLocal the export files\n| TAR_EXP_FILES                   | POST_TASK        | Archive the export files\n| SFTP_PUT_EXP_FILE_TO_REDIS      | POST_TASK        | Tranfers the Archived file to Redis VM\n| LOAD_DATA_TO_REDIS_DB           | LOAD_REDIS       | Upload the Archived file into Redis database \n| MAIN_END                        | JOB_END          | Procedure Completed\n+---------------------------------+------------------+--------------------------------------------------------------------------------------------------\n```\n### Monitoring database Queries\nMySQL queries to retrieve messages from the most recent CSI-Redis execution.\n- List messages of the last load  \n`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n```sql\n    select \n      execution_id, component, job, operative_partition,  \n      status, system_ts, substr(message,1,50) msg\n    from jobstatus a where 1=1\n    and upper(application)='CSI'\n    and execution_id in (select max(execution_id) from jobstatus where upper(application)='CSI' and upper(job)='DATA_PREPARATION')\n    order by a.id\n    ;\n```\n### Monitoring Health-Check\nHow to check the status of the monitoring application and restart it if needed.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 9"
        }
    },
    "85": {
        "page_content": "order by a.id\n    ;\n```\n### Monitoring Health-Check\nHow to check the status of the monitoring application and restart it if needed.\n- Check Monitoring status.  \n```\tbash\n$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\t\n{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n```  \n- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\nHow to respond to errors or failed jobs, including checking logs and identifying root causes.\nAn email will be sent by the system with the point of failure.  \ni.e.\n<pre>\nFrom: abc_bigd@abc.gr  \nSubject: CSI - DATA_AGGREGATION: FAILED  \n<b>\nData preparation:top_worst_delta_csi_metrics_inc\nExec_id:20230401_102030\n</b>\nThis is an automated e-mail.  \nPlease do not reply.  \n</pre>\n**Actions**  \n1. Write down the value of `Exec_id` described in the alert email  \n\ti.e. Exec_id:`1673849411`\n2. Log files are stored in HDFS in archived files.  \n```bash\n$ hdfs dfs -ls /user/rediscsi/log/\n-rw-r--r--   3 rediscsi rediscsi  366865842 2023-04-20 23:35 /user/rediscsi/log/csiRedis.20230420.20230420_230010.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  361801963 2023-04-21 23:38 /user/rediscsi/log/csiRedis.20230421.20230421_230010.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  358913487 2023-04-22 23:41 /user/rediscsi/log/csiRedis.20230422.20230422_230013.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  364564867 2023-04-23 23:42 /user/rediscsi/log/csiRedis.20230423.20230423_230010.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  359603322 2023-04-24 23:38 /user/rediscsi/log/csiRedis.20230424.20230424_230009.tar.gz\n```\nEach archived file contains a set of logs related to the specific flow run (execution ID).",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 10"
        }
    },
    "86": {
        "page_content": "```\nEach archived file contains a set of logs related to the specific flow run (execution ID).  \nThe filename contains info about the `<partition data>` and the `<execution ID>`  \n`csiRedis.<partition data>.<execution ID>.tar.gz`\n3. Get the log file  \n- create a new dir to store the log file\n```bash\nmkdir -p /tmp/csi_redis_log\ncd /tmp/csi_redis_log\n```\n- Copy from the HDFS log dir the proper log file according to the `<execution ID>` mentioned in the alert email\n`hdfs dfs -get /user/rediscsi/log/csiRedis.20230401.20230401_102030.tar.gz`\n- Extract the archived log file\n`tar xvfz ${archFile} --strip-components 9 -C .`\n- Searches for Exception messages in log files  \n`egrep -i '(Exception:|Caused by)' *.log`  \n4. In case of failure, the flow will try to load the data in the next run.  \njkl-Telecom is not aware of how the data files are produced or the contents in them.  \nThe Spark jobs that are used by the flow, have been developed by a partner of abc (an India company).  \njkl-Telecom is responsible for \n- the execution of Spark jobs to produce the export data files, \n- the collection of the export data files (if any), \n- the transfer of them in Redis node \n- and finally the loading of the export files into the Redis database (using specific Spark jobs).",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "CSI-Redis_Flow.md - Part 11"
        }
    },
    "87": {
        "page_content": "---\ntitle: Brond Retrains Flow - End-to-End File Ingestion and Hive Loading via Oozie\ndescription: Detailed documentation of the Brond Retrains pipeline, including SFTP retrieval, parsing, HDFS loading, Hive integration, monitoring via MySQL, and Oozie-based orchestration across BigStreamer infrastructure.\ntags:\n  - mno\n  - bigstreamer\n  - brond\n  - retrains\n  - oozie\n  - sftp\n  - hive\n  - impala\n  - beeline\n  - data pipeline\n  - monitoring\n  - ftp ingestion\n  - kerberos\n  - hdfs\n  - compute stats\n  - metadata ingestion\n  - jobstatus\n  - partition management\n  - manual rerun\n  - alert resolution\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  job_name: BROND_RETRAINS\n  component: MAIN\n  target_table: brond.brond_retrains_hist\n  host: un-vip.bigdata.abc.gr\n  coordinator: Brond_Load_Retrains_Coord_NEW\n  workflow: Brond_Load_Retrains_WF_NEW\n  owner: brond\n  system: BigStreamer\n  root_cause_keywords:\n    - no raw files found\n    - missing files\n    - hive partition missing\n    - failed workflow execution\n    - kerberos expiration\n  monitoring_db_host: 999.999.999.999\n  ssh_script_host: un-vip.bigdata.abc.gr\n  oozie_main_script_path: /user/brond/000.Brond_Retrains_Oozie_Main.sh\n  hive_db: brond\n  hive_table: brond_retrains_hist\n  log_file_pattern: 002.Brond_Retrains_Load.<YYYYMMDD>.log\n  manual_triggerable: true\n  default_schedule: [04:10, 05:10, 06:10, 10:10 UTC]\n---\n# Brond Retrains Flow\n## Installation info\nThis section outlines the setup details for the Brond Retrains pipeline, including input files, directories, scripts, logging, and Oozie scheduling.\n### Data Source File\nDetails on the raw input files retrieved via SFTP, including naming patterns, SFTP credentials, and local/HDFS paths.\n- Source system: FTP Server  \n  - host :`999.999.999.999`\n  - port :`22`\n  - protocol :`SFTP`\n  - user : `bigd`\n  - spool area : `/ADSL_Brond`\n  - file_type : `Counter_Collection_24H.*.csv.gz`\n  - load_suffix : `LOADED`\n- Local FileSystem Directories",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 1"
        }
    },
    "88": {
        "page_content": "- port :`22`\n  - protocol :`SFTP`\n  - user : `bigd`\n  - spool area : `/ADSL_Brond`\n  - file_type : `Counter_Collection_24H.*.csv.gz`\n  - load_suffix : `LOADED`\n- Local FileSystem Directories\n\t- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n\t- landing_zone : `/data/1/brond_retr_LZ`\n\t- archive_dir : `/data/1/brond_retr_LZ/archives`\n\t- work_dir : `/shared/brond_retr_repo`\n- HDFS Directories\n\t- hdfs_Bin : `/user/brond`\n\t- hdfs_dir : `/ez/warehouse/brond.db/landing_zone/brond_retrains`\n\t- hdfs_pending_dir : `/ez/warehouse/brond.db/landing_zone/brond_retrains/not_loaded`\n\t- hdfs_stats_dir : `/ez/warehouse/brond.db/landing_zone/brond_retrains/stats`\n### Scripts-Configuration Location\nPaths to parsing scripts and configuration `.trn` files used for file handling and flow control.\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- scripts path : `/shared/abc/brond/DataParser/scripts`\n-\tconfigurations path : `/shared/abc/brond/DataParser/scripts/transferlist/*.trn` (i.e. brond_retrains.trn)\n### Logs Location\nLocation and naming convention for logs generated by each retrains load run.\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- path : `/shared/abc/brond/DataParser/scripts/log`\n- log file: `002.Brond_Retrains_Load.<YYYYMMDD>.log`\n### Oozie Scheduling\nInformation on the Oozie coordinator, workflow, execution schedule, and trigger script.\n- user : `brond`\n- Coordinator :`Brond_Load_Retrains_Coord_NEW`  \n\truns at : `04:10, 05:10, 06:10, 10:10 UTC`\n- Workflow : `Brond_Load_Retrains_WF_NEW`  \n- Main script : `HDFS:/user/brond/000.Brond_Retrains_Oozie_Main.sh`\n- SSH Identity file : `HDFS:/user/brond/id_rsa`\nNdef: **Main Script** runs `oozie_brond_retrains.sh` located on `un-vip.bigdata.abc.gr` using **ssh** as user **brond**  \n`$ ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa brond@un-vip.bigdata.abc.gr \"/shared/abc/brond/DataParser/scripts/oozie_brond_retrains.sh\"`\n### Hive Tables",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 2"
        }
    },
    "89": {
        "page_content": "`$ ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa brond@un-vip.bigdata.abc.gr \"/shared/abc/brond/DataParser/scripts/oozie_brond_retrains.sh\"`\n### Hive Tables\nThe target Hive table used for storing parsed retrains data.\n- Target Database: `brond`\n- Target Tables: `brond.brond_retrains_hist`\n### Beeline-Impala Shell commands\nCommands for querying and managing data using Hive (Beeline) and Impala.\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/default;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala-shell: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k`\n## Data process: How raw FTP files get into Hive\nStep-by-step process of how retrains data files move from the FTP server to the final Hive table, including renaming, parsing, HDFS upload, and Hive loading.\n1. sftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```bash\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 3"
        }
    },
    "90": {
        "page_content": "sftp> ls -l ADSL_Brond\n-rw-r--r-- 0 507 500 21902115 Nov 28 07:02 ADSL_Brond/Counter_Collection_24H.328_2022_11_28.csv.gz.LOADED\n-rw-r--r-- 0 507 500 21925249 Nov 29 06:22 ADSL_Brond/Counter_Collection_24H.329_2022_11_29.csv.gz.LOADED\n-rw-r--r-- 0 507 500 22107252 Nov 30 06:52 ADSL_Brond/Counter_Collection_24H.330_2022_11_30.csv.gz\n```\n2. rename the raw file(s) in remdef SFTP server by adding the suffix .LOADED\n`echo \"rename /ADSL_Brond/Counter_Collection_24H.330_2022_11_30.csv.gz /ADSL_Brond/Counter_Collection_24H.330_2022_11_30.csv.gz.LOADED\" | sftp -oport=22 bigd@999.999.999.999`\n3. unzip raw files using `gzip -d` command in `/data/1/brond_retr_LZ`\n4. parsing raw files in `/data/1/brond_retr_LZ`\n- removes the headers (1st line)\n- removes double-qudefs chars\n- defines the PAR_DT value from the filename (i.e. Counter_Collection_24H.330_2022_11_30.csv.gz convert to 20221130)\n- add the prefix `RETR___` to raw file\n- add the suffix `<load time>.parsed` to raw file  \nLoad time format:`<YYYYMMDD_HHMISS>`  \ni.e. `RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed`\n5. put raw files into HDFS landingzone\n`hdfs dfs -put /data/1/brond_retr_LZ/RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed /ez/warehouse/brond.db/landing_zone/brond_retrains/RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed`\n6. clean-up any copy of the raw files from local filesystem  \n`/data/1/brond_retr_LZ`  \n`/shared/brond_retr_repo`  \n7. load HDFS files into hive table `brond.brond_retrains_hist`\n`beeline -e \"LOAD DATA INPATH '/ez/warehouse/brond.db/landing_zone/brond_retrains/RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed' OVERWRITE INTO TABLE brond.brond_retrains_hist PARTITION (par_dt='20221130')\"`\n8. execute compute stats using impala-shell  \n```bash\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\t\ncompute incremental stats brond.brond_retrains_hist;\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 4"
        }
    },
    "91": {
        "page_content": "8. execute compute stats using impala-shell  \n```bash\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\t\ncompute incremental stats brond.brond_retrains_hist;\n```\nIn the event where multiple files are transfered (files refer to the same data), we proceed with overwriting data in brond table. Deletion of those multiple files is abc's responsibility.\n## Monitoring\nHow the pipeline\u2019s execution is tracked using monitoring logs and component-level messages.\n### Monitoring connection details\nDatabase connection info for querying monitoring logs.\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \nConnection command: `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n### Monitoring Message list\nExample logs recorded during successful pipeline executions.\nFor each load the following set of messages will be recorded in the Monitoring database.\n```\nid    | execution_id | application | job            | component                        | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n------+--------------+-------------+----------------+----------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n15807 | 1659939004   | BROND       | BROND_RETRAINS | MAIN                             | 20220808            | SUCCESS | 2022-08-08 09:10:04 | 2022-08-08 09:10:48 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n15809 | 1659939004   | BROND       | BROND_RETRAINS | GET_RAW_RETRAIN_FILES            | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 5"
        }
    },
    "92": {
        "page_content": "15811 | 1659939004   | BROND       | BROND_RETRAINS | RENAME_FILES_@SFTP_SERVER        | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     |                        | brond | un2.bigdata.abc.gr\n15813 | 1659939004   | BROND       | BROND_RETRAINS | UNZIP_FILES                      | 20220808            | SUCCESS | 2022-08-08 09:10:07 |                     |                        | brond | un2.bigdata.abc.gr\n15815 | 1659939004   | BROND       | BROND_RETRAINS | PARSING_FILES                    | 20220808            | SUCCESS | 2022-08-08 09:10:13 |                     |                        | brond | un2.bigdata.abc.gr\n15817 | 1659939004   | BROND       | BROND_RETRAINS | LOAD_HDFS_LANDINGZONE            | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n15819 | 1659939004   | BROND       | BROND_RETRAINS | CLEAN-UP_THE_INPUT_FILES         | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n15821 | 1659939004   | BROND       | BROND_RETRAINS | LOAD_HDFS_FILES_INTO_HIVE_TABLES | 20220808            | SUCCESS | 2022-08-08 09:10:37 |                     |                        | brond | un2.bigdata.abc.gr\n15823 | 1659939004   | BROND       | BROND_RETRAINS | POST_SCRIPT                      | 20220808            | SUCCESS | 2022-08-08 09:10:48 |                     |                        | brond | un2.bigdata.abc.gr\n```\n### Monitoring Component list\nDescriptions of each component and what task it performs during execution.\n|Component | Description \n|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED| \n|GET_RAW_RETRAIN_FILES|sftp-get the raw files from the remdef server.<br />i.e.<br />Counter_Collection_24H.218_2022_08_08.csv.gz",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 6"
        }
    },
    "93": {
        "page_content": "|GET_RAW_RETRAIN_FILES|sftp-get the raw files from the remdef server.<br />i.e.<br />Counter_Collection_24H.218_2022_08_08.csv.gz\n|RENAME_FILES_@SFTP_SERVER| Rename the raw files in remdef SFTP server by adding the suffix .LOADED<br />i.e.<br />Counter_Collection_24H.218_2022_08_08.csv.gz.LOADED\n|UNZIP_FILES| unzip the raw files using `gzip -d` command\n|PARSING_FILES| removes any control chars (if any) from the raw files\n|LOAD_HDFS_LANDINGZONE|PUT the parsing files into HDFS landingzone `/ez/warehouse/brond.db/landing_zone/brond_retrains`\n|CLEAN-UP_THE_INPUT_FILES|Clean-up any copy of the raw files from the filesystem (`/data/1/brond_retr_LZ`, `/shared/brond_retr_repo`)\n|LOAD_HDFS_FILES_INTO_HIVE_TABLE| Load raw data (files) into the tables<br />`brond.brond_retrains_hist`\n|POST_SCRIPT| Execute Compute Statistics using impala-shell.<br />`compute incremental stats brond.brond_retrains_hist;`\n### Monitoring database Queries\nSQL queries to retrieve logs from the monitoring database for recent or failed loads.\n- List messages of the last load  \n`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n```sql\nselect \nexecution_id, id, application, job, component, operative_partition,  \nstatus, system_ts, system_ts_end, message, user,host   \nfrom jobstatus a where upper(job) like 'BROND_RETRAINS%'   \nand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'BROND_RETRAINS%');\n\texecution_id | id    | application | job            | component                        | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n\t-------------+-------+-------------+----------------+----------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 7"
        }
    },
    "94": {
        "page_content": "1659939004   | 15807 | BROND       | BROND_RETRAINS | MAIN                             | 20220808            | SUCCESS | 2022-08-08 09:10:04 | 2022-08-08 09:10:48 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n\t1659939004   | 15809 | BROND       | BROND_RETRAINS | GET RAW RETRAIN FILES            | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n\t1659939004   | 15811 | BROND       | BROND_RETRAINS | RENAME FILES @SFTP SERVER        | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15813 | BROND       | BROND_RETRAINS | UNZIP FILES                      | 20220808            | SUCCESS | 2022-08-08 09:10:07 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15815 | BROND       | BROND_RETRAINS | PARSING FILES                    | 20220808            | SUCCESS | 2022-08-08 09:10:13 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15817 | BROND       | BROND_RETRAINS | LOAD HDFS LANDINGZONE            | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15819 | BROND       | BROND_RETRAINS | CLEAN-UP THE INPUT FILES         | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15821 | BROND       | BROND_RETRAINS | LOAD HDFS FILES INTO HIVE TABLES | 20220808            | SUCCESS | 2022-08-08 09:10:37 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15823 | BROND       | BROND_RETRAINS | POST SCRIPT                      | 20220808            | SUCCESS | 2022-08-08 09:10:48 |                     |                        | brond | un2.bigdata.abc.gr\n\t```\n### Monitoring Health-Check",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 8"
        }
    },
    "95": {
        "page_content": "```\n### Monitoring Health-Check\nHow to verify the health of the monitoring application and steps to restart if needed.\n- Check Monitoring status.  \n```\tbash\n$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\n{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n```  \n- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\nCommon issues, log search patterns, and diagnostic steps when the retrains flow fails.\n- An email will be sent by the system with the point of failure.\n- Check the log file for errors/exceptions  \n`egrep -i 'error|fail|exception|problem' /shared/abc/brond/DataParser/scripts/log/002.Brond_Retrains_Load.<YYYYMMDD>.log`\n- List Failed Monitoring messages of the last load  \n```bash\n/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\nselect * from jobstatus where upper(job) like 'BROND_RETRAINS%' \nand status='FAILED'\nand operative_partition=(select max(operative_partition) from jobstatus where upper(job) like 'BROND_RETRAINS%' and operative_partition regexp '[0-9]{8}')\n\torder by id;\n\tid    | execution_id | application | job            | component             | operative_partition | status | system_ts           | system_ts_end       | message            | user  | host                  \n\t------+--------------+-------------+----------------+-----------------------+---------------------+--------+---------------------+---------------------+--------------------+-------+-----------------------\n\t14621 |              | BROND       | BROND_RETRAINS | MAIN                  | 20220801            | FAILED | 2022-08-01 16:13:13 | 2022-08-01 16:13:14 | No raw files found | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 9"
        }
    },
    "96": {
        "page_content": "14623 |              | BROND       | BROND_RETRAINS | GET RAW RETRAIN FILES | 20220801            | FAILED | 2022-08-01 16:13:14 |                     | No raw files found | brond | un2.bigdata.abc.gr\n\t```\n### Common errors  \n- `No raw files found`, there are no raw files available for loading at remdef server.  \nA WARNING message will be sent to abc by email.\nNo actions required from OBSS. Responsible abc. \n- Other factors not related to the specific flow\n- impala/hive availability\n- Kerberos authentication (A.  \n> Ndef: The flow checks if the ticket is still active before any HDFS action.  \nIn case of expiration the flow performs a `kinit` command*\n## Manually triggering the workflow\nInstructions for rerunning the workflow when files are uploaded after the scheduled execution time.\nThere are cases where abc might upload new files after the scheduled workflow timing, and request these files to be\nprocessed in the same day. This can only be done by manually triggering the workflow. Before doing so, you need to make some checks first:\n### Check workflow logs\n1. Login to https://999.999.999.999:8888/hue/accounts/login?next=/hue using the brond account\n2. Go to \"Jobs\" > \"Workflows\"\nIf all workflow executions (\"Brond_Load_Retrains_WF_NEW\") were successful, you can proceed by checking that the file(s)\nabc added, were copied after the scheduled timings of the workflow\n### Check added files\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond_DWH\n-rw-r--r--    0 507      500      150589497 Nov 29 05:34 ADSL_Brond_DWH/DWH_VDSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      150823890 Nov 29 13:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_29.csv.gz\n```\nThe second file was indeed added after the scheduled time and has not been picked up by the workflow.\n### Trigger workflow\nYou can now proceed to manually trigger the workflow:\n1. Go to HUE and select \"Jobs\"",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 10"
        }
    },
    "97": {
        "page_content": "### Trigger workflow\nYou can now proceed to manually trigger the workflow:\n1. Go to HUE and select \"Jobs\"\n2. Go to \"Workflow\" and select \"Brond_Load_Retrains_WF_NEW\"\n3. In the next screen, select \"Rerun\"\n4. Wait for the workflow to successfully end\n5. If no errors occur, proceed with:\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond_DWH\n-rw-r--r--    0 507      500      150589497 Nov 29 05:34 ADSL_Brond_DWH/DWH_VDSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      150823890 Nov 29 13:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_29.csv.gz\n```\n## Data Check\nSteps to validate that retrains data has been properly loaded and partitioned in the Hive table.\n- **Check final tables for new partitions**:\n- Impala-shell: \n```bash\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\nrefresh brond.brond_retrains_hist;  \nshow partitions brond.brond_retrains_hist;  \n\t\n\tpar_dt   | #Rows   | #Files | Size     | Bytes Cached | Cache Replication | Format | Incremental stats | Location                                                                     \n\t---------+---------+--------+----------+--------------+-------------------+--------+-------------------+------------------------------------------------------------------------------\n\t20221130 | 2784494 |      1 | 146.16MB | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/brond.db/brond_retrains_hist/par_dt=20221130\n\tTotal    | 5569421 |      1 | 146.16MB | 0B           |                   |        |                   |                                                                              \n```\n- **Check the amount of data in final tables**:\n- Impala-shell: \n```bash\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\nselect par_dt, count(*) as cnt from brond.brond_retrains_hist group by par_dt order by 1;\n\tpar_dt   | cnt    \n\t---------+--------",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 11"
        }
    },
    "98": {
        "page_content": "```bash\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\nselect par_dt, count(*) as cnt from brond.brond_retrains_hist group by par_dt order by 1;\n\tpar_dt   | cnt    \n\t---------+--------\n\t20221130 | 2784494\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Brond_Retrains_Flow.md - Part 12"
        }
    },
    "99": {
        "page_content": "---\ndocument_owner: \"Intra team\"\nsystems_involved:\n  - Traffica\n  - SFTP\n  - Hive\n  - Impala\n  - HDFS\n  - Grafana\nscheduling:\n  - Java Spring Boot\ndata_sources:\n  - trafficaftp@cne.def.gr:/data/1/trafficaftp/Traffica_XDR\ntarget_tables:\n  - sai.sms_load\n  - sai.sms_raw_text\n  - sai.sms_raw\n  - sai.voice_load\n  - sai.voice_raw_text\n  - sai.voice_raw_text_c2c\n  - sai.voice_raw\nscripts_location: \"/shared/abc/traffica\"\nhdfs_paths:\n  - /ez/warehouse/sai.db/landing_zone/sms\n  - /ez/warehouse/sai.db/landing_zone/voice\nresponsible_users:\n  - traffica\nmonitored: true\nalerts_handling: \"Email alerts on failure; Grafana dashboard for monitoring error metrics\"\nsummary: >\n  This document describes the Traffica data ingestion flows for SMS and VOICE XDR files. \n  It includes file retrieval via SFTP, local and HDFS staging, table loading to Hive and Impala, \n  cleanup logic, scheduling via Spring Boot, monitoring via Grafana, and alerting via email notifications.\ntags:\n  - traffica\n  - traffica_sms\n  - traffica_voice\n  - sms xdr flow\n  - voice xdr flow\n  - sftp ingestion\n  - springboot scheduler\n  - hive landing zone\n  - impala raw table\n  - hdfs staging\n  - bigstreamer ingestion\n  - .LOADED suffix\n  - grafana monitoring\n  - xdr pipeline\n  - application pause resume\n---\n# Traffica Flow\n## Useful links\n- [Wiki](https://metis.ghi.com/obss/bigdata/abc/etl/traffica/traffica-devops/-/wikis/home)\n- [Infrastructure](https://metis.ghi.com/obss/bigdata/abc/etl/traffica/traffica-devops/-/wikis/Infrastructure)\n## SMS Flow\n> The SMS flow ingests XDR files from the remote SFTP server, stages them on the local disk and HDFS, \n> loads them into Hive staging tables, and inserts them into the final Impala table.\n``` mermaid\n     graph TD\n      A0[\"abc Flow <br> User: trafficaftp\"]\n      A1[\"Host: cne.def.gr <br> Path: /data/1/trafficaftp/Traffica_XDR\"]\n      A2[\"Staging Directory <br> Path: /data/1/traffica_LZ/sms\"]\n      A3(\"Staging HDFS Directory <br> Path: /ez/warehouse/sai.db/landing_zone/sms\")",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "traffica.md - Part 1"
        }
    },
    "100": {
        "page_content": "A2[\"Staging Directory <br> Path: /data/1/traffica_LZ/sms\"]\n      A3(\"Staging HDFS Directory <br> Path: /ez/warehouse/sai.db/landing_zone/sms\")\n      A4(\"Staging Table <br> Hive: sai.sms_load\")\n      A5(\"Staging Table <br> Hive: sai.sms_raw_text\")\n      A6(\"Table <br> Impala: sai.sms_raw\")\n      A7(\"Cleanup <br> Add .LOADED suffix to local raw files <br> Clean local staging directories <br> Clean HDFS tables/directories\")\n    \n      A0 -->|SFTP| A1\n      A1 --> |Merge files| A2\n      A2 --> |HDFS Load| A3\n      A3 --> |Hive Load| A4\n      A4 --> |Hive Insert| A5\n      A5 --> |Impala Insert| A6\n      A6 --> |Successful loaded files only| A7\n```\n**Flow Summary:**\n- SFTP pull from cne.def.gr to local staging\n- Local merge and upload to HDFS\n- Load to Hive staging tables\n- Insert into Impala target table\n- Cleanup: Add `.LOADED` suffix to processed files and clean staging dirs\n**Schedule**: `every 35 minutes`  \n**Scheduler**: `Java Springboot Application`  \n**User**: `traffica`  \n**Active Node**: `unc2.bigdata.abc.gr`  \n**Backup Node**: `unc1.bigdata.abc.gr`  \n**Installation directory**: `/shared/abc/traffica`  \n**Logs**: `/shared/abc/traffica/logs`  \n**Configuration File**: `/shared/abc/traffica/config/application.yml`\n**Start command**: `supervisorctl start traffica_sms`  \n**Stop command**: `supervisorctl stop traffica_sms`  \n**Enable command (un-pause)**: `curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\"`\n**Alerts**:\n- Mail with subject: `Traffica Application failed`\nPossible messages:\n1. `Traffica sms main flow failed.`\n2. `Traffica sms application has been paused due to multiple sequential failures. Manual actions are needed.`\n3. `Traffica sms application has been paused due to inability to rename local files. Manual actions are needed.`\n4. `Traffica sms application has been paused due to inability to clean up files. Manual actions are needed.`\n**Troubleshooting steps**:\n- Check to see if the application is running:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "traffica.md - Part 2"
        }
    },
    "101": {
        "page_content": "4. `Traffica sms application has been paused due to inability to clean up files. Manual actions are needed.`\n**Troubleshooting steps**:\n- Check to see if the application is running:\n  From `unc2` with personal account:\n  ``` bash\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\"\n  ```\n- Check the logs for errors to identify the root cause\n  From `unc2` as `traffica`:\n  ``` bash\n  # For the current log file\n  grep -i -e error -e exception /shared/abc/traffica/logs/traffica-sms.log\n  # For older compressed files\n  zgrep -i -e error -e exception /shared/abc/traffica/logs/<yearMonthFolder>/<name_of_logfile>\n  ```\n- Check metrics and error rates from Grafana\n  Open Firefox using VNC and go to `https://unc1.bigdata.abc.gr:3000/d/qIM5rod4z/traffica`\n  Use panels ending in `Err` to identify problematic components and steps.\n  Use `Files`,`Size`,`Rows` to identify if input has changed\n- If there is a problem renaming files with the `.LOADED` suffix\n  From `unc2` as `traffica`:\n  ``` bash\n  # Get files that where processed correctly\n  grep 'filelist=' /shared/abc/traffica/logs/traffica-sms.log \n  # Move files pending rename from the list above\n  cd /data/1/trafficaftp/Traffica_XDR\n  mv <file>{,.LOADED}\n  ```\n- If the root cause is resolved resume normal operation.\n  The flow has been designed with auto-recovery. It marks only successfully loaded files as `.LOADED` and handles/cleans up all staging directories on each run.\n  From `unc2` with personal user:\n  ``` bash\n  # Check if scheduling is enabled \n  curl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disabled\"\n  # If the above command returns true\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\"\n  ```\n  - If `unc2` is down, then manually start the application on `unc1` this requires that VIP `cne.def.gr` has been migrated and SFTP is working on `unc1`\n**Ndefs**:\n  From `unc2` with personal account:\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "traffica.md - Part 3"
        }
    },
    "102": {
        "page_content": "**Ndefs**:\n  From `unc2` with personal account:\n  ``` bash\n  curl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/info/health\" # HTTP 200 if app is up\n  curl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/info/check\" # returns message if up\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/shutdown/gracefully\" # shutdown application. If flow is running, then wait to finish. App should terminate ONLY with this method.\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disable\" # enable flow scheduling\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\" # enable flow scheduling\n  curl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disabled\" # true if disabled, else false\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/cleanup/all\" # Run cleanup on demand\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/main/run\" # Run flow on demand\n  # UI with endpoints available from VNC: http://unc2.bigdata.abc.gr:11483/traffica/swagger-ui/index.html#/\n  ```\n## Voice Flow\n> The Voice flow processes CDR files similarly to SMS, but includes two Hive staging tables before \n> merging everything into the final Impala voice table.\n``` mermaid\n     graph TD\n      A0[\"abc Flow <br> User: trafficaftp\"]\n      A1[\"cne.def.gr <br> Path: /data/1/trafficaftp/Traffica_XDR\"]\n      A2[\"Staging Directory <br> Path: /data/1/traffica_LZ/voice\"]\n      A3(\"Staging HDFS Directory <br> Path: /ez/warehouse/sai.db/landing_zone/voice\")\n      A4(\"Staging Table <br> Hive: sai.voice_load\")\n      A5(\"Staging Table <br> Hive: sai.voice_raw_text\")\n      A6(\"Staging Table <br> Hive: sai.voice_raw_text_c2c\")\n      A7(\"Table <br> Impala: sai.voice_raw\")\n      A8(\"Cleanup <br> Add .LOADED suffix to local raw files <br> Clean local staging directories <br> Clean HDFS tables/directories\")\n    \n      A0 -->|SFTP| A1",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "traffica.md - Part 4"
        }
    },
    "103": {
        "page_content": "A8(\"Cleanup <br> Add .LOADED suffix to local raw files <br> Clean local staging directories <br> Clean HDFS tables/directories\")\n    \n      A0 -->|SFTP| A1\n      A1 --> |Merge files| A2\n      A2 --> |HDFS Load| A3\n      A3 --> |Hive Load| A4\n      A4 --> |Hive Insert| A5\n      A4 --> |Hive Insert| A6\n      A5 --> |Impala Insert| A7\n      A6 --> |Impala Insert| A7\n      A7 --> |Successful loaded files only| A8\n```\n**Flow Summary:**\n- SFTP pull from cne.def.gr to local staging\n- Merge and stage to HDFS\n- Load to two Hive raw text tables (voice_raw_text, voice_raw_text_c2c)\n- Insert into final Impala voice_raw table\n- Cleanup: Mark files as `.LOADED`, clean Hive + HDFS staging dirs\n**Schedule**: `every 20 minutes`  \n**Scheduler**: `Java Springboot Application`  \n**User**: `traffica`  \n**Active Node**: `unc2.bigdata.abc.gr`\n**Backup Node**: `unc1.bigdata.abc.gr`\n**Installation directory**: `/shared/abc/traffica`\n**Logs**: `/shared/abc/traffica/logs`\n**Configuration File**: `/shared/abc/traffica/config/application.yml`\n**Start command**: `supervisorctl start traffica_voice`  \n**Stop command**: `supervisorctl stop traffica_voice`  \n**Enable command (un-pause)**: `curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/enable\"`\n**Alerts**:\n- Mail with subject: `Traffica Application failed`\nPossible messages:\n1. `Traffica voice main flow failed.`\n2. `Traffica voice application has been paused due to multiple sequential failures. Manual actions are needed.`\n3. `Traffica voice application has been paused due to inability to rename local files. Manual actions are needed.`\n4. `Traffica voice application has been paused due to inability to clean up files. Manual actions are needed.`\n**Troubleshooting steps**:\n- Check to see if the application is running:\n  From `unc2` with personal account:\n  ``` bash\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\"\n  ```\n- Check the logs for errors to identify the root cause",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "traffica.md - Part 5"
        }
    },
    "104": {
        "page_content": "From `unc2` with personal account:\n  ``` bash\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\"\n  ```\n- Check the logs for errors to identify the root cause\n  From `unc2` as `traffica`:\n  ``` bash\n  # For the current log file\n  grep -i -e error -e exception /shared/abc/traffica/logs/traffica-voice.log\n  # For older compressed files\n  zgrep -i -e error -e exception /shared/abc/traffica/logs/<yearMonthFolder>/<name_of_logfile>\n  ```\n- Check metrics and error rates from Grafana\n  Open Firefox using VNC and go to `https://unc1.bigdata.abc.gr:3000/d/qIM5rod4z/traffica`\n  Use panels ending in `Err` to identify problematic components and steps.\n  Use `Files`,`Size`,`Rows` to identify if input has changed\n- If there is a problem renaming files with the `.LOADED` suffix\n  From `unc2` as `traffica`:\n  ``` bash\n  # Get files that where processed correctly\n  grep 'filelist=' /shared/abc/traffica/logs/traffica-voice.log \n  # Move files pending rename from the list above\n  cd /data/1/trafficaftp/Traffica_XDR\n  mv <file>{,.LOADED}\n  ```\n- If the root cause is resolved resume normal operation.\n  The flow has been designed with auto-recovery. It marks only successfully loaded files as `.LOADED` and handles/cleans up all staging directories on each run.\n  From `unc2` with personal user:\n  ``` bash\n  # Check if scheduling is enabled \n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/disabled\"\n  # If the above command returns true\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/enable\"\n  ```\n  - If `unc2` is down, then manually start the application on `unc1` this requires that VIP `cne.def.gr` has been migrated and SFTP is working on `unc1`\n**Ndefs**:\n  From `unc2` with personal account:\n  ``` bash\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/health\" # HTTP 200 if app is up\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\" # returns message if up",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "traffica.md - Part 6"
        }
    },
    "105": {
        "page_content": "curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/health\" # HTTP 200 if app is up\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\" # returns message if up\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/shutdown/gracefully\" # shutdown application. If flow is running, then wait to finish. App should terminate ONLY with this method.\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/disable\" # enable flow scheduling\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/enable\" # enable flow scheduling\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/disabled\" # true if disabled, else false\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/cleanup/all\" # Run cleanup on demand\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/main/run\" # Run flow on demand\n  # UI with endpoints available from VNC: http://unc2.bigdata.abc.gr:11482/traffica/swagger-ui/index.html#/\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "traffica.md - Part 7"
        }
    },
    "106": {
        "page_content": "---\ntitle: DWHFixed Full and Delta Load \u2013 Oracle to Hive/Impala ETL\ndescription: Daily and bi-hourly ETL flows for ingesting data from Oracle SAS_VA views to Hive and Impala using Sqoop, Beeline, and Impala-shell, with monitoring, logging, retry mechanisms, and Grafana dashboards.\njob_name: FULL / DELTA\ncomponent: DWHFixed\nsystem: BigStreamer\nhost: un-vip.bigdata.abc.gr\nsource: Oracle (SAS_VA_VIEW)\ntarget_system: Hive / Impala\ntarget_tables: dwhfixed.*_hist\ncoordinator_full: DWHFixed - Full Coordinator\nworkflow_full: DWHFixed - Full Workflow\ncoordinator_delta: DWHFixed - Delta Coordinator\nworkflow_delta: DWHFixed - Delta Workflow\noracle_user: dm_sas_va\noracle_password_link: https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx\nexecution_schedule_full: Daily at 15:30 & 18:30 UTC\nexecution_schedule_delta: Every 2 hours from 01:30 to 23:30 UTC\nhdfs_paths:\n  full: /user/dwhfixed/full\n  delta: /user/dwhfixed/delta\n  config: /user/dwhfixed/dwhfixed.config\n  log: /user/dwhfixed/log\n  retention: /user/dwhfixed/HDFS_LOG_Retention\nmonitoring:\n  application: DWHFIXED\n  api_url: http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find\n  dashboard: https://unc1.bigdata.abc.gr:3000/d/DNM-sBo4z/dwhfixed-dashboard?orgId=1&from=now-2d&to=now\nhue_login_user: dwhfixed\nalert_subject_format: DWHFIXED - {FULL|DELTA}: FAILED\nalert_source_system: Oracle\nalert_target_system: Hive / Impala\nretention_policy_days: 9\nowner: dwhfixed\nkeywords:\n  - oracle\n  - hive\n  - impala\n  - sqoop\n  - beeline\n  - impala-shell\n  - hdfs\n  - monitoring\n  - grafana\n  - full load\n  - delta load\n  - partitioned data\n  - devpasswd\n  - SAS_VA_VIEW\n  - jdbc\n  - load failure alerts\n  - control table\n  - jceks\n  - oozie\n  - retry\n  - logging\n  - alerting\n  - automation\n  - curl monitoring\n  - log retention\n  - hue workflows\n---\n# Full Load",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 1"
        }
    },
    "107": {
        "page_content": "- jdbc\n  - load failure alerts\n  - control table\n  - jceks\n  - oozie\n  - retry\n  - logging\n  - alerting\n  - automation\n  - curl monitoring\n  - log retention\n  - hue workflows\n---\n# Full Load\nThis section describes the full data pipeline executed twice daily, transferring data from Oracle SAS_VA views to partitioned Hive/Impala history tables via Sqoop and Beeline.\n- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - Full Workflow`, \n- **COORDINATOR**: `DWHFixed - Full Coordinator`\n- **HDFS path**: `/user/dwhfixed/full`\n- **Runs**: `15:30, 18:30 (UTC)` \n- **Config file**: `hdfs: /user/dwhfixed/dwhfixed.config`\n- **Tables config/list**: `hdfs: /user/dwhfixed/full/tables_full.config`\n- **Oracle user**: `dm_sas_va`\n- **Oracle pass**: `https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx`. You need password to open this file and get the user's password.\nMoves data from the source tables to a `yesterday` partition of the target tables.\n## Oracle Tables (source)\nThese are the source tables in Oracle used in the Full Load process. The data from each table is transferred to Hive/Impala with daily partitions.\n- `SAS_VA_VIEW.V_BOX_DIM`\n- `SAS_VA_VIEW.V_FIXED_CABLE_DIM`\n- `SAS_VA_VIEW.V_KV_DIM`\n- `SAS_VA_VIEW.V_DSLAM_DIM`\n- `SAS_VA_VIEW.V_SR_TYPE_DIM`\n- `SAS_VA_VIEW.V_SRIA_ADDINFO_DIM`\n- `SAS_VA_VIEW.V_SRIA_STATUS_DIM`\n- `SAS_VA_VIEW.V_SRIA_SERV_PRODUCT_CAT_DIM`\n- `SAS_VA_VIEW.V_SRIA_PRIORITY_DIM`\n- `SAS_VA_VIEW.V_PROVIDER_DIM`\n- `SAS_VA_VIEW.V_def_NETWORK_DIM`\n- `SAS_VA_VIEW.V_def_DIVISION_DIM`\n- `SAS_VA_VIEW.V_SRIA_TYPE_DIM`\n- `SAS_VA_VIEW.V_SRIA_ACT_STATUS_DIM`\n- `SAS_VA_VIEW.V_SRIA_SUBAREA_DIM`\n- `SAS_VA_VIEW.V_POSITION_DIM`\n- `SAS_VA_VIEW.V_CAUSE_DIM`\n- `SAS_VA_VIEW.V_ACTION_DIM`\n- `SAS_VA_VIEW.V_ADSL_DIM`\n- `SAS_VA_VIEW.V_SRIA_AREA_DIM`\n## Hive - Impala Tables (target)\nThese are the target history tables in Hive and Impala, where data is written using LOAD DATA INPATH and REFRESH statements.\n- `dwhfixed.v_box_dim_hist`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 2"
        }
    },
    "108": {
        "page_content": "## Hive - Impala Tables (target)\nThese are the target history tables in Hive and Impala, where data is written using LOAD DATA INPATH and REFRESH statements.\n- `dwhfixed.v_box_dim_hist`\n- `dwhfixed.v_fixed_cable_dim_hist`\n- `dwhfixed.v_kv_dim_hist`\n- `dwhfixed.v_dslam_dim_hist`\n- `dwhfixed.v_sr_type_dim_hist`\n- `dwhfixed.v_sria_addinfo_dim_hist`\n- `dwhfixed.v_sria_status_dim_hist`\n- `dwhfixed.v_sria_serv_product_cat_dim_hist`\n- `dwhfixed.v_sria_priority_dim_hist`\n- `dwhfixed.v_provider_dim_hist`\n- `dwhfixed.v_def_network_dim_hist`\n- `dwhfixed.v_def_division_dim_hist`\n- `dwhfixed.v_sria_type_dim_hist`\n- `dwhfixed.v_sria_act_status_dim_hist`\n- `dwhfixed.v_sria_subarea_dim_hist`\n- `dwhfixed.v_position_dim_hist`\n- `dwhfixed.v_cause_dim__hist`\n- `dwhfixed.v_action_dim_hist`\n- `dwhfixed.v_adsl_dim_hist`\n- `dwhfixed.v_sria_area_dim_hist`\n## Data Flow\nThe diagram below illustrates the end-to-end data movement from Oracle to Hive and Impala using Sqoop, Beeline, and Impala-shell.\n``` mermaid\n    graph TD\n        A1(<b>Oracle Table</b>)\n        A2(\"<b>HDFS</b> <br> /user/hive/warehouse/dwhfixed.db/landing_zone/{table_name}\")\n        A3(<b>Hive table</b>)\n        A4(\"<b>Impala table</b>\")",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 3"
        }
    },
    "109": {
        "page_content": "A1 --> |\"\"\"<b>sqoop: import</b> <br> \n                jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD <br>\n                user: dm_sas_va <br>\n                password file jceks://hdfs/user/dwhfixed/dwhfixed <br>\n                password-alias: dm_sas_va.pass\"\"\"| A2\n        A2 --> |\"<b>beeline: load data inpath</b>  <br>\n                 un-vip.bigdata.abc.gr:10000/default\"| A3\n        A3 --> |\"<b>impala-shell: refresh</b> <br>\n                 un-vip.bigdata.abc.gr\"| A4\n```\n## Logs\nLogs for each execution of the workflow can be viewed in the Hue interface under the dwhfixed user, or in HDFS log paths for historical runs.\nApplication logs can be found in each Workflow in Hue as user `dwhfixed`. Oozie Coordinator result can be seen into HUE (login as `dwhfixed` user) and go to `Jobs -> Workflows` and look for `DWHFixed - Full Workflow`\n## Monitoring messages\nThis section explains how monitoring tracks execution status for each table and component. Each job run has a unique executionId.\n- All monitoring messages have the following constant values:\\\n  **application=DWHFIXED**\\\n  **job=FULL**\n- All monitoring messages of the same execution have a **unique executionId**\n- Every component of one execution has a unique row that is updated between the following status values: RUNNING to SUCCESS or FAILED.\n```mermaid\n    graph TD\n        A1(\"component=MAIN_START_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A2(\"component=SQOOP_IMPORT_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A3(\"component=BEELINE_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A4(\"component=IMPALA_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A5(\"component=MAIN_END_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 4"
        }
    },
    "110": {
        "page_content": "A1 --> A2\n        A2 --> A3\n        A3 --> A4\n        A4 --> A5\n```\n- Check monitoring app for successful executions:  \nFrom `un2` with personal account:\n``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=FULL$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'\n```\n- Check monitoring app for failed executions:  \nFrom `un2` with personal account:\n``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=FULL$status=FAILED&operativePartition=<timestamp e.g.:20220518>'\n```\n- Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n### Grafana dashboard\nA Grafana dashboard provides visual monitoring for DWHFixed executions and error trends. Use the link below to access it.\nGrafana: `https://unc1.bigdata.abc.gr:3000/d/DNM-sBo4z/dwhfixed-dashboard?orgId=1&from=now-2d&to=now`\n## Alerts (Mail)\nEmail alerts are triggered automatically in case of failure in any component \u2014 Oracle, Hive, Impala. The subject and body format is detailed below.\n**Subject**: `DWHFIXED - FULL: FAILED`  \n**Body**: \n- `${ORACLE_TABLE}: failed to perform Oracle query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to fetch data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: fetched no data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to execute beeline query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n`${ORACLE_TABLE}`: Can be any of the tables from section `1.1. Oracle (source)`  \n`${datetime_now}`: Current datetime in `%Y-%m-%d %H:%M` format\nThe application sends an email in each case of the following failures (for each table):\n### Oracle failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 5"
        }
    },
    "111": {
        "page_content": "`${datetime_now}`: Current datetime in `%Y-%m-%d %H:%M` format\nThe application sends an email in each case of the following failures (for each table):\n### Oracle failure\n- Sqoop eval failure.\n- Sqoop import failure.\n- Sqoop has fetched no data.\nHow to check Oracle:\n```bash\nssh <username>@un2\nsh -l intra\njsec_file=jceks://hdfs/ez/intra.Sqoop.Creds.jceks\nsqoop eval \\\n-Dhadoop.security.credential.provider.path=${jsec_file} \\\n--password-alias dm_sas_va.pass \\\n--connect jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD \\\n--username dm_sas_va \\\n--query \"select * FROM SAS_VA_VIEW.V_DW_CONTROL_TABLE where 1=1\" ;\n```\nAlternative way of checking Oracle:\n```bash\nssh <username>@undt1\nsu -l intra\nexport ORACLE_HOME=/usr/lib/oracle/11.2/client64\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:${LD_LIBRARY_PATH}\nexport PATH=$ORACLE_HOME/bin:${PATH}\nexport SQLPATH=$ORACLE_HOME/lib:${SQLPATH}\nexport NLS_DATE_FORMAT='yyyy-mm-dd hh24:mi:ss'\nexport HISTSIZE=0\nsqlplus dm_sas_va/<Oracle-pass>@999.999.999.999:6644/DWHPRD\n```\n### Hive/Impala failure\n- Beeline load data inpath has failed.\n- Impala refresh has failed.\nHow to check Hive/Impala:\n- In the Hue UI mentioned above, click on editor -> Hive/Impala\n- Run the following query on each:`select * from dwhfixed.v_sria_subarea_dim_hist limit 3;`\n### Actions\n//TODO\n# Delta Load\nThe delta load flow runs every 2 hours and loads only new partitions by consulting the control table in Oracle before running the main pipeline.\n- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - Delta Workflow`, \n- **COORDINATOR**: `DWHFixed - Delta Coordinator`\n- **HDFS path**: `/user/dwhfixed/delta`\n- **Runs**: `1:30,3:30,5:30,7:30,9:30,11:30,13:30,15:30,19:30,21:30,23:30 (UTC)`\n- **Config file**: `hdfs: /user/dwhfixed/dwhfixed.config`\n- **Tables config/list**: `hdfs: /user/dwhfixed/delta/tables_delta.config`\n- **Oracle user**: `dm_sas_va`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 6"
        }
    },
    "112": {
        "page_content": "- **Config file**: `hdfs: /user/dwhfixed/dwhfixed.config`\n- **Tables config/list**: `hdfs: /user/dwhfixed/delta/tables_delta.config`\n- **Oracle user**: `dm_sas_va`\n- **Oracle pass**: `https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx`. You need password to open this file and get the user's password.\nRuns every 2h, checks control table: `SAS_VA_VIEW.V_DW_CONTROL_TABLE` in order to evaluate if a new partition has been added to the source tables. After that, it runs data flow.\n## Oracle (source)\n- `SAS_VA_VIEW.V_SRIA_SERVICE_REQUESTS_FCT`\n- `SAS_VA_VIEW.V_ACS_MODEMS_ACTIVE_FCT`\n- `SAS_VA_VIEW.V_LL_DIM`\n- `SAS_VA_VIEW.V_SRIA_INTERACT_ACTIVITY_FCT`\n- `SAS_VA_VIEW.V_DW_CONTROL_TABLE`\n- `SAS_VA_VIEW.V_FAULT_NTT_NETWORK_ELEM_FCT`\n- `SAS_VA_VIEW.V_SRIA_SITE_DIM`\n- `SAS_VA_VIEW.V_SR_AFF_CUST_FCT`\n## Hive - Impala (target)\n- `dwhfixed.v_sria_service_requests_fct_hist`\n- `dwhfixed.v_acs_modems_active_fct_hist`\n- `dwhfixed.v_ll_dim_hist`\n- `dwhfixed.v_sria_interact_activity_fct_hist`\n- `dwhfixed.v_fault_ntt_network_elem_fct_hist`\n- `dwhfixed.v_sria_site_dim_hist`\n- `dwhfixed.v_sr_aff_cust_fct_hist`\n## Data Flow\n``` mermaid\n    graph TD\n        A1(<b>Oracle Table</b>)\n        A2(\"<b>HDFS</b> <br> /user/hive/warehouse/dwhfixed.db/landing_zone/{table_name}\")\n        A3(<b>Hive table</b>)\n        A4(\"<b>Impala table</b>\")",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 7"
        }
    },
    "113": {
        "page_content": "A1 --> |\"\"\"<b>sqoop: import</b> <br> \n                jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD <br>\n                user: dm_sas_va <br>\n                password file jceks://hdfs/user/dwhfixed/dwhfixed <br>\n                password-alias: dm_sas_va.pass\"\"\"| A2\n        A2 --> |\"<b>beeline: load data inpath</b>  <br>\n                 un-vip.bigdata.abc.gr:10000/default\"| A3\n        A3 --> |\"<b>impala-shell: refresh</b> <br>\n                 un-vip.bigdata.abc.gr\"| A4\n```\n## Logs\nApplication logs can be found in each Workflow in Hue as user `dwhfixed`. Oozie Coordinator result can be seen into HUE (login as `dwhfixed` user) and go to `Jobs -> Workflows` and look for `DWHFixed - Delta Workflow`\n## Monitoring messages\n- All monitoring messages have the following constant values:\\\n  **application=DWHFIXED**\\\n  **job=DELTA**\n- All monitoring messages of the same execution have a **unique executionId**\n- Every component of one execution has a unique row that is updated between the following status values: RUNNING to SUCCESS or FAILED.\n```mermaid\n    graph TD\n        A1(\"component=MAIN_START_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A2(\"component=SQOOP_IMPORT_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A3(\"component=BEELINE_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A4(\"component=IMPALA_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A5(\"component=MAIN_END_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 8"
        }
    },
    "114": {
        "page_content": "A1 --> A2\n        A2 --> A3\n        A3 --> A4\n        A4 --> A5\n```\n- Check monitoring app for successful executions:  \nFrom `un2` with personal account:\n``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=DELTA$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'\n```\n- Check monitoring app for failed executions:  \nFrom `un2` with personal account:\n``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=DELTA$status=FAILED&operativePartition=<timestamp e.g.:20220518>'\n```\n- Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n### Grafana dashboard\nGrafana: `https://unc1.bigdata.abc.gr:3000/d/DNM-sBo4z/dwhfixed-dashboard?orgId=1&from=now-2d&to=now`\n## Alerts (Mail)\n**Subject**: `DWHFIXED - DELTA: FAILED`  \n**Body**: \n- `${ORACLE_TABLE}: failed to perform Oracle query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to fetch data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: fetched no data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to execute beeline query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n`${ORACLE_TABLE}`: Can be any of the tables from section `1.1. Oracle (source)`  \n`${datetime_now}`: Current datetime in `%Y-%m-%d %H:%M` format\nThe application sends an email in each case of the following failures (for each table):\n### Oracle failure\n- Sqoop eval failure.\n- Sqoop import failure.\n- Sqoop has fetched no data.\nHow to check Oracle:\n```bash\nssh <username>@un2\nsh -l intra\njsec_file=jceks://hdfs/ez/intra.Sqoop.Creds.jceks\nsqoop eval \\\n-Dhadoop.security.credential.provider.path=${jsec_file} \\",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 9"
        }
    },
    "115": {
        "page_content": "How to check Oracle:\n```bash\nssh <username>@un2\nsh -l intra\njsec_file=jceks://hdfs/ez/intra.Sqoop.Creds.jceks\nsqoop eval \\\n-Dhadoop.security.credential.provider.path=${jsec_file} \\\n--password-alias dm_sas_va.pass \\\n--connect jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD \\\n--username dm_sas_va \\\n--query \"select * FROM SAS_VA_VIEW.V_DW_CONTROL_TABLE where 1=1\" ;\n```\nAlternative way of checking Oracle:\n```bash\nssh <username>@undt1\nsu -l intra\nexport ORACLE_HOME=/usr/lib/oracle/11.2/client64\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:${LD_LIBRARY_PATH}\nexport PATH=$ORACLE_HOME/bin:${PATH}\nexport SQLPATH=$ORACLE_HOME/lib:${SQLPATH}\nexport NLS_DATE_FORMAT='yyyy-mm-dd hh24:mi:ss'\nexport HISTSIZE=0\nsqlplus dm_sas_va/<Oracle-pass>@999.999.999.999:6644/DWHPRD\n```\n### Hive/Impala failure\n- Beeline load data inpath has failed.\n- Impala refresh has failed.\nHow to check Hive/Impala:\n- In the Hue UI mentioned above, click on editor -> Hive/Impala\n- Run the following query on each:`select * from dwhfixed.v_sria_subarea_dim_hist limit 3;`\n### Actions\nIn case of any of alert, do nothing. The flow will try to re-run in 2 hours. If everything is OK, then it will load successfully the partitions. After 2-2:30 hours check the status of the next run. \nIf the error persists:\n- In case of infastructure error (ex. HDFS, Hive are down), do nothing.\n- In case of other error, contact BigData Developer team.\n# HDFS Log Files Retention \nA daily HDFS workflow archives execution logs for historical reference. Default retention is 9 days.\nArchiving of the log files produced in every run of the flow and store them in HDFS directory.  \nIt is useful for investigation in case of errors.  \nThe retention of the log files is configurable. Default value: 9 days  \n- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - HDFS_Log_Retention_Workf`, \n- **COORDINATOR**: `DWHFixed - HDFS_Log_Retention_Coord`\n- **Runs**: `${coord:days(1)} 23:00 (Europe/Athens)`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 10"
        }
    },
    "116": {
        "page_content": "- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - HDFS_Log_Retention_Workf`, \n- **COORDINATOR**: `DWHFixed - HDFS_Log_Retention_Coord`\n- **Runs**: `${coord:days(1)} 23:00 (Europe/Athens)`\n- **HDFS Retention path**: `/user/dwhfixed/HDFS_LOG_Retention`\n- **HDFS Log path**: `/user/dwhfixed/log`\n- **HDFS Log Files**: `DWHFIXED.*.Archive.tar.gz`\n# Useful Links\n[Home](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/dwhfixed/-/wikis/home)\n[Infrastructure](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/dwhfixed/-/wikis/Infrastructure)",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "dwhfixed.md - Part 11"
        }
    },
    "117": {
        "page_content": "---\ntitle: TrustCenter Data Export Flows\ndescription: Overview and support guide for TrustCenter-related export workflows including Location Mobility, Router Analytics, Application Usage Insights (AUI), and Customer Satisfaction Index (CSI). Describes scheduling, file formats, SFTP transfers, Impala sources, Oozie jobs, and troubleshooting procedures.\nauthor: mtuser / intra / ABC BigStreamer Team\nupdated: 2025-05-01\ntags:\n  - trustcenter\n  - location mobility\n  - lm\n  - router analytics\n  - ra\n  - application usage insights\n  - aui\n  - customer satisfaction index\n  - csi\n  - oozie\n  - sftp\n  - export flows\n  - bigstreamer\n  - impala\n  - reconciliation logs\n---\n# TrustCenter Flows\nThis is a document that will assist on support. Business documents can be found [here](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/tree/master/mini%20projects/location_mobility/docs).\n## Location Mobility\nLocation Mobility (LM) reffers to extraction of data from BigStreamer into files.  \nThe output files are transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them.  \nThese files are:\n- `LM_02_lte_yyyyMMdd_xxx.txt`\n- `LM_03_smsIn_yyyyMMdd_xxx.txt`\n- `LM_04_smsOut_yyyyMMdd_xxx.txt`\n- `LM_05_voiceInOut_yyyyMMdd_xxx.txt`\n- `LM_06_voiceIn_yyyyMMdd_xxx.txt`\n- `LM_07_voiceOut_yyyyMMdd_xxx.txt`\n- `LM_08_cellHist_yyyyMMdd_xxx.txt`\nAlong with those, the reconciliation files are produced and sent for each one.  \nThey give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\n``` bash\ncat /shared/abc/location_mobility/logging/LM_05_voiceInOut_reconciliation.log\n#e.g for LM_05_voiceInOut and 1st of February 2022\n2022-02-01 08:06:33 LM_05_voiceInOut_20220201_00001.txt 20220201 20906\n2022-02-01 10:02:36 LM_05_voiceInOut_20220201_00002.txt 20220201 23810\n2022-02-01 12:02:45 LM_05_voiceInOut_20220201_00003.txt 20220201 179719",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 1"
        }
    },
    "118": {
        "page_content": "2022-02-01 10:02:36 LM_05_voiceInOut_20220201_00002.txt 20220201 23810\n2022-02-01 12:02:45 LM_05_voiceInOut_20220201_00003.txt 20220201 179719\n2022-02-01 14:03:45 LM_05_voiceInOut_20220201_00004.txt 20220201 876051\n2022-02-01 16:05:13 LM_05_voiceInOut_20220201_00005.txt 20220201 1581201\n2022-02-01 20:02:00 LM_05_voiceInOut_20220201_00006.txt 20220201 1606966\n```\n**Reconcilication Files**:  \n`/shared/abc/location_mobility/logging/LM_*` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check to see if the file was produced at the right time and contained the expected number of rows.\n### LM_02_lte\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala table `eea.eea_hour`.  \nThe filename format is `LM_02_lte_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_02_lte_20220301_00002.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\n  graph TD \n  A[Impala: eea.eea_hour] -->| Impala Query | B1[location_mobility.lm_lte_exp]\n  B1 --> |Merge HDFS files to a single file | B2[File: LM_02_lte_yyyyMMdd_000xx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B2 --> |SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 2"
        }
    },
    "119": {
        "page_content": "```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/eea_hour.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    # e.g for 2021-02-22\n    [...] - INFO: end_date=2021-02-22 09:00:00\n    [...] - INFO: max_date=2021-02-22 09:00:00\n    ```\n    If end_date is newer or equal to max_date, it means that table `eea.eea_hour` does not contain new data and therefore there is nothing to be done during this execution.  \n\t\tabc should load data in `eea.eea_hour` table first and then execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \nFor example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 3"
        }
    },
    "120": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_03_smsIn\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.sms_raw_v, osix.osix_sms_raw` that fulfill some conditions.  \nThe filename format is `LM_03_smsIn_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_03_smsIn_20220301_00002.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD\n  A[Impala: sai.sms_raw_v] --> |union all | D[Impala: osix.osix_sms_raw ] --> | Impala Query | B[File: LM_03_smsIn_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 4"
        }
    },
    "121": {
        "page_content": "```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_in_v2.sql` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/sms_in.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that tables `sai.sms_raw_v` or `osix.osix_sms_raw` do not contain new data and therefore there is nothing to be done during this execution.  \n    New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.sms_raw`, updated by TRAFFICA flow (`sai.sms_raw_v` is a view on `sai.sms_raw` table).  \n\t\t- `osix.osix_sms_raw`, responsible abc\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_sms_in_v2_mon.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/sms_in.lock` and execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 5"
        }
    },
    "122": {
        "page_content": "- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \nFor example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag.  \nFor example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_04_smsOut\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.sms_raw_v, osix.osix_sms_raw` that fulfill some conditions.  \nThe filename format is `LM_04_smsOut_yyyyMMdd_xxx.txt` where `xx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_04_smsOut_20220301_00002.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 6"
        }
    },
    "123": {
        "page_content": "**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.sms_raw_v] --> |union all | D[Impala: osix.osix_sms_raw ] -->| Impala Query | B[File: LM_04_smsOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_out_v2.sql` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/sms_out.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that table `sai.sms_raw_v` or `osix.osix_sms_raw` do not contain new data and therefore there is nothing to be done during this execution.  \n    New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.sms_raw`, updated by TRAFFICA flow (`sai.sms_raw_v` is a view on `sai.sms_raw` table).  \n\t\t- `osix.osix_sms_raw`, responsible abc\n- If failed execution's log contains the message\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_sms_out_v2_mon.sh` return no process means the previous execution was forcefully stopped.  \n\t\tDelete the lock file `/shared/abc/location_mobility/run/sms_out.lock` and execute the script.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 7"
        }
    },
    "124": {
        "page_content": "Delete the lock file `/shared/abc/location_mobility/run/sms_out.lock` and execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \nFor example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d') 2>&1\n    ```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_05_voiceInOut\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions. \nThe filename format is `LM_05_voiceInOut_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_05_voiceInOut_20220301_00002.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 8"
        }
    },
    "125": {
        "page_content": "B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] --> | Impala Query | B[File: LM_05_voiceInOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2.sql` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/voice_inout.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution.\n    New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \n\t\t- `osix.osix_voice_raw`, responsible abc",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 9"
        }
    },
    "126": {
        "page_content": "- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \n\t\t- `osix.osix_voice_raw`, responsible abc\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_voice_inout_v2_mon.sh` return no process means the previous execution was forcefully stopped.  \n\t\tDelete the lock file `/shared/abc/location_mobility/run/voice_inout.lock` and execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \nFor example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_06_voiceIn\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 10"
        }
    },
    "127": {
        "page_content": "Under normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions.  \nThe filename format is `LM_06_voiceIn_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_06_voiceIn_20220301_00002.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] -->| Impala Query | B[File: LM_06_voiceIn_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_in_v2.sql` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/voice_in.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 11"
        }
    },
    "128": {
        "page_content": "**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution. \n    New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \n\t\t- `osix.osix_voice_raw`, responsible abc\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_voice_in.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/voice_in.lock` and execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up. For example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 12"
        }
    },
    "129": {
        "page_content": "/shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_07_voiceOut\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions.  \nThe filename format is `LM_07_voiceOut_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_07_voiceOut_20220301_00002.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] -->| Impala Query | B[File: LM_07_voiceOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 13"
        }
    },
    "130": {
        "page_content": "B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_out_v2.sql` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/voice_out.lock`\n**Troubleshooting Steps**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution. \n    New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \n\t\t- `osix.osix_voice_raw`, responsible abc\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_voice_out_v2_mon.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/voice_out.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 14"
        }
    },
    "131": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N 2-hour intervals. This is not needed if 4 or less files were missed in which case the procedure will automatically catch up. For example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_08_cellHist\nUnder normal circumstances this file is produced every day and contains yesterday's data from the Impala table `refdata.rd_cells_v`.  \nThe filename format is `LM_08_cellHist_yyyyMMdd_00001.txt`.  \nFor example, if the file contains data for the 1st of March 2022 the filename will be `LM_08_cellHist_20220301_00001.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_Daily_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 07:00`  \n**Coordinator**: `Location_Mobility_Daily_CO`\n**Master Script**: `000.Location_Mobility_Daily_Oozie_Main.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 15"
        }
    },
    "132": {
        "page_content": "**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 07:00`  \n**Coordinator**: `Location_Mobility_Daily_CO`\n**Master Script**: `000.Location_Mobility_Daily_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_daily.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: refdata.rd_cells_v] -->| Impala Query | B[File: LM_08_cellHist_yyyyMMdd_00001.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/rd_cells.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: max_date=yyyyMMdd and export_date=yyyyMMdd\n    ```\n    If the desired export_date is newer than max_date, it means that table `refdata.rd_cells_v` does not contain new data and therefore there is nothing to be done during this execution.  \n\t\tLoad table `refdata.rd_cells` first and then execute the script.\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_rd_cells.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/rd_cells.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 16"
        }
    },
    "133": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more dates weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N dates.  \nThis is not needed if 4 or less dates were missed in which case the procedure will automatically catch up.  \nFor example if 6 dates were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log 2>&1\n    ```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh -t 20220313 >> /shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log 2>&1\n    ```\n## Router Analytics\nRouter Analytics (RA) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\n- `RA_01_yyyymmdd_00001_x.gz` \n- `RA_02_yyyymmdd_00001_x.gz`\n- `RA_03_yyyymmdd.gz`\nAlong with those, the reconciliation files are produced and sent for each one. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\n``` bash\ncat /shared/abc/location_mobility/logging/RA_BS_01_reconciliation.log\n#e.g for LM_05_voiceInOut and 31st of January 2022\n2022-02-01 09:06:39 RA_01_20220131_00001_[0-5] 20220131 68579162\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 17"
        }
    },
    "134": {
        "page_content": "``` bash\ncat /shared/abc/location_mobility/logging/RA_BS_01_reconciliation.log\n#e.g for LM_05_voiceInOut and 31st of January 2022\n2022-02-01 09:06:39 RA_01_20220131_00001_[0-5] 20220131 68579162\n```\n**Reconcilication Files**: `/shared/abc/location_mobility/logging/RA_BS_*` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check to see if the file was produced at the right time and contained the expected number of rows.\n### RA_01\nUnder normal circumstances this file is produced every day and contains yesterday's data from the Impala table `npce.device_session`. The filename format is `RA_01_yyyymmdd_00001_x.gz` where `x` is a serial number between `1` and `5` as due to size the file is split into subfiles. For example, the files containing data for the 1st of March 2022 will be `RA_01_20220301_00001_[0-5].gz`.\n``` mermaid\n  graph TD\n  A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 07:00`  \n**Coordinator**: `export_Router_Analytics_files_daily`\n**Master Script**: `/shared/abc/location_mobility/run/run_ra_exports.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: npce.device_session] -->| Impala Query | B[File: RA_01_yyyymmdd_00001_x.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_01.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/ra_01.lock`\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 18"
        }
    },
    "135": {
        "page_content": "**Script**: `/shared/abc/location_mobility/run/export_ra_bs_01.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/ra_01.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2021-02-01\n    [...] - INFO: max_date=20220131 and export_date=20220131\n    ```\n    If the desired export_date is newer than max_date, it means that table `npce.device_session` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_ra_bs_01.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_01.lock` and execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 4 or more dates weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N dates. This is not needed if 3 or less dates were missed in which case the procedure will automatically catch up. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 5 dates were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_01.sh --max-files 6 >> /shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 19"
        }
    },
    "136": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_01.sh --max-files 6 >> /shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_01.sh -t 20220313 >> /shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n### RA_02\nUnder normal circumstances this file is produced every day and contains yesterday's data from the Impala table `npce.device_traffic`. The filename format is `RA_02_yyyymmdd_00001_x.gz` where `x` is a serial number between `1` and `5` as due to size the file is split into subfiles. For example, the files containing data for the 1st of March 2022 will be `RA_02_20220301_00001_[0-5].gz`.\n``` mermaid\n  graph TD\n  A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 07:00`  \n**Coordinator**: `export_Router_Analytics_files_daily`\n**Master Script**: `/shared/abc/location_mobility/run/run_ra_exports.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: npce.device_traffic] -->| Impala Query | B[File: RA_02_yyyymmdd_00001_x.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 20"
        }
    },
    "137": {
        "page_content": "**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_02.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/ra_02.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2021-02-01\n    [...] - INFO: max_date=20220131 and export_date=20220131\n    ```\n    If the desired export_date is newer than max_date, it means that table `npce.device_traffic` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_ra_bs_02.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_02.lock` and execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 4 or more dates weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N dates. This is not needed if 3 or less dates were missed in which case the procedure will automatically catch up. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 5 dates were not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 21"
        }
    },
    "138": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_02.sh --max-files 6 >> /shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_02.sh -t 20220313 >> /shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n### RA_03\nUnder normal circumstances this file is produced every Wednesday and contains past week's data from the Impala table `npce.device_dms`. The filename format is `RA_03_yyyymmdd.gz`. For example, the files containing data up to the 2nd of March 2022 will be `RA_03_20220302.gz`.\n``` mermaid\n  graph TD\n  A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every Wednesday at 16:00`  \n**Coordinator**: `export_Router_Analytics_files_to_mediation_ra_03_weekly`\n**Master Script**: `/shared/abc/location_mobility/run/run_ra_03_export.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: npce.device_dms] -->| Impala Query | B[File: RA_03_yyyymmdd.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_03.sh` on `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 22"
        }
    },
    "139": {
        "page_content": "**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_03.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/ra_03.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2021-01-26\n    [...] - INFO: max_date=20220126 and export_date=20220202\n    ```\n    If the desired export_date is newer than max_date, it means that table `npce.device_dms` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_ra_bs_03.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_03.lock` and execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 2 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N executions. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 2 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_03.sh --max-files 2 >> /shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 23"
        }
    },
    "140": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_03.sh --max-files 2 >> /shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 16th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_03.sh -t 20220316 >> /shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n## Application Data Usage Insights\nApplication Data Usage Insights (AUI) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\n- `AUI_01_yyyymmdd_0000x.txt`\nAlong with those, a reconciliation file are produced and sent. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\n``` bash\ncat /shared/abc/location_mobility/logging/AUI_BS_01_reconciliation.log\n#e.g for AUI_01 and 21st of February 2022\n2021-02-22 06:00:09 AUI_01_20210221_00005.txt 20210221 15\n```\n**Reconcilication File**: `/shared/abc/location_mobility/logging/AUI_BS_01_reconciliation.log` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check to see if the file was produced at the right time and contained the expected number of rows.\n### AUI_01\nUnder normal circumstances this file is produced every 4 hours and contains data from 6 to 2 hours ago of the Impala table `npce.abc_apps_raw_events`. The filename format is `AUI_01_yyyymmdd_0000x.txt` where `x` is a serial number between `1` and `6`. For example, the files containing data for the 1st of March 2022 from 00:00 to 04:00 will be `AUI_01_20220301_00001.txt`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 24"
        }
    },
    "141": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie: export_Application_Data_Usage_Insights_files_4_hours] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 4 hours`  \n**Coordinator**: `export_Application_Data_Usage_Insights_files_4_hours`\n**Master Script**: `/shared/abc/location_mobility/run/run_aui_exports.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: npce.abc_apps_raw_events] -->| Impala Query | B[File: AUI_01_yyyymmdd_0000x.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /aui]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/aui`\n**Logs**: ```/shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/location_mobility/run/export_aui.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs\n    date: invalid date \u2018NULL 6 hours ago\u2019\n    ```\n    This means that table `npce.abc_apps_raw_events` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 25"
        }
    },
    "142": {
        "page_content": "- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N 4-hour intervals. This is not needed if 4 or less dates were missed in which case the procedure will automatically catch up. For example if 6 dates were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_aui.sh --max-files 6 >> /shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_aui.sh -t 20220313 >> /shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n## Customer Satisfaction Index\nCustomer Satisfaction Index (CSI) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\n- `CSI_fix_mmddyyyy_wXX.txt`\n- `CSI_mob_mmddyyyy_mmddyyyy.txt`\nAlong with those, a reconciliation file are produced and sent. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\n``` bash\ncat /shared/abc/export_sai_csi/logging/CSI_mob_reconciliation.log\n#e.g for CSI_mob and 30th of January 2022\n2022-01-30 09:02:42  CSI_mob_01242022_01302022.txt  20220124  4223904\n```\n**Reconcilication File**: `/shared/abc/export_sai_csi/logging/CSI_*` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check to see if the file was produced at the right time and contained the expected number of rows.\n### CSI_fix",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 26"
        }
    },
    "143": {
        "page_content": "**Troubleshooting Steps**:\n- Check to see if the file was produced at the right time and contained the expected number of rows.\n### CSI_fix\nUnder normal circumstances this file is produced every 4 hours and contains data from 2 days ago ago of the Impala table `sai.cube_indicators_it`. The filename format is `CSI_fix_mmddyyyy_wXX.txt` where `XX` is a serial number between `1` and `52` for the week of the year. For example, the file containing data for the 2nd of February 2022 which belongs to the 5th week of the year, will be `CSI_fix_02042022_w05.txt`.\n``` mermaid\n  graph TD\n  A[Oozie: export_CSI_fix_and_mobile_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 7:00`  \n**Coordinator**: `export_CSI_fix_and_mobile_daily`\n**Master Script**: `/shared/abc/location_mobility/run/run_csi_exports_daily.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.cube_indicators_it] -->| Impala Query | B[File: CSI_fix_mmddyyyy_wXX.txt <br> Server: un2.bigdata.abc.gr <br> Path: /shared/abc/export_sai_csi/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /csi]\n```\n**User**: `mtuser`\n**Local path**: `/shared/abc/export_sai_csi/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/csi`\n**Logs**: ```/shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/export_sai_csi/run/export_csi_fix.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2022-01-10\n    Problem with 20220108.\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 27"
        }
    },
    "144": {
        "page_content": "- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2022-01-10\n    Problem with 20220108.\n    ```\n    This means that table `sai.cube_indicators_it` does not contain new data and therefore there is nothing to be done during this execution. Load table `brond.cube_indicators` first and then execute the script.\n**Ndefs**:\n- If one date was missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If you need to export file for a specific date execute the script with argument `<yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/export_sai_csi/run/export_csi_fix.sh 20220313 >> /shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log 2>&1\n    ```\n### CSI_mob\nUnder normal circumstances this file is produced every day and contains data for the current week of the Impala table `sai.sub_aggr_csi_it`. The filename format is `CSI_mob_mmddyyyy_mmddyyyy.txt` where the first date is the last loaded Monday and the second the current date. For example, the file containing data for the 2nd of February 2022 will be `CSI_mob_01312022_02022022.txt`.\n``` mermaid\n  graph TD\n  A[Oozie: export_CSI_fix_and_mobile_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 7:00`  \n**Coordinator**: `export_CSI_fix_and_mobile_daily`\n**Master Script**: `/shared/abc/location_mobility/run/run_csi_exports_daily.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 28"
        }
    },
    "145": {
        "page_content": "**Master Script**: `/shared/abc/location_mobility/run/run_csi_exports_daily.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.sub_aggr_csi_it] -->| Impala Query | B[File: CSI_mob_mmddyyyy_mmddyyyy.txt <br> Server: un2.bigdata.abc.gr <br> Path: /shared/abc/export_sai_csi/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /csi]\n```\n**User**: `mtuser`\n**Local path**: `/shared/abc/export_sai_csi/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/csi`\n**Logs**: ```/shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/export_sai_csi/run/export_csi_mob_daily.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2022-01-10\n    Problem with 20220108.\n    ```\n    This means that table `sai.sub_aggr_csi_it` does not contain new data and therefore there is nothing to be done during this execution. Load table `sai.sub_aggr_csi_it` first and then execute the script.\n**Ndefs**:\n- If one date was missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If you need to export file for a specific date execute the script with argument `<yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/export_sai_csi/run/export_csi_mob_daily.sh 20220313 >> /shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "trustcenter_flows.md - Part 29"
        }
    },
    "146": {
        "page_content": "---\ndocument_owner: \"Intra team\"\nsystems_involved:\n  - TeMIP\n  - BigStreamer\n  - Kudu\n  - Impala\n  - Wildfly\n  - Oozie\n  - Oracle\n  - Sqoop\nscheduling:\n  - oozie\ndata_sources:\n  - TeMIP server (live alarms)\n  - Oracle TeMIP DB (archived alarms)\ntarget_tables:\n  - temip.temip_kudu_active_alarms\n  - temip.temip_kudu_terminated_alarms\n  - temip.temip_kudu_historic_events\n  - temip.temip_impala_terminated_alarms\n  - temip.temip_impala_historic_events\n  - temip.temip_alert_table\nscripts_location:\n  - hdfs:/user/temip/\n  - /shared/abc/temip_oozie_production_scripts/\n  - /usr/icom/scripts/\nresponsible_users:\n  - temip\nmonitored: false\nalerts_handling: \"Email alerts if no alarms received (via Alert Mail flow)\"\nsummary: >\n  This document details the end-to-end flow for ingesting TeMIP alarms into BigStreamer, including the Java-based live feed, Oozie-scheduled data migration \n  from Kudu to Impala, alert notification checks, manual recovery procedures from Oracle in case of data loss, and administration of the Wildfly server.\n---\n# TeMIP\n## Overview\nThe `abc TeMIP alarms live feed to BigStreamer` application is a Java application hosted on a Wildfly application server. The objective of the application is to receive and store (in near real time) the TeMIP alarms (from specific TeMIP Operation Contexts) into the BigStreamer\u2122 ecosystem. The `Apache Kudu` storage engine was selected in order to achieve near real time CRUD operations (Create, Read, Update, Delete). The `Apache Impala` is used for extended data retention (6 months). The `Apache Oozie` scheduler  is used in order to automatically run the necessary scripts.\n- **Ndef:** All the needed **passwords** can be found [**here**](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx).\n## Flows\nThe `TeMIP Flow` consists of 4 components/flows:\n1. Initialization/Synchronization flow\n1. Main Application flow\n1. Move Kudu to Impala flow\n1. Alert Mail flow\n### Main Application",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 1"
        }
    },
    "147": {
        "page_content": "## Flows\nThe `TeMIP Flow` consists of 4 components/flows:\n1. Initialization/Synchronization flow\n1. Main Application flow\n1. Move Kudu to Impala flow\n1. Alert Mail flow\n### Main Application\nHandles live ingestion of alarms from TeMIP server into Kudu tables via a Java app hosted on Wildfly.\nThe `Main Application Flow` contains our `TeMIP application` deployed to the `Wildfly Server` which receives the TeMIP alarms and stores them into Kudu tables.\n``` mermaid\n  flowchart TD\n  A[TeMIP Server] \n  B[Wildfly Server]\n  A --> |Sends TeMIP alarms| B\n  B --> |Stores TeMIP alarms| D[(Kudu Storage Engine)]\n  D --- E[Kudu: temip.temip_kudu_active_alarms]\n  D --- Z[Kudu: temip.temip_kudu_terminated_alarms]\n  D --- K[Kudu: temip.temip_kudu_historic_events]\n  style A fill: #45b39d\n```\n- **TeMIP Server**\n  - **Host:** `999.999.999.999`\n  - **Port:** `7180`\n- **Wildfly Server**\n  - **Servers:**\n    - `temip1 (999.999.999.999)` Standby Server\n    - `temip2 (999.999.999.999)` Active Server\n  - **User:** `temip`\n  - **Installation Path:** `/opt/wf_cdef_temip/`\n  - **Deployments Path:** `/opt/wf_cdef_temip/standalone/deployments`\n  - **Application Logs:** `/opt/wf_cdef_temip/standalone/log/server.log`\n  - **Access Logs:** `/opt/wf_cdef_temip/standalone/log/access.log`\n  - **Configuration:** `/opt/wf_cdef_temip/standalone/configuration/BigStreamer/config/`\n    - **File:** `temip.properties`\n**Alerts:**\n- **Mail executed by [Alert Mail](#alert-mail)**\n  - **Subject:** `\"[ Temip ] No alarms available.\"`\n  - **Body:** `\"There are no Temip alarms  available for the last hour. Corrective action may be needed.\"`\n**Troubleshooting Steps:**\n1. Check `logs` (application and access) with `temip-tailog` for any `ERROR` message that can occur.  \nIf TeMIP Server is running correctly, we should see lines like the following:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 2"
        }
    },
    "148": {
        "page_content": "1. Check `logs` (application and access) with `temip-tailog` for any `ERROR` message that can occur.  \nIf TeMIP Server is running correctly, we should see lines like the following:  \n`INFO [com.jkl.bigstreamer.abc.temip.core.service.TemipService] (default task-173) Counter= 3064020, handle= 968, batchName= batch_2, timesRestartedFromLastSync= 1, aoExtractDataList size= 1`\n1. Check if `TeMIP Server` is up by executing `ping 999.999.999.999`.\n1. Contact a `TeMIP admin` to see if there are any server side related issues\n### Initialization/Synchronization\nEnsures communication between Wildfly and TeMIP server is established post-deployment via Oozie.\nThe `Initialization/Synchronization Flow` consists of an OOZIE Coordinator called `TeMIP_Synchronization_CO`. The coordinator is responsible for establishing the **connection** and **communication** of the `Wildfly Server` (containing our TeMIP Application) with the `TeMIP Server`.\nEvery time the `Main Application` is successfully deployed to `Wildfly Server` or gets restarted, this `coordinator` **must be run manually** to initiate the above procedure, it does not happen automatically. If the `Wildfly Server` is up and running, the `coordinator` executes on specific days of every month to perform maintenance tasks.\n``` mermaid\n  flowchart TD\n  A[OOZIE Server] -->|SHELL Action as user temip| B[un-vip.bigdata.abc.gr <br> User: temip]\n  B -->|REST message protocol| C[Main Application]\n  C <--> |SOAP message protocol| D[TeMIP Server]\n  style C fill: #45b39d\n```\n- **Oozie Coordinator**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_Synchronization_CO`\n    - **Execution:** `2,7,12,17,22,27 of every month at 03:00 local time`\n    - **Approximate Duration:** `45 minutes`\n    - **Workflow:** `TeMIP_Synchronization_WF`\n      - **Master Script:** `hdfs:/user/temip/100.TeMIP_Synchronization_Oozie_Main.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 3"
        }
    },
    "149": {
        "page_content": "- **Approximate Duration:** `45 minutes`\n    - **Workflow:** `TeMIP_Synchronization_WF`\n      - **Master Script:** `hdfs:/user/temip/100.TeMIP_Synchronization_Oozie_Main.sh`\n      - **Remdef Script:** `un-vip:/shared/abc/temip_oozie_production_scripts/101.temip_synchronization_Main.sh`\n      - **Server:** `un-vip.bigdata.abc.gr`\n      - **SSH User:** `temip`\n      - **Logs:** `un-vip:/shared/abc/temip_oozie_production_scripts/log/102.temip_synchronization.$(date '+%Y%m%d').log`\n**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**\n1. Check logs for any errors\n1. If workflow `TeMIP_Synchronization_WF` has been run manually, login to `Hue` with `temip` user `after 45 minutes` and execute the following `impala query` editor: `select * from temip.temip_kudu_configs`.  \nIt should return `15 rows`. If not, re run the `TeMIP_Synchronization_WF` workflow\n### Move Kudu to Impala\nDaily Oozie flow to offload older alarm data from Kudu to Impala for long-term storage.\nThe `Move Kudu to Impala` flow consists of a coordinator called `TeMIP_kudu_2_Impala_CO` which executes once a day and is responsible for moving the alarms from kudu to the equivalent impala table.\n``` mermaid\n flowchart TD\n  A[OOZIE Job] \n  Z[Kudu: temip.temip_kudu_terminated_alarms]\n  K[Kudu: temip.temip_kudu_historic_events]\n  A --> Z\n  A --> K\n  Z --> |Move older alarms to impala|H[Impala: temip.temip_impala_terminated_alarms]\n  K --> |Move older events to impala|L[Impala: temip.temip_impala_historic_events]\n```\n- **Oozie Coordinator**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_kudu_2_Impala_CO`\n    - **Execution:** `everyday at 06:00 local time`\n    - **Approximate Duration:** `15 minutes`\n    - **Workflow:** `TeMIP_kudu_2_Impala_WF`\n      - **Script:** `hdfs:/user/temip/temip_kudu_to_impala.sh`\n      - **Logs:** `Through Oozie Job in HUE`\n**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 4"
        }
    },
    "150": {
        "page_content": "- **Script:** `hdfs:/user/temip/temip_kudu_to_impala.sh`\n      - **Logs:** `Through Oozie Job in HUE`\n**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**\n1. Check that workflow `TeMIP_kudu_2_Impala_WF` runs successfully. This can be done through accessing `Hue` with `temip` user and selecting `Jobs`. Then filter the jobs with `user:temip` and look for a `job` with the below properties:\n    - **Name:**  `oozie:launcher:T=shell:W=temip_kudu_to_impala:A=shell-661a:*`\n    - **Type:** `Oozie Launcher`\n    - **Execution Time:** `06:00 AM`\n    Select the desired shell execution, view its logs and search for any ERRORS.\n1. After the root cause of the problem has been identified, re-rerun the failed execution. This can be done through accessing `Hue` with `temip` user.\n### Alert Mail\nHourly job that checks for gaps in alarm reception and sends notifications if needed.\nThe `Alert Mail` flow consists of a coordinator called `TeMIP_Alert_Mail_CO` which runs every hour and checks if the application receives any alarms from the TeMIP Server. The `TeMIP Server` sends alarms continuously. If in the last hour, the application has not detected any new alarms, an email is sent to jkl Engineers to inform that there might be a issue. The check is performed by comparing the number of alarms from the previous execution stored in `temip.temip_alert_table` table with the current one.\n``` mermaid\n flowchart TD\n  A[OOZIE Server] -->|SHELL Action as user temip| B[un-vip.bigdata.abc.gr <br> User: temip]\n  B --> C[201.temip_alert_mechanism_Main.sh]\n```\n- **Oozie Scheduler**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_Alert_Mail_CO`\n    - **Execution:** `every hour`\n    - **Workflow:** `TeMIP_Alert_Mail_WF`\n      - **Master Script:** `hdfs:/user/temip/200.TeMIP_Alert_Mail_Oozie_Main.sh`\n      - **Remdef Script:** `un-vip/shared/abc/temip_oozie_production_scripts/201.temip_alert_mechanism_Main.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 5"
        }
    },
    "151": {
        "page_content": "- **Master Script:** `hdfs:/user/temip/200.TeMIP_Alert_Mail_Oozie_Main.sh`\n      - **Remdef Script:** `un-vip/shared/abc/temip_oozie_production_scripts/201.temip_alert_mechanism_Main.sh`\n      - **Server:** `un2.bigdata.abc.gr`\n      - **SSH User:** `temip`\n      - **Logs:** `un-vip:/shared/abc/temip_oozie_production_scripts/log/202.temip_alert_mechanism.$(date '+%Y%m%d').log`\n**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**\n1. Check for any failed executions. This can be done through accessing `Hue` with `temip` user\n1. Check for any cluster related problems during the failed execution.\n## Manual Actions\n### Restart Wildfly Server\nManual steps to safely restart the application server and coordinate Oozie flows.\n---\n**_Ndef:_** TEMIP runs only in one node. Second node is in standby mode.\n1. **Only if requested by TeMip Administrators** In `Hue` with `temip` user, clear table `temip.temip_kudu_active_alarms` by executing `delete from temip.temip_kudu_active_alarms;`\n1. `Shutdown Wildfly Server`\n    1. Login as `temip` user in `temip2`.\n    1. Execute `/bin/bash` and then run `temip-stop` to stop wildfly and check logs with `temip-tailog`.\n    1. Suspend the temip Coordinators (`Hue` with `temip` user):\n        - `TeMIP_kudu_2_Impala_CO`\n        - `TeMIP_Synchronization_CO`\n        - `TeMIP_Alert_Mail_CO`\n    1. Clear table `temip.temip_kudu_active_alarms` by executing `delete from temip.temip_kudu_active_alarms;` in `Hue` as `temip` user \n1. `Startup Wildfly Server`\n    1. Login as `temip` user in `temip2`.\n    1. Start wildfly by executing `/bin/bash` and then running `temip-start` and check logs with `temip-tailog`.\n    1. Resume the three temip coordinators (`Hue` with `temip` user):\n        - `TeMIP_kudu_2_Impala_CO`\n        - `TeMIP_Synchronization_CO`\n        - `TeMIP_Alert_Mail_CO`\n    1. Workflows:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 6"
        }
    },
    "152": {
        "page_content": "1. Resume the three temip coordinators (`Hue` with `temip` user):\n        - `TeMIP_kudu_2_Impala_CO`\n        - `TeMIP_Synchronization_CO`\n        - `TeMIP_Alert_Mail_CO`\n    1. Workflows:\n        - The two workflows `TeMIP_kudu_2_Impala_WF` and `TeMIP_Alert_Mail_WF` should run automatically when oozie scheduler detects that it was suspended.\n        - The third workflow `TeMIP_Synchronization_WF` should be run manually. Specifically, `login` as `temip` to `Hue` and run manually with no parameters. Make sure that it will not also be executed by the corresponding coordinator.\n    1. At `HUE` with `temip` user, open the impala editor and execute the following command in order to refresh e-mail alert script:  \n      `insert overwrite temip.temip_alert_table values(1);`\n1. `Sanity Checks`\n    1. Login as `temip` user in `temip2`\n    1. Check `logs` with `temip-tailog` and search for any errors.\n    1. After `45 minutes`, login to `Hue` with `temip` user and execute the following impala query editor:  \n    `select * from temip.temip_kudu_configs`  \n      It should return 15 rows. If not, `re run` the `TeMIP_Synchronization_WF` workflow.\n    1. Login to `Hue` with `temip` and perform the below impala queries with a temip admin (Ioanna Bekiari) in order to established if everything is running okay. If the results are the same or really similar, the synchronization is considered successful.\n        ``` sql\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ENM_BASEBAND%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ERICOSS2G%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ERICOSS-LTE%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%NOKIA3G%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%.ATHENS-OC%';",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 7"
        }
    },
    "153": {
        "page_content": "select count(*) from temip_kudu_active_alarms where operation_context like '%NOKIA3G%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%.ATHENS-OC%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%NNM_FIXED%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%U2000-OC%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%1350OMS%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%HUAWEI_IMS%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%AUMS-OC%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '.def.A5529.A5520_AMS-OC';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%2000_DSLAM%';\n        ```\n### Load Terminated Alarms from TeMIP Oracle Database\nProcedure to backfill alarm data in case of outages by extracting historical records from the Oracle TeMIP DB.\n#### In case of data loss\n---\nIn case there is a loss of alarms for any reason, eg our application or TeMIP outage, we may be asked to load historical data directly from TeMIP Oracle Database into our terminated alarms table. In order to start this operation we must wait for some days, so that all alarms are transferred to the Oracle table. Whole procedure is described in detail below:\n1. Wait `7 days` from the day you want to `re-load` in order for terminated alarms to be refreshed in Oracle table.\n1. Connect as `temip` in `un2` and run `ping 999.999.999.999`, in order to see if `Temip Server` is up and running.\n1. From impala shell `secimp`(as `temip` in `un2`) or `Hue`(as `temip`):\n\t1. Check missing partitions in `temip.temip_kudu_terminated_alarms` and `temip.temip_impala_terminated_alarm` by running  \n  `select count(*), par_dt from <database>.<table> where par_dt='<partition>' group by par_dt;` on both tables.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 8"
        }
    },
    "154": {
        "page_content": "`select count(*), par_dt from <database>.<table> where par_dt='<partition>' group by par_dt;` on both tables.  \n\t\tWe receive TeMIP alarms every day. So if there are general ERRORS(logs) or we have partitions containing less alarms than usual(eg. count), it suggests that there might be problems with the TeMIP server or our application and in need of investigating.\n\t1. Delete existing wrong partitions that overlap with the required interval, either from kudu table `temip.temip_kudu_terminated_alarms` or from impala table `temip.temip_impala_terminated_alarms`.\n\t\t- If wrong partitions are contained in kudu table (only 10 most recent days are in kudu), do:  \n`ALTER table temip.temip_kudu_terminated_alarms DROP IF EXISTS RANGE PARTITION 'v1'<= values < 'v2';`,   \nwhere v1 and v2 the required interval.\n\t\t- If wrong partitions are contained in impala table (10 days past the current date), do:  \n`ALTER table temip.temip_impala_terminated_alarms DROP IF EXISTS PARTITION (par_dt='v');`,   \nwhere v is the wrong partition.\n\t1. In order to not transfer again old data that have remained, run `truncate table temip.temipdb_term_alarms_load_par;`.\n1. As `temip` in `un2` Run the script with arguments\n    ``` bash\n    sh /usr/icom/scripts/Sqoop_Oracle_HDFS_Impala_Load_TeMIP_v832.sh \"temipaharchi.alarmobject0\" identifier 30 <current-pardt> \"terminationtimestamp>='v1' and terminationtimestamp<'v2'\"\n    ```\n    - **current-pardt:** is the `today` par_dt, the day the script is run. Format `YYYYMMDD`. It has no significant value to the internal process.\n    - **v1, v2:** Use values for `terminationtimestamp` that are between the start and end of the interval you want to load from Oracle. Format `01-MAY-22`.\n    Example for day 20220501:\n    ``` bash\n    sh /usr/icom/scripts/Sqoop_Oracle_HDFS_Impala_Load_TeMIP_v832.sh \"temipaharchi.alarmobject0\" identifier 30 20230104 \"terminationtimestamp>='01-MAY-22' and terminationtimestamp<'02-MAY-22'\"\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 9"
        }
    },
    "155": {
        "page_content": "```\n    The data will be **loaded** into table `temip.temipdb_term_alarms_load_par`.\n1. From impala shell `secimp`(as `temip` in `un2`) or `Hue`(as `temip`):\n    1. Refresh the staging table in the impala shell:  \n      `refresh temip.temipdb_term_alarms_load_par;`\n    1. Run the following sql command, which transfers automatically all data to the right partition (par_dt) of temip.temip_impala_terminated_alarms:\n        ``` sql\n        insert overwrite temip.temip_impala_terminated_alarms partition (par_dt)\n        select concat('OPERATION_CONTEXT ',split_part(upper(ocname),':',2),' ALARM OBJECT ', identifier) outentityspec,\n        null last_Modification_Timestamp,\n        split_part(upper(ocname),':',2) operation_context,\n        cast(identifier as bigint) identifier,\n        \"Terminated-Oracle\" state,\n        \"Closed\" problem_status,\n        case when clearancereportflag = \"1\" then true else false end clearance_report_flag,\n        acknowledgementuseride as acknowledgement_user_identifier,\n        handledby as handled_by,\n        closedby as closed_by,\n        handleduseridentifier as handled_user_identifier,\n        releaseuseridentifier as release_user_identifier,\n        closeuseridentifier as close_user_identifier,\n        terminationuseridentif as termination_user_identifier,\n        acknowledgementtimesta as acknowledgement_time_stamp,\n        handletimestamp as handle_time_stamp,\n        closetimestamp as close_time_stamp,\n        terminationtimestamp as termination_time_stamp,\n        releasetimestamp as release_time_stamp,\n        null automatic_terminate_on_close,\n        creationtimestamp as creation_timestamp,\n        archivetimestamp as archive_time_stamp,\n        clearancetimestamp as clearance_time_stamp,\n        null previous_state,\n        managedobject as managed_object,\n        targetentities as target_entities,\n        --targetentities60512 as target_entities,\n        alarmtype as alarm_type,\n        eventtime as event_time,",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 10"
        }
    },
    "156": {
        "page_content": "managedobject as managed_object,\n        targetentities as target_entities,\n        --targetentities60512 as target_entities,\n        alarmtype as alarm_type,\n        eventtime as event_time,\n        probablecause as probable_cause,\n        securityalarmcause as security_alarm_cause,\n        specificproblems as specific_problems,\n        --specificproblems (id)-8eloume to join kai edw,\n        null backed_up_status,\n        backupobject as backup_object,\n        trendindication as trend_indication,\n        thresholdinfo as threshold_info,\n        cast(notificationidentifier as bigint) notification_identifier,\n        correlnotifinfo as correl_notif_info,\n        monitoredattributes as monitored_attributes,\n        proposedrepairactions as proposed_repair_actions,\n        null additional_information,\n        domain as domain,\n        securityalarmdetector as security_Alarm_Detector,\n        null service_User,\n        null service_Provider,\n        ocname as oc_Name,\n        cast(parentalarmobject as bigint) parent_alarm_object,\n        null severity_changed_time_stamp,\n        alarmcomment as alarm_comment,\n        agentalarmidentifier as agent_alarm_identifier,\n        agententity as agent_entity,\n        perceivedseverity as perceived_Severity,\n        additionaltext as additional_Text,\n        alarmobjectoperatorno as alarm_Object_Operator_Ndef,\n        originalseverity as original_Severity,\n        originaleventtime as original_Event_Time,\n        0 useridentifier,\n        usertext as user_Text,\n        cast(satotal as bigint) sa_total,\n        null deleted,\n        from_timestamp(to_timestamp(terminationtimestamp,'yyyy-MM-dd HH:mm:ss'),'yyyyMMdd') as par_dt\n        --,*\n        from temip.temipdb_term_alarms_load_par a;\n        ```\n        **Ndef:** There are comments that might affect the query if not handled carefully.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 11"
        }
    },
    "157": {
        "page_content": "--,*\n        from temip.temipdb_term_alarms_load_par a;\n        ```\n        **Ndef:** There are comments that might affect the query if not handled carefully. \n    1. Check if data transferred successfully by running the command below and comparing the result with the number of retrieved records in the logs produced by the script.  \n      ``` sql\n      SELECT par_dt, count(*) FROM temip.temip_impala_terminated_alarms where par_dt > 'v';\n      Eg. Logs: INFO mapreduce.ImportJobBase: Retrieved 1113488 records.  \n      +----------+  \n      | count(*) |  \n      +----------+  \n      | 1113488  |  \n      +----------+  \n      ```\n## TeMIP Wildfly Server\nIn order to change the `logging level` of any of the `categories` of the `TeMIP Wildfly Server` access the `WildFly Management`.\n### Logging\nIn order to change the `logging level` of any of the categories of the `TeMIP Wildfly Server` access the `WildFly Management`.\n- [Home URL](https://999.999.999.999:8888/)\n- **User:** `admin`\n- **Password:** [abc-syspasswd.kdbx](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/abc-syspasswd.kdbx)\n#### Change logging level\n1. Login to the `WildFly Management` by following the [home url](https://999.999.999.999:10213/console/App.html#home), select `Configuration` ~> `Subsystems` ~> `Logging` and select `View`.\n1. Select `LOG CATEGORIES`\n1. From the table select the `category` you desire to change its **logging level** e.g. com.jkl.bigstreamer.abc.temip is our main TeMIP App.\n1. Press the `Edit` option below the table\n1. Select the desired `Level` between the given options. \n1. Click `Save`\n1. Ssh from `un2` with `temip` to `temip1` or `temip2` with `temip` user and check that in the file `/opt/wf_cdef_temip/standalone/configuration/standalone-full.xml` the level of the previously configured logger has changed successfully. It should be configured **automatically**. \n## Useful Links\n- [TeMIP Dir](https://metis.ghi.com/obss/bigdata/abc/temip)",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 12"
        }
    },
    "158": {
        "page_content": "## Useful Links\n- [TeMIP Dir](https://metis.ghi.com/obss/bigdata/abc/temip)\n- [TeMIP Application Deployment](https://metis.ghi.com/obss/bigdata/abc/temip/temip-devops/-/wikis/Application-Deployment)\n- [TeMIP Wiki](https://metis.ghi.com/obss/bigdata/abc/temip/temip-devops/-/wikis/home)\n#tags:\n# - temip\n# - kudu\n# - impala\n# - alarms\n# - wildfly\n# - oozie\n# - synchronization\n# - alerting\n# - oracle\n# - sqoop\n# - impala tables\n# - kudu tables\n# - alarm ingestion\n# - terminated alarms\n# - temip recovery\n# - active alarms\n# - logging level\n# - coordinator workflows\n# - manual actions",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "temip.md - Part 13"
        }
    },
    "159": {
        "page_content": "---\ntitle: IPVPN Application Flow  \ndescription: Comprehensive architecture and pipeline for collecting, transforming, and exporting IP VPN network KPIs (CPU, memory, QoS, interface metrics) from NNM, SNMP, and UI-managed MySQL configurations into BigStreamer and the SQM system.  \nsystem: BigStreamer  \ncomponent: IPVPN  \njob_name: IPVPN_KPIs_Collection  \nsource_systems:  \n  - HP Network Node Manager (NNM)  \n  - SNMP Custom Poller  \n  - Wildfly + MySQL (TrustCenter)  \ndestination_tables:  \n  - bigcust.nnm_ipvpn_componentmetrics_hist  \n  - bigcust.nnmcp_ipvpn_slametrics_hist  \n  - bigcust.perf_interfacemetrics_ipvpn_hist  \n  - bigcust.customer_pl  \n  - bigcust.net_to_sm_customer  \n  - bigcust.customer_sla_config_ipvpn  \n  - bigcust.component_metrics  \n  - bigcust.sla_metrics  \n  - bigcust.interface_metrics  \nschedule:  \n  component_metrics_ingestion: every 5 minutes  \n  sla_metrics_ingestion: every 5 minutes  \n  interface_metrics_ingestion: every 2 minutes  \n  postgres_config_sync: daily at 3:00 UTC  \n  mysql_config_sync: daily at 4:00 UTC  \n  kpi_exports_to_sqm: every 5 minutes  \n  output_config_xml: every 4 hours  \nsource_transfer_protocols:  \n  - SFTP  \n  - Sqoop  \n  - Spark  \n  - Flume + Morphline  \n  - SSH trigger  \nkey_users:  \n  - ipvpn  \n  - intra  \n  - trustuser  \nkey_hosts:  \n  - un2.bigdata.abc.gr  \n  - nnmprd01.abc.gr  \n  - nnmdis01.abc.gr  \n  - unekl1.bigdata.abc.gr  \n  - unekl2.bigdata.abc.gr  \n  - unc1.bigdata.abc.gr  \n  - unc2.bigdata.abc.gr  \nsftp_paths:  \n  - /var/opt/OV/shared/nnm/databases/custompoller/export/final  \n  - /home/custompoller/ipvpn/out  \n  - /home/custompoller/nnm_interface_metrics  \nintermediate_paths:  \n  - /data/1/nnm_components_LZ/spooldir  \n  - /data/1/nnm_custompoller_ipvpn_LZ  \n  - /shared/abc/ip_vpn/interfaces_flow/repo  \nlandingzone_paths:  \n  - /ez/landingzone/nnm_custompoller_ipvpn/raw  \n  - /shared/abc/ip_vpn/out/saismpm  \nimpala_tables:  \n  - bigcust.nnm_ipvpn_componentmetrics_hist",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 1"
        }
    },
    "160": {
        "page_content": "landingzone_paths:  \n  - /ez/landingzone/nnm_custompoller_ipvpn/raw  \n  - /shared/abc/ip_vpn/out/saismpm  \nimpala_tables:  \n  - bigcust.nnm_ipvpn_componentmetrics_hist  \n  - bigcust.nnmcp_ipvpn_slametrics_hist  \n  - bigcust.perf_interfacemetrics_ipvpn_hist  \n  - bigcust.customer_sla_config_ipvpn  \n  - bigcust.customer_pl  \n  - bigcust.net_to_sm_customer  \n  - bigcust.pe_interfaces  \n  - bigcust.sla_configurations  \n  - bigcust.component_metrics  \n  - bigcust.sla_metrics  \n  - bigcust.interface_metrics  \nmysql_tables:  \n  - trustcenter.IPVPNCUUI_CUSTOMER_SLA_CONFIG  \n  - trustcenter.IPVPNSLA_CUSTOMER_PL  \n  - trustcenter.IPVPNSLA_NET_TO_SM_CUSTOMER  \n  - trustcenter.IPVPNSLA_PE_INTERFACES  \npostgre_tables:  \n  - nnm.nms_iface  \n  - nnm.nms_ip_addr  \n  - nnm.nms_node  \nscripts:  \n  - load_data.pl  \n  - spark-submit.sh  \n  - update_pl_customer.sh  \n  - update_net_to_sm_customer.sh  \n  - update_customer_sla_config.sh  \n  - update_pe_interfaces.sh  \n  - initiate_export_components.sh  \n  - initiate_export_sla.sh  \n  - initiate_export_interfaces.sh  \n  - populate_components_metrics_table.sh  \n  - populate_sla_metrics_table.sh  \n  - populate_interface_metrics_table.sh  \n  - sftp.sh (XML Export)  \nkeywords:  \n  - ipvpn  \n  - vpn  \n  - sla  \n  - kpi  \n  - snmp  \n  - nnm  \n  - wildfly  \n  - trustcenter  \n  - impala  \n  - hdfs  \n  - spark  \n  - sqoop  \n  - cron  \n  - sftp  \n  - flume  \n  - morphline  \n  - postgres  \n  - mysql  \n  - haproxy  \n  - interface metrics  \n  - qos  \n  - memory  \n  - cpu  \n  - sqm  \n  - perf metrics  \n  - realtime kpis  \n  - sla config  \n  - oozie  \n  - bigcust  \n  - un2  \n  - pipeline monitoring  \n  - export retry  \n---\n# IPVPN\n## Overview\nDescribes the architecture and flows that support the collection, processing, and storage of network KPIs for VPN customers.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 2"
        }
    },
    "161": {
        "page_content": "- pipeline monitoring  \n  - export retry  \n---\n# IPVPN\n## Overview\nDescribes the architecture and flows that support the collection, processing, and storage of network KPIs for VPN customers.\nIP VPN is an application that receives metrics about the network quality for the abc VPN Customers and produces Key Performance Indicators (KPIs) regarding Memory Usage, CPU Load, Provider Edge (PE) Interface, PE Branch Availability and PE Branch Quality of Service (QoS), which are collected and processed by the Service Management (SM) system of abc.\nThis is a document that will assist on support. Business documents can be found [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ipvpnsla-customers-devops/-/tree/master/docs).\n## Input Performance Data\nOutlines how raw performance metrics are collected from NNM and SNMP systems, transferred to BigStreamer, and ingested into Impala.\nThere are two source systems, HP Network Node Manager (NNM) and SNMP Custom Poller application, that poll periodically network elements and produces raw files with the instantaneous metrics. These files are then parsed by one or more procedures. Finally they are loaded to BigStreamer cluster in Impala tables. There are 3 flows of input performance data that are being described in detail below.\n### Component Metrics\nCovers memory and CPU usage metrics collection from NNM and their ingestion via Flume.\n#### Creation of raw files\nThe source system in this case is NNM. For high availability there are two instances of NNM on two separate servers and they operate in active-standby fashion. The whole infrastructure is entirely managed by abc. The raw files produced contain component metrics for CPU load and memory usage of the network elements and are stored in local paths on those servers.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 3"
        }
    },
    "162": {
        "page_content": "``` mermaid\n  graph TD\n  A[Service: NNM <br> Host: nnmprd01.abc.gr] --> B[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz <br> Path: /var/opt/OV/shared/nnm/databases/custompoller/export/final <br> Host: nnmprd01.abc.gr]\n  C[Service: NNM <br> Host: nnmdis01.abc.gr] -.->|Stopped| D[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz <br> Path: /var/opt/OV/shared/nnm/databases/custompoller/export/final <br> Host: nnmdis01.abc.gr]\n```\n- **Path**: `/var/opt/OV/shared/nnm/databases/custompoller/export/final` on `nnmprd01.abc.gr` or `nnmdis01.abc.gr`\n- **File**: `BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz`\n- **Schedule**: `Every 5 minutes`\n#### Transfer to BigStreamer nodes\nA Perl script executed periodically by user `ipvpn` collects the raw files locally via passwordless SFTP, decompresses them and moves them to a local directory.\n``` mermaid\n  graph TD\n  A[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz <br> Path: /var/opt/OV/shared/nnm/databases/custompoller/export/final <br> Host: nnmprd01.abc.gr] -->|SFTP| B[Path: /data/1/nnm_components_LZ <br> Host: un2.bigdata.abc.gr <br> User: ipvpn]\n  B -->|Decompress/Move|C[Path: /data/1/nnm_components_LZ/spooldir <br> Host: un2.bigdata.abc.gr <br> User: ipvpn]\n  D[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz <br> Path: /var/opt/OV/shared/nnm/databases/custompoller/export/final <br> Host: nnmdis01.abc.gr] -.->|Stopped| B\n```\n- **User**: `ipvpn`\n- **Scheduler**: `Cron`\n- **Schedule**: `Every minute`\n- **SFTP Path**: `/var/opt/OV/shared/nnm/databases/custompoller/export/final`\n- **SFTP user**: `custompoller`\n- **Intermediate Path**: `/data/1/nnm_components_LZ`\n- **Destination Path**: `/data/1/nnm_components_LZ/spooldir`\n- **Logs**: /shared/abc/ip_vpn/log/nnm_component_metrics.cron.`date +%Y%m%d`.log\n- **Configuration**: `/shared/abc/ip_vpn/DataParser/scripts/transferlist/cpu_mem.trn`\n- **Script**: `/shared/abc/ip_vpn/DataParser/scripts/load_data.pl` on `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 4"
        }
    },
    "163": {
        "page_content": "- **Configuration**: `/shared/abc/ip_vpn/DataParser/scripts/transferlist/cpu_mem.trn`\n- **Script**: `/shared/abc/ip_vpn/DataParser/scripts/load_data.pl` on `un2.bigdata.abc.gr`\n#### Load to BigStreamer cluster\nDecompressed files are read on the spot by the Flume agent running on `un2.bigdata.abc.gr`. It first parses them using Morphline and then loads them into an Impala table.\n``` mermaid\n  graph TD\n  A[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv <br> Path: /data/1/nnm_components_LZ/spooldir <br> Host: un2.bigdata.abc.gr]\n  B[Morphline Parsing]\n  C[Impala Table: bigcust.nnm_ipvpn_componentmetrics_hist]\n  A -->|Read| B\n  B -->|Load| C\n```\n- **User**: `ipvpn`\n- **Name**: `Flume-IPVPN` on `un2.bigdata.abc.gr`\n- **Schedule**: `Always`\n- **Source Path**: `/data/1/nnm_components_LZ/spooldir`\n- **Morphline JAR**: `/home/users/ipvpn/flume-ipvpn/jars/nnmmetrics/lib/ipvpnsla-customers-abc-flume-2.0.0-SNAPSHOT.jar`\n- **Morphline Configuration**: `/shared/abc/ip_vpn/flume/nnm_component_metrics/morphline_nnmMetricsCsvToRecord_ipvpn_user.conf`\n- **Impala Table**: `bigcust.nnm_ipvpn_componentmetrics_hist`\n- **Logs**: `/var/log/flume-ng/flume-cmf-flume5-AGENT-un2.bigdata.abc.gr.log*`\n### SLA Metrics\nDescribes how SLA-related metrics are gathered, parsed, and loaded for QoS and availability KPIs.\n#### Creation of raw files\nThe source system in this case is SNMP Custom Poller application. For high availability there are two deployments of the application on two seperate servers and they operate in active-standby fashion. While the servers are managed by abc, the application is managed by jkl. The raw files produced contain SLA metrics for QoS and availability of the network elements and are stored in local paths on those servers.\n``` mermaid\n  graph TD\n  A[Service: SNMP Custom Poller <br> Host: nnmprd01.abc.gr] -.->|Stopped| B[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/ipvpn/out <br> Host: nnmprd01.abc.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 5"
        }
    },
    "164": {
        "page_content": "``` mermaid\n  graph TD\n  A[Service: SNMP Custom Poller <br> Host: nnmprd01.abc.gr] -.->|Stopped| B[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/ipvpn/out <br> Host: nnmprd01.abc.gr]\n  C[Service: SNMP Custom Poller <br> Host: nnmdis01.abc.gr] --> D[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/ipvpn/out <br> Host: nnmdis01.abc.gr]\n```\n- **User**: `custompoller`\n- **Scheduler**: `Cron`\n- **Schedule**: `Every 5 minutes`\n- **Path**: `/home/custompoller/ipvpn/out`\n- **Elements Configuration**: `/home/custompoller/ipvpn/conf/vpn.config`\n- **Logs**: /home/custompoller/ipvpn/log/ipvpn-`date +%Y%m%d`.log\n- **Script**: `/home/custompoller/ipvpn/run/run_ipvpn.sh` on `nnmprd01.abc.gr` and `nnmdis01.abc.gr`\n#### Transfer to BigStreamer nodes\nA Perl script executed periodically by user `ipvpn` collects the raw files locally via passwordless SFTP, concatenates them into one for every 5 minute interval and uploads them to an HDFS directory.\n``` mermaid\n  graph TD\n  A[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/ipvpn/out <br> Host: nnmprd01.abc.gr]\n  E[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/ipvpn/out <br> Host: nnmdis01.abc.gr]\n  B[Path: /data/1/nnm_custompoller_ipvpn_LZ <br> Host: un2.bigdata.abc.gr]\n  C[File: nnm_poller_ipvpn.yyyymmddHHMM.yyyymmdd_HHMMss.group.parsed <br> Path: /data/1/nnm_custompoller_ipvpn_LZ <br> Host: un2.bigdata.abc.gr]\n  D[Path: hdfs://nameservice1/ez/landingzone/nnm_custompoller_ipvpn/raw]\n  A -.->|Stopped|B\n  E --> |SFTP|B\n  B -->|Concat|C\n  C -->|Upload HDFS|D\n```\n- **User**: `ipvpn`\n- **Scheduler**: `Executed from the previous step`\n- **SFTP Path**: `/home/custompoller/ipvpn/out`\n- **SFTP User**: `custompoller`\n- **Intermediate Path**: `/data/1/nnm_custompoller_ipvpn_LZ`\n- **Destination Path**: `hdfs://nameservice1/ez/landingzone/nnm_custompoller_ipvpn/raw`\n- **Logs**: /shared/abc/nnm_custompoller_ipvpn/log/nnmcustompoller_ipvpn_cron.`date +%Y%m%d`.log",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 6"
        }
    },
    "165": {
        "page_content": "- **Destination Path**: `hdfs://nameservice1/ez/landingzone/nnm_custompoller_ipvpn/raw`\n- **Logs**: /shared/abc/nnm_custompoller_ipvpn/log/nnmcustompoller_ipvpn_cron.`date +%Y%m%d`.log\n- **Configuration**: `/shared/abc/nnm_custompoller_ipvpn/DataParser/scripts_nnmprod/nnm_custompoller_ipvpn.trn`\n- **Script**: `/shared/abc/nnm_custompoller_ipvpn/DataParser/scripts_nnmprod/nnm_custompoller_ipvpn.pl` on `un2.bigdata.abc.gr`\n#### Load to BigStreamer cluster\nA Spark job executed by the previous step parses the concatenated files and loads them into an Impala table.\n> Ndef\n>`spark-submit.sh` script is triggered in the following manner:\n> 1.`run_ipvpn.sh` is run on nnmdis01 which triggers the execution of\n> 2.`nnm_custompoller_ipvpn.pl` on un2 via ssh which runs\n> 3. `spark_submit.sh` passed via `post_script` variable\n``` mermaid\n  graph TD\n  A[File: nnm_poller_ipvpn.yyyymmddHHMM.yyyymmdd_HHMMss.group.parsed <br> Path: hdfs://nameservice1/ez/landingzone/nnm_custompoller_ipvpn/raw]\n  B[Parsing]\n  C[Impala Table: bigcust.nnmcp_ipvpn_slametrics_hist]\n  A -->|Read| B\n  B -->|Load| C\n```\n- **User**: `ipvpn`\n- **Scheduler**: `Executed from the previous step`\n- **Job Name**: `com.jkl.bigstreamer.ipvpnslacustomers.spark.snmp.SnmpETLTopologyRunner`\n- **JAR**: `/home/users/ipvpn/run/ipvpnsla-customers-abc-spark.jar`\n- **Logs**: /shared/abc/nnm_custompoller_ipvpn/log/nnmcustompoller_ipvpn_cron.`date +%Y%m%d`.log\n- **Submit Script**: `/home/users/ipvpn/run/spark-submit.sh` on `un2.bigdata.abc.gr`\n- **Impala Table**: `bigcust.nnmcp_ipvpn_slametrics_hist`\n### Interface Metrics\nDetails metrics related to PE interface traffic and availability.\n#### Creation of raw files",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 7"
        }
    },
    "166": {
        "page_content": "- **Impala Table**: `bigcust.nnmcp_ipvpn_slametrics_hist`\n### Interface Metrics\nDetails metrics related to PE interface traffic and availability.\n#### Creation of raw files\nThe source system in this case is NNM. For high availability there are two instances of NNM on two seperate servers and they operate in active-standby fashion. The whole infrastructure is entirely managed by abc. The raw files produced contain interface metrics for overall interfaces' usage of the network elements and are stored in local paths on those servers.\n``` mermaid\n  graph TD\n  A[Service: NNM <br> Host: nnmprd01.abc.gr] --> B[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica <br> Host: nnmprd01.abc.gr]\n  C[Service: NNM <br> Host: nnmdis01.abc.gr] -.->|Stopped| D[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica <br> Host: nnmdis01.abc.gr]\n```\n- **Source Path**: `/files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica` on `nnmprd01.abc.gr` or `nnmdis01.abc.gr`\n- **File**: `InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz`\n- **Schedule**: `Every 5 minutes`\nThen a shell script running on the same servers, copies the files to the appropriate directory .\n``` mermaid\n  graph TD\n  A[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica <br> Host: nnmprd01.abc.gr]\n  B[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica <br> Host: nnmdis01.abc.gr]\n  C[Script: transfer-new-files.sh <br> Host: nnmprd01.abc.gr <br> User: custompoller]\n  D[Script: transfer-new-files.sh <br> Host: nnmdis01.abc.gr <br> User: custompoller]\n  E[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /home/custompoller/nnm_interface_metrics <br> Host: nnmprd01.abc.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 8"
        }
    },
    "167": {
        "page_content": "E[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /home/custompoller/nnm_interface_metrics <br> Host: nnmprd01.abc.gr]\n  F[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /home/custompoller/nnm_interface_metrics <br> Host: nnmdis01.abc.gr]\n  A --> C\n  C --> E\n  B -.->|Stopped| D\n  D -.-> F\n```\n- **User**: `custompoller`\n- **Scheduler**: `Cron`\n- **Schedule**: `Every minute`\n- **Path**: `/home/custompoller/nnm_interface_metrics`\n- **Logs**: /home/custompoller/export_metrics/log/transfer-new-files.`date +%Y%m%d`.log\n- **Script**: `/home/custompoller/export_metrics/transfer-new-files.sh` on `nnmprd01.abc.gr` or `nnmdis01.abc.gr`\n#### Load to BigStreamer cluster\nA Perl script executed periodically by user `ipvpn` collects the raw files locally via passwordless SFTP, decompresses them and uploads them to an Impala table.\n``` mermaid\n  graph TD\n  A[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /home/custompoller/nnm_interface_metrics <br> Host: nnmprd01.abc.gr]\n  B[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /home/custompoller/nnm_interface_metrics <br> Host: nnmdis01.abc.gr]\n  C[Path: /shared/abc/ip_vpn/interfaces_flow/repo <br> Host: un2.bigdata.abc.gr]\n  D[Impala Table: bigcust.perf_interfacemetrics_ipvpn_hist]\n  A -->|SFTP| C\n  B -.->|Stopped|C\n  C -->|Decompress/Load| D\n```\n- **User**: `ipvpn`\n- **Scheduler**: `Cron`\n- **Schedule**: `Every 2 minutes`\n- **SFTP Path**: `/home/custompoller/nnm_interface_metrics`\n- **SFTP user**: `custompoller`\n- **Intermediate Path**: `/shared/abc/ip_vpn/interfaces_flow/repo`\n- **Impala Table**: `bigcust.perf_interfacemetrics_ipvpn_hist`\n- **Logs**: /shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/log/nnm_interface_metrics.cron.`date +%Y%m%d`.log\n- **Configuration**: `/shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/transferlist/config_Interface_metrics.trn`\n- **Script**: `/shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/load_data.pl` on `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 9"
        }
    },
    "168": {
        "page_content": "- **Script**: `/shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/load_data.pl` on `un2.bigdata.abc.gr`\n## Input Configuration Data\nExplains how network topology and configuration data from Postgres (NNM DB) are loaded to Impala for enrichment and join operations.\n### NNM Postgres\nThe active NNM preserves network configuration data of the elements in a Postgres database, which is managed by abc. Every day these data are transferred to BigStreamer with Sqoop and used in computations of output performance data. Since slave nodes cannot connect directly to the Postgres database, firewall rules have been setup to `un1.bigdata.abc.gr` and `un2.bigdata.abc.gr` in order to forward requests to the external database.\n``` bash\n[root@un2 ~] firewall-cmd --list-all\npublic (active)\n  target: ACCEPT\n  icmp-block-inversion: no\n  interfaces: bond0 bond0.100 bond0.2000 bond0.300 bond0.951 em2 lo p3p2\n  sources:\n  services: dhcpv6-client ssh\n  ports:\n  protocols:\n  masquerade: yes\n  forward-ports:\n    ...\n\tport=6535:proto=tcp:toport=5432:toaddr=999.999.999.999\n  source-ports:\n  icmp-blocks:\n  rich rules:\n```\n#### Table nms_iface\n``` mermaid\n  graph TD\n  A[Postgres <br> Host: nnmprd01.abc.gr <br> Table: nnm.nms_iface ]\n  B[Postgres <br> Host: nnmdis01.abc.gr <br> Table: nnm.nms_iface ]\n  A -->|Sqoop| C[Impala Table: nnmnps.conf_nms_iface ]\n  B -.->|Stopped| C\n```\n- **User**: `intra`\n- **Scheduler**: `Oozie`\n- **Schedule**: `Every day at 3:00 (UTC)`\n- **Coordinator**: `Coord_nnmdb.nms_iface`\n- **Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.\n- Check if data have been loaded for a specific date using Impala shell or editor.\n```bash\n# e.g for 01-10-2018\nselect count(*) from nnmnps.conf_nms_iface where par_dt='20181001';\n```\n- Check if connectivity to Postgres works from `un2.bigdata.abc.gr`.\n#### Table nms_ip_addr\n``` mermaid\n  graph TD\n  A[Postgres <br> Host: nnmprd01.abc.gr <br> Table: nnm.nms_ip_addr ]",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 10"
        }
    },
    "169": {
        "page_content": "```\n- Check if connectivity to Postgres works from `un2.bigdata.abc.gr`.\n#### Table nms_ip_addr\n``` mermaid\n  graph TD\n  A[Postgres <br> Host: nnmprd01.abc.gr <br> Table: nnm.nms_ip_addr ]\n  B[Postgres <br> Host: nnmdis01.abc.gr <br> Table: nnm.nms_ip_addr ]\n  A -->|Sqoop| C[Impala Table: nnmnps.nms_ip_addr ]\n  B -.->|Stopped| C\n```\n- **User**: `intra`\n- **Scheduler**: `Oozie`\n- **Schedule**: `Every day at 3:00 (UTC)`\n- **Coordinator**: `Coord_nnmdb.nms_ip_addr`\n- **Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.\n- Check if data have been loaded for a specific date using Impala shell or editor.\n```bash\n# e.g for 01-10-2018\nselect count(*) from nnmnps.nms_ip_addr where par_dt='20181001';\n```\n- Check if connectivity to Postgres works from `un2.bigdata.abc.gr`.\n#### Table nms_node\n``` mermaid\n  graph TD\n  A[Postgres <br> Host: nnmprd01.abc.gr <br> Table: nnm.nms_node ]\n  B[Postgres <br> Host: nnmdis01.abc.gr <br> Table: nnm.nms_node ]\n  A -->|Sqoop| C[Impala Table: nnmnps.nms_node ]\n  B -.->|Stopped| C\n```\n- **User**: `intra`\n- **Scheduler**: `Oozie`\n- **Schedule**: `Every day at 3:00 (UTC)`\n- **Coordinator**: `Coord_nnmdb.nms_node`\n- **Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.\n- Check if data have been loaded for a specific date using Impala shell or editor.\n```bash\n# e.g for 01-10-2018\nselect count(*) from nnmnps.nms_node where par_dt='20181001';\n```\n- Check that Coordinator runs correctly.  \n```sql\nSELECT MAX(par_dt) FROM nnmnps.nms_node WHERE par_dt>= from_timestamp(now() - interval 15 days,'yyyyMMdd')\n```\nmust return the yesterday's date. \n- Check if connectivity to Postgres works from `un2.bigdata.abc.gr`.\n### User Interface\nDocuments how user-managed configurations are handled by a web UI and synced to BigStreamer from MySQL.\n#### Stack",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 11"
        }
    },
    "170": {
        "page_content": "### User Interface\nDocuments how user-managed configurations are handled by a web UI and synced to BigStreamer from MySQL.\n#### Stack\nUsers can manage configurations used during the computation of the output performance data, using the CustomApps UI. The load balancer sends traffic to two Wildfly instances who keep track of users' changes in MySQL. They are updated into BigStreamer daily. The whole stack is managed by jkl.\n``` mermaid\n  graph TD\n  A[Users] --> B[Load Balancer]\n  B --> C[Wildfly <br> Host: unekl1.bigdata.abc.gr]\n  B --> D[Wildfly <br> Host: unekl2.bigdata.abc.gr]\n  C --> E[MySQL]\n  D --> E\n  E --> F[Sync MySQL and BigStreamer]\n  F -->G[Impala]\n```\n##### Load Balancer\nFor load balancing the HaProxy service is used.\n- **URL**: `https://cne.def.gr:8643/landing/#/login`\n- **Controlled By**: `systemctl`\n- **Configuration**: `/etc/haproxy/haproxy.cfg`\n- **Host**: `unc1.bigdata.abc.gr` and `unc2.bigdata.abc.gr`\n##### Wildfly\nWildfly is the application server used for the UI of CustomApps including IP VPN.\n- **User**: `trustcenter`\n- **Installation Path**: `/opt/trustcenter/wf_cdef_trc` on `unekl1.bigdata.abc.gr` and `unekl2.bigdata.abc.gr`\n- **Deployments Path**: `/opt/trustcenter/wf_cdef_trc/standalone/deployments`\n- **General Configuration Path**: `/opt/trustcenter/wf_cdef_trc/standalone/configuration/standalone-full.xml`\n- **Application Configuration Path**: `/opt/trustcenter/wf_cdef_trc/standalone/configuration/ServiceWeaver/beanconfig/`\n- **Application Logs**: `/opt/trustcenter/wf_cdef_trc/standalone/log/server.log`\n- **Access Log**: `/opt/trustcenter/wf_cdef_trc/standalone/log/access_log.log`\n##### MySQL\nMySQL is the database used for the UI. Apart from other things, configuration data managed by users are stored there.\n- **MySQL Servers**: `db01.bigdata.abc.gr` and `db02.bigdata.abc.gr` with vIP `999.999.999.999`\n- **MySQL Schema**: `trustcenter`\n#### Data Synchronization",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 12"
        }
    },
    "171": {
        "page_content": "- **MySQL Servers**: `db01.bigdata.abc.gr` and `db02.bigdata.abc.gr` with vIP `999.999.999.999`\n- **MySQL Schema**: `trustcenter`\n#### Data Synchronization\nSynchronization of MySQL and BigStreamer's data is done using Sqoop. These are data necessary for computation of the output performance metrics from MySQL used for UI to BigStreamer. Also steps to load configuration tables are done for improving query performance. The master scripts executes jobs in order to update the following Impala tables.\n``` mermaid\n  graph TD\n  A[Oozie: Coord_IPVPN_load_mysql_to_Impala] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to ipvpn| C[Master Script]\n```\n- **User**: `intra`\n- **Scheduler**: `Oozie`\n- **Schedule**: `Every day at 4:00 (UTC)`\n- **Coordinator**: `Coord_IPVPN_load_mysql_to_Impala`\n- **Master Script**: `/shared/abc/ip_vpn/run/run_load_mysql_to_impala.sh`\n- **Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.\n##### Table customer_pl\nCustomer PL indicates which Packet Loss formula type is going to be used for each customer. Default is 1.\n``` mermaid\ngraph TD\nA[MySQL Table: trustcenter.IPVPNSLA_CUSTOMER_PL] -->|Sqoop| B[Impala Table: bigcust.customer_pl]\n```\n- **User**: `ipvpn`\n- **Logs**: /shared/abc/ip_vpn/log/update_pl_customer.`date +%Y%m%d`.log\n- **Script**: `/shared/abc/ip_vpn/run/update_pl_customer.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n##### Table net_to_sm_customer\nNet to SM Customer is a translation for customer names from NNM to SM.\n``` mermaid\ngraph TD\nA[MySQL Table: trustcenter.IPVPNSLA_NET_TO_SM_CUSTOMER] -->|Sqoop| B[Impala Table: bigcust.net_to_sm_customer]\n```\n- **User**: `ipvpn`\n- **Logs**: /shared/abc/ip_vpn/log/update_net_to_sm_customer.`date +%Y%m%d`.log\n- **Script**: `/shared/abc/ip_vpn/run/update_net_to_sm_customer.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Identify system or service errors in the log file.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 13"
        }
    },
    "172": {
        "page_content": "- **Script**: `/shared/abc/ip_vpn/run/update_net_to_sm_customer.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n##### Table customer_sla_config_ipvpn\nSLA configurations specify how each QoS metric is computed.\n``` mermaid\ngraph TD\nA[MySQL Table: trustcenter.IPVPNCUUI_CUSTOMER_SLA_CONFIG] -->|Sqoop| B[Impala Table: bigcust.customer_sla_config_ipvpn]\n```\n- **User**: `ipvpn`\n- **Logs**: /shared/abc/ip_vpn/log/update_customer_sla_config.`date +%Y%m%d`.log\n- **Script**: `/shared/abc/ip_vpn/run/update_customer_sla_config.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n##### Table pe_interfaces\nPE interfaces specify for which elements interface KPIs will be exported. The MySQL table is transferred to BigStreamer, enriched with NPS data and populates the Impala table. The enriched data are transferred back to MySQL.\n``` mermaid\ngraph TD\nA[MySQL Table: trustcenter.IPVPNSLA_PE_INTERFACES] -->|Sqoop| B[Impala Table: bigcust.pe_interfaces]\nB -->|Sqoop| C[MySQL Table: trustcenter.IPVPNSLA_PE_INTERFACES_INTERMEDIATE]\nC -->|MySQL Query|D[MySQL Table: trustcenter.IPVPNSLA_PE_INTERFACES]\n```\n- **User**: `ipvpn`\n- **Logs**: /shared/abc/ip_vpn/log/update_pe_interfaces.`date +%Y%m%d`.log\n- **Script**: `/shared/abc/ip_vpn/run/update_pe_interfaces.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n##### Table sla_configurations\nData from the above tables are combined and loaded into an Impala table used for queries of output performance metrics.\n``` mermaid\ngraph TD\nA[Impala Table: bigcust.customer_sla_config_ipvpn] --> B[Impala Table: bigcust.sla_configurations]\nC[Impala Table: bigcust.customer_pl] --> B\nD[Impala Table: bigcust.net_to_sm_customer] --> B\n```\n- **User**: `ipvpn`\n- **Logs**: /shared/abc/ip_vpn/log/update_customer_sla_config.`date +%Y%m%d`.log",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 14"
        }
    },
    "173": {
        "page_content": "C[Impala Table: bigcust.customer_pl] --> B\nD[Impala Table: bigcust.net_to_sm_customer] --> B\n```\n- **User**: `ipvpn`\n- **Logs**: /shared/abc/ip_vpn/log/update_customer_sla_config.`date +%Y%m%d`.log\n- **Script**: `/shared/abc/ip_vpn/run/update_customer_sla_config.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n## Output Performance Data\nDescribes how final KPIs (CPU, QoS, interface metrics) are calculated, stored, and sent to the SQM system.\nFor every 5-minute interval 5 HTTP requests containing CPU load, memory utilization, QoS, availability and interface utilization are sent to SQM server containing instantenous metrics for network elements and are moved to an exchange directory. Also these metrics are stored into an Impala table.\n### CPU and Memory\n#### Main script\nA procedure is executed periodically that triggers IPVPN-SM App to compute and transmit to the SQM server via HTTP the metrics for CPU and memory KPIs of the network elements. The procedure also inserts the KPIs into an Impala table. These three sub-steps are executed in parallel.\n- **User**: `ipvpn`\n- **Scheduler**: `Cron`\n- **Schedule**: `Every 5 minutes`\n- **Exchange user**: `saismpm`\n- **Exchange path**: `/shared/abc/ip_vpn/out/saismpm`\n- **Logs**: /shared/abc/ip_vpn/log/initiate_export_components.cron.`date '+%Y%m%d'`.log\n- **Script**: `/shared/abc/ip_vpn/run/initiate_export_components.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Check IPVPN-SM script and app [logs](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ipvpn_sm_replacement.md#logs)\n##### Load component_metrics\n``` mermaid\ngraph TD\n  A[Impala Table: bigcust.nnm_ipvpn_componentmetrics_hist]\n  B[Computation of CPU & Memory KPIs]\n  C[Impala Table: bigcust.component_metrics]\n  A -->|Query|B\n  B -->|Load|C\n```\n- **User**: `ipvpn`\n- **Impala Table**: `bigcust.component_metrics`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 15"
        }
    },
    "174": {
        "page_content": "B[Computation of CPU & Memory KPIs]\n  C[Impala Table: bigcust.component_metrics]\n  A -->|Query|B\n  B -->|Load|C\n```\n- **User**: `ipvpn`\n- **Impala Table**: `bigcust.component_metrics`\n- **Logs**: /shared/abc/ip_vpn/log/populate_components_metrics_table.`date +%Y%m%d`.log\n- **Script**: `/shared/abc/ip_vpn/run/populate_components_metrics_table.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Identify system or service errors in the log files e.g failed Impala query.\n### QoS and Availability\n#### Main script\nA procedure is executed periodically that triggers IPVPN-SM App to compute and transmit to the SQM server via HTTP the metrics for QoS and availability KPIs of the network elements. The procedure also inserts the KPIs into an Impala table. These three sub-steps are executed in parallel.\n- **User**: `ipvpn`\n- **Scheduler**: `Cron`\n- **Schedule**: `Every 5 minutes`\n- **Exchange user**: `saismpm`\n- **Exchange path**: `/shared/abc/ip_vpn/out/saismpm`\n- **Logs**: /shared/abc/ip_vpn/log/initiate_export_sla.cron.`date '+%Y%m%d'`.log\n- **Script**: `/shared/abc/ip_vpn/run/initiate_export_sla.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Check IPVPN-SM script and app [logs](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ipvpn_sm_replacement.md#logs)\n##### Load sla_metrics\n``` mermaid\ngraph TD\n  A[Impala Table: bigcust.nnmcp_ipvpn_slametrics_hist]\n  B[Computation of QoS and AV KPIs]\n  C[Impala Table: bigcust.sla_metrics]\n  A -->|Query|B\n  B -->|Load|C\n```\n- **User**: `ipvpn`\n- **Impala Table**: `bigcust.sla_metrics`\n- **Logs**: /shared/abc/ip_vpn/log/populate_sla_metrics_table.`date +%Y%m%d`.log\n- **Script**: `/shared/abc/ip_vpn/run/populate_sla_metrics_table.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Identify system or service errors in the log files e.g failed Impala query.\n#### Support actions",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 16"
        }
    },
    "175": {
        "page_content": "- **Troubleshooting Steps**:\n- Identify system or service errors in the log files e.g failed Impala query.\n#### Support actions\nProvides detailed instructions for identifying and resolving issues at any stage of the pipeline, from ingestion to SQM export.\n##### Spark failure\nIf the ingestion of the SLA metrics failed during the spark job execution (meaning that the files are successfully moved to hdfs dir `/ez/landingzone/nnm_custompoller_ipvpn/raw` and they are ready to be loaded on the Impala table) then we can re-submit the spark job in the following manner:\n1. Connect to un2 \n``` bash\nssh un2\nsu - ipvpn\nkinit -kt /home/users/ipvpn/ipvpn.keytab ipvpn\n```\n2. Execute the spark-submit as seen on file `/home/users/ipvpn/run/spark-submit.sh` with the appropriate startMin and endMin parameters\n   e.g. for loading the metrics that correspond to the 5min interval `2023-11-29 11:20:00` :\n```\nspark-submit \\\n  --verbose  \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --num-executors 4 \\\n  --files /home/users/ipvpn/ipvpn.keytab#ipvpn.keytab,/etc/hive/conf/hive-site.xml,/home/users/ipvpn/conf/ipvpn-log4j.xml#ipvpn-log4j.xml \\\n  --conf \"spark.executor.extraJavaOptions=-Dlog4j.configuration=/home/users/ipvpn/conf/ipvpn-log4j.xml\" \\\n  --conf \"spark.driver.extraJavaOptions=-Dlog4j.configuration=/home/users/ipvpn/conf/ipvpn-log4j.xml\" \\\n  --conf \"spark.driver.extraJavaOptions=-DlogFilename=/home/users/ipvpn/log/ipvpn-\" \\\n  --conf \"spark.executor.extraJavaOptions=-DlogFilename=/home/users/ipvpn/log/ipvpn-\" \\\n  --conf \"spark.executor.instances=4\" \\\n  --conf \"spark.executor.cores=1\" \\\n  --conf \"spark.executor.memory=4g\" \\\n  --conf \"spark.executor.memoryOverhead=600\" \\\n  --conf \"spark.driver.cores=1\" \\\n  --conf \"spark.driver.memory=4g\" \\\n  --conf \"spark.driver.memoryOverhead=600\" \\\n  --conf \"spark.dynamicAllocation.enabled=false\" \\\n  --class com.jkl.bigstreamer.ipvpnslacustomers.spark.snmp.SnmpETLTopologyRunner /home/users/ipvpn/run/ipvpnsla-customers-abc-spark.jar \\",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 17"
        }
    },
    "176": {
        "page_content": "--conf \"spark.dynamicAllocation.enabled=false\" \\\n  --class com.jkl.bigstreamer.ipvpnslacustomers.spark.snmp.SnmpETLTopologyRunner /home/users/ipvpn/run/ipvpnsla-customers-abc-spark.jar \\\n  -baseDirectory \"hdfs://nameservice1/ez/landingzone/nnm_custompoller_ipvpn/raw/\" -startMin 202311281120 -endMin 202311281120 -impalaTableName \"bigcust.nnmcp_ipvpn_slametrics_hist\" -counter32List \"NumOfRTT,SumOfRTT,PacketLostSD,PacketLostDS,PacketMIA,NumJitOpCompletions,SumOfPosJitterSD,SumOfNegJitterSD,NumOfPosJitterSD,NumOfNegJitterSD,SumOfPosJitterDS,SumOfNegJitterDS,NumOfPosJitterDS,NumOfNegJitterDS,OperationCompletions,OperationTotInitiations\" -totalDatasetPartitions 30\n```\n3. Check metrics are loaded\n```\nrefresh nnmcp_ipvpn_slametrics_hist; \nselect count(*) from nnmcp_ipvpn_slametrics_hist where n5_minute='2023-11-28 11:20:00';\n```\n### Interfaces\n#### Main script\nA procedure is executed periodically that triggers IPVPN-SM App to compute and transmit to the SQM server via HTTP the metrics for interface KPIs of the network elements. The procedure also inserts the KPIs into an Impala table. These two sub-steps are executed in parallel.\n- **User**: `ipvpn`\n- **Scheduler**: `Cron`\n- **Schedule**: `Every 5 minutes`\n- **Exchange user**: `saismpm`\n- **Exchange path**: `/shared/abc/ip_vpn/out/saismpm`\n- **Logs**: /shared/abc/ip_vpn/log/initiate_export_interfaces.cron.`date '+%Y%m%d'`.log\n- **Master Script**: `/shared/abc/ip_vpn/run/initiate_export_interfaces.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Check IPVPN-SM script and app [logs](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ipvpn_sm_replacement.md#logs)\n##### Load interface_metrics\n``` mermaid\ngraph TD\n  A[Impala Table: bigcust.perf_interfacemetrics_ipvpn_hist]\n  B[Computation of IF KPIs]\n  C[Impala Table: bigcust.interface_metrics]\n  A -->|Query|B\n  B -->|Load|C\n```\n- **User**: `ipvpn`\n- **Impala Table**: `bigcust.interface_metrics`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 18"
        }
    },
    "177": {
        "page_content": "B[Computation of IF KPIs]\n  C[Impala Table: bigcust.interface_metrics]\n  A -->|Query|B\n  B -->|Load|C\n```\n- **User**: `ipvpn`\n- **Impala Table**: `bigcust.interface_metrics`\n- **Logs**: /shared/abc/ip_vpn/log/populate_interface_metrics_table.`date +%Y%m%d`.log\n- **Script**: `/shared/abc/ip_vpn/run/populate_interface_metrics_table.sh` on `un2.bigdata.abc.gr`\n- **Troubleshooting Steps**:\n- Identify system or service errors in the log files e.g failed Impala query.\n## Output Configuration Data\nPeriodically an XML file that contains information about VPN customers with configuration changes, is produced by one of the Wildfly instances and is transferred to an exchange directory using a shell script.\n``` mermaid\ngraph TD\n  A[Wildfly <br> Host: unekl1.bigdata.abc.gr <br> Status: Master] -->| Executes | B[Script: /opt/trustcenter/wf_cdef_trc/ipvpnExports/sftp.sh <br> Host: unekl1.bigdata.abc.gr <br> User: trustuser]\n  B -->|Export| C[File: SD_Inventory_DB_yyyy_mm_dd_HH:MM:SS.xml <br> Server: unekl1.bigdata.abc.gr]\n  E[Wildfly <br> Host: unekl2.bigdata.abc.gr <br> Status: Slave] -.->| Executes | F[Script: /opt/trustcenter/wf_cdef_trc/ipvpnExports/sftp.sh <br> Host: unekl2.bigdata.abc.gr <br> User: trustuser]\n  F -.->|Export| G[File: SD_Inventory_DB_yyyy_mm_dd_HH:MM:SS.xml <br> Server: unekl1.bigdata.abc.gr]\n  C -->|SFTP| D[User: sd <br> Server: eeaadaptprd.def.gr <br> Path: /sd/classic/scripts/install/inventory/export/IPVPN_Inventory]\n  G -.->|SFTP| D\n```\n- **User**: `trustuser`\n- **Scheduler**: `Wildfly`\n- **Schedule**: `Every 4 hours`\n- **SFTP user**: `sd`\n- **SFTP Server**: `eeaadaptprd.def.gr`\n- **SFTP path**: `/sd/classic/scripts/install/inventory/export/IPVPN_Inventory`\n- **Logs**: /opt/trustcenter/wf_cdef_trc/standalone/log/server.log*\n- **Script**: `/opt/trustcenter/wf_cdef_trc/ipvpnExports/sftp.sh` on `unekl1.bigdata.abc.gr` and `unekl2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "ip_vpn.md - Part 19"
        }
    },
    "178": {
        "page_content": "---\ndocument_owner: \"Intra team\"\nsystems_involved:\n  - SNMP Custom Poller\n  - BigStreamer\n  - Impala\n  - Hive\n  - HDFS\n  - MySQL\n  - Spark\n  - Sqoop\nscheduling: \n  - cron\n  - oozie\ndata_sources:\n  - SNMP Poller raw files\n  - MySQL SZXUI_MASTER_ENTRY\n  - Impala nnmnps.* metrics tables\ntarget_tables:\n  - nnmnps.nnmcp_qametrics_hist\n  - nnmnps.cpe_metrics\n  - nnmnps.core_metrics\n  - nnmnps.report_daily_*\n  - nnmnps.report_monthly_*\nscripts_location: \"/shared/abc/nnmnps/bin/\"\nhdfs_paths:\n  - \"/ez/landingzone/nnm_custompoller/raw\"\n  - \"/ez/tmp/cpe/exp\"\nresponsible_users:\n  - intra\n  - custompoller\n  - syzefxis\nmonitored: false\nalerts_handling: \"Manual checks or automated email alerts depending on flow\"\nsummary: >\n  This document describes the end-to-end data pipeline for Syzefxis performance metrics. \n  It includes raw data generation from SNMP Custom Poller, periodic transfer to HDFS, Spark transformations,\n  KPI enrichment flows (via shell & SQL scripts), and final reporting via Impala tables and email exports.\n---\n# Syzefxis Flows\n## Useful Links\n- [Business Documents](https://metis.ghi.com/obss/bigdata/abc/sizefxis/bigstreamer-sizefxis-devops/-/tree/master/docs)\n- [MoP documents](https://metis.ghi.com/obss/bigdata/abc/sizefxis/bigstreamer-sizefxis-devops/-/tree/master/MOPs)\n- Users **keePass file**: [abc-devpasswd.kdbx](../../../abc-devpasswd.kdbx)  \n- **Troubleshooting Steps**: Refer to MoPs files in [devops repository](https://metis.ghi.com/obss/bigdata/abc/sizefxis/bigstreamer-sizefxis-devops/-/blob/master/MOPs/README.md?ref_type=heads) of the project\n## Step 1: Ingest Raw SLA Data from Custom Poller\nThis section explains how raw performance metrics are generated on-prem via SNMP Custom Poller, collected, and uploaded to the HDFS landing zone.\n### Step 1.1: Raw File Creation from SNMP Custom Poller\n#### Creation of raw files",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "syzefxis_flows.md - Part 1"
        }
    },
    "179": {
        "page_content": "### Step 1.1: Raw File Creation from SNMP Custom Poller\n#### Creation of raw files\nThe source system in this case is SNMP Custom Poller application. For high availability there are two deployments of the application on two seperate servers and they operate in active-standby fashion. While the servers are managed by abc, the application is managed by jkl. The raw files produced contain SLA metrics for QoS and availability of the network elements and are stored in local paths on those servers.\n``` mermaid\n  graph TD\n  A[Service: SNMP Custom Poller <br> Host: nnmprd01.abc.gr] --> B[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/out <br> Host: nnmprd01.abc.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "syzefxis_flows.md - Part 2"
        }
    },
    "180": {
        "page_content": "C[Service: SNMP Custom Poller <br> Host: nnmdis01.abc.gr] -.->|Stoped| D[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/out <br> Host: nnmdis01.abc.gr]\n```\n**Server**: `nnmprd01.abc.gr` (backup server `nnmdis01.abc.gr`)  \n**User**: `custompoller`  \n**Password**: `Passwordless SSH from intra@un2.bigdata.abc.gr`  \n**Scheduler**: `Cron`  \n**Schedule**: `Every 5 minutes`  \n**Path**: `/home/custompoller/out`  \n**Elements Configuration**: `/home/custompoller/conf/syzeyksis_syze1.config`  \n**Logs**: ```/home/custompoller/log/syzeyksis-`date +%Y%m%d`.log```  \n**Script**: `/home/custompoller/run/run_syzeyksis_standby.sh` on `nnmprd01.abc.gr` and `nnmdis01.abc.gr`  \n**Alerts**:\n- Not monitored\n### Step 1.2: Transfer Raw Files to BigStreamer HDFS\nBash scripts executed periodically by user `intra` collects the raw files locally via passwordless SFTP, concatenates them into one for every 5 minute interval and uploads them to an HDFS directory.\n``` mermaid\n  graph TD\n  A[SFTP Server: nnmprd01.abc.gr<br>SFTP User: custopoller<br>SFTP Path: ./out] --> |SFTP GET|B[Local Staging Path: /data/1/nnm_custompoller_LZ/archives<br>FILES: nnm_poller.YYYYMMDDhhmm.YYYYMMDD_hhmmss.group.parsed]\n  -->|HDFS PUT| C[HDFS: /ez/landingzone/nnm_custompoller/raw/YYYYMMDDhhmm<br>FILE PATTERN: nnm_poller.YYYYMMDDhhmm.YYYYMMDD_hhmmss.group.parsed ]\n  D[SFTP Server: nnmdis01.abc.gr<br>SFTP User: custopoller<br>SFTP Path: ./out] -.-> |Stopped|B\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Scheduler**: `Cron`  \n**Schedule**: `\u0395very 10 minutes`  \n**SFTP Server**:  `nnmprd01.abc.gr` (backup: `nnmdis01.abc.gr`)  \n**SFTP Path**:  `./out`  \n**SFTP User**: `custompoller`  \n**SFTP Password**: `Passwordless Authentication with SSH Key`  \n**Local Staging Path**: `/data/1/nnm_custompoller_LZ/archives`  \n**HDFS Destination Path**: `/ez/landingzone/nnm_custompoller/raw/YYYYMMDDhhmm`  \n**Logs**: `/shared/abc/nnm_custompoller/log/nnmcustompoller_cron.YYYYMMDD.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "syzefxis_flows.md - Part 3"
        }
    },
    "181": {
        "page_content": "**HDFS Destination Path**: `/ez/landingzone/nnm_custompoller/raw/YYYYMMDDhhmm`  \n**Logs**: `/shared/abc/nnm_custompoller/log/nnmcustompoller_cron.YYYYMMDD.log`  \n**Configuration**: `/shared/abc/nnm_custompoller/DataParser/scripts/transferlist/nnm_custompoller.trn`  \n**Script**:  `/shared/abc/nnm_custompoller/DataParser/scripts/nnm_custompoller.pl`  \n**Alerts**:\n- Not monitored\n### Step 1.3: Spark Load to nnmnps.nnmcp_qametrics_hist\nSpark Job to pivot raw data and make temporal calculation every 30 minutes to append the `nnmnps.nnmcp_qametrics_hist` impala table. This table will be the source data for further calculations and reports.\n``` mermaid\n  graph TD\nA[HDFS: /ez/landingzone/nnm_custompoller/raw/YYYYMMDDhhmm<br>FILE PATTERN: nnm_poller.YYYYMMDDhhmm.YYYYMMDD_hhmmss.group.parsed]-->B[Spark]-->C[Impala: nnmnps.nnmcp_qametrics_hist]\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `syzefxis`  \n**Scheduler**: `Cron`  \n**Schedule**: `Every 30 minutes`  \n**Logs**: `/home/users/syzefxis/DataTransformation/log/syzefxis-YYYY-MM-DD.log`  \n**Script**: `/home/users/syzefxis/DataTransformation/run/spark-submit.sh`  \n**Alerts**:\n- Not monitored\n## Step 2: Generate and Categorize Metrics (CPE & Core)\nDescribes shell and SQL workflows that transform the raw metrics into categorized KPI metrics, including CPE and Core metrics. Scripts run in parallel daily.\nMaster script that launches sub-scripts in parallel. Each sub-script is a separate section below.\n``` mermaid\n  graph TD\n  A[001_CP_nnmnps_Metrics.sh]\n  A -.->B[100_Sqoop_MySql_HDFS_Load.sh]\n  A -.->C[401_CP_cpe_metrics.sh]\n  A -.->D[403_CP_cpe_E2E_metrics.sh]\n  A -.->E[402_CP_core_metrics.sh]\n  A -.->F[404_core_E2E_metrics.sh]\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**MySQL User**: `syzeyksis`  \n**MySQL Host**: `db-vip.bidata.abc.gr`  \n**Scheduler**: `Cron`  \n**Schedule**: `Every day at 6:30`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "syzefxis_flows.md - Part 4"
        }
    },
    "182": {
        "page_content": "**MySQL User**: `syzeyksis`  \n**MySQL Host**: `db-vip.bidata.abc.gr`  \n**Scheduler**: `Cron`  \n**Schedule**: `Every day at 6:30`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`  \n**Script**: `/shared/abc/nnmnps/bin/001_CP_nnmnps_Metrics.sh`  \n**Alerts**:\n- Not monitored\n\t### Step 2.1: Sqoop Import from MySQL to refdata.rd_cpe\n``` mermaid\n  graph TD\n  S[MySQL: syzeyksis.SZXUI_MASTER_ENTRY]-->|Sqoop|T1[HDFS: /ez/tmp/cpe/exp] -->|Impala Insert|T[Impala: refdata.rd_cpe]\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**MySQL User**: `syzeyksis`\n**MySQL Host**: `db-vip.bidata.abc.gr`\n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`\n**Script**: `/shared/abc/nnmnps/bin/100_Sqoop_MySql_HDFS_Load.sh`  \n**Alerts**:\n- Not monitored\n### Step 2.2: Generate CPE Metrics\n``` mermaid\n  graph TD\n  T[Impala: refdata.rd_cpe]\n  A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.cpe_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`\n**Script**: `/shared/abc/nnmnps/bin/401_CP_cpe_metrics.sh`  \n**Alerts**:\n- Not monitored\n### Step 2.3: Generate CPE E2E Metrics\n``` mermaid\n  graph TD\n  T[Impala: refdata.rd_cpe]\n  A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.cpe_e2e_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`  \n**Script**: `/shared/abc/nnmnps/bin/403_CP_cpe_E2E_metrics.sh`  \n**Alerts**:\n- Not monitored\n### Step 2.4: Generate Core Metrics\n``` mermaid\n  graph TD\n  T[Impala: refdata.rd_cpe]\n  A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.core_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "syzefxis_flows.md - Part 5"
        }
    },
    "183": {
        "page_content": "B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.core_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`  \n**Script**: `/shared/abc/nnmnps/bin/402_CP_core_metrics.sh`  \n**Alerts**:\n- Not monitored\n### Step 2.5: Generate Core E2E Metrics\n``` mermaid\n  graph TD\n  T[Impala: refdata.rd_cpe]\n  A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.core_e2e_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`\n**Script**: `/shared/abc/nnmnps/bin/404_core_E2E_metrics.sh`  \n**Alerts**:\n- Not monitored\n## Step 3: Daily KPI Calculation (Oozie Spark Job)\nOutlines the Oozie-scheduled Spark job that calculates daily KPI aggregates from the metrics tables.\nA Spark application is executed on a daily basis, which calculates the results according to the Metrics tables and stores the data in the output tables\n``` mermaid\n  graph TD\nA[Impala Tables:<br> nnmnps.cpe_metrics<br>nnmnps.core_metrics]-->B[Spark]-->C[Impala Tables:<br>nnmnps.report_daily_core<br>nnmnps.report_daily_cpe]\n```\n**User**: `intra`  \n**Scheduler**: `Oozie`  \n**Schedule**: `Every day at 8:00 (UTC)`  \n**Oozie Coordinator**: `DailySyzefxisCoordinator`  \n**Oozie workflow**: `Syzefxis_Daily_Spark`  \n**Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n**Alerts**:\n- Not monitored\n## Step 4: Monthly KPI Calculation (Oozie Spark Job)\nExplains how a separate Spark job runs monthly to compute longer-term performance metrics for reporting.\nA Spark application is executed on a monthly basis, which calculates the results according to the Metrics tables and stores the data in the output tables\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "syzefxis_flows.md - Part 6"
        }
    },
    "184": {
        "page_content": "A Spark application is executed on a monthly basis, which calculates the results according to the Metrics tables and stores the data in the output tables\n``` mermaid\n  graph TD\nA[Impala Tables:<br> nnmnps.cpe_metrics<br>nnmnps.core_metrics]-->B[Spark]-->C[Impala Tables:<br>nnmnps.report_monthly_core<br>nnmnps.report_monthly_cpe]\n```\n**User**: `intra`  \n**Scheduler**: `Oozie`  \n**Schedule**: `Every 1st day of month at 10:00 (UTC)`  \n**Oozie Coordinator**: `MonthlySyzefxisCoordinator`  \n**Oozie workflow**: `Syzefxis_Monthly_Spark`  \n**Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n**Alerts**:\n- Not monitored\n## Step 5: Monthly Report Export & Email to Customer\nDetails the monthly export logic that generates CSV reports and emails them to stakeholders automatically.\nOn the 2nd day of each month, a shell script runs via Crontab that exports the data into the necessary csv files and mails them in ZIP format to the customer.\n```mermaid\ngraph TD\n  A1[Impala Tables<br>nnmnps.report_monthly_core<br>nnmnps.report_monthly_cpe<br>nnmnps.report_monthly_islet]\n  --> B1[Impala SELECT]\n  --> C1[FILE:<br>network_report_1_monthly_syzefxis.YYYYMM.csv<br>network_report_13_monthly_syzefxis.YYYYMM.csv<br>network_report_14_monthly_syzefxis.YYYYMM.csv<br>network_report_16_monthly_syzefxis.YYYYMM.csv<br>network_report_19_monthly_syzefxis.YYYYMM.csv<br>network_report_20_monthly_syzefxis.YYYYMM.csv<br>Staging Directory:<br>/shared/abc/nnmnps/tmp]\n  A2[Impala Tables:<br>nnmnps.report_daily_core<br>nnmnps.report_daily_cpe]\n  --> B2[Impala SELECT]\n  --> C2[FILES:<br>1. network_report_1_daily_syzefxis.YYYYMM.csv<br>2. network_report_14_daily_syzefxis.YYYYMM.csv<br>3. network_report_16_daily_syzefxis.YYYYMM.csv<br><strong>Staging Directory:</strong><br>/shared/abc/nnmnps/tmp]\n  C1-->Z[Zip file:<br>YYYYMM.zip]\n  C2-->Z\n  Z-->M[mail<br>Recipients:<br>dvordonis fa:fa-at def.gr,sla fa:fa-at def.gr,ATraianou fa:fa-at abc.gr]\n```\n**Server**: `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "syzefxis_flows.md - Part 7"
        }
    },
    "185": {
        "page_content": "C1-->Z[Zip file:<br>YYYYMM.zip]\n  C2-->Z\n  Z-->M[mail<br>Recipients:<br>dvordonis fa:fa-at def.gr,sla fa:fa-at def.gr,ATraianou fa:fa-at abc.gr]\n```\n**Server**: `un2.bigdata.abc.gr`  \n**Scheduler**: `Cron`  \n**Schedule**: `At the second day of each month at 6:00`  \n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/901_Export_CSV_reports.cron.log`  \n**Scripts**:\n- `/shared/abc/nnmnps/bin/901_Export_CSV_reports.sh`  \n- `/shared/abc/nnmnps/bin/902_Mail_Exported_Files.sh`\n**Export folder**: `/shared/abc/nnmnps/tmp`\n**Alerts**:\n- Not monitored\n## Step 6: Troubleshooting Raw File Generation\nProvides steps to check if raw data generation is stuck and explains how to resolve common lock-related issues in SNMP Poller.\nCheck if the custompoller continuously generates raw files.  \n1. ssh ipvpn@un2\n1. ssh custompoller@nnmprd01 (active node)\n1. cd /home/custompoller/out\n1. ls -ltr nnmcp.saa-syze1.*.txt\n```bash\n-rw-r--r-- 1 custompoller custompoller 32195036 Jan 16 16:40 nnmcp.saa-syze1.202401161640.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32196182 Jan 16 16:45 nnmcp.saa-syze1.202401161645.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32197733 Jan 16 16:50 nnmcp.saa-syze1.202401161650.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32199664 Jan 16 16:55 nnmcp.saa-syze1.202401161655.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32201772 Jan 16 17:00 nnmcp.saa-syze1.202401161700.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32203880 Jan 16 17:05 nnmcp.saa-syze1.202401161705.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32138587 Jan 16 17:10 nnmcp.saa-syze1.202401161710.txt\n-rw-r--r-- 1 custompoller custompoller 32154540 Jan 16 17:15 nnmcp.saa-syze1.202401161715.txt\n-rw-r--r-- 1 custompoller custompoller 32179152 Jan 16 17:20 nnmcp.saa-syze1.202401161720.txt\n```\nNdef: `.LOADED` suffix indicates that the specific raw files has been transfered into cluster `un2:/data/1/nnm_custompoller_LZ/archives`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "syzefxis_flows.md - Part 8"
        }
    },
    "186": {
        "page_content": "```\nNdef: `.LOADED` suffix indicates that the specific raw files has been transfered into cluster `un2:/data/1/nnm_custompoller_LZ/archives`\nUnder normal circumstances, custompoller creates the following files every 5 minutes\n- a new raw file with filename format `nnmcp.saa-syze1.<yyyymmddHHMM>.txt`\n- a lock file with the same filename, `saa-syze1.lock`\ni.e.\n```\n-rw-r--r-- 1 custompoller custompoller        0 Jan 16 17:20 saa-syze1.lock\n-rw-r--r-- 1 custompoller custompoller 32179152 Jan 16 17:20 nnmcp.saa-syze1.202401161720.txt\n```\nNdef: Once the raw file completed, the lock file will be removed.  \nIn case an old lock file remains then the custompoller will stop generating new raw files.  \nLog file: `/home/custompoller/log/syzeyksis-2024-01-10.log`\n```\n16:10:02.429 ERROR [Thread-1] [saa-syze1] SNMPWalkTool: snmpWalkByOidsException: \njava.lang.IllegalStateException: Lock file /home/custompoller/out/saa-syze1.lock already exists.\n        at com.jkl.bigstreamer.snmp.tools.snmp4jwalk.SNMPWalkTool.createLockFile(SNMPWalkTool.java:198) ~[bigstreamer-snmp-tools-1.1.1-fixed.jar:1.1.1]\n        at com.jkl.bigstreamer.snmp.tools.snmp4jwalk.SNMPWalkTool.snmpWalkByOids(SNMPWalkTool.java:73) [bigstreamer-snmp-tools-1.1.1-fixed.jar:1.1.1]\n        at com.jkl.bigstreamer.snmp.tools.wrapper.runnables.NodeRunner.run(NodeRunner.java:33) [bigstreamer-snmp-tools-1.1.1-fixed.jar:1.1.1]\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]\n```\n### Solution for Stuck Raw File Generation\nIf raw file generation is stuck and a `.lock` file persists, SSH into the custompoller node and manually delete the stale lock file:\n```bash\nrm /home/custompoller/out/saa-syze1.lock\n#syzefxis #snmp #custompoller #rawdata #bigstreamer #hdfs #spark #impala #hive #metrics #kpis #sqoop #mysql #oozie #cron #dailyreport #monthlyreport #sla #performance-monitoring #pipeline #dataflow #export #troubleshooting\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "syzefxis_flows.md - Part 9"
        }
    },
    "187": {
        "page_content": "---\ntitle: Piraeus Cisco VDSL2 SNMP Polling and Delivery Flow\ndescription: Kubernetes-based SNMP polling application for Piraeus Bank's Cisco VDSL2 elements that collects, transforms, merges, and delivers network metrics to both HDFS and SFTP every 5 minutes, with automated monitoring and retry handling.\ncomponent: piraeus-cisco-vdsl2-app\nsystem: BigStreamer\nenvironment: Kubernetes\nnamespace: piraeus-cisco-vdsl2-deployment\nowner: root\ncluster_user: ipvpn\nsftp_host: un-vip.bigdata.abc.gr\nsftp_path: /shared/abc/ip_vpn/out/vdsl2\nhdfs_path: /ez/warehouse/bigcust.db/landing_zone/ext_tables/piraeus_vdsl2\ntarget_table: bigcust.vdsl2\nschedule: every 5 minutes\nmonitoring: enabled\nlast_updated: 2025-05-01\nkeywords:\n  - bigstreamer\n  - kubernetes\n  - snmp\n  - polling\n  - vdsl2\n  - piraeus\n  - ciscodevices\n  - metrics\n  - sftp\n  - hdfs\n  - cronjob\n  - pod\n  - spark\n  - transformation\n  - file merger\n  - delivery flow\n  - application.yaml\n  - network monitoring\n  - root user\n  - kerberos\n  - hadoop\n  - logs\n  - automation\n  - monitoring\n  - external delivery\n---\n# PIRAEUS CISCO VDSL2\n## Useful links\n- [Piraeus Cisco VDSL2 App](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-app)\n- [Piraeus Cisco VDSL2 DevOps](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-devops)\n- [Piraeus Cisco VDSL2 Deployment](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-deployment)\n- [Wiki Page](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-devops/-/wikis/home)\n- [File Definitions](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-devops/-/wikis/File-Definitions)\n- [Monitoring](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/home)\n- [Deployment Instructions](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-deployment/-/blob/main/Readme.md)",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "cisco_vdsl2.md - Part 1"
        }
    },
    "188": {
        "page_content": "- [Deployment Instructions](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-deployment/-/blob/main/Readme.md)\n## Overview\n`Piraeus Cisco Vdsl2 App` is an application that polls data every 5 minutes using SNMP, transforms the SNMPs output files, concatenates the files to one output file and then places it to an SFTP server and an HDFS directory, in order to be retrieved by the customer. The application runs in a Kubernetes pod. [Monitoring App](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/home#prod) is used for monitoring and health checks.\n**Pod User:** `root`  \n**Pod Scheduler:** `Cron`  \n**Kubernetes Namespace:** `piraeus-cisco-vdsl2-deployment`  \n**Cluster User:** `ipvpn`  \n**Container Registry:** `kubemaster-vip.bigdata.abc.gr/piraeus-cisco-vdsl2-app`  \n**Schedule:** `Every 5 minutes`  \n**Pod Script:** `/app/run_vdsl2.sh`  \n**Main Configuration File:** `/app/conf/application.yaml`  \n**Logs:** Use `kubectl logs` to view `stdout`  \n**Hadoop Table:** `bigcust.vdsl2`\n## Application Components\n### SNMP Polling of Elements\nThis step performs SNMP polling on VDSL2 network elements to collect performance metrics, storing the raw output files inside the pod.\n``` mermaid\n  graph TD\n  A[Piraeus Bank VTUs] -->|SNMP Polling| B[File: nnmcp.vdsl-g*.\\*.txt <br> Path: /app/work/data <br> Pod ]\n```\nThe application polls data using SNMP. The raw files produced, contain component metrics ([4 metrics](#metrics) for each Element) of the network elements and are stored in local path inside a Kubernetes pod.\n**Output Path Configuration:** `/app/conf/application.yaml` -> `snmppoller.dataDir`  \n**Output File Name Pattern:** `nnmcp.vdsl-g*.\\*.txt`  \n**Elements Configuration File:** `/app/conf/application.yaml` -> `snmppoller.endpoints`  \n**Keystore Path Configuration:** `/app/conf/application.yaml` ->  `snmppoller.keyStoreFilePath`\n### Transformation of SNMP files",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "cisco_vdsl2.md - Part 2"
        }
    },
    "189": {
        "page_content": "**Keystore Path Configuration:** `/app/conf/application.yaml` ->  `snmppoller.keyStoreFilePath`\n### Transformation of SNMP files\nIn this step, raw SNMP files are transformed into CSV format to standardize their structure for downstream processing.\n``` mermaid\n  graph TD\n  B[File: nnmcp.vdsl-g*.\\*.txt <br> Path: /app/work/data <br> Pod ] --> |Transform|D[File: nnmcp.vdsl-g*.\\*.txt.csv <br> Path: /app/work/data <br> Pod ]\n```\nAfter the data has been polled, the application transforms the output files to respective CSV files while formatting the data to fit the desired formation. The files are stored in local path inside a Kubernetes pod.\n**Output Path Configuration:** `/app/conf/application.yaml` -> `vdsl2.dataDir`  \n**Output File Name Pattern Configuration:** `/app/conf/application.yaml` -> `vdsl2.filePattern`  \n**Input File Pattern:** `nnmcp.vdsl-g*.\\*.txt.csv`  \n### Merging of transformed files\nAll transformed CSV files with matching timestamps are merged into a single consolidated deliverable file.\n``` mermaid\n  graph TD\n  D[File: nnmcp.vdsl-g*.\\*.txt.csv <br> Path: /app/work/data <br> Pod ] --> |Merge|E[File: VDSL2_*.csv <br> Path: /app/work/data <br> Pod ]\n```\nThe application then merges all the output csv files, which have the same timestamp, and produce a single deliverable file.\n**Output Path Configuration:** `/app/conf/application.yaml` -> `fmerger.dataDir`  \n**Output File Name Pattern Configuration:** `/app/conf/application.yaml` -> `fmerger.filePattern`  \n**Input File Pattern:** `VDSL2_*.csv`  \n### SFTP Transfer\nThe consolidated CSV file is sent to a remote SFTP server so that external systems can retrieve the data.\n``` mermaid\n  graph TD\n  E[File: VDSL2_*.csv <br> Path: /app/work/data <br> Pod ] --> |SFTP|F[File: VDSL2_*.csv <br> Path:/shared/abc/ip_vpn/out/vdsl2 <br> Host: un-vip.bigdata.abc.gr/999.999.999.999]\n```\nThe Piraeus Cisco Vdsl2 App places the deliverable file in an sftp server.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "cisco_vdsl2.md - Part 3"
        }
    },
    "190": {
        "page_content": "```\nThe Piraeus Cisco Vdsl2 App places the deliverable file in an sftp server.\n**Input Step Configuration:** `/app/conf/application.yaml` -> `ssh.inputFrom`\n**Input File Source Path Configuration:** `/app/conf/application.yaml` -> `ssh.source`  \n**SFTP Destination Path Configuration:** `/app/conf/application.yaml` -> `ssh.remdef`  \n**SFTP Host Configuration:** `/app/conf/application.yaml` -> `ssh.host`  \n**SFTP User Configuration:** `/app/conf/application.yaml` -> `ssh.user`  \n**SFTP Key Configuration:** `/app/conf/application.yaml` -> `ssh.prkey`  \n### HDFS transfer\nThe same output file is also stored in HDFS for long-term archival and analytics use by internal systems.\n``` mermaid\n  graph TD\n  E[File: VDSL2_*.csv <br> Path: /app/work/data <br> Pod ] --> |HDFS|G[File: VDSL2_*.csv <br> Path:/ez/warehouse/bigcust.db/landing_zone/ext_tables/piraeus_vdsl2 <br> HDFS]\n```\nThe Piraeus Cisco Vdsl2 App places the deliverable file in a hdfs directory for archiving.\n**Input Step Configuration:** `/app/conf/application.yaml` -> `hdfsput.inputFrom` (Step that provides files to HDFS upload)\n**Input File Path Configuration:** `/app/conf/application.yaml` -> `hdfsput.dataDir`  \n**Output HDFS URL Configuration:** `/app/conf/application.yaml` -> `hdfsput.hdfsURL`  \n**Output HDFS Destination Path Configuration:** `/app/conf/application.yaml` -> `hdfsput.hdfsPath`  \n**Hadoop User Configuration:** `/app/conf/application.yaml` -> `hdfsput.hadoopUser`  \n**Hadoop Site Configuration:** `/app/conf/application.yaml` -> `hdfsput.hadoopSite`  \n**Kerberos Configuration File:** `/app/conf/application.yaml` -> `hdfsput.kerberos.krb5conf`  \n**Kerberos User Keytab Path Configuration:** `/app/conf/application.yaml` -> `hdfsput.kerberos.keytab`  \n**Kerberos Principal Configuration:** `/app/conf/application.yaml` -> `hdfsput.kerberos.principal`\n## Metrics",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "cisco_vdsl2.md - Part 4"
        }
    },
    "191": {
        "page_content": "**Kerberos Principal Configuration:** `/app/conf/application.yaml` -> `hdfsput.kerberos.principal`\n## Metrics\n| Requirement                         | Metric                          | Metric OID                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| ----------------------------------- | ------------------------------- | --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "cisco_vdsl2.md - Part 5"
        }
    },
    "192": {
        "page_content": "| Connected (actual) Speed downstream | xdsl2LineStatusAttainableRateDs | transmission.999.999.999.999.1.20 | Maximum Attainable Data Rate Downstream. The maximum downstream net data rate currently attainable by the xTU-C transmitter and the xTU-R receiver, coded in bit/s.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| Connected (actual) Speed upstream   | xdsl2LineStatusAttainableRateUs | transmission.999.999.999.999.1.21 | Maximum Attainable Data Rate Upstream. The maximum upstream net data rate currently attainable by the xTU-R transmitter and the xTU-C receiver, coded in bit/s.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "cisco_vdsl2.md - Part 6"
        }
    },
    "193": {
        "page_content": "| Signal Attenuation per band         | xdsl2LineBandStatusSigAtten     | transmission.999.999.999.999.1.3  | When referring to a band in the downstream direction, it is the measured difference in the total power transmitted by the xTU-C and the total power received by the xTU-R over all  subcarriers of that band during Showtime. When referring to a band in the upstream direction, it is the measured difference in the total power transmitted by the xTU-R and the total power received by the xTU-C over all subcarriers of that band during Showtime. Values range from 0 to 1270 in units of 0.1 dB (physical values are 0 to 127 dB). A special value of 0x7FFFFFFF (2147483647) indicates the line attenuation is out of range to be represented. A special value of 0x7FFFFFFE (2147483646) indicates the line attenuation measurement is unavailable. |\n| Noise margin per band               | xdsl2LineBandStatusSnrMargin    | transmission.999.999.999.999.1.4  | SNR Margin is the maximum increase in dB of the noise power received at the xTU (xTU-R for a band in the downstream direction and xTU-C for a band in the upstream direction), such that the BER requirements are met for all bearer channels received at the xTU.  Values range from -640 to 630 in units of 0.1 dB (physical values are -64 to 63 dB). A special value of 0x7FFFFFFF (2147483647) indicates the SNR Margin is out of range to be represented. A special value of 0x7FFFFFFE (2147483646) indicates the SNR Margin measurement is currently unavailable.                                                                                                                                                                                     |",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "cisco_vdsl2.md - Part 7"
        }
    },
    "194": {
        "page_content": "---\ntitle: Energy-Efficiency Pollaploi Load to Impala  \ndescription: Nightly Oozie workflow that ingests energy efficiency data from .txt files (zipped via SFTP) into the energy_efficiency.pollaploi Impala table using HDFS staging and SSH-based script execution.  \njob_name: coord_energy_efficiency_load_pollaploi  \ncomponent: energy_efficiency_load_pollaploi  \nsystem: BigStreamer  \nhost: un2.bigdata.abc.gr  \ntarget_table: energy_efficiency.pollaploi  \nsource_file_format: zip (.txt inside)  \nsource_transfer_protocol: SFTP  \nsource_server: 999.999.999.999:/energypm  \ncoordinator: coord_energy_efficiency_load_pollaploi  \nworkflow: energy_efficiency_load_pollaploi  \nworkflow_user: intra  \nworkflow_script_path: /shared/abc/energy_efficiency/load_pollaploi/pollaploi/pollaploi.sh  \nworkflow_execution_node: un-vip.bigdata.abc.gr  \nworkflow_ssh_user: intra2  \nworkflow_schedule_winter: 21:00 local time  \nworkflow_schedule_dst: 22:00 local time  \nexecution_duration: ~8 seconds  \ntemp_dir: /shared/abc/energy_efficiency/load_pollaploi/pollaploi_temp  \ncurr_dir: /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr  \nlog_dir: /shared/abc/energy_efficiency/load_pollaploi/log  \nhdfs_input_dir: /ez/landingzone/energy_temp/  \nimpala_db: energy_efficiency  \nimpala_table: pollaploi  \nimpala_load_command: LOAD DATA INPATH  \nlog_retention: none  \npassword_vault: https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx  \nexample_data_file: https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/blob/master/FLOWS/energy_efficiency/TEST%20%CE%91%CE%A1%CE%A7%CE%95%CE%99%CE%91/2019_05_pollaploi.txt  \ngitlab_repo: https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/tree/master/FLOWS/energy_efficiency  \nkeywords:  \n  - energy  \n  - efficiency  \n  - pollaploi  \n  - ssh  \n  - impala  \n  - oozie  \n  - sftp  \n  - txt  \n  - hdfs  \n  - hive  \n  - metadata refresh  \n  - landingzone  \n  - table load  \n  - un2  \n  - intra",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "energy_efficiency_pollaploi.md - Part 1"
        }
    },
    "195": {
        "page_content": "- efficiency  \n  - pollaploi  \n  - ssh  \n  - impala  \n  - oozie  \n  - sftp  \n  - txt  \n  - hdfs  \n  - hive  \n  - metadata refresh  \n  - landingzone  \n  - table load  \n  - un2  \n  - intra  \n  - passwordless auth  \n  - file comparison  \n  - zipped transfer  \n  - workflow troubleshooting  \n  - devpasswd  \n---\n# Energy-Efficiency Pollaploi\n## Overview\nThis section explains the Oozie job responsible for importing energy efficiency data from flat files into Hive/Impala tables.\nThis is an `Oozie Flow` responsible to **load data** from **txt files** into **impala tables**. Through the **Oozie Workflow** a **ssh** action is performed which executes the `pollaploi.sh` script. \n- **Utility Node / Server:** `un2.bigdata.abc.gr`\n  - **User:** `intra`\n  - [Password](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)\n  - **Main File Directory:** `/shared/abc/energy_efficiency/load_pollaploi/`\n- **Oozie Coordinator**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `intra`\n  - **Coordinator:** `coord_energy_efficiency_load_pollaploi`\n    - **Execution:** \n      - **Winter time:** `every day at 21:00 local time (9PM)`\n      - **Daylight saving time:** `every day at 22:00 local time (10PM)`\n    - **Approximate Duration:** `8 seconds`\n    - **Workflow:** `energy_efficiency_load_pollaploi`\n      - **SSH Server:** `un-vip.bigdata.abc.gr`\n      - **SSH User:** `intra2`\n      - [Script](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/blob/master/FLOWS/energy_efficiency/PROD/load_pollaploi/pollaploi/pollaploi.sh)\n      - **Logs:** `view through job run - NO LOGS`\n## Pollaploi Flow",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "energy_efficiency_pollaploi.md - Part 2"
        }
    },
    "196": {
        "page_content": "- **Logs:** `view through job run - NO LOGS`\n## Pollaploi Flow\nThe `pollaploi flow` gets a .txt file from a remdef sftp directory and moves it to a temporary directory on the utility node. Then it unzips the file that was just transferred and compares it to another.txt file in the curr directory on the utility node. If those files are the same then it does nothing, since it means that the file has already been processed by the flow. If the file names are different then it removes the old file in the curr directory and moves the new file from the temp to the curr directory. After that the new file in the curr directory is put in a hdfs path. From there impala queries are executed clearing the pollaploi table, loading the data from the new file and refreshing the pollaploi table. \n- **SFTP:** \n   - **Initiator:** `intra` user\n\t- **User:** `bigd`\n\t- **Password:** `passwordless`\n\t- **Server:** `999.999.999.999`\n\t- **Path:** `/energypm/`\n\t- **Compressed File:** `*_pollaploi.zip` containing `*_pollaploi.txt`\n- **Utility Node Directories:**\n\t- **Curr:** `/shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr`\n\t- **Temp:** `/shared/abc/energy_efficiency/load_pollaploi/pollaploi_temp`\n  - **Scripts:** `/shared/abc/energy_efficiency/load_pollaploi/pollaploi`\n- **HDFS:**\n\t- **Path:** `/ez/landingzone/energy_temp/`\n- **Impala:** \n\t- **Database:** `energy_efficiency`\n\t- **Table Name:** `pollaploi`\n\t- **Retention:** `-`\n- **Logs:**\n  - **Path:** `/shared/abc/energy_efficiency/load_pollaploi/log`\n  - **Retention:** `none` (since 15/12/2019)",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "energy_efficiency_pollaploi.md - Part 3"
        }
    },
    "197": {
        "page_content": "- **Table Name:** `pollaploi`\n\t- **Retention:** `-`\n- **Logs:**\n  - **Path:** `/shared/abc/energy_efficiency/load_pollaploi/log`\n  - **Retention:** `none` (since 15/12/2019)\n**_Ndef:_** One of the `impala-shell` queries executed is the `LOAD DATA INPATH <hdfs_path>/<filename>` As seen in this [article](https://impala.apache.org/docs/build/html/topics/impala_load_data.html) the LOAD DATA INPATH command moves the loaded data file (not copies) into the Impala data directory. So the log entry `rm: /ez/landingzone/energy_temp/2023_03_01_pollaploi.txt': No such file or directory` is not something to worry about. \n**_[Sample data from pollaploi table](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/blob/master/FLOWS/energy_efficiency/TEST%20%CE%91%CE%A1%CE%A7%CE%95%CE%99%CE%91/2019_05_pollaploi.txt)_** \n## Troubleshooting Steps\nGuidance for identifying if a file has been successfully picked up, processed, and loaded.\nDue to the occurance of these tickets (**SD2179931** and **SD2021989**) the below steps should be followed for troubleshooting.\n1. Check that a new file `*pollaploi.zip` is placed in the `remdef SFTP directory`. Because the `workflow` runs in the evening (9PM or 10PM), if a file is placed earlier in the remdef SFTP directory and the client asks why it hasn't been loaded, wait until the next day and follow the steps mentioned here to see its execution.\n1. Check that a file `*pollaploi.txt` exists in `/shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr`.\n1. Based on the `date` the file has in `/shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr` check the log file `pollaploi.<YYYYMMDD>.log` of that specific day. E.g. \n```bash\n$ ls -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/\n> -rw-r--r-- 1 intra intra 1034132960 Mar 22 18:34 2023_03_01_pollaploi.txt\n$ less /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.20230322.log\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "energy_efficiency_pollaploi.md - Part 4"
        }
    },
    "198": {
        "page_content": "> -rw-r--r-- 1 intra intra 1034132960 Mar 22 18:34 2023_03_01_pollaploi.txt\n$ less /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.20230322.log\n```\n1. In `Hue` go to `Jobs` and search `energy` in the search bar. View the last executed `workflow` and see if it has run successfully.   \n### Possible Response to Ticket\nHow to respond to common tickets from clients regarding missing table updates.\n**_Ticket:_**\n``` \nthe new pollaploi file has been uploaded but the corresponding table has not been updated yet\n```\n**_Response:_** (example)\n```\nThere seems to be no new file in the sftp directory /energypm. The workflow that loads the table runs every day at 9PM/10PM, so if a file is added today it will be loaded in the evening. The last file that has been loaded is named 2023_03_01_pollaploi and from the logs on 2023-03-22 it seems to have been loaded normally.\n```\n## Useful Links\nCode and documentation related to the pollaploi workflow.\n- **[GitLab Repo](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/tree/master/FLOWS/energy_efficiency)**",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "energy_efficiency_pollaploi.md - Part 5"
        }
    },
    "199": {
        "page_content": "---\ntitle: Reference Data ETL Flow\nsystem: BigStreamer\ncomponent: Reference Data\njob_name: refdata_etl\nsource_system: Local Filesystem (vantagerd)\nsource_tables:\n  - cells_YYYYMMDD.csv.gz\n  - crm_YYYYMMDD.csv.gz\n  - devices_YYYYMMDD.csv.gz\n  - services_YYYYMMDD.csv.gz\ndestination_system: Hive\ndestination_tables:\n  - refdata.rd_cells_load\n  - refdata.rd_services_load\n  - refdata.rd_crm_load\n  - refdata.rf_devices_load\nschedule: daily at 00:05\ncoordinator: none (cron-based)\nworkflow: 210_refData_Load.sh / 220_refData_Daily_Snapshot.sh\nscript_path: /shared/abc/refdata/bin\nmonitoring_table: none\nowner: intra\ntags:\n  - Reference Data\n  - refdata\n  - rd_cells_load\n  - rd_crm_load\n  - rd_services_load\n  - rf_devices_load\n  - Hive\n  - Snapshot\n  - Crontab\n  - Bash Scripts\n  - Troubleshooting\n  - Data Validation\n  - Vantagerd\n---\n# Reference Data Flow\nThis document describes the reference data ingestion process for cells, CRM, devices, and services. It includes file locations, loading scripts, cron scheduling, Hive targets, and troubleshooting steps.\n## Installation info\nDescribes the input/output directories, involved nodes and users, script and log locations, and crontab setup for automatic file loading.\n### Data Source File\n- Local FileSystem Directories\n  - host :`un2.bigdata.abc.gr (999.999.999.999)`\n  - user : `vantagerd`\n  - spool area : `/data/1/vantage_ref-data/REF-DATA/` link points to `/shared/vantage_ref-data/REF-DATA`\n  - file_types : `<refType>_<refDate>.csv.gz`  \n*\\<refType\\>: `cells, crm, devices, services` (i.e. cells_20230530.csv.gz)*\n  - load_suffix : `<YYYYMMDD>.LOADED` *(i.e. cells_20230530.csv.cells_20230531.LOADED)*\n- HDFS Directories\n\t- hdfs landingzone : `/ez/landingzone/REFDATA`\n### Scripts-Logs Locations\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\n- user : `intra`\n- script path : `/shared/abc/refdata/bin`\n- script files: \n\t- `210_refData_Load.sh`\n\t- `220_refData_Daily_Snapshot.sh`\n- log path : `/shared/abc/refdata/log`\n- log files:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Reference_Data_Flow.md - Part 1"
        }
    },
    "200": {
        "page_content": "- user : `intra`\n- script path : `/shared/abc/refdata/bin`\n- script files: \n\t- `210_refData_Load.sh`\n\t- `220_refData_Daily_Snapshot.sh`\n- log path : `/shared/abc/refdata/log`\n- log files: \n\t- `210_refData_Load.<YYYYMM>.log`\n\t- `220_refData_Daily_Snapshot.<YYYYMM>.log`\n### Crontab Scheduling\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\n- user : `intra`  \n\truns at : Daily at 00:05\n\t`5 0 * * * /shared/abc/refdata/bin/210_refData_Load.sh CELLS $(date '+\\%Y\\%m\\%d' -d \"yesterday\")`\nNdef1: The entry above loads reference data for CELLS.  \nNdef2: For each reference type a new entry of the `210_refData_Load.sh` is required passing the following parameters  \n\t\\<reference Type\\> : `cells, crm, devices, services`  \n\t\\<reference Date\\> : `yesterday` is the default value  \n### Hive Tables\n- Target Database: `refdata`\n- Target Tables: \n\t1. `rd_cells_load`\n\t1. `rd_services_load`\n\t1. `rd_crm_load`\n\t1. `rf_devices_load`\n| Table Name         | Description                      |\n|--------------------|----------------------------------|\n| rd_cells_load      | Historical data of cell metadata |\n| rd_services_load   | Historical data of services      |\n| rd_crm_load        | Historical data of CRM records   |\n| rf_devices_load    | Historical data of device info   |\n## Data process\nHigh-level overview of how reference files are prepared, parsed, and loaded into historical and snapshot Hive tables.\n### High Level Overview\n![High_Level_Overview](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/refdata/-/raw/main/docs/ReferenceData.High_Level_Overview.png)\n##### Steps 1-3: \nabc is responsible for the preparation/creation of the Reference Data flat files.  \nThese files are stored into a specific directory in `UN2` node using the SFTP-PUT method as user `vantagerd`  \n##### Steps 4-5:\nScript `210_refData_Load.sh` is responsible to read, parse and load the contents of reference files into HIVE tables (aka LOAD tables).",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Reference_Data_Flow.md - Part 2"
        }
    },
    "201": {
        "page_content": "##### Steps 4-5:\nScript `210_refData_Load.sh` is responsible to read, parse and load the contents of reference files into HIVE tables (aka LOAD tables).  \nThese tables keep the data of all completed loads. That is, they contain all the historicity of the reference data.  \nThe data of each load is stored in a separate partition identified by the date of the loading (i.e. par_dt=20230530)  \n##### Steps 6-7:\nScript `220_refData_Daily_Snapshot.sh` reads the most recently added data from the LOAD table and store them as a snapshot into a separate table (aka snapshot tables).  \nThe data of these tables are used for the enrichment of various fact data (i.e. Traffica (SAI))\n## Manually Run\nInstructions for manually triggering the data load process for a specific file and date using the load script.\n`210_refData_Load.sh` script is responsible for the loading of a reference file.\nTo run the script two arguments are required  \n`/shared/abc/refdata/bin/210_refData_Load.sh <refType>  <refDate>`  \n1st: **\\<refType\\>**, the Reference Type\n```\n- CELLS\n- CRM\n- DEVICES\n- SERVICES\n```\n2nd: **\\<refDate\\>**, the date that the flat file contains in its filename  \n\ti.e.\n```\ncells_20220207.csv.gz\ncells_20220208.csv.gz\ncells_20220209.csv.gz\nservices_20220207.csv.gz\ndevices_20220208.csv.gz\ncrm_20220209.csv.gz\n```\nIn case of loading the files above we should execute the following commands\n```bash\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20220207\n/shared/abc/refdata/bin/210_refData_Load.sh cells 20220208\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20220209\n/shared/abc/refdata/bin/210_refData_Load.sh services 20220207\n/shared/abc/refdata/bin/210_refData_Load.sh DEVICES 20220208\n/shared/abc/refdata/bin/210_refData_Load.sh crm 20220209\n```\nOnce the loads completed, the flat files renamed to <CURRENT_YYYYMMDD>.LOADED\n```\ncells_20220207.csv.20230531.LOADED\ncells_20220208.csv.20230531.LOADED\ncells_20220209.csv.20230531.LOADED\nservices_20220207.csv.20230531.LOADED",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Reference_Data_Flow.md - Part 3"
        }
    },
    "202": {
        "page_content": "```\ncells_20220207.csv.20230531.LOADED\ncells_20220208.csv.20230531.LOADED\ncells_20220209.csv.20230531.LOADED\nservices_20220207.csv.20230531.LOADED\ndevices_20220208.csv.20230531.LOADED\ncrm_20220209.csv.20230531.LOADED\n```\n## Troubleshooting\nExplains how to identify and resolve issues with reference data loads using error logs and manual script execution.\n**Currently, the Reference Data flow does not support the `Monitoring` services.**  \n- An email will be sent by the system with the point of failure.\ni.e.\n```\nSubject: ALERT: Reference data Loading, Type:CELL,  File:cells_20220207.csv\nBody: \n\tReference Type  : CELL\n\tReference File  : cells_20220207.csv\n\tReference Scirpt: 210_refData_Load.sh\n\t------------------------------------------\n\tERROR:$(date '+%F %T'), ALTER TABLE or LOAD DATA command failed.\n```\n- Check the log files for errors/exceptions  \n```bash\negrep -i 'error|fail|exception|problem' /shared/abc/refdata/log/210_refData_Load.YYYYMM.log\negrep -i 'error|fail|exception|problem' /shared/abc/refdata/log/220_refData_Daily_Snapshot.YYYYMM.log\n```\nIn case of failure follow the instructions described in **`Manually Run`**\n### Common errors  \n- Reference data file is empty or the contents of the file is not the expected.  \nIf this is the case, update abc that the file is invalid and ask them to send a new.  \n- Other factors not related to the specific flow\n\t- impala/hive availability\n\t- Kerberos authentication\n\t*Ndef: The flow checks if the ticket is still active before any HDFS action.  \n\tIn case of expiration the flow performs a `kinit` command*\n## Data Check\nProvides queries and instructions to verify data load completeness and partition existence in Hive tables.\n- **Check final tables for new partitions**:\n\t```\n\tsu - intra\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k -q \"refresh refdata.rd_cells_load; show partitions refdata.rd_cells_load;\"\n\t\n\t+----------+-----------+--------+---------+\n\t| par_dt   | #Rows     | #Files | Size    |",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Reference_Data_Flow.md - Part 4"
        }
    },
    "203": {
        "page_content": "+----------+-----------+--------+---------+\n\t| par_dt   | #Rows     | #Files | Size    |\n\t+----------+-----------+--------+---------+\n\t| 20220227 | 98090     | 1      | 41.88MB |\n\t| 20220228 | 98021     | 1      | 41.84MB |\n\t| 20220301 | 97353     | 1      | 41.76MB |\n\t| Total    | 142404322 | 1500   | 59.63GB |\n\t+----------+-----------+--------+---------+\n\t```\n- **Check the amount of data in final tables**:\n```bash\nsu - intra\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k -q \"select par_dt, count(*) as cnt from refdata.rd_cells_load group by par_dt order by 1;\"\n\tpar_dt   | cnt    \n\t---------+--------\n\t20221130 | 2784494\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "Reference_Data_Flow.md - Part 5"
        }
    },
    "204": {
        "page_content": "---\ntitle: Radius ETL Flow and Kudu Housekeeping\nsystem: BigStreamer\ncomponent: Radius\njob_name: Radius_Load_Workflow\nsource_system: SFTP Server (prdts)\nsource_tables:\n  - radarchive CSV files\n  - radacct CSV files\ndestination_system: Hive & Kudu\ndestination_tables:\n  - radius.radarchive\n  - radius.radacct\n  - radius.radreference\nschedule: every 1h and 30min\ncoordinator: Radius_Load_Coordinator\nworkflow: Radius_Load_Workflow\nscript_path: /user/radius\nmonitoring_table: monitoring.jobstatus\nowner: radius\ntags:\n  - Radius\n  - ETL\n  - radarchive\n  - radacct\n  - Kudu\n  - Hive\n  - BigStreamer\n  - SFTP\n  - Monitoring\n  - Oozie\n  - Retention\n  - Impala Stats\n  - Workflow Troubleshooting\n---\n# Radius\nThe Radius ETL process ingests user accounting data and session logs from the Trustcenter SFTP server. It transforms and loads the files into Hive and Kudu tables through hourly workflows and maintains data freshness using regular housekeeping and statistics recomputation.\n## Main Flow\nThis section describes the main ETL pipeline of the Radius flow, which loads and processes radarchive and radacct CSVs from an external SFTP server into Hive and Kudu tables.\n``` mermaid\n    graph TD\n    subgraph AA[Startup]\n      direction TB\n      AA1(\"Cleanup HDFS Folder:\" /ez/warehouse/radius.db/tmp)-->\n      AA2(\"Create Local Folder:\" ./sftp_files)-->\n      AA3(\"Create Local Folder:\" ./exported_files)-->\n      AA4(\"Housekeep Trustcenter SFTP server\")\n    end\n    subgraph AB[Load]\n      AB1(\"SFTP Server\") -->|Excluding:<br>Non-valid filenames<br>Files with suffix .LOADED<br>Files older than `days_back` according to filename | AB2(\"Temporary Directory on Nodemanager: ./sftp_files\")\n      AB1 -->|Add suffix .LOADED to<br>filenames in HDFS file:<br>/user/radius/unrenamed_files|AB1\n      AB2 -->|Decompress| AB3(\"Temporary Directory on Nodemanager: ./sftp_files\")\n    end\n    subgraph AC[radarchive]",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "radius.md - Part 1"
        }
    },
    "205": {
        "page_content": "AB2 -->|Decompress| AB3(\"Temporary Directory on Nodemanager: ./sftp_files\")\n    end\n    subgraph AC[radarchive]\n      AC1(\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\") -->|Hive LOAD|AC2(\"Hive: radius.radarchive_stg\")\n      AC2 -->|Refresh/Impala Insert|AC3(\"Impala: radius.radarchive\")\n      AC2 -->|Refresh/Impala Upsert<br>Reduced fields|AC4(\"Kudu: radius.radreference\")\n    end\n    subgraph AD[radacct]\n      subgraph ADA[File Export]\n        direction TB\n        AD1(\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\") -->|Hive LOAD|AD2(\"Hive: radius.radacct_stg\")\n        AD6(\"npce.fixed_super_repo<br>Responsibility: abc\") -->AD8\n        AD7(\"demo.dummy_radius_dslams<br>Responsibility: abc\") -->AD8\n        AD2 --> AD8(\"Join\")\n        AD8 -->|Refresh/Impala Select|AD9(\"Temporary Directory on Nodemanager:<br> ./exported_files/radacct_enriched_yearMonthDay_startHour-endHour.csv\")\n        AD9 -->|SFTP Put|AD10(\"Trustcenter SFTP\")\n      end\n      subgraph ADB[Populate Table]\n        direction TB\n        AD3(\"Kudu: radius.radreference\") -->AD4\n        AD11(\"Hive: radius.radacct_stg\") -->AD4(\"Join on username\")\n        AD4 -->|Refresh/Impala Insert|AD5(\"Impala: radius.radacct\")\n      end\n      AD10 -.->|On successful SFTP PUT<br>Populate the table| AD3\n    end\n    subgraph AE[Finish]\n      AE1(\"SFTP Server: Add suffix .LOADED to file\")\n    end\n    AA4 --> AB1\n    AB3 -->|Filename: radarchive_yearMonthDay_startHour-endHour.csv|AC1\n    AB3 -->|Filename: radacct_yearMonthDay_startHour-endHour.csv|AD1\n    AC4 -->|On Success|AE1\n    AD5 -->|On Success|AE1\n```\n- **User**: `radius`  \n- **Coordinator**: `Radius_Load_Coordinator`  \n- **Workflow**: `Radius_Load_Workflow`\n- **HDFS path**: `/user/radius`\n- **Runs**: `every 1h and 30mins`\n- **Config file**: `HDFS: /user/radius/config/settings_prod.ini`\n- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n- **Input SFTP Server**:\n  - **Host**: `999.999.999.999`\n  - **Port**: `22`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "radius.md - Part 2"
        }
    },
    "206": {
        "page_content": "- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n- **Input SFTP Server**:\n  - **Host**: `999.999.999.999`\n  - **Port**: `22`\n  - **User**: `prdts`\n  - **Remdef Files Folder**: `/home/prdts/transfer`\n  - **Port Forward**:\n    - **Host**: `un-vip.bigdata.abc.gr`\n    - **Port**: `2222`\n- **Trustcenter SFTP Server**:\n  - **Host**: `unc2.bigdata.abc.gr`\n  - **Port**: `22`\n  - **User**: `trustcenterftp`\n  - **Remdef Files Folder**: `/rd`\n**Alerts**:\n- Mail\n  - Subject: Radius Flow failed\n  - Alerts that indicate problem with Input SFTP server:\n    - Body starts with: `No upcoming files for more than 3h`\n    - Body starts with: `Files found with a late timestamp.`\n    - Body starts with: `Could not rename file`\n  - Alerts that indicate problem with Trustcenter SFTP Server:\n    - Body starts with: `Could not perform radacct_enriched housekeeping on sftp server`\n  - Alerts that indicate general failures without specific cause:\n    - Body starts with: `Insert data failed` and then lists the status for each file\n**Troubleshooting Steps**:\n- If an alert has been received, use the message from the e-mail to determine the root cause for the failure.\n  - For failures with Input SFTP server:\n    - Inform abc in order to check the Input SFTP Server\n    - If abc does not detect any problems, check connectivity:\n      From `un2.bigdata.abc.gr` with personal user:\n      ``` bash\n      su - radius\n      sftp prdts@999.999.999.999\n      # Check for files\n      sftp> ls -l\n      ```\n  - For failures with Trustcenter SFTP Server:\n    - Check `unc2.bigdata.abc.gr` for any errors\n- If no alerts have been received or the problem is not specified determine the root cause for the failure with the following steps:\n  - Check for failed executions\n    From `un2.bigdata.abc.gr` with personal user:\n    ```bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "radius.md - Part 3"
        }
    },
    "207": {
        "page_content": "- Check for failed executions\n    From `un2.bigdata.abc.gr` with personal user:\n    ```bash\n    curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=RADIUS&status=FAILED&operativePartition=<date in YYYYMMDD e.g.:20220518>'\n    ```\n    For additional query parameters check [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n    or check via the corresponding [Grafana dashboard](https://unc1.bigdata.abc.gr:3000/d/J4KPyBoVk/radius-dashboard?orgId=1&from=now-2d&to=now)\n  - Check the logs for the failed execution.\n- After the underlying problem has been corrected the next execution of the flow will load any files that have been delayed or failed to be loaded in a previous execution.\n- If the customer requests to load specific files after they have been processed by the flow. **In order to avoid duplicate data we have to reload the whole partition.**\n  - Ensure the files are still available on the SFTP server\n    From `un2.bigdata.abc.gr` with personal user:\n    ``` bash\n    su - radius\n    sftp prdts@999.999.999.999\n    # Check for files covering the whole date affected\n    sftp> ls -l *.LOADED\n    ```\n  - Suspend coordinator `Radius_Load_Coordinator`\n  - Drop partitions that were not properly loaded from following tables\n    From `un2.bigdata.abc.gr` with personal user:\n    ``` bash\n    su - radius\n    kinit -kt /home/users/radius/radius.keytab radius\n    impala-shell -i un-vip.bigdata.abc.gr -k --ssl\n    ```\n    - In case of radarchive category of file:\n      ```sql\n      alter table radius.radarchive drop partitions (par_dt=\"<date in YYYYMMDD e.g.: 20220915>\")\n      ```\n    - In case of radacct category of file:\n      ```sql\n      alter table radius.radacct drop partitions (par_dt=\"<date in YYYYMMDD e.g.: 20220915>\")\n      ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "radius.md - Part 4"
        }
    },
    "208": {
        "page_content": "```\n    - In case of radacct category of file:\n      ```sql\n      alter table radius.radacct drop partitions (par_dt=\"<date in YYYYMMDD e.g.: 20220915>\")\n      ```\n  - Connect to SFTP server and rename files with filename timestamp equal to above-mentioned partition(s) and the same category:\n    From `un2.bigdata.abc.gr` with personal user:\n    ``` bash\n    su - radius\n    sftp prdts@999.999.999.999\n    # For every file you need to reload\n    sftp> rename <filename>.LOADED <filename>\n    ```\n  - Resume coordinator `Radius_Load_Coordinator`\n## Kudu Housekeeping and Compute Statistics Flow\nThis secondary flow enforces retention on the Kudu table radius.radreference and recomputes Impala statistics for radarchive and radacct daily.\n```mermaid\n  graph TD\n  A[Apply 5-day rention to<br>Kudu: radius.radreference]\n  B[Compute Impala Statistics for<br>radius.radarchive<br>radius.radacct]\n```\n- **User**: `radius`  \n- **Coordinator**: `Radius_Kudu_Retention_Coordinator` \n- **Workflow**: `Radius_Kudu_Retention_Workflow`\n- **HDFS path**: `/user/radius`\n- **Runs**: `once a day at 2:25 (UTC)`\n- **Config file**: `hdfs: /user/radius/Kudu_Retention_And_Compute_Stats/settings.ini`\n- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n**Alerts**:\n- Not monitored\n**Troubleshooting Steps**:\n- Check the logs for the failed execution.\n- After the underlying problem has been corrected the next execution of the flow will apply the correct retention to the Kudu table and compute the missing statistics.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_abc",
            "name": "radius.md - Part 5"
        }
    },
    "209": {
        "page_content": "---\ntitle: \"Online\"\ndescription: \"Real-time and batch ingestion pipeline for Online transactions. Covers Wildfly, Kafka, Spark Streaming, daily batch jobs, hourly merging, and query handling for downstream applications and reporting.\"\ntags:\n  - online\n  - ingestion\n  - kafka\n  - spark\n  - kudu\n  - hbase\n  - wildfly\n  - streaming\n  - batch\n  - parquet\n  - queries\n  - impala\n  - retention\n  - oozie\n  - prodrest\n---\n# Online\n## Stream\nProcesses Online transaction events in real-time from backend servers via Wildfly and Kafka, storing them through Spark into Kudu and HBase.\n### Wildfly Transaction Ingestion\nWildfly instances receive HTTP POST events from backend servers, loadbalanced through NetScaler, and forward them to Kafka topics at the active site.\nUser generated events are sent to Wildfly instances installed at edge servers as HTTP POST requests. The requests originate from the backend servers of Internet Banking. Requests are loadbalanced by NetScaler (Loadbalancer) that is managed by mno's networking department. The events are then sent to a Kafka topic. Usually only one site is active (Primary Site).\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\n  B -->|Active| C[pr1edge01.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]\n  C -->|Kafka Producer| G[Primary Site's Kafka: prod-trlog-online-ingest-stream-mir]\n  D -->|Kafka Producer| G\n  E -.->|Stopped| H[Disaster Site's Kafka: prod-trlog-online-ingest-stream-mir]\n  F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodreston`\n**Deployments Path**: `/opt/wildfly/default/prodreston/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 1"
        }
    },
    "210": {
        "page_content": "**Application Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodreston/server.log`\n**Access Log**: `/var/log/wildfly/prodreston/access.log`\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\n**Alerts**:\n- [PR][ONLINE] REST Endpoints Average Response Time\n- [DR][ONLINE] REST Endpoints Average Response Time\n- [PR][ONLINE] Ingestion Average Error rate\n- [DR][ONLINE] Ingestion Average Error rate\n**Troubleshooting Steps**:\n- Check application logs for error messages.\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\n- If the logs do not indicate a specific problem, check for any problems regarding Kafka performance.\n### Kafka Mirroring via MirrorMaker\nExplains how Kafka MirrorMaker replicates `-mir` topics between sites to ensure high availability and simplify failover.\nAfter being published to Kafka, the traffic is mirrored to the opposite site via Kafka MirrorMaker. In order to be able to have both Sites active at any given time and simplify the failover procedure the each Site produce messages to a topic with the suffix `-mir`. The topics with the `-mir` suffix contain events published to the Site. The `-mir` topics are then copied via Kafka MirrorMakers to the final topics (no `-mir` suffix), that has all the messages from both Sites.\n#### PR replication\n```mermaid\n  graph TD\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-online-ingest-stream-mir]\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-online-ingest-stream-mir]\n  E[dr1edge02.mno.gr] -.->|Stopped| F",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 2"
        }
    },
    "211": {
        "page_content": "B[pr1edge02.mno.gr] -->|Kafka Producer| C\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-online-ingest-stream-mir]\n  E[dr1edge02.mno.gr] -.->|Stopped| F\n  C -->|Kafka MirrorMakers <br> pr1node01.mno.gr/pr1node04.mno.gr| G[PR Site Kafka: prod-trlog-online-ingest-stream]\n  F -->|Kafka MirrorMakers <br> pr1node05.mno.gr/pr1node06.mno.gr| G[PR Site Kafka: prod-trlog-online-ingest-stream]\n```\n#### DR replication\n``` mermaid\n  graph TD\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-online-ingest-stream-mir]\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-online-ingest-stream-mir]\n  E[dr1edge02.mno.gr] -.->|Stopped| F\n  F -->|Kafka MirrorMakers <br> dr1node01.mno.gr/dr1node04.mno.gr| H[DR Site Kafka: prod-trlog-online-ingest-stream]\n  C -->|Kafka MirrorMakers <br> dr1node05.mno.gr/dr1node06.mno.gr| H[DR Site Kafka: prod-trlog-online-ingest-stream]\n```\n**MirrorMaker User**: `kafka`\n**Configuration**: Cloudera Manager\n**Logs**: Cloudera Manager\n**Instructions for start/stop**: [Here](../procedures/manage_mirrormaker.md)\n**Alerts**:\n- Cloudera Manager alerts regarding Kafka\n### Spark Streaming Topologies\nDescribes the Spark Streaming topologies that consume Kafka data and write them to Kudu and HBase, including support for oversized columns.\nAfter being sent to the final Kafka topic, the records are consumed by two separate Spark Streaming topologies\n#### Prod_Online_IngestStream\nThis topology takes all records from the final Kafka topics, extracts some values and writes them to a Kudu table. It also inserts some columns that are too big for Kudu to an HBase table. This topology runs independently **on both clusters**.\n``` mermaid\n  graph TD\n  A[Kafka: prod-trlog-online-ingest-stream] --> B[Spark: Prod_Online_IngestStream]\n  B --> C[Kudu: prod_trlog_online.service_audit_stream]\n  B --> D[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM]\n```\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 3"
        }
    },
    "212": {
        "page_content": "B --> C[Kudu: prod_trlog_online.service_audit_stream]\n  B --> D[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM]\n```\n**User**: `PRODREST`\n**Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Submit Script**: `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- [PR][ONLINE] Spark Waiting Batches\n- [DR][ONLINE] Spark Waiting Batches\n**Troubleshooting Steps**:\n- If the alert chart has **no line**, ensure that the application is not running (Cloudera Manager > YARN > Applications) and then submit the topology using the **Submit Script**. After resubmitting the application check the old application logs for the root cause that it failed\n- If the alert has exceeded the threshold, investigate HBase/Kudu for delays and check the application logs for any errors. You can also try to kill (use Cloudera Manager or yarn command) and then re-submit the topology.\n## Batch\nOutlines the batch workflows that enrich and aggregate daily Online transactions, merge data across systems, and prepare output for consumption.\n### Daily Batch Controller Script\nMain daily batch coordinator that orchestrates MergeBatch, aggregation, retention cleanup, and reporting steps across both clusters.\nAs mentioned before, the information processed by the [Prod_Online_IngestStream](#prod_online_ingeststream) topology is stored in Kudu/HBase tables. At the end of each day (at **4:15 am in PR & DR site** by **Cron**) the records from HBase/Kudu is enriched with additional information and the transfered to a parquet table. After the data are inserted to the parquet table, sequent jobs are triggered to produce aggregations and data for external systems. This procedure runs **independantly on both sites**.\n**User**: `PRODREST`\n**Logs**: `/var/log/ingestion/PRODREST/online/log/online_daily_batch_jobs.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 4"
        }
    },
    "213": {
        "page_content": "**User**: `PRODREST`\n**Logs**: `/var/log/ingestion/PRODREST/online/log/online_daily_batch_jobs.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/online_daily_batch_jobs_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- _See below_\n**Troubleshooting Steps**:\nThis procedure runs multiple steps which we will describe below with seperate troubleshooting steps for each step. Use the following steps for all alarms as a general guide:\n- Identify the failed step using the alarm name\n- Identify the root cause for the failed job based on the logs of the submit script/cluster application/query\n- If the problem is with an external system, ask the customer to inform the owners of the external system\n- if the problem is temporary or if you cannot determine the root cause of the problem, try to re-run the failed application\n### Sub-steps\nThe following steps run **on both clusters independently**, unless specified otherwise.\n#### Merge Batch to Final Table\nThis step transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and deletes the data from the original tables.\n``` mermaid\n  graph TD\n  A[Kudu: prod_trlog_online.service_audit_stream] --> B[Spark: PROD_Online_MergeBatch]\n  C[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM] --> B\n  B --> D[Impala: prod_trlog_online.service_audit]\n  ```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/cronExecutor_OnlineBatch_full.log`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Script**: `/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- Online_Ingestion MergeBatch JOB\n**Troubleshooting Steps**:\n- Use the script/spark logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 5"
        }
    },
    "214": {
        "page_content": "**Alerts**:\n- Online_Ingestion MergeBatch JOB\n**Troubleshooting Steps**:\n- Use the script/spark logs to identify the cause of the failure\n- Ensure that no records are present in prod_trlog_online.service_audit\n  ``` bash\n  # eg. 09-11-2019\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_online.service_audit where par_dt='20191109';\"\n  ```\n- If no records exist and no other process is up, you can ran the script again.\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_Online_MergeBatch LOCK_ONLINE_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_Online_MergeBatch  /opt/ingestion/PRODREST/online/lock/ FULL 1800 \"`date --date='-1 day' '+%Y-%m-%d 00:00:00'`\" \"`date '+%Y-%m-%d 00:00:00'`\"  >> /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log 2>&1 &\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_Online_MergeBatch LOCK_ONLINE_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_Online_MergeBatch  /opt/ingestion/PRODREST/online/lock/ FULL 1800 \"2019-11-09 00:00:00\" \"2019-11-10 00:00:00\"   >> /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log 2>&1 &\n    ```\n- The process runs for well 30 minutes under normal circumstances or even longer for heavy load. Use of `screen` command advised\n- If the problem is with resources (out-of-memory errors):\n  - You can adjust the values at `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh`. At the moment these values are as high as they can go.\n  - You can run the MergeBatch for parts of the day\n    ``` bash\n    # eg. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 6"
        }
    },
    "215": {
        "page_content": "- You can run the MergeBatch for parts of the day\n    ``` bash\n    # eg. 09-11-2019\n    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 00:00:00\" \"2019-11-09 12:00:00\"\n    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 12:00:00\" \"2019-11-09 18:00:00\"\n    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 18:00:00\" \"2019-11-10 00:00:00\"\n    ```\n- Run the rest of the steps\n#### Report stats to Graphite\nReports statistics about the ingestion process.\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/cron_report_stats.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- This process is not monitored\n- You can safely skip this step\n- Sample execution:\n  ``` bash\n  nohup  /opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh prod_trlog_online.service_audit online >> /var/log/ingestion/PRODREST/online/log/cron_report_stats.log\n  ```\n- Run the rest of the steps\n#### Drop Hourly MergeBatch Partitions (DR)\nNdef: **ONLY DR SITE**\nDrop partitions from the [Hourly Merge Batch](#hourly-merge-batch) tables\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/drop_hourly_partitions.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh`\n**Alerts**:\n- Online_Migration Drop hourly partitions JOB\n**Troubleshooting Steps**:\n- Ensure that you are running steps for the DR site\n- If you are running the steps for the Primary skip this step\n- Use the script logs to identify the cause of the failure\n- For the previous day:\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 7"
        }
    },
    "216": {
        "page_content": "- If you are running the steps for the Primary skip this step\n- Use the script logs to identify the cause of the failure\n- For the previous day:\n  ``` bash\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \"prod_trlog_online.service_audit_hourly\" \"`date -d '-1 day' '+%Y%m%d'`\" >> /var/log/ingestion/PRODREST/online/log/drop_hourly_partitions.log 2>&1 &\n  ```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \"prod_trlog_online.service_audit_hourly\" \"20191109\" >> /var/log/ingestion/PRODREST/online/log/drop_hourly_partitions.log 2>&1 &\n  ```\n- Run the rest of the steps\n#### Aggregation to Analytical Tables\nThis flow computes aggregations for use with the [Queries](#queries).\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh`\n**Alerts**:\n- Online_Migration Aggregations JOB\n- Online_Migration Aggregation_SA Impala_Insert\n- Online_Migration Aggregation_SA_Index Kudu_Insert\n**Troubleshooting Steps**:\n- For the previous day:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh prod_trlog_online.service_audit prod_trlog_online.aggr_service_audit prod_trlog_online.aggr_service_audit_idx  >> /var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log 2>&1 &\n  ```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019\n  /opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh prod_trlog_online.service_audit prod_trlog_online.aggr_service_audit prod_trlog_online.aggr_service_audit_idx 20191109 >> /var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log 2>&1\n  ```\n- Run the rest of the steps\n#### Send Daily Duplicate Reports",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 8"
        }
    },
    "217": {
        "page_content": "```\n- Run the rest of the steps\n#### Send Daily Duplicate Reports\nSends email reports to users for duplicate transaction IDS and leftover transactions in `prod_trlog_online.service_audit_stream`.\n**User**: `PRODREST`\n**Script Logs**: `-`\n**Script**: `-`\n**Alerts**:\n- Online_Ingestion GUID_Report Impala\n- Online_Ingestion GUID_Report JOB\n**Troubleshooting Steps**:\n- Check `/var/log/ingestion/PRODREST/online/log/online_report_duplicate_identical.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/online_report_duplicate_identical_STABLE.sh  `date -d '-1 day' '+%Y%m%d'`  prod_trlog_online service_audit service_audit_duplicates >> /var/log/ingestion/PRODREST/online/log/online_report_duplicate_identical.log 2>&1 &\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_online.service_audit_stream prod_trlog_online.service_audit_old online >> /var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log 2>&1 &\n  ```\n#### Duplicates between Impala and Kudu/HBase\nDelete data which exist both Impala and Kudu. Sometimes MergeBatch fails because YARN kills executors for exceeding memory limits. AS a result, transactions are inserted to impala and job fails before drop them from kudu and HBase\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- Check `/var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 9"
        }
    },
    "218": {
        "page_content": "- Check `/var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_online.service_audit_stream prod_trlog_online.service_audit_old online >> /var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log 2>&1\n  ```\n### Hourly Merge to Intermediate Table\nEvery hour a  transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and without deleting the data from the original tables. This table is used by mno to query the latest transactions.\n``` mermaid\n  graph TD\n  A[Kudu: prod_trlog_online.service_audit_stream] --> B[Spark: PROD_Online_MergeBatch_Hourly]\n  C[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM] --> B\n  B --> D[Impala: prod_trlog_online.service_audit_hourly]\n  ```\n**User**: `PRODREST`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Alerts**:\n- Not Monitored*\n**Troubleshooting Steps**:\n- Use the spark logs to identify the cause of the failure\n- The data in the Hourly tables is only for the current day. No need to fill missing hours, as the next MergeBatch will insert the data to the final table.\n## Queries\nDetails the query layer used by Online applications to fetch transaction records through Impala via REST APIs hosted in Wildfly.\nThe ingested data are queried in order to be displayed by the Online application (used by branches). The queries are submitted to the cluster as HTTP requests using the same Wildfly instances as the [Stream](#stream) flow.\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\n  B -->|Active| C[pr1edge01.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 10"
        }
    },
    "219": {
        "page_content": "B -->|Active| C[pr1edge01.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]\n  C -->|Impala JDBC | G[Primary Site]\n  D -->|Impala JDBC | G\n  E -.->|Stopped| H[Disaster Site]\n  F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodreston`\n**Deployments Path**: `/opt/wildfly/default/prodreston/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodreston/server.log`\n**Access Log**: `/var/log/wildfly/prodreston/access.log`\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\n**Alerts**:\n- [PR][ONLINE] Query Average Response Time\n- [DR][ONLINE] Query Average Response Time\n- [PR][ONLINE] Query Average Error rate\n- [DR][ONLINE] Query Average Error rate\n- [PR][ONLINE] REST Endpoints Average Response Time\n- [DR][ONLINE] REST Endpoints Average Response Time\n**Troubleshooting Steps**:\n- Check in Grafana dashboard `MONITORING/Monitoring Alerts` the charts that are in alerting state\n- Check if the response time is constant, a single query can increase the mean value of the response time. Check the access log for entries with large response time for the specific endpoint. If it is an isolated query the customer is not impacted.\n- Check application logs for error messages.\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\n- If the logs do not indicate a specific problem, check for any problems regarding Impala/Kudu\n### Query Endpoints\nQueries regarding Online query Impala tables stored in both HDFS and Kudu.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 11"
        }
    },
    "220": {
        "page_content": "- If the logs do not indicate a specific problem, check for any problems regarding Impala/Kudu\n### Query Endpoints\nQueries regarding Online query Impala tables stored in both HDFS and Kudu.\n**Endpoints**:\n- dynamic search\n- by-id\n- by-core-fields\n- by-application\n- top-by-branchcode-clientusername\n- first-by-branchcode-computername\n- aggr-computernum-usernum-transnum-groupby-branchcode\n- aggr-transnum-groupby-branchcode-clientusername-by-branchcode\n- aggr-opcodenum-transnum-groupby-opclass\n- aggr-transnum-groupby-opclass-opcode-by-opclass\n## Retention Mechanism\nRetention scripts and cron jobs that purge old transaction data from Impala, HDFS, and HBase based on date thresholds to manage system size.\n### Impala Retention\n#### DEV\nEvery day (at **16:05 on DR site only** by **Cron**). This script drops partitions from impala table `dev_trlog_online.service_audit` older than 60 days.\n**User**: `DEVREST`\n**Script Logs**: `/var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log`\n**Script**: `/opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh`\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, run manually the following command\n  ``` bash  \n  /opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh >> /var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log 2>&1\n  ```\n### Additional Tables\nKudu table's `prod_trlog_online.service_audit_stream` partitions are maintained by an Oozie jobs every day. See section [Create next week kudu partitions](#create-next-week-kudu-partitions).\nParquet table's `prod_trlog_online.service_audit_hourly` partitions are dropped after each successful Merge run. See section [Drop hourly partitions](#drop-hourly-partitions).\n### HBase retention",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 12"
        }
    },
    "221": {
        "page_content": "Parquet table's `prod_trlog_online.service_audit_hourly` partitions are dropped after each successful Merge run. See section [Drop hourly partitions](#drop-hourly-partitions).\n### HBase retention\nEvery day (at **16:15 in both sites** by **Cron**) This script deletes rows from hbase `PROD_ONLINE:SERVICE_AUDIT_STREAM` which refere to transactions inserted 7 days ago. Those rows may have been not deleted properly after merge batch job. To do so the topology queries rows from impala table `prod_trlog_online.service_audit` inside partition with par_dt refering 7 days ago.\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/daily_tools_cleanupHBaseSAS.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/daily_tools_cleanupHBaseSAS.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr`\n**Alerts**:\n- Retention OnlineCleanupHbaseSAS JOB\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, check on `/opt/ingestion/PRODREST/no_bkp/duplicate_cid_hbase` if a manual re-run must be done for a specific date\n  - For a specified date eg 2022-06-30:\n    ``` bash\n    /opt/ingestion/PRODREST/common/spark/submit/submitmnoSparkTopology_tools_cleanupHbaseSAS prod_trlog_online.service_audit PROD_ONLINE:SERVICE_AUDIT_STREAM LIST 20220630\n    ```\n##  Oozie Jobs\nScheduled Oozie jobs for importing lookup tables and managing future Kudu partitions for Online service audit tables.\n###  Lookup tables\nEvery day (at 07:00 by Oozie on DR & PR site ), we transfers 3 tables with reference data from the legacy MSSQL server, which is managed by mno, to the cluster. We keep only latest version to BigData (no partition).\n**User**: `PRODREST`\n**Coordinator**: `Coord_OnlineLookupTables_PROD`\n**Workflow**: `ImportLookupTables`\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/lookupTables/import_lookup_tables.sh`\n**Logs**: from HUE\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 13"
        }
    },
    "222": {
        "page_content": "**Workflow**: `ImportLookupTables`\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/lookupTables/import_lookup_tables.sh`\n**Logs**: from HUE\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs\n###  Create next week kudu partitions\nEvery day (at 10:10 by Oozie on DR site and 10:30 by Oozie on PR site ), we delete old Kudu partitions and add new Kudu partitions on `prod_trlog_online.service_audit_stream`.\n**User**: `PRODREST`\n**Coordinator**: `Coord_OnlineCreateKuduPartitionsPROD`\n**Workflow**: `CreateKuduPartitionsPROD`\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\n**Logs**: from HUE\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs\nThere is a case that merge batch has not properly run for a specific date, **even though** the transactions for that specific date are present on impala `service_audit`. For example by moving a service_audit partiton from DR to PR or vice versa, or when merge batch failed **after** deleting transactions from kudu.\nIn that case we can manually mark merge batch as complete for that date, and therefore next oozie job created by `Coord_OnlineCreateKuduPartitionsPROD` will delete that partition from kudu, by manipulating HBase table `PROD_ONLINE:MERGE_BATCH_STATE_INFO`.\nExample for a specific date (10/10/2022):\n - Run HBase shell\n ```\n   hbase shell\n   ```\n - And then inside HBase shell:\n ```\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase', 'true'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase_time','0'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu', 'true'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu_time', '0'",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 14"
        }
    },
    "223": {
        "page_content": "put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu_time', '0'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running', 'false'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running+time', '0'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala', 'true'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala_time', '0'\n   ```\n#### DEV\nEvery day (at **11:00 by Oozie** on **DR site only** ), we delete old Kudu partitions and add new Kudu partitions on `dev_trlog_online.service_audit_stream`.\n**User**: `DEVREST`\n**Coordinator**: `Coord_OnlineCreateKuduPartitionsDEV`\n**Workflow**: `CreateKuduPartitionsDEV`\n**Local path**: `/opt/ingestion/DEVREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\n**Logs**: from HUE\n**Alerts**:\n- Not Monitored",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "online.md - Part 15"
        }
    },
    "224": {
        "page_content": "---\ntitle: Datawarehouse IBank Extract and Export Processes\ndescription: Comprehensive operational guide for extracting and exporting Internet Banking service data from `prod_trlog_ibank.service_audit` to detail tables and then to MSSQL via Sqoop. Includes scheduler setup (UC4), script paths, Spark and Sqoop jobs, troubleshooting, and historical retention notes.\nauthor: produser / mno big data engineering\nupdated: 2025-05-01\ntags:\n  - datawarehouse\n  - ibank\n  - internet banking\n  - spark\n  - sqoop\n  - uc4\n  - dwh\n  - produser\n  - impala\n  - extract\n  - export\n  - transfer\n  - payment\n  - card\n  - loan payment\n  - cancel payment\n  - time deposit\n  - mass debit\n  - man date\n  - my bank\n  - service audit\n  - yarn\n  - staging\n  - reconciliation\n  - retention\n  - monitoring\n  - logs\n---\n# Datawarehouse ibank\n## Extract\n**Extraction of detail tables**\nOur spark application will extract information from prod_trlog_ibank.service_audit table to different detail tables based on different service names with the help of lookup table service_name where needed. The columns of the detail tables are produced either from a non json column of the input table (eg. client_username) or from parsing the json fields request_text_data and response_text_data and extracting specific fields based on their name. When two fields have the same name then we will keep the greater value either for a number field or for a string based on lexicographic order.\nThese jobs are executed by the bank's scheduler. There is an agent installed on the BDA edge node which is called UC4. The UC4 agent executes the below script for every job:\n**UC4 Agent Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh` on `dr1edge01.mno.gr` **OR** `pr1edge01.mno.gr` (according to which site is the active for the UC4 agent)\n**User**: `PRODUSER`\n**Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 1"
        }
    },
    "225": {
        "page_content": "**User**: `PRODUSER`\n**Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\nThe above script executes the below scripts for every job and the below scripts and each of these scripts executes this generic spark-submit script:\n**Generic Spark Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submitter.sh`\nThe jobs which perform the extraction of the details from service_audit are:\n### Transfer Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_transfer_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_transfer]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_transfer.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT TRANSFER\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t transfer\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t transfer -p 20191109\n    ```\n### Payment Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_payment_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_payment]\n  ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 2"
        }
    },
    "226": {
        "page_content": "``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_payment_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_payment]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_payment.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t payment\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t payment -p 20191109\n    ```\n### Loan Payment Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_loanPayment_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_loan_payment]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_loan_payment.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT LOAN_PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 3"
        }
    },
    "227": {
        "page_content": "**Alert**:\n- DWH_IBank EXTRACT LOAN_PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t loanPayment\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t loanPayment -p 20191109\n    ```\n### Cancel Payment Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_cancelPayment_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_cancel_payment]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_cancel_payment.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT CANCEL_PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t cancelPayment\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 4"
        }
    },
    "228": {
        "page_content": "```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t cancelPayment -p 20191109\n    ```\n### Card Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_card_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_card]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_card.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT CARD\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t card\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t card -p 20191109\n    ```\n### Stock Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_stock_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_stock]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_stock.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 5"
        }
    },
    "229": {
        "page_content": "**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT STOCK\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t stock\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t stock -p 20191109\n    ```\n### Time Deposit Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_timeDeposit_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_time_deposit]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_time_deposit.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT TIME_DEPOSIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 6"
        }
    },
    "230": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t timeDeposit\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t timeDeposit -p 20191109\n    ```\n### Mass Debit Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_massDebit_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_mass_debit]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_mass_debit.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT MASS_DEBIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t massDebit\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t massDebit -p 20191109\n    ```\n### Man Date Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_manDate_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_man_date]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_man_date.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 7"
        }
    },
    "231": {
        "page_content": "B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_man_date]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_man_date.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT MAN_DATE\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -p 20191109\n    ```\n### My Bank Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_myBank_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_my_bank]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_my_bank.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT MY_BANK\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 8"
        }
    },
    "232": {
        "page_content": "**Alert**:\n- DWH_IBank EXTRACT MY_BANK\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t myBank\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t myBank -p 20191109\n    ```\n---\n## Export\n**Export of details tables and part of service_audit columns to mno datawarehouse**\nThe data are copied temporary to an internal staging table with an impala query and then we use sqoop-export to export the data to Datawarehouse.\nThese jobs are executed by the bank's scheduler. There is an agent installed on the BDA edge node which is called UC4. The UC4 agent executes the below script for every job:\n**UC4 Agent Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh` on `dr1edge01.mno.gr` **OR** `pr1edge01.mno.gr` (according to which site is the active for the UC4 agent)\n**User**: `PRODUSER`\n**Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\nThe above script executes the below scripts for every job and the below scripts and each of these scripts executes this generic sqoop script:\n**Generic Sqoop Job Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_to_dwh.sh`\nThe jobs which perform the export of the details to the MSSQL Server are:\n### Transfer Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_transfer] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_transfer_stg]",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 9"
        }
    },
    "233": {
        "page_content": "### Transfer Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_transfer] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_transfer_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_TransferDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.TransferDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_transfer.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT TRANSFER\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t transfer\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t transfer -p 20191109\n    ```\n### Payment Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_payment] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_payment_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_PaymentDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.PaymentDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_payment.sh`\n**User**: `PRODUSER`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 10"
        }
    },
    "234": {
        "page_content": "C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.PaymentDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_payment.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT TRANSFER\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t payment\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t payment -p 20191109\n    ```\n### Loan Payment Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_loan_payment] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_loan_payment_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_LoanPaymentDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.LoanPaymentDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_loan_payment.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT LOAN_PAYMENT\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 11"
        }
    },
    "235": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT LOAN_PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment -p 20191109\n    ```\n### Cancel Payment Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_cancel_payment] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_cancel_payment_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_CancelPaymentDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.CancelPaymentDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_cancel_payment.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT CANCEL_PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 12"
        }
    },
    "236": {
        "page_content": "**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t cancelPayment\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t cancelPayment -p 20191109\n    ```\n### Card Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_card] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_card_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_CardDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.CardDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_card.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT CARD\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t card\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 13"
        }
    },
    "237": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t card\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t card -p 20191109\n    ```\n### Stock Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_stock] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_stock_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_StockDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.StockDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_stock.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT STOCK\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t stock\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t stock -p 20191109\n    ```\n### Time Deposit Export\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 14"
        }
    },
    "238": {
        "page_content": "/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t stock -p 20191109\n    ```\n### Time Deposit Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_time_deposit] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_time_deposit_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_TimeDepositDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.TimeDepositDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_time_deposit.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT TIME_DEPOSIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t timeDeposit\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t timeDeposit -p 20191109\n    ```\n### Mass Debit Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_mass_debit] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_mass_debit_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_MassDebitDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.MassDebitDetails]\n  ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 15"
        }
    },
    "239": {
        "page_content": "B --> C[Sqoop: PROD_IBank_DWH_EXPORT_MassDebitDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.MassDebitDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_mass_debit.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT MASS_DEBIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t massDebit\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t massDebit -p 20191109\n    ```\n### Man Date Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_man_date] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_man_date_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_ManDateDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.TransferDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_man_date.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 16"
        }
    },
    "240": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT MAN_DATE\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t manDate\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t manDate -p 20191109\n    ```\n### My Bank Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_my_bank] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_my_bank_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_MyBankDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.MyBankDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_my_bank.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT MY_BANK\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 17"
        }
    },
    "241": {
        "page_content": "- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t myBank\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t myBank -p 20191109\n    ```\n### Service Audit Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_service_audit_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_ServiceAuditDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.ServiceAudit]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_service_audit.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT SERVICE_AUDIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t serviceAudit\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 18"
        }
    },
    "242": {
        "page_content": "```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t serviceAudit -p 20191109\n    ```\n## Retention Mechanism (Suspended)\nNdef: **This mechanism is suspended. DO NOT Troubleshoot**. As of 22/09/2023 all tables listed bellow have been dropped by mno (**Mail Subject: Incident IM2220978 receipt confirmation. Related Interaction: SD2285018**).\nInformation shown here is for completeness.\n**Execution**: Every day (at **3:30 pm in DR site** by **Cron**) \n**Description**: This script drops partitions from impala tables `prod_trlog_ibank_analytical.dwh_details*` older than 10 days.\n**User**: `PRODUSER`\n**Script Logs**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily.log`\n**Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily_STABLE.sh` on `dr1edge01.mno.gr`\n**Alerts**:\n- Retention DWH_retention {$table}\nWhere $table can be\n- prod_trlog_ibank_analytical.dwh_details_cancel_payment\n- prod_trlog_ibank_analytical.dwh_details_card\n- prod_trlog_ibank_analytical.dwh_details_loan_payment\n- prod_trlog_ibank_analytical.dwh_details_man_date\n- prod_trlog_ibank_analytical.dwh_details_mass_debit\n- prod_trlog_ibank_analytical.dwh_details_my_bank\n- prod_trlog_ibank_analytical.dwh_details_payment\n- prod_trlog_ibank_analytical.dwh_details_stock\n- prod_trlog_ibank_analytical.dwh_details_time_deposit\n- prod_trlog_ibank_analytical.dwh_details_cancel_transfer\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, run manually the following command\n  - To keep the only last 10 days:\n    ``` bash\n        /opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily_STABLE.sh >> /opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily.log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "datawarehouse-ibank.md - Part 19"
        }
    },
    "243": {
        "page_content": "---\ntitle: Internet Banking - Data Ingestion and Processing\ndescription: Complete documentation of the Internet Banking data pipeline: streaming ingestion, batch migration, Spark processing, Kafka, Wildfly, retention policies, Oozie jobs, and operational troubleshooting across DR/PR clusters.\ntags:\n  - ibank\n  - internet banking\n  - spark streaming\n  - kafka\n  - kudu\n  - hbase\n  - wildfly\n  - retention\n  - migration\n  - sqoop\n  - batch processing\n  - cron jobs\n  - troubleshooting\n  - monitoring\n  - cloudera manager\n  - impala\n  - data pipeline\n  - RAG\n  - BigStreamer\n---\n# Internet Banking\n## Stream\nProcesses user transaction events in real-time from Wildfly servers to Kafka, then through Spark Streaming to Kudu and HBase for persistent storage.\n### Wilfly Transaction Receiver\nWildfly applications receive HTTP POST requests with transaction events from Internet Banking backend servers and forward them to Kafka topics.\nUser generated events are sent to Wildfly instances installed at edge servers as HTTP POST requests. The requests originate from the backend servers of Internet Banking. Requests are loadbalanced by NetScaler (Loadbalancer) that is managed by mno's networking department. The events are then sent to a Kafka topic. Usually only one site is active (Primary Site).\n```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\n  B -->|Active| C[pr1edge01.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]\n  C -->|Kafka Producer| G[Primary Site's Kafka: prod-trlog-ibank-ingest-stream-mir]\n  D -->|Kafka Producer| G\n  E -.->|Stopped| H[Disaster Site's Kafka: prod-trlog-ibank-ingest-stream-mir]\n  F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodrestib`\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 1"
        }
    },
    "244": {
        "page_content": "F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodrestib`\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodrestib/server.log`\n**Access Log**: `/var/log/wildfly/prodrestib/access.log`\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\n**Alerts**:\n- [PR][IBANK] REST Endpoints Average Response Time\n- [DR][IBANK] REST Endpoints Average Response Time\n- [PR][IBANK] Ingestion Average Error rate\n- [DR][IBANK] Ingestion Average Error rate\n**Troubleshooting Steps**:\n- Check application logs for error messages.\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\n- If the logs do not indicate a specific problem, check for any problems regarding Kafka performance.\n### Kafka Topic Mirroring via MirrorMaker\nDescribes the Kafka MirrorMaker setup that replicates `-mir` topics from the active site to final shared topics used by both clusters.\nAfter being published to Kafka, the traffic is mirrored to the opposite site via Kafka MirrorMaker. In order to be able to have both Sites active at any given time and simplify the failover procedure the each Site produce messages to a topic with the suffix `-mir`. The topics with the `-mir` suffix contain events published to the Site. The `-mir` topics are then copied via Kafka MirrorMakers to the final topics (no `-mir` suffix), that has all the messages from both Sites.\n#### PR replication\n```mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 2"
        }
    },
    "245": {
        "page_content": "#### PR replication\n```mermaid\n  graph TD\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\n  E[dr1edge02.mno.gr] -.->|Stopped| F\n  C -->|Kafka MirrorMakers <br> pr1node01.mno.gr/pr1node04.mno.gr| G[PR Site Kafka: prod-trlog-ibank-ingest-stream]\n  F -->|Kafka MirrorMakers <br> pr1node05.mno.gr/pr1node06.mno.gr| G[PR Site Kafka: prod-trlog-ibank-ingest-stream]",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 3"
        }
    },
    "246": {
        "page_content": "```\n#### DR replication\n``` mermaid\n  graph TD\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\n  E[dr1edge02.mno.gr] -.->|Stopped| F\n  F -->|Kafka MirrorMakers <br> dr1node01.mno.gr/dr1node04.mno.gr| H[DR Site Kafka: prod-trlog-ibank-ingest-stream]\n  C -->|Kafka MirrorMakers <br> dr1node05.mno.gr/dr1node06.mno.gr| H[DR Site Kafka: prod-trlog-ibank-ingest-stream]\n```\n**MirrorMaker User**: `kafka`\n**Configuration**: Cloudera Manager\n**Logs**: Cloudera Manager\n**Instructions for start/stop**: [Here](../procedures/manage_mirrormaker.md)\n**Alerts**:\n- Cloudera Manager alerts regarding Kafka\n### Spark Streaming Pipelines\nSpark topologies consume Kafka events and write them to Kudu and HBase, separating full stream and visible-only transactions.\nAfter being sent to the final Kafka topic, the records are consumed by two separate Spark Streaming topologies\n#### Prod_IBANK_IngestStream\nThis topology takes all records from the final Kafka topics, extracts some values and writes them to a Kudu table. It also inserts some columns that are too big for Kudu to an HBase table. This topology runs independently **on both clusters**.\n``` mermaid\n  graph TD\n  A[Kafka: prod-trlog-ibank-ingest-stream] --> B[Spark: Prod_IBANK_IngestStream]\n  B --> C[Kudu: prod_trlog_ibank.service_audit_stream]\n  B --> D[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM]\n```\n**User**: `PRODREST`\n**Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Submit Script**: `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- [PR][IBANK] Spark Waiting Batches\n- [DR][IBANK] Spark Waiting Batches\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 4"
        }
    },
    "247": {
        "page_content": "**Alerts**:\n- [PR][IBANK] Spark Waiting Batches\n- [DR][IBANK] Spark Waiting Batches\n**Troubleshooting Steps**:\n- If the alert chart has **no line**, ensure that the application is not running (Cloudera Manager > YARN > Applications) and then submit the topology using the **Submit Script**. After resubmitting the application check the old application logs for the root cause that it failed\n- If the alert has exceeded the threshold, investigate HBase/Kudu for delays and check the application logs for any errors. You can also try to kill (use Cloudera Manager or yarn command) and then re-submit the topology.\n#### Prod_IBANK_IngestStream_Visible\n**Visible transaction**: Visible are considered transactions from applications that have show_customer marked as true in the Impala table `prod_trlog_ibank.service_name`.\nThis topology takes **visible** records from the final Kafka topics and writes them to HBase tables. This topology runs independently **on both clusters**.\n``` mermaid\n  graph TD\n  A[Kafka: prod-trlog-ibank-ingest-stream] --> C[Spark: Prod_IBANK_IngestStream_Visible]\n  B[Impala: prod_trlog_ibank.service_name] -->|Queried every 6 hours| C[Spark: Prod_IBANK_IngestStream_Visible]\n  C --> D[HBase: PROD_IBANK:SERVICE_AUDIT]\n  C --> E[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_APP]\n  C --> F[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_ID]\n  C --> G[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_SNTRNCAT]\n  C --> H[HBase: PROD_IBANK:SERVICE_AUDIT_OBSCURE]\n```\n**User**: `PRODREST`\n**Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Submit Script**: `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_VISIBLE_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- [PR][IBANK Visible] Spark Waiting Batches\n- [DR][IBANK Visible] Spark Waiting Batches\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 5"
        }
    },
    "248": {
        "page_content": "**Alerts**:\n- [PR][IBANK Visible] Spark Waiting Batches\n- [DR][IBANK Visible] Spark Waiting Batches\n**Troubleshooting Steps**:\n- If the alert chart has **no line**, ensure that the application is not running (Cloudera Manager > YARN > Applications) and then submit the topology using the **Submit Script**.\n- If the alert has exceeded the threshold, investigate HBase for delays and check the application logs for any errors. You can also try to kill (use Cloudera Manager or yarn command) and then re-submit the topology.\n## Batch\nDescribes the daily and hourly batch flows that process and enrich ingested data, merge and de-duplicate them, and make them available to external systems.\n### Main Batch Job Entry Point\nMain batch entry point that coordinates migration, merge, enrichment, and final insertions to operational tables.\nAs mentioned before, the information processed by the [Prod_IBANK_IngestStream](#prod_ibank_ingeststream) topology is stored in Kudu/HBase tables. At the end of each day (at **1:00 am in PR site** and **2:00 am in DR site** by **Cron**) the records from HBase/Kudu is enriched with additional information and the transfered to a parquet table. After the data are inserted to the parquet table, sequent jobs are triggered to produce aggregations and data for external systems. This procedure runs **independantly on both sites**.\n**User**: `PRODREST`\n**Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log`\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- _See below_\n**Troubleshooting Steps**:\nThis procedure runs multiple steps which we will describe below with seperate troubleshooting steps for each step. Use the following steps for all alarms as a general guide:\n- Identify the failed step using the alarm name",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 6"
        }
    },
    "249": {
        "page_content": "- Identify the failed step using the alarm name\n- Identify the root cause for the failed job based on the logs of the submit script/cluster application/query\n- If the problem is with an external system, ask the customer to inform the owners of the external system\n- if the problem is temporary or if you cannot determine the root cause of the problem, try to re-run the failed application\n### Sub-steps\nThe following steps run **on both clusters independently**, unless specified otherwise.\n#### MSSQL Sqoop Import (Migration)\nThis step transfers transactions from the legacy MSSQL server, which is managed by mno, to the cluster as part of the daily data migration from the legacy system.\n``` mermaid\n  graph TD\n  A[MSSQL] -->|Sqoop Import| B[Impala: prod_trlog_ibank.historical_service_audit_raw_v2]\n  B -->|Impala Insert| C[Impala: prod_trlog_ibank.historical_service_audit_v1]\n```\n**User**: `PRODREST`\n**Script Logs**: ```/opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log```\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_migration/sqoop_import_ibank_hist_recursive_prod_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- IBank_Migration Historical JOB\n- IBank_Migration Historical Sqoop_Import\n- IBank_Migration Historical Impala_Insert\n**Troubleshooting Steps**:\n- Use the script/sqoop logs to identify the cause of the failure\n- If the alert is Sqoop_Import, you can safely execute the script again\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 7"
        }
    },
    "250": {
        "page_content": "- Use the script/sqoop logs to identify the cause of the failure\n- If the alert is Sqoop_Import, you can safely execute the script again\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODREST/historical/ibank_migration/sqoop_import_ibank_hist_recursive_prod_STABLE.sh /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2 FandFUser prod_trlog_ibank.historical_service_audit_raw_v2 prod_trlog_ibank.historical_service_audit_v1 `date +%Y-%m-%d` 1 1 4 jceks://hdfs/user/PRODREST/migration/credentials/mssql_ibank.jceks mssqlprod.password >>  /opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log 2>&1\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 10-11-2019\n    nohup /opt/ingestion/PRODREST/historical/ibank_migration/sqoop_import_ibank_hist_recursive_prod_STABLE.sh /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2 FandFUser prod_trlog_ibank.historical_service_audit_raw_v2 prod_trlog_ibank.historical_service_audit_v1 2019-11-10 1 1 4 jceks://hdfs/user/PRODREST/migration/credentials/mssql_ibank.jceks mssqlprod.password &>>  /opt/icomdev/ibank_hist/prod_migrate/sqoop_20191110_1_1_20191112_0900.log &\n    ```\n- If the alert is Impala_Insert, check the that the records from the MSSQL server and the `prod_trlog_ibank.historical_service_audit_raw_v2` have the same count\n  - MSSQL server\n    ``` bash\n    # Replace the sample date 16/11/2019-17/11/2019\n\tsource /opt/ingestion/PRODREST/historical/ibank_migration/config",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 8"
        }
    },
    "251": {
        "page_content": "- MSSQL server\n    ``` bash\n    # Replace the sample date 16/11/2019-17/11/2019\n\tsource /opt/ingestion/PRODREST/historical/ibank_migration/config\n    sqoop-eval  -Dhadoop.security.credential.provider.path=jceks://hdfs/user/PRODREST/migration/credentials/mssql_ibank.jceks --connect 'jdbc:jtds:sqlserver://999.999.999.999:2544;useNTLMv2=true;databaseName=InternetBankingAudit' --username FandFUser --password-alias mssqlprod.password --query \"select count() from ServiceAudit WITH (nolock, INDEX(IX_ServiceAuditNew_Timestamp)) where Timestamp >= '2019-11-16 00:00:00' and Timestamp < '2019-11-17 00:00:00'\"\n    ```\n  - prod_trlog_ibank.historical_service_audit_raw_v2\n    ``` bash\n    impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.historical_service_audit_raw_v2;\"\n    ```\n- In case the counts are different between the two try again with Hive:\n  - prod_trlog_ibank.historical_service_audit_raw_v2\n    ``` bash\n    # For Primary Site\n    beeline -u \"jdbc:hive://pr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAP.mno.GR;ssl=true\" -e \"select  count(*) from prod_trlog_ibank.historical_service_audit_raw_v2;\"\n    # For Disaster Site\n    beeline -u \"jdbc:hive://dr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAD.mno.GR;ssl=true\" -e \"select  count(*) from prod_trlog_ibank.historical_service_audit_raw_v2;\"\n    ```\n- If the counts are the same with Hive:\n  ``` bash\n  # For Primary Site\n  beeline -u \"jdbc:hive://pr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAP.mno.GR;ssl=true\"\n  # For Disaster Site\n  beeline -u \"jdbc:hive://dr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAD.mno.GR;ssl=true\"\n  ```\n  And run the insert:\n  ``` SQL",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 9"
        }
    },
    "252": {
        "page_content": "# For Disaster Site\n  beeline -u \"jdbc:hive://dr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAD.mno.GR;ssl=true\"\n  ```\n  And run the insert:\n  ``` SQL\n  INSERT INTO prod_trlog_ibank.historical_service_audit_v1 partition (par_dt) SELECT id, to_timestamp(trtimestamp, 'yyyy-MM-dd HH:mm:ss.SSSSSSS'), trim(host), trim(servicename), trim(type), trim(application), cast(substr(executiontime,1,2)as BIGINT)36001000000000 + castsubstr(executiontime,4,2)as BIGINT)601000000000+ cast(substr(executiontime,7,2)as BIGINT)*1000000000 + cast(substr(executiontime,10,7)as BIGINT)*100, clientsession, trim(clientusername), cast(clientcustomer AS bigint), clientipadress, clientdetails, trim(clientrequestpath), cast(clientlongitude AS FLOAT), cast(clientlatitude AS FLOAT), cast(substr(clientexecutiontime,1,2)as BIGINT)36001000000000 + cast(substr(clientexecutiontime,4,2)as BIGINT)601000000000+ cast(substr(clientexecutiontime,7,2)as BIGINT)*1000000000 + cast(substr(clientexecutiontime,10,7)as BIGINT)*100, webhost, responsetextdata, requesttextdata, errordata, replace(to_date(trtimestamp),'-','') AS par_dt FROM prod_trlog_ibank.historical_service_audit_raw_v2;\n  ```\n  And then refresh the table\n  ``` bash\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"SET DDL_SYNC=true; refresh prod_trlog_ibank.historical_service_audit_v1;\"\n  ```\n- Run the rest of the steps\n#### Insert to Service Audit\nThis step transfers transactions migrated from the legacy MSSQL server to the final table.\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.historical_service_audit_v1] -->|Impala Insert| B[Impala: prod_trlog_ibank.service_audit]\n  ```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_migrate_hist_to_service_audit.log`\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_migrate_hist_to_service_audit_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- IBank_Migration Historical to SA JOB",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 10"
        }
    },
    "253": {
        "page_content": "**Alerts**:\n- IBank_Migration Historical to SA JOB\n- IBank_Migration Historical to SA Impala_Insert\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- The script cleans up after failure, so if the problem was temporary run the script again\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODREST/historical/ibank_migrate_hist_to_service_audit_STABLE.sh prod_trlog_ibank.historical_service_audit_v1 prod_trlog_ibank.service_audit `date +%Y%m%d` 1 1 >> /var/log/ingestion/PRODREST/ibank/log/ibank_migrate_hist_to_service_audit.log  2>&1\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODREST/historical/ibank_migrate_hist_to_service_audit_STABLE.sh prod_trlog_ibank.historical_service_audit_v1 prod_trlog_ibank.service_audit 20191110 1 1 >> /var/log/ingestion/PRODREST/ibank/log/ibank_migrate_hist_to_service_audit.log  2>&1 &\n    ```\n- Run the rest of the steps\n#### Merge Batch\nThis step transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and deletes the data from the original tables.\n``` mermaid\n  graph TD\n  A[Kudu: prod_trlog_ibank.service_audit_stream] --> B[Spark: PROD_IBank_MergeBatch]\n  C[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM] --> B\n  B --> D[Impala: prod_trlog_ibank.service_audit_old]\n  ```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Script**: `/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- IBank_Ingestion MergeBatch JOB\n**Troubleshooting Steps**:\n- Use the script/spark logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 11"
        }
    },
    "254": {
        "page_content": "**Alerts**:\n- IBank_Ingestion MergeBatch JOB\n**Troubleshooting Steps**:\n- Use the script/spark logs to identify the cause of the failure\n- Ensure that no records are present in prod_trlog_ibank.service_audit_old\n  ``` bash\n  # eg. 09-11-2019\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.service_audit_old where par_dt='20191109';\"\n  ```\n- If no records exist and no other process is up, you can ran the script again.\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_IBank_MergeBatch LOCK_IBANK_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_IBank_MergeBatch  /opt/ingestion/PRODREST/ibank/lock/ FULL 1800 \"`date --date='-1 day' '+%Y-%m-%d 00:00:00'`\" \"`date '+%Y-%m-%d 00:00:00'`\"   >> /var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log 2>&1\n    ```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_IBank_MergeBatch LOCK_IBANK_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_IBank_MergeBatch  /opt/ingestion/PRODREST/ibank/lock/ FULL 1800 \"2019-11-09 00:00:00\" \"2019-11-10 00:00:00\"   >> /var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log 2>&1 &\n    ```\n- The process runs for well over an hour under normal circumstances or even longer for heavy load. Use of `screen` command advised\n- If the problem is with resources (out-of-memory errors):\n  - You can adjust the values at `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh`. At the moment these values are as high as they can go.\n  - You can run the MergeBatch for parts of the day\n    ``` bash\n    # eg. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 12"
        }
    },
    "255": {
        "page_content": "- You can run the MergeBatch for parts of the day\n    ``` bash\n    # eg. 09-11-2019\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 00:00:00\" \"2019-11-09 12:00:00\"\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 12:00:00\" \"2019-11-09 18:00:00\"\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 18:00:00\" \"2019-11-10 00:00:00\"\n    ```\n- Run the rest of the steps\n#### Distinct join to Service Audit\nSome records that are ingested by the [Stream](#stream) can also be present in the MSSQL server. In this step we insert to the final table the transactions that are unique to the [Stream](#stream), excluding the ones that are already present in the final table due to the data migration by MSSQL.\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit_old] -->|Impala Insert| B[Impala: prod_trlog_ibank.service_audit]\n```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- IBank_Migration Enrich SA from SA_old JOB\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- Ensure that only records coming from prod_trlog_ibank.historical_service_audit_v1 are present in prod_trlog_ibank.service_audit. These records come from Insert to Service Audit [sub-step](#insert-to-service-audit) and their number should match.\n  ``` bash\n  # eg. 09-11-2019\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\"",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 13"
        }
    },
    "256": {
        "page_content": "``` bash\n  # eg. 09-11-2019\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\"\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.historical_service_audit_v1 where par_dt like '20191109';\"\n  ```\n- If these records match and no other process is up, you can run the script again.\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n    ```\n  - For a specified date:\n    ``` bash\n    # e.g. 09-11-2019\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n    ```\n- Run the rest of the steps\n#### Report stats to Graphite\nReports statistics about the ingestion process.\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/cron_report_stats.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- This process is not monitored\n- You can safely skip this step\n- Sample execution:\n  ``` bash\n  nohup  /opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh prod_trlog_ibank.service_audit ibank >> /var/log/ingestion/PRODREST/ibank/log/cron_report_stats.log\n  ```\n- Run the rest of the steps\n#### Trigger external flows\nNdef: **ONLY DR SITE**\nCreates a trigger file for external flows. Related to [Datawerehouse](./ibank_dwh.md)\n**User**: `PRODREST`\n**Script Logs**: `-`\n**Script**: `-`\n**Alerts**:\n- IBank_Migration Create UC4 file Create UC4 file\n**Troubleshooting Steps**:\n- Ensure that you are running steps for the DR site",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 14"
        }
    },
    "257": {
        "page_content": "**User**: `PRODREST`\n**Script Logs**: `-`\n**Script**: `-`\n**Alerts**:\n- IBank_Migration Create UC4 file Create UC4 file\n**Troubleshooting Steps**:\n- Ensure that you are running steps for the DR site\n- If you are running the steps for the Primary skip this step\n- Execution:\n  ``` bash\n  touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY\n  touch /opt/applications/landing_zone/PRODUSER/triggers/IBANK_SA_`date '+%Y%m%d'`.READY\n  ```\n- Run the rest of the steps\n#### Drop Hourly Batch Partitions (DR Site)\nNdef: **ONLY DR SITE**\nDrop partitions from the [Hourly Merge Batch](#hourly-merge-batch) tables\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/drop_hourly_partitions.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh`\n**Alerts**:\n- IBank_Migration Drop hourly partitions JOB\n**Troubleshooting Steps**:\n- Ensure that you are running steps for the DR site\n- If you are running the steps for the Primary skip this step\n- Use the script logs to identify the cause of the failure\n- For the previous day:\n  ``` bash\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \"prod_trlog_ibank.service_audit_hourly\" \"`date -d '-1 day' '+%Y%m%d'`\" >> /var/log/ingestion/PRODREST/ibank/log/drop_hourly_partitions.log 2>&1 &\n  ```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \"prod_trlog_ibank.service_audit_hourly\" \"20191109\" >> /var/log/ingestion/PRODREST/ibank/log/drop_hourly_partitions.log 2>&1 &\n  ```\n- Run the rest of the steps\n#### Execute aggregations\nNdef: **This flow is supspended. DO NOT EXECUTE**. Information listed here are for completeness.\nThis flow computes aggregations for use with the [Queries](#queries).\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 15"
        }
    },
    "258": {
        "page_content": "This flow computes aggregations for use with the [Queries](#queries).\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/update_ibank_aggr_tables_STABLE.sh`\n**Alerts**:\n- IBank_Migration Aggregations JOB\n- IBank_Migration Aggregations HBase\n- IBank_Migration Aggregations Kudu\n**Troubleshooting Steps**:\n- **DO NOT RUN THIS STEP**\n- For the previous day:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/update_ibank_aggr_tables_STABLE.sh prod_trlog_ibank.service_audit prod_trlog_ibank.aggr_service_audit_clun_app prod_trlog_ibank.aggr_service_audit_clun >> /var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log 2>&1\n  ```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019\n  /opt/ingestion/PRODREST/common/scripts/update_ibank_aggr_tables_STABLE.sh prod_trlog_ibank.service_audit prod_trlog_ibank.aggr_service_audit_clun_app prod_trlog_ibank.aggr_service_audit_clun 20191109 >> /var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log 2>&1\n  ```\n- Run the rest of the steps\n#### Upsert to HBase (Migration)\nThis step enriches the HBase visible tables with the transactions that are unique to MSSQL server.\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> C[Impala Insert/Spark]\n  B[Impala: prod_trlog_ibank.service_audit_old] --> C\n  C --> D[HBase: PROD_IBANK:SERVICE_AUDIT]\n  C --> E[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_APP]\n  C --> F[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_ID]\n  C --> G[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_SNTRNCAT]\n  C --> H[HBase: PROD_IBANK:SERVICE_AUDIT_OBSCURE]\n```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\n**Alerts**:\n- IBank_Migration Enrich hbase tables JOB\n- IBank_Migration Enrich hbase tables Impala_insert",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 16"
        }
    },
    "259": {
        "page_content": "**Alerts**:\n- IBank_Migration Enrich hbase tables JOB\n- IBank_Migration Enrich hbase tables Impala_insert\n- IBank_Migration Enrich hbase tables Spark\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n  Ndef: If job failed and the following error appears :`ERROR: RetriesExhaustedWithDetailsException: Failed <num> actions: CallTimeoutException: <num> times, servers with issues: [dr/pr]1node02.mno.gr`,  execute script again. The error has to do with HBase merging/spliting on a region server, but a detailed reason is unknown.\n- The script uses upsert and can be safely run many times.\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n    ```\n  - For a specified date:\n    ``` bash\n    # e.g. 09-11-2019\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20191109  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n    ```\n- Run the rest of the steps\n#### Send reports to business users\nSends email reports to users for duplicate transaction IDS and leftover transactions in `prod_trlog_ibank.service_audit_stream`.\n**User**: `PRODREST`\n**Script Logs**: `-`\n**Script**: `-`\n**Alerts**:\n- IBank_Migration GUID_Report JOB\n- IBank_Migration GUID_Report Impala\n**Troubleshooting Steps**:\n- Check `/var/log/ingestion/PRODREST/ibank/log/ibank_report_duplicate_identical.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n   ``` bash\n  /opt/ingestion/PRODREST/common/scripts/ibank_report_duplicate_identical_STABLE.sh  prod_trlog_ibank service_audit_old service_audit_duplicates >> /var/log/ingestion/PRODREST/ibank/log/ibank_report_duplicate_identical.log 2>&1 &\n   ```\n#### Duplicates between Impala and Kudu/HBase",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 17"
        }
    },
    "260": {
        "page_content": "```\n#### Duplicates between Impala and Kudu/HBase\nDelete data which exist both Impala and Kudu. Sometimes MergeBatch fails because YARN kills executors for exceeding memory limits. AS a result, transactions are inserted to impala and job fails before drop them from kudu and HBase\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- Check `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_ibank.service_audit_stream prod_trlog_ibank.service_audit_old ibank >> /var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log 2>&1\n  ```\n- Run the rest of the steps\n#### Update monitoring postgres database\nNdef: **IF AND ONLY IF**  all steps performed succesfully and grafana monitoring isn't updated, proceed with the following:\nUpdated the monitoring postgres database to appeared green/success in Grafana.\n- For a specified date:\n```bash\n# e.g 2023-03-30\nssh Exxxx@pr1edge01.mno.gr\nsudo -i -u postgres\npsql -d monitoring\nselect * from prod.monitoring where par_dt = 20230330;\nINSERT INTO prod.monitoring (application, job_name,component,status,par_dt,start_time,end_time,description,params,host) VALUES ('IBank_Migration','Enrich SA from SA_old','JOB',0,20230330,'2023-03-31 03:18:30.000','2023-03-31 05:00:42.000','','','pr1edge01.mno.gr') ON CONFLICT (application, job_name,component,par_dt) DO UPDATE SET status=0, start_time='2023-03-31 03:18:30.000', end_time='2023-03-31 05:00:42.000',description='';\n```\n- Check from Grafana that the failed job is now succeded",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 18"
        }
    },
    "261": {
        "page_content": "```\n- Check from Grafana that the failed job is now succeded\n### Hourly Merge Batch\nEvery hour a  transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and without deleting the data from the original tables. This table is used by mno to query the latest transactions.\n``` mermaid\n  graph TD\n  A[Kudu: prod_trlog_ibank.service_audit_stream] --> B[Spark: PROD_IBank_MergeBatch_Hourly]\n  C[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM] --> B\n  B --> D[Impala: prod_trlog_ibank.service_audit_hourly]\n  ```\n**User**: `PRODREST`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- Use the spark logs to identify the cause of the failure\n- The data in the Hourly tables is only for the current day. No need to fill missing hours, as the next MergeBatch will insert the data to the final table.\n## Queries\nExplains how Internet Banking queries are handled using REST endpoints and backend access to Impala and HBase.\nThe ingested data are queried in order to be displayed by the Internet Banking application (under the Calendar/\u0397\u03bc\u03b5\u03c1\u03bf\u03bb\u03cc\u03b3\u03b9\u03bf application). The application displays to the user only **Visible** transactions. The queries are submitted to the cluster as HTTP requests using the same Wildfly instances as the [Stream](#stream) flow.\n```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\n  B -->|Active| C[pr1edge01.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]\n  C -->|Impala JDBC / HBase client| G[Primary Site]\n  D -->|Impala JDBC / HBase client| G\n  E -.->|Stopped| H[Disaster Site]\n  F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodrestib`\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 19"
        }
    },
    "262": {
        "page_content": "F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodrestib`\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodrestib/server.log`\n**Access Log**: `/var/log/wildfly/prodrestib/access.log`\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\n**Alerts**:\n- [PR][IBANK] Query Average Response Time\n- [DR][IBANK] Query Average Response Time\n- [PR][IBANK] Query Average Error rate\n- [DR][IBANK] Query Average Error rate\n**Troubleshooting Steps**:\n- Check in Grafana dashboard `MONITORING/Monitoring Alerts` the charts that are in alerting state\n- If the response time is for _Old implementation_ (see below) queries check if the response time is constant, a single query can increase the mean value of the response time. Check the access log for entries with large response time for the specific endpoint. If it is an isolated query the customer is not impacted.\n- Check application logs for error messages.\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\n- If the logs do not indicate a specific problem, check for any problems regarding Impala/Kudu/HBase.\n### Old implementation\nThere are two versions for each query. The old implementention queries Impala tables stored in both HDFS and Kudu/HBase. This implementation had performance problems for many concurrent users.\n**Endpoints**:\n- auditCount **NOT USED BY mno**\n- auditSearch **NOT USED BY mno**\n- selectById **USED BY mno**\n### New implementation",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 20"
        }
    },
    "263": {
        "page_content": "**Endpoints**:\n- auditCount **NOT USED BY mno**\n- auditSearch **NOT USED BY mno**\n- selectById **USED BY mno**\n### New implementation\nThe new implementation uses a subset of the data (only visible transactions) stored in HBase. Queries required to access **non-Visible** transactions have to rely on the old implementation.\n**Endpoints**:\n- auditCountVisible **NOT USED BY mno**\n- auditSearchVisible **USED BY mno**\n- selectByIdVisible **NOT USED BY mno**\n## Retention Mechanism\nDescribes automated retention flows that drop old partitions from Impala, HDFS, and HBase to manage storage over time.\n### Impala Retention\nEvery day (at **3:00 pm in both sites** by **Cron**) This script drops partitions from impala tables `prod_trlog_ibank.service_audit_old` and `prod_trlog_ibank.historical_service_audit_v1` older than 10 days. It also removes a HDFS directory under `/mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2_vault/` that correspond to 30 days before.\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/common/log/retention_mechanism_daily.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/retention_mechanism_daily_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr`\n**Alerts**:\n- Retention prod_trlog_ibank.service_audit_old JOB\n- Retention prod_trlog_ibank.historical_service_audit_v1 JOB\n- Retention /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2_vault/`date -d \"-30 day\" +%Y-%m-%d`_`date -d \"-29 day\" +%Y-%m-%d` JOB\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, run manually the following commands\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019\n    impala-shell -k -i ${HOSTNAME/01/} --ssl --query \"set SYNC_DDL=true;alter table prod_trlog_ibank.service_audit_old drop partition ( par_dt <= 20191109 ) purge;\"",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 21"
        }
    },
    "264": {
        "page_content": "``` bash\n    # eg. 09-11-2019\n    impala-shell -k -i ${HOSTNAME/01/} --ssl --query \"set SYNC_DDL=true;alter table prod_trlog_ibank.service_audit_old drop partition ( par_dt <= 20191109 ) purge;\"\n    impala-shell -k -i ${HOSTNAME/01/} --ssl --query \"set SYNC_DDL=true;alter table prod_trlog_ibank.historical_service_audit_v1 drop partition ( par_dt <= '20191109' ) purge;\"\n    hdfs dfs -rm -R -skipTrash /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2_vault/2019-11-09_2019-11-10\n    ```\n#### Additional Tables\nKudu table's `prod_trlog_ibank.service_audit_stream` partitions are maintained by an Oozie jobs every day. See section [Create next week kudu partitions](#create-next-week-kudu-partitions).",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 22"
        }
    },
    "265": {
        "page_content": "Parquet table's `prod_trlog_ibank.service_audit_hourly` partitions are dropped after each successful Merge run. See section [Drop hourly partitions](#drop-hourly-partitions).\n#### DEV\nEvery day (at **16:05 on DR site only** by **Cron**). This script drops partitions from impala table `dev_trlog_ibank.service_audit` older than 60 days.\n**User**: `DEVREST`\n**Script Logs**: `/var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log`\n**Script**: `/opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh`\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, run manually the following command\n  ``` bash  \n  /opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh >> /var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log 2>&1\n  ```\n### HBase retention\nEvery day (at **16:00 in both sites** by **Cron**) This script deletes rows from hbase `PROD_IBANK:SERVICE_AUDIT_STREAM` which refere to transactions inserted 7 days ago. Those rows may have been not deleted properly after merge batch job. To do so the topology queries rows from impala table `prod_trlog_ibank.service_audit` inside partition with par_dt refering 7 days ago.\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/daily_tools_cleanupHBaseSAS.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/daily_tools_cleanupHBaseSAS.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr`\n**Alerts**:\n- Retention IbankCleanupHbaseSAS JOB\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, check on `/opt/ingestion/PRODREST/no_bkp/duplicate_cid_hbase` if a manual re-run must be done for a specific date\n  - For a specified date eg 2022-06-30:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 23"
        }
    },
    "266": {
        "page_content": "- For a specified date eg 2022-06-30:\n    ``` bash\n    /opt/ingestion/PRODREST/common/spark/submit/submitmnoSparkTopology_tools_cleanupHbaseSAS prod_trlog_ibank.service_audit PROD_IBANK:SERVICE_AUDIT_STREAM LIST 20220630\n    ```\n##  Oozie Jobs\nOozie workflows that import lookup tables and manage weekly Kudu partitioning.\n###  Lookup tables\nEvery day (at 07:15 by Oozie on DR & PR site ), we transfers 3 tables with reference data from the legacy MSSQL server, which is managed by mno, to the cluster. We keep only latest version to BigData (no partition).\n**User**: `PRODREST`\n**Coordinator**: `Coord_IbankLookupTables_PROD`\n**Workflow**: `ImportLookupTables`\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/lookupTables/import_lookup_tables.sh`\n**Logs**: from HUE\n**Alerts**:\n- Not Monitored\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs\n###  Create next week kudu partitions\nEvery day (at 10:00 by Oozie on DR site and 10:40 by Oozie on PR site ), we delete old Kudu partitions and add new Kudu partitions on `prod_trlog_ibank.service_audit_stream`.\n**User**: `PRODREST`\n**Coordinator**: `Coord_IBankCreateKuduPartitionsPROD`\n**Workflow**: `CreateKuduPartitionsPROD`\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\n**Logs**: from HUE\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs\nThere is a case that merge batch has not properly run for a specific date, **even though** the transactions for that specific date are present on impala `service_audit`. For example by moving a service_audit partiton from DR to PR or vice versa, or when merge batch failed **after** deleting transactions from kudu.",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 24"
        }
    },
    "267": {
        "page_content": "In that case we can manually mark merge batch as complete for that date, and therefore next oozie job created by `Coord_IBankCreateKuduPartitionsPROD` will delete that partition from kudu, by manipulating HBase table `PROD_IBANK:MERGE_BATCH_STATE_INFO`.\nExample for a specific date (10/10/2022):\n - Run HBase shell\n ```\n   hbase shell\n   ```\n - And then inside HBase shell:\n ```\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase', 'true'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase_time','0'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu', 'true'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu_time', '0'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running', 'false'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running+time', '0'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala', 'true'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala_time', '0'\n   ```\n#### DEV\nEvery day (at **11:10 by Oozie** on **DR site only** ), we delete old Kudu partitions and add new Kudu partitions on `dev_trlog_ibank.service_audit_stream`.\n**User**: `DEVREST`\n**Coordinator**: `Coord_IBankCreateKuduPartitionsDEV`\n**Workflow**: `CreateKuduPartitionsDEV`\n**Local path**: `/opt/ingestion/DEVREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\n**Logs**: from HUE\n**Alerts**:\n- Not Monitored",
        "metadata": {
            "category": "applicationFlows",
            "client": "Client_mno",
            "name": "ibank.md - Part 25"
        }
    },
    "268": {
        "page_content": "---\ntitle: \"Streamsets \u2013 Energy Efficiency Duplicate Resolution\"\ndescription: \"Guide for accessing Streamsets for the Energy Efficiency pipeline and resolving duplicates in the 'energy_efficiency.cell' table using Impala.\"\ntags:\n  - streamsets\n  - energy efficiency\n  - duplicates\n  - impala\n  - sftp\n  - hive\n  - overwrite\n  - bigd\n  - par_dt\n  - cell table\n  - un2\n---\n# Streamsets - Energy Efficiency\nThis guide outlines how to access Streamsets for the Energy Efficiency pipeline, detect duplicate records in the `energy_efficiency.cell` table using Impala, and safely remove them by overwriting partitions.\n## Streamsets Access\n- **Login URL**: [https://999.999.999.999:18636/](https://999.999.999.999:18636/)\n- **File Transfer (from `un2` using `sdc` user)**:\n```bash\nsftp bigd@999.999.999.999\ncd /ossrc\n```\n## Check for Duplicates in Table\nRun the following Impala queries to inspect duplicate records in the energy_efficiency.cell table:\n```sql\n-- Total rows by partition\nSELECT count(*), par_dt \nFROM energy_efficiency.cell \nWHERE par_dt > '202111201' \nGROUP BY par_dt \nORDER BY par_dt DESC;\n-- Check for duplicates on a specific date\nSELECT count(*) \nFROM (\n  SELECT DISTINCT * \n  FROM energy_efficiency.cell \n  WHERE par_dt = '20211210'\n) a;\n```\n## Reolve Duplicates\n### 1. Create a Backup Table\n```sql\nCREATE TABLE energy_efficiency.cell_bak LIKE energy_efficiency.cell;\nINSERT INTO TABLE energy_efficiency.cell_bak PARTITION (par_dt)\nSELECT * FROM energy_efficiency.cell;\n```\n### 2. Overwrite the Original Table\nUpdate the table using only distinct records for the specified partition range:\n```sql\nINSERT OVERWRITE TABLE energy_efficiency.cell PARTITION (par_dt)\nSELECT DISTINCT * \nFROM energy_efficiency.cell \nWHERE par_dt BETWEEN '20211210' AND '20211215';\n```\n### 3. Drop the Backup Table (if cleanup confirmed)\n```sql\nDROP TABLE energy_efficiency.cell_bak;\n```\n---\ntags:\n  - streamsets\n  - impala\n  - hive\n  - data deduplication\n  - energy efficiency\n  - par_dt",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "fix_energy_efficiency_duplicates.md - Part 1"
        }
    },
    "269": {
        "page_content": "```sql\nDROP TABLE energy_efficiency.cell_bak;\n```\n---\ntags:\n  - streamsets\n  - impala\n  - hive\n  - data deduplication\n  - energy efficiency\n  - par_dt\n  - partition overwrite\n  - data cleanup\n  - sftp access\n---",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "fix_energy_efficiency_duplicates.md - Part 2"
        }
    },
    "270": {
        "page_content": "---\ntitle: \"Fixing OpenLDAP Replication Issues\"\ndescription: \"Step-by-step instructions for resolving OpenLDAP replication failures between kerb1 and kerb2, including password updates, slapcat/slapadd procedures, and verification via ldapsearch.\"\ntags:\n  - openldap\n  - replication\n  - ldap\n  - slapcat\n  - slapadd\n  - kerb1\n  - kerb2\n  - phpldapadmin\n  - ldapsearch\n  - slapd\n  - sync\n  - mirror mode\n  - user creation\n  - credentials\n  - config.ldif\n  - data.ldif\n  - restore ldap\n  - slapd.d\n---\n# How to fix openldap replication\nThis guide documents how to fix broken OpenLDAP replication between kerb1 and kerb2, addressing two scenarios: a Manager password change or corruption due to events like power outages. It includes configuration updates, slapcat/slapadd restore steps, verification procedures, and UI-based user creation checks via phpLDAPadmin.\nThe broken replication between `kerb1`/`kerb2` could happened in case any of the below cases appeared/happened.\n- Case 1: You changed the `Manager` password of openldap instance\n- Case 2: Replication broken for any other reason (power outage etc) between `kerb1` and `kerb2`\nFor every ldasearch the password of `Manager` is [here](KnowledgeBase/prodsyspasswd.kdbx)\n## For Case 1 follow the below steps:\nLogin into kerb1 node as root\n```bash\nssh kerb1\nsudo -i\n```\nBackup:\n```bash\nslapcat -n 0 -l config.ldif\nslapcat -n 2 -l data.ldif\n```\nCreate ldif file replication fix\n```bash\nvi replication_config.ldif\ndn: olcDatabase={0}config,cn=config\nchangetype:modify\nreplace: olcSyncrepl\nolcSyncrepl: rid=001\n  provider=ldaps://kerb1.bigdata.abc.gr/\n  binddn=\"cn=config\"\n  bindmethod=simple\n  credentials=\"new password\"\n  searchbase=\"cn=config\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nolcSyncrepl: rid=002\n  provider=ldaps://kerb2.bigdata.abc.gr/\n  binddn=\"cn=config\"\n  bindmethod=simple\n  credentials=\"new password\"\n  searchbase=\"cn=config\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nadd: olcMirrorMode",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "fix_openldap_replication_procedure.md - Part 1"
        }
    },
    "271": {
        "page_content": "binddn=\"cn=config\"\n  bindmethod=simple\n  credentials=\"new password\"\n  searchbase=\"cn=config\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nadd: olcMirrorMode\nolcMirrorMode: TRUE\ndn: olcDatabase={2}bdb,cn=config\nchangetype:modify\nreplace: olcSyncrepl\nolcSyncrepl: rid=003\n  provider=ldaps://kerb1.bigdata.abc.gr/\n  binddn=\"cn=Manager,dc=bigdata,dc=abc,dc=gr\"\n  bindmethod=simple\n  credentials=`new password`\n  searchbase=\"dc=bigdata,dc=abc,dc=gr\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nolcSyncrepl: rid=004\n  provider=ldaps://kerb2.bigdata.abc.gr/\n  binddn=\"cn=Manager,dc=bigdata,dc=abc,dc=gr\"\n  bindmethod=simple\n  credentials=`new password`\n  searchbase=\"dc=bigdata,dc=abc,dc=gr\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nadd: olcMirrorMode\nolcMirrorMode: TRUE\n```\nFix the replication:\n```bash\nldapmodify  -H ldaps://kerb1.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\nldapmodify  -H ldaps://kerb2.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\n```\nChecks:\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:\nLogin into admin node as root:\nOpen firefox\n```bash\nfirefox\n```\nphpldapadmin link: (https://kerb1.bigdata.abc.gr/phpldapadmin/)\n## Steps to create an ldap user\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\n2. Expand tree `people`\n3. Click `create a new entry here`\n4. The type of account will be `User account`\n5. Fill all the empty boxes. \n6. The user will be `no login` and the group/gid `disabled`\n7. Create object\nAfter succesfully creation of `testuser` check if exist on `kerb2` from the dropdown menu or via ldapsearch\n```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=testuser'\n```\nIf user exist then replication fixed. Just delete the `testuser`.\n## Steps to delete an ldap user",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "fix_openldap_replication_procedure.md - Part 2"
        }
    },
    "272": {
        "page_content": "```\nIf user exist then replication fixed. Just delete the `testuser`.\n## Steps to delete an ldap user\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\n2. Expand tree `people`\n3. Check the new user and from the right bar click `delete this entry`\n## For Case 2 follow the below steps:\nIdentify which `kerb` ldap instance has the issue. For example check if they had the same amount of `users` and `groups` with `ldapsearch` commands from checks\nFrom the `kerb` ldap instance without corruption :\n```bash\nslapcat -n 0  config.ldif\nslapcat -n 1 data.ldif\nscp *.ldif `ldap_instance_with_corruption`:/tmp\n```\nGo to corrupted `kerb` instance:\nBackup:\n```bash\ncp -rp /etc/openldap/ /tmp/openldap.bak\ncp -rp /var/lib/ldap/ /tmp\n```\nClear:\n```bash\nsystemctl stop slapd\nsystemctl status slapd\ncd /etc/openldap/\nrm \u2013Rf slapd.d\ncd /etc/openldap\nmkdir slapd.d\ncd /var/lib/ldap/\nrm \u2013Rf *\n```\nRestore:\n```bash\nslapadd -n 0 -F /etc/openldap/slapd.d -l /tmp/config.ldif\nslapadd -n 2 -F /etc/openldap/slapd.d -l /tmp/data.ldif -w\n```\nOwnership:\n```bash\nchown -R ldap:ldap /var/lib/ldap\n```\nStart `slapd` service:\n```bash\nsystemctl start slapd\nsystemctl status slapd\n```\nChecks:\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:\nThe procedure to create a new user described [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/openldap_change_manager_password.md)\nLogin into admin node as root:\nOpen firefox\n```bash\nfirefox\n```\nphpldapadmin link: (https://kerb1.bigdata.abc.gr/phpldapadmin/)\nAfter successfully creation of `testuser` check if exist on `kerb2` from the dropdown menu or via ldapsearch\n```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=testuser'\n```\nIf user exist then replication fixed. Just delete the `testuser`.",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "fix_openldap_replication_procedure.md - Part 3"
        }
    },
    "273": {
        "page_content": "```\nIf user exist then replication fixed. Just delete the `testuser`.\nThe procedure to delete a new user described [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/openldap_change_manager_password.md)\n---\ntags:\n  - ldap\n  - openldap\n  - kerberos\n  - slapcat\n  - slapadd\n  - phpldapadmin\n  - ldap replication\n  - directory service\n  - slapd\n  - user management\n  - config.ldif\n  - data.ldif\n---",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "fix_openldap_replication_procedure.md - Part 4"
        }
    },
    "274": {
        "page_content": "---\ntitle: RCPE Integration with GROUPNET\ndescription: Procedure for migrating RCPE domain from central-domain.root.def.gr to GROUPNET including SSL setup, domain/user creation, sso-security.xml update, and MySQL user migration.\ntags:\n  - rcpe\n  - groupnet\n  - sso\n  - ldap\n  - domain-migration\n  - mysql\n  - kubernetes\n  - user-management\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  systems: unc2, rcpe1, rcpe2\n  domains: GROUPNET, central-domain.root.def.gr\n  app_links:\n    - https://cne.def.gr:8843/rcpe/#/login\n    - https://999.999.999.999:8743/rcpe/\n---\nThis procedure documents the complete migration of RCPE domain authentication from central-domain.root.def.gr to GROUPNET. It includes verifying LDAP certificate connectivity, creating the new domain in the RCPE UI, updating the sso-security.xml file on the backend application node, restarting RCPE, and migrating user domain assignments in the MySQL SSO_USERS table. It also covers domain deletion steps, how to verify login using the new domain, and all necessary backups. This guide is useful for recurring domain transitions, SSO reconfiguration, and LDAP troubleshooting in BigStreamer environments.\n# abc - [One Domain] RCPE integration with GROUPNET\n## Description\nThis document describes the procedure of changing current domain from `central-domain.root.def.gr` to `groupnet` along with their users\n## Servers:\n999.999.999.999 PVDCAHR01.groupnet.gr\n999.999.999.999 PVDCLAM01.groupnet.gr\n## Useful info:\nPROD\n- rcpe1.bigdata.abc.gr, rcpe2.bigdata.abc.gr, \n- https://999.999.999.999:8843/rcpe/#/login\n- https://999.999.999.999:8843/rcpe/#/login\n- https://cne.def.gr:8843/rcpe/#/login\nTEST\n- unc2.bigdata.abc.gr\n- https://999.999.999.999:8743/rcpe/\n> Ndef: Following procedure occurs for test. Be sure to apply the same steps for prod \n## Prerequisites\n1. Check if the ssl certificates of the groupnet have already been imported\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "RCPE_Change_Domain_Procedure.md - Part 1"
        }
    },
    "275": {
        "page_content": "> Ndef: Following procedure occurs for test. Be sure to apply the same steps for prod \n## Prerequisites\n1. Check if the ssl certificates of the groupnet have already been imported\n```bash\n[root@unc2 ~]# openssl s_client -connect PVDCAHR01.groupnet.gr:636\n[root@unc2 ~]# openssl s_client -connect PVDCLAM01.groupnet.gr:636\n```\nIf they are not been imported, you should import them using formula at `admin:etc/salt/salt/tls/certificate_authority/import_ca.sls`.\n2. Customer should send an active user that belongs to the new domain for testing \n3. `/etc/hosts` file must be updated to all  BigStreamer servers with the new domain \n4. Perform an ldap search for the given user:\n```\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -W -b \"dc=groupnet,dc=gr\" -D \"<Bind User sAMAccountName>\" '(sAMAccountName=...)'\n```\n## New Domain Creation\n1. Login to https://999.999.999.999:8743/rcpe/ with the credentials you have\n2. On the main screen select **User Management** on the left of the page\n3. Select **Domain** from the tabs on the left\n4. Select **Create New** button at the bottom of the view.\n5. Enter the name and description of the new domain (DOMAINS_NAME: groupnet.gr, DOMAINS_DESCRIPTION: GROUPNET Domain)\n6. Select **Create** button at the bottom of the view.\n### Create users for the new domain\n> Ndef: This section should be only followed in case the given user does not belong to RCPE. You can check that from **Users** Tab and search for the username. \n1. Select **Users** from the tabs on the left.\n2. Select **Create New** button at the bottom of the view to create a new user\n3. Enter the username and the required information for the newly user given by the customer ( Domain Attribute included ). \n> Ndef: You should not add a password here\n5. Select **Create** button at the bottom of the view.\n6. Click on **Fetch All** to view existing users including the new one",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "RCPE_Change_Domain_Procedure.md - Part 2"
        }
    },
    "276": {
        "page_content": "> Ndef: You should not add a password here\n5. Select **Create** button at the bottom of the view.\n6. Click on **Fetch All** to view existing users including the new one\n7. Click on the **magnifying glass** button next to the name of the newly created user in order to assign roles and click on button **USERS_ASSIGN_ROLES** , add SSO-Administrator and click on **Submit**.\n**Time to update sso-configuration**\n1. Login to `test_r_cpe` user\n```bash\nssh unc2\nsu - test_r_cpe #r_cpe for prod\n```\n2. Check trcpe status\n```bash\n[test_r_cpe@unc2 ~]$ trcpe-status #rcpe-status for prod\n```\n3. Stop trcpe\n```bash\n[test_r_cpe@unc2 ~]$ trcpe-stop #rcpe-stop for prod\n```\n4. Back up sso configuration for central\n```bash\n[test_r_cpe@unc2 ~]$ cp /opt/test_r_cpe/standalone/configuration/ServiceWeaver/sso/sso-security.xml /opt/test_r_cpe/standalone/configuration/ServiceWeaver/sso/sso-security-backup.xml\n#/opt/r_cpe/standalone/configuration/ServiceWeaver/sso/sso-security.xml path for prod \n```\n5. Move the new SSO configuration file from `/home/users/ilpap/sso-security-groupnet.xml` to the following path:\n`[test_r_cpe@unc2 ~]$ mv /home/users/ilpap/sso-security-groupnet.xml /opt/test_r_cpe/standalone/configuration/ServiceWeaver/sso/sso-security.xml`\n6. Start trcpe and check status\n```bash\ntrcpe-start\ntrcpe-status\n```\n7. Login to https://999.999.999.999:8743/rcpe/ with user and shared credentials. You must be able to see the newly created domain.\n### Move users to the created domain\n1. Back up mysql SSO_USERS table:\n```bash\nmysqldump -u root  -p test_r_cpe SSO_USERS --single-transaction > /tmp/SSO_USERS_BACKUP.sql\n```\n2. Move all users that have domain `central-domain.root.def.gr` to `groupnet.gr`\n```bash\n[root@db01 ~]# mysql -u root -p;\nmysql> use test_r_cpe;\nmysql> show tables;\nmysql> select * FROM SSO_DOMAINS LIMIT 5; #newly domain_ID is 5\nmysql> show create table SSO_USERS; #Domain_ID is currently 3\nmysql> UPDATE SSO_USERS SET DOMAIN_ID=5 WHERE DOMAIN_ID=3;",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "RCPE_Change_Domain_Procedure.md - Part 3"
        }
    },
    "277": {
        "page_content": "mysql> select * FROM SSO_DOMAINS LIMIT 5; #newly domain_ID is 5\nmysql> show create table SSO_USERS; #Domain_ID is currently 3\nmysql> UPDATE SSO_USERS SET DOMAIN_ID=5 WHERE DOMAIN_ID=3;\nmysql> select * FROM SSO_USERS where DOMAIN_ID=5;\n```\n## Domain Deletion\n1. Login with a user authorized with SSO access rights on the application\n2. On the main screen select User Management on the left of the page\n3. Select Domain from the tabs on the left\n4. Select the domain you want to delete by clicking on the left of the record\n5. Select Delete Row(s) button at the bottom of the view.\n6. Verify deletion  ( select Yes, delete on the pop-up view )\n**Congrats!**",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "RCPE_Change_Domain_Procedure.md - Part 4"
        }
    },
    "278": {
        "page_content": "---\ntitle: \"Nagios Alarms & Errors - Fork, SSH, Return Code 255\"\ndescription: \"Steps to resolve common Nagios issues on BigStreamer admin nodes including 'fork: retry', 'ssh_exchange_identification: Connection closed', and 'Return code 255 is out of bounds' by adjusting ulimits, SSH command options, and Nagios configuration settings.\"\ntags:\n  - nagios\n  - bigstreamer\n  - monitoring\n  - fork error\n  - ssh_exchange_identification\n  - return code 255\n  - max_concurrent_checks\n  - bashrc\n  - nagios.cfg\n  - commands.cfg\n  - ssh\n  - alerts\n  - admin\n  - abc\n---\n# Nagios Alarms & Errors\n**Component**: Nagios  \n**Environment**: BigStreamer  \n**Owner**: kpar  \n**Status**: Closed  \n**Date**: 2021-05-12  \n**Issue Number**: -  \n## Description\nThis document describes how to resolve the following Nagios errors:\n- `/etc/bashrc: fork: retry: Resource temporarily unavailable`\n- `ssh_exchange_identification: Connection closed by remote host`\n- `Return code of 255 is out of bounds`\n## Resolution Steps\n### 1. Fix \"fork: retry\" Error\nAs root or as the `nagios` user, edit the `.bashrc` file:\n```bash\nvi /home/nagios/.bashrc\n```\nAdd the following lines:\n```bash\nulimit -u 8888\nulimit -n 2222\n```\n---\n### 2. Fix SSH \"Connection closed by remote host\" Error\nAs root, edit the following Nagios command file:\n```bash\nvi /usr/local/nagios/etc/objects/commands.cfg\n```\nReplace this line:\n```bash\n$USER1$/check_by_ssh  -H $HOSTADDRESS$ -t 30 -C \"/usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$ -p $ARG3$\"\n```\nWith this:\n```bash\n$USER1$/check_by_ssh -E 8 -o StrictHostKeyChecking=no -H $HOSTADDRESS$ -t 30 -C \"/usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$ -p $ARG3$\"\n```\n---\n### 3. Fix \"Return code of 255 is out of bounds\" Error\nAs root, edit the Nagios configuration file:\n```bash\nvi /usr/local/nagios/etc/nagios.cfg\n```\nFind and change the following setting:\n```bash\nmax_concurrent_checks=50\n```\nThen restart the Nagios service:",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "nagios-errors.md - Part 1"
        }
    },
    "279": {
        "page_content": "```bash\nservice nagios restart\n```\n---\n## Keywords\nlogs, fork, bounds, connection closed, ssh, ulimit, max_concurrent_checks, Nagios admin",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "nagios-errors.md - Part 2"
        }
    },
    "280": {
        "page_content": "---\ntitle: SpagoBI Integration with GROUPNET Domain\ndescription: Procedure for migrating SpagoBI authentication from central-domain.root.def.gr to GROUPNET, including LDAP search, HAProxy update, config changes, and user migration in the spagobi MySQL database.\ntags:\n  - spagobi\n  - groupnet\n  - ldap\n  - haproxy\n  - user-migration\n  - domain-change\n  - mysql\n  - authentication\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  systems:\n    - un5.bigdata.abc.gr\n    - db01\n    - un1 (HAProxy)\n  domains:\n    from: central-domain.root.def.gr\n    to: groupnet\n  app_url: https://cne.def.gr/SpagoBI\n  bind_server: PVDCAHR01.groupnet.gr\n---\n# abc - [One Domain] SpagoBI integration with GROUPNET\nThis document describes the procedure of changing current domain from `central-domain.root.def.gr` to `groupnet` along with their users.\nServer to use: PVDCAHR01.groupnet.gr\nProd Server: un5.bigdata.abc.gr\nURL: https://cne.def.gr/SpagoBI\n999.999.999.999 cne.def.gr cne\n### Prerequisites\nVerify if GROUPNET LDAP server's SSL certificate is already trusted by un5\n1. Check if the ssl certificates of the groupnet have already been imported\n```bash\n[root@un5 ~]# openssl s_client -connect PVDCAHR01.groupnet.gr:636\n```\nIf it is not been imported, you should import them using formula  `admin:etc/salt/salt/tls/certificate_authority/import_ca.sls`.\n2. Customer should send an active user that belongs to the new domain so we can verify that the change is succesfully made. \nVaggos username in our case (enomikos)\n3. Customer should also send a bind user that we will use for groupnet domain configuration.\n4. `/etc/hosts` file at `un5` must be updated to all  BigStreamer servers with the new domain \n5. Perfom an ldap search for the given bind user. \n```bash\n[root@unrstudio1 ~]# ldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D \"t1-svc-cnebind\" -W -b \"dc=groupnet,dc=gr\" '(sAMAccountName=enomikos)'\n```\n### Backup\nBackup current SpagoBI user database in case rollback is needed",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 1"
        }
    },
    "281": {
        "page_content": "```\n### Backup\nBackup current SpagoBI user database in case rollback is needed\n1. Backup spagobi mysql database:\n```bash\n[root@db01 ~]# mysqldump -u root -p spagobi --single-transaction > /tmp/spagobi.sql\n```\n2. Back up `ldap_authorizations.xml`:\n```bash\n[root@un5 ~]# cp -ap /usr/lib/spagobi/webapps/SpagoBIProject/WEB-INF/conf/webapp/ldap_authorizations.xml /usr/lib/spagobi/webapps/SpagoBIProject/WEB-INF/conf/webapp/ldap_authorizations-central.xml\n```\n3. Back up haproxy:\n```bash\n[root@un1 ~]# cp -ap /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak\n```\n### Actions\n1. Login to `https://cne.def.gr/SpagoBI` with the credentials you have and create groupnet user for user `enomikos`:\n- User Management\n- Click on **Add**\n- Fill in with the user ID and full name\n- Add roles\n- Save\n2. Verify that the user is successfully created using following commands:\n```bash\n[root@db01 ~]# mysql -u spagobi -p;\nmysql> use spagobi;\nmysql> show tables;\nmysql> select * FROM SBI_USER WHERE USER_ID='enomikos@groupnet';\n```\n3. Stop SpagoBI process:\n```bash\n[root@un5 ~]# docker stop prod-spagobi-7.0.105\n```\n4. Update SpagoBI LDAP config to use new GROUPNET bind credentials and base DN. Edit the following lines at `un5:/usr/lib/spagobi/webapps/SpagoBIProject/WEB-INF/conf/webapp/ldap_authorizations.xml`:\n```bash\n<!--  SERVER -->\n                <HOST>un1.bigdata.abc.gr</HOST>\n                <PORT>863</PORT>        \n                <ADMIN_USER>replace_with_name_of_admin</ADMIN_USER>\n                <ADMIN_PSW>replace_with_password</ADMIN_PSW> <!-- password in clear text -->\n                <BASE_DN>dc=groupnet,dc=gr</BASE_DN> <!-- base domain, if any -->\n```\n5. Expose GROUPNET AD through HAProxy with LDAPS support to be reachable from SpagoBI. Update reverse proxy at `un1` so that Groupnet AD can be reached directly from spagobi app.\nAdd the following at `un1:/etc/haproxy/haproxy.cfg`\n```bash\nlisten def-ad-ldaps\n    bind *:863 ssl crt /opt/security/haproxy/node.pem\n    mode tcp",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 2"
        }
    },
    "282": {
        "page_content": "Add the following at `un1:/etc/haproxy/haproxy.cfg`\n```bash\nlisten def-ad-ldaps\n    bind *:863 ssl crt /opt/security/haproxy/node.pem\n    mode tcp\n    balance     source\n    server def_ad1 PVDCAHR01.groupnet.gr:636 ssl check ca-file /etc/ssl/certs/ca-bundle.crt\n```\n6. Test and reload haproxy in order changes to take effect\n```bash\n[root@un1 ~]# haproxy -f /etc/haproxy/haproxy.cfg -c\n[root@un1 ~]# systemctl reload haproxy\n[root@un1 ~]# systemctl status haproxy\n```\n7. Start SpagoBI app:\n```bash\n[root@un5 ~]# docker start prod-spagobi-7.0.105\n```\n8. Check if `enomikos` can sign in. If yes, then go to the next step\n9. Migrate all users in SpagoBI MySQL DB from central-domain to groupnet. Move all users that have domain `central-domain.root.def.gr` to `groupnet.gr`\n```bash\n[root@db01 ~]# mysql -u root -p;\nmysql> use spagobi;\nselect * from SBI_USER WHERE USER_ID LIKE '%@central-domain%'; #check existing users that belong to central-domain\nUPDATE SBI_USER SET USER_ID = REPLACE(USER_ID,'@central-domain','@groupnet') WHERE USER_ID LIKE '%@central-domain%';\nselect * from SBI_USER WHERE USER_ID LIKE '%@central-domain%'; #check that no user left to central-domain\n```\n> Ndef: Before moving all users at once to the new domain you can first test just one. For example:\nUPDATE SBI_USER SET USER_ID = REPLACE(USER_ID,'@groupnet.gr','@groupnet') WHERE USER_ID LIKE '%enomikos@groupnet.gr%'\nselect * from SBI_USER WHERE USER_ID LIKE '%enomikos@groupnet.gr%'",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 3"
        }
    },
    "283": {
        "page_content": "---\ntitle: \"GROUPNET - Change Bind Users' Passwords\"\ndescription: \"Step-by-step guide for updating LDAP bind user passwords for RAN.AI Geolocation and R-Studio Connect services in the GROUPNET domain, including license reactivation if needed.\"\ntags:\n  - ldap\n  - bind user\n  - password rotation\n  - rstudio connect\n  - ran.ai\n  - groupnet\n  - license activation\n  - cyberark\n  - keycloak\n  - ldap integration\n  - rstudio-connect.gcfg\n  - authentication\n---\n# GROUPNET - Change bind users' passwords\nThis document outlines the procedure for rotating LDAP bind user passwords for the GROUPNET domain, covering both RAN.AI Geolocation (`t1-svc-cneranaibind`) and R-Studio Connect (`t1-svc-cnebind`). It also includes instructions for resolving expired license issues following a restart.\n- [GROUPNET - Change bind users' passwords](#groupnet---change-bind-users-passwords)\n  - [RAN.AI Geolocation - t1-svc-cneranaibind](#ranai-geolocation---t1-svc-cneranaibind)\n  - [R-Studio Connect - t1-svc-cnebind](#r-studio-connect---t1-svc-cnebind)\n    - [Re-activate License for R-Studio Connect](#re-activate-license-for-r-studio-connect)\nThe purpose of this document is to describe the procedure on how to change the passwords for the bind users used for querying the LDAP of GROUPNET domain.\n## RAN.AI Geolocation - t1-svc-cneranaibind\n1. Inform users that the authentication with GROUPNET accounts may encounter errors during the procedure (approximate 1 hour). No pod will be restarted.\n2. Go to [https://cne.def.gr/auth/admin](https://cne.def.gr/auth/admin)\n3. Login with an administrative account\n4. Navigate to User Federation > GROUPNET\n5. Request password update `t1-svc-cneranaibind` via Remedy and obtain the new password\n6. Update `Bind Credential` field and press `Save`\n7. Press `Test authentication`\n## R-Studio Connect - t1-svc-cnebind\n1. Inform users for downtime of approximate 1 hour\n2. Login to `unrstudio1.bigdata.abc.gr` as root via Cyberark",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "groupnet_change_bind_users_passwords.md - Part 1"
        }
    },
    "284": {
        "page_content": "7. Press `Test authentication`\n## R-Studio Connect - t1-svc-cnebind\n1. Inform users for downtime of approximate 1 hour\n2. Login to `unrstudio1.bigdata.abc.gr` as root via Cyberark\n3. Request password update `t1-svc-cnebind` via Remedy and obtain the new password\n4. Edit `/etc/rstudio-connect/rstudio-connect.gcfg`\n``` bash\nvi  /etc/rstudio-connect/rstudio-connect.gcfg\n# Update **BindPassword** with the password obtained in step 3 and save\n```\n5. Restart R-Studio Connect\n``` bash\nsystemctl restart rstudio-connect\n```\n6. Check R-Studio Connect status\n``` bash\nsystemctl status rstudio-connect\n```\n7. Verify LDAP authentication by logging in to [https://unrstudio1.bigdata.abc.gr/connect](https://unrstudio1.bigdata.abc.gr/connect)\n8. Due to the fact that the server is not directly connected to the Internet, R-Studio Connect might display an error about expired license after the reboot. In this case follow the steps listed [below](#re-activate-license-for-r-studio-connect).\n9. Inform users that the application is available.\n### Re-activate License for R-Studio Connect\n1. Login to `unrstudio1.bigdata.abc.gr` as root via Cyberark\n2. Ensure that time is accurate and the time zone is correct for the machine.\n```bash\ntimedatectl\n```\n3. Sync date and time to hardware clock of the machine.\n``` bash\nhwclock -w\n```\n4. Deactivate license\n``` bash\nexport http_proxy=http://un-vip.bigdata.abc.gr:5555\nexport https_proxy=http://un-vip.bigdata.abc.gr:5555\n/opt/rstudio-connect/bin/license-manager deactivate\n```\n5. Activate license\n``` bash\nexport http_proxy=http://un-vip.bigdata.abc.gr:5555\nexport https_proxy=http://un-vip.bigdata.abc.gr:5555\n/opt/rstudio-connect/bin/license-manager activate <product-key>\n# This should display Activation status as Activated \n```\n6. In case you  receive the following:\n```text",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "groupnet_change_bind_users_passwords.md - Part 2"
        }
    },
    "285": {
        "page_content": "/opt/rstudio-connect/bin/license-manager activate <product-key>\n# This should display Activation status as Activated \n```\n6. In case you  receive the following:\n```text\nError activating product key: (13): The activation has expired or the system time has been tampered with. Ensure your time, timezone, and date settings are correct. If you're sure the license is not expired, try performing the following steps, in order: \n1. Fix the timezone on your system.\n2. Fix the date on your system.\n3. Fix the time on your system.\n4. Perform a system restart (important!)\n```\nFix any time/date issues and **reboot the server**.\n7. Verify license status\n``` bash\n/opt/rstudio-connect/bin/license-manager status\n/opt/rstudio-connect/bin/license-manager verify\n```\n8. Restart R-Studio Connect\n``` bash\nsystemctl restart rstudio-connect\n```\n9. Check R-Studio Connect status\n``` bash\nsystemctl status rstudio-connect\n```\n10. Verify LDAP authentication by logging in to [https://unrstudio1.bigdata.abc.gr/connect](https://unrstudio1.bigdata.abc.gr/connect)",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "groupnet_change_bind_users_passwords.md - Part 3"
        }
    },
    "286": {
        "page_content": "---\ntitle: \"Kubernetes User Environment Setup\"\ndescription: \"Step-by-step guide to install kubectl and helm, create Kubernetes service accounts, and generate user-specific kubeconfig for RAN.AI environments.\"\ntags:\n  - kubernetes\n  - kubectl\n  - service account\n  - kubeconfig\n  - user setup\n  - helm\n  - role binding\n  - authentication\n  - secret\n  - RAN.AI\n---\n# Kubernetes User Environment Setup\nThis guide explains how to set up a user environment for Kubernetes access in a RAN.AI cluster. It covers kubectl and helm installation, service account creation, role bindings, secrets, and kubeconfig generation\u2014manually or using a plugin.\n## Tools\nThe main tool that needs to be installed is **kubectl**, instructions for which can be found\n[here](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/).\n### Install bash completion for kubectl:\nAdditionally after\ninstallation, completion can be enabled by executing:\n```bash\nmkdir -p /etc/bash_completion.d\nkubectl completion bash | sudo tee /etc/bash_completion.d/kubectl\n```\nIf possible a nice to have plugin for setting up kubeconfigs can be found at\n[kubectl-view-serviceaccount-kubeconfig-plugin](https://github.com/superbrothers/kubectl-view-serviceaccount-kubeconfig-plugin/releases). Simply place the binary under `/usr/local/bin/`.\nAdditionally in order to install **helm**, follow the [instructions](https://helm.sh/docs/intro/install/)\nand set up completion by executing the following:\n```bash\nhelm completion bash | sudo tee /etc/bash_completion.d/helm\n```\n## Service Account\nCreate the following YAML files, that contain the definition for the service account and its\nrole binding:\n- **`service_account.yml`**\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: <Account Name>\n  namespace: <RAN.AI Namespace>\n```\n- **`role_binding.yml`**\n```yaml\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: <Binding Name>\n  namespace: <RAN.AI Namespace>\nsubjects:\n- kind: ServiceAccount",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "create_ranai_kubernetes_user.md - Part 1"
        }
    },
    "287": {
        "page_content": "```\n- **`role_binding.yml`**\n```yaml\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: <Binding Name>\n  namespace: <RAN.AI Namespace>\nsubjects:\n- kind: ServiceAccount\n  name: <Account Name>\n  namespace: <RAN.AI Namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit\n```\n### Apply service account and role binding\n```bash\nkubectl apply -f service_account.yml\nkubectl apply -f role_binding.yml\n```\n### User Secret\nFor Kubernetes versions over 1.24 when creating a service account it's secret is not automatically created\nand mounted, so in that case create the following secret:\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <Account Secret Name>\n  namespace: <RAN.AI Namespace>\n  annotations:\n    kubernetes.io/service-account.name: <Account Name>\ntype: kubernetes.io/service-account-token\n```\n### Generate user kubeconfig using plugin:\nExecute the following to get the new users config. If the plugin is not available the config must be constructed manually:\n```\nkubectl view-serviceaccount-kubeconfig -n <RAN.AI Namespace> <Account Name> > <User's Home>/.kube/config\n```\nFor reference the config looks like this:\n```yaml\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: ...\n    server: https://<Server>:<Port>\n  name: <Cluster Name>\ncontexts:\n- context:\n    cluster: <Cluster Name>\n    namespace: <RAN.AI Namespace>\n    user: <Account Name>\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: <Account Name>\n  user:\n    token: ...\n```\nIn the above segment `certificate-authority-data` and `token` can be obtained by executing:\n```bash\n# Find the Account's Secret Token name\nkubectl get secrets -n <RAN.AI Namespace>\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.ca\\.crt}'\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.token}' | base64 --decode\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "create_ranai_kubernetes_user.md - Part 2"
        }
    },
    "288": {
        "page_content": "---\ntitle: \"MoPs Index\"\ndescription: \"Central index of MoPs (Methods of Procedure) maintained by the Big Data team. Includes a link to the DevOps GitLab page where the full list of MoPs is stored.\"\ntags:\n  - mops\n  - methods of procedure\n  - operations\n  - devops\n  - big data\n  - gitlab\n  - documentation index\n  - platform procedures\n---\n# MoPs Index\nThis document serves as a reference point to access the full list of MoPs used in the Big Data environment. MoPs (Methods of Procedure) are detailed technical instructions for executing recurring or critical tasks. The full index is maintained in the DevOps GitLab repository.\n[View the MoPs Index in GitLab](https://metis.ghi.com/obss/bigdata/abc/devops/devops-projects/-/blob/master/MoPs_index.md)\n---\ntags:\n  - mops\n  - devops\n  - bigdata\n  - procedures\n  - operations\n  - documentation\n---",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "Dev_MoPs_index.md - Part 1"
        }
    },
    "289": {
        "page_content": "---\ntitle: \"Certificate Authority Installation\"\ndescription: \"Step-by-step procedure for installing a root certificate authority using SaltStack, including OS trust and Java keystore integration.\"\ntags:\n  - certificate\n  - saltstack\n  - tls\n  - root ca\n  - jssecacerts\n  - java keystore\n  - tls internal certificate\n  - infrastructure\n  - security\n  - ssl\n---\n### Certificate Authority installation\nThis guide explains how to install a root certificate authority in a system using SaltStack automation, covering both OS-level trust and Java keystore (`jssecacerts`) integration.\nThe following procedure describes how to install a root certificate authority using SaltStack:\n 1.  Move given certificate under `admin:/etc/salt/salt/tls/internal_certificate/root_certificate/`\n 2.  Rename `.cer` file to `.crt`.\n- If certificate has `.crt` suffix you should first verify that it base64 encoded by opening file and make sure it starts with `-----BEGIN CERTIFICATE----`\n ```bash\nmv /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.crt /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.cer\n```\n3. Install certificate by using saltStack formula:\n```bash\n###Test what actions will take affect before actually run the installation formula\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os test=True\n### Install the certificate\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os\n```\n4. Install jssecacerts by using saltStack formula:\n```bash\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_jssecacerts\n```\n> Ndef: Keep in mind that above command will fail if there is no java installed at the specified node\n**Congratulations!** Certificate installation is complete!",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "Certificate_authority_installation.md - Part 1"
        }
    },
    "290": {
        "page_content": "---\ntitle: \"MoP - AppEmptyQueryException Alerts Resolution\"\ndescription: \"Step-by-step resolution procedure for AppEmptyQueryException alerts caused by missing interface and CPU/MEM metrics in BigStreamer\u2019s bigcust tables, including log inspection and verification on un2 and nnmprd01.\"\ntags:\n  - mop\n  - AppEmptyQueryException\n  - bigcust\n  - ipvpn\n  - interface metrics\n  - cpu metrics\n  - memory metrics\n  - csv transfer\n  - nnmprd01\n  - un2\n  - ip_vpn\n  - dataparser\n  - sftp\n  - missing data\n  - bigstreamer\n  - alerts\n---\nThis MoP describes how to investigate and resolve `AppEmptyQueryException` alerts triggered due to missing data in the `bigcust` interface and CPU/MEM metrics tables. The procedure involves log review on the ingestion and source systems (un2 and nnmprd01) and CSV generation verification.\n# Method of Procedure (MoP) - AppEmptyQueryException Alerts Resolution\n## Description\nThis MoP document outlines the resolution steps for addressing AppEmptyQueryException alerts related to missing data from bigcust tables. The alerts pertain to two categories: IF (Interface Metrics) and CPU/MEM (CPU and Memory Metrics).\n## Affected Alerts\n1. IF Alerts:\n   - Occurred for 5-minute intervals starting from `2023-10-12 14:30:00` until `2023-10-12 14:50:00`.\n2. CPU/MEM Alerts:\n   - Occurred for 5-minute intervals starting from `2023-10-12 14:30:00` until `2023-10-12 14:45:00` and then for `2023-10-12 14:55:00` & `2023-10-12 15:00:00`.\n## Resolution Steps\n### Resolution for IF Alerts:\n1. Check `bigcust.perf_interfacemetrics_ipvpn_hist` table to confirm the missing data for the mentioned 5-minute intervals.\n2. Review the CSV file loading process into HDFS for errors:\n- Connect to `un2` as `ipvpn` and inspect the log file:\n```bash\nssh root@un2\nsu ipvpn\ntail -f /shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/log/nnm_interface_metrics.cron.20231012.log\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md - Part 1"
        }
    },
    "291": {
        "page_content": "- Connect to `un2` as `ipvpn` and inspect the log file:\n```bash\nssh root@un2\nsu ipvpn\ntail -f /shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/log/nnm_interface_metrics.cron.20231012.log\n```\n- The last loaded CSV file was `InterfaceMetrics_20231012142500238.csv.gz` at `2023-10-12 14:28:00`. No files were loaded from that point until 14:50, with the message: \"File '/home/custompoller/nnm_interface_metrics/InterfaceMetrics*.csv.gz' not found.\"\n3. Verify the transfer process on `nnmprd01` for errors:\n- Connect to `nnmprd01` via passwordless SSH from `un2`:\n```bash\nssh custompoller@nnmprd01\n```\n- Review the transfer log:\n```ssh\ntail -f /home/custompoller/export_metrics/log/transfer-new-files-cut.20231012.log\n```\nThe last file moved from the source directory to the destination directory on `nnmprd01` was `InterfaceMetrics_20231012142500238.csv.gz` at `14:28:01`. This continues until `14:50`, aligning with the observations in the `bigcust.perf_interfacemetrics_ipvpn_hist` table.\n#### Conclusion for IF Alerts\nRoot Cause: No Interface Metrics files were generated by NNM between `14:30` and `14:50`, explaining the IF alerts.\n### Resolution for CPU/MEM Alerts:\n1. Check `bigcust.nnm_ipvpn_componentmetrics_hist` table to confirm the missing data for the mentioned 5-minute intervals.\n2. Examine the SFTP transfer process from `nnprd01` for errors:\n- Navigate to the log file on `ipvpn@un2`:\n```bash\ntail -f /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20231012.log\n```\nThe last SFTP transfer was for the file `BIG-CUSTOMERS-CPU-MEM-UTIL_20231012142323337.csv.gz` at `14:26`. Subsequently, the logs show the message \"No file found.. sleep and try again..\" until `14:58`, when a CSV file was found.\n#### Conclusion for CPU/MEM Alerts\nRoot Cause: No CPU/MEM files were generated by NNM between `14:30` and `15:00`, explaining the CPU/MEM alerts.",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md - Part 2"
        }
    },
    "292": {
        "page_content": "---\ntitle: \"abc - Retention and Anonymization Job Status Checks\"\ndescription: \"Instructions for checking the status and logs of Retention_Dynamic_Drop_DDL and Anonymize_Data_Main shell scripts on abc using Snapshot ID and RunID from log files.\"\ntags:\n  - abc\n  - BigStreamer\n  - retention\n  - anonymization\n  - job monitoring\n  - script status\n  - log analysis\n  - shell commands\n  - troubleshooting\n  - snapshot\n  - runid\n---\n## Retention Check\n### Step 1 \u2013 Initial Status Check\nLogin to `un2` as `intra` and run the following command:\n```bash\ngrep \"Script Status\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.202012.log | tail -n1\n```\nExample output:\nScript Status ==> Scr:203.Retention_Dynamic_Drop_DDL.sh, Dt:2020-12-18 08:13:12, Status:0, Snapshot:1608267602, RunID:1608271202, ExpRows:3327, Secs:790, 00:13:10\nIf Status != 0, the script has failed.\n---\n### Step 2 \u2013 Deeper Investigation\nExtract the Snapshot value from the above output (e.g. 1608267602) and check for any logged problems:\n```bash\negrep -i '(error|problem|except|fail)' /shared/abc/cdo/log/Retention/*1608267602*.log\n```\nIf the result has fewer than 10 lines, it\u2019s usually not concerning. A large number of matches indicates an issue.\n## Anonymization Check\n### Step 1 \u2013 Initial Status Check\n``` bash\ngrep \"Script Status\" /shared/abc/cdo/log/100.Anonymize_Data_Main.202012.log | tail -n1\n```\nExample output:\nScript Status ==> Scr:100.Anonymize_Data_Main.sh, Dt:2020-12-17 21:01:03, Status:, RunID:1608228002, Secs:3661, 01:01:01\n### Step 2 \u2013 Check Detailed Errors\nExtract the RunID (e.g. 1608228002) and inspect logs:\n```bash\negrep '(:ERROR|with errors)' /shared/abc/cdo/log/Anonymize/*1608228002*.log | less\n```\nIf the command returns output, there is a problem.\n## Notes\n- All paths refer to shared log directories on abc BigStreamer.\n- Retention jobs refer to dropping old data.\n- Anonymization jobs refer to data privacy transformations.\n---",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "check_anonymization.md - Part 1"
        }
    },
    "293": {
        "page_content": "---\ntitle: \"Manage Connectivity with Viavi Kafka\"\ndescription: \"Configuration and operational steps for setting up HAProxy, DNS, and connectivity between BigStreamer and Viavi's Kafka cluster using a single node (Incelligent) with TCP passthrough and TLS encryption.\"\ntags:\n  - viavi\n  - kafka\n  - haproxy\n  - vlan300\n  - tls\n  - tcp\n  - kafka-proxy\n  - kafka-client\n  - vpn\n  - dns\n  - incelligent\n  - bigstreamer\n  - vlan\n  - geolocation\n  - spark-streaming\n  - devops\n  - network\n  - connectivity\n  - procedure\n  - admin\n---\nThis document describes the configuration of secure connectivity between BigStreamer and Viavi's Kafka brokers via HAProxy using internal VLAN 300. It includes HAProxy setup, DNS configuration, and procedures for managing services and validating access.\n# Viavi Kafka Integration via HAProxy\n- [Manage Connectivity with Viavi Kafka](#manage-connectivity-with-viavi-kafka)\n  - [Setup](#setup)\n    - [HAProxy Configuration](#haproxy-configuration)\n  - [Procedure](#procedure)\n    - [Manage HAProxy](#manage-haproxy)\n    - [Manage DNS entries](#manage-dns-entries)\n## Setup\nIncelligent wants to develop an application with Viavi's Kafka cluster as the datasource.This project aims to replace the current feed that powers RAN.AI Geolocation that is based on SFTP file transfers with a Spark Streaming application. Kafka Cluster is secured using internal/local users and TLS encryption.\nabc requested to expose Viavi's Kafka Cluster to BigStreamer's datanodes and enable the development of the application.\nIn order to achieve the connectivity we have added three new IPs (VLAN 300) to the Incelligent node and we have assigned the hostnames from the Kafka Brokers to these internal IPs with DNS entries on the cluster's internal DNS.\nThe reason we have used only one node for the connectivity is that the traffic from this flow can possibly saturate all uplinks of the BigStreamer which would impact other flows.\n``` mermaid\ngraph TD",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 1"
        }
    },
    "294": {
        "page_content": "The reason we have used only one node for the connectivity is that the traffic from this flow can possibly saturate all uplinks of the BigStreamer which would impact other flows.\n``` mermaid\ngraph TD\n  subgraph internal [Internal BigStreamer Network - VLAN 300]\n  A[Kafka Client]\n  A-->A1\n  A-->B1\n  A-->C1\n  subgraph proxy [incelligent node]\n    A1[999.999.999.999:9093<br>geolightgr35.geo.abc.gr - Intenral DNS]\n    B1[999.999.999.999:9093<br>geolightgr36.geo.abc.gr - Intenral DNS]\n    C1[999.999.999.999:9093<br>geolightgr37.geo.abc.gr - Intenral DNS]\n  end \n  end\n  subgraph kafka [Viavi's Kafka]\n    A2[999.999.999.999:9093<br>geolightgr35.geo.abc.gr - Actual Broker]\n    B2[999.999.999.999:9093<br>geolightgr36.geo.abc.gr - Actual Broker]\n    C2[999.999.999.999:9093<br>geolightgr37.geo.abc.gr - Actual Broker]\n  end\n  A1-->|HAProxy - Mode TCP|A2\n  B1-->|HAProxy - Mode TCP|B2\n  C1-->|HAProxy - Mode TCP|C2\n```\n### HAProxy Configuration\n```conf\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         999.999.999.999 local2",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 2"
        }
    },
    "295": {
        "page_content": "chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 999.999.999.999/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n\nlisten viavi-megafeed-kafka1\n\n   bind 999.999.999.999:9092\n   mode tcp\n   balance leastconn\n\n   server viavi-megafeed-kafka1 999.999.999.999:9092\n\nlisten viavi-megafeed-kafka2\n\n   bind 999.999.999.999:9092\n   mode tcp\n   balance leastconn\n\n   server viavi-megafeed-kafka2 999.999.999.999:9092\n\nlisten viavi-megafeed-kafka3\n\n   bind 999.999.999.999:9092\n   mode tcp\n   balance leastconn\n\n   server viavi-megafeed-kafka3 999.999.999.999:9092\n\nlisten viavi-megafeed-kafka1_ssl\n\n   bind 999.999.999.999:9093\n   mode tcp\n   balance leastconn\n\n   server viavi-megafeed-kafka1 999.999.999.999:9093\n\nlisten viavi-megafeed-kafka2_ssl\n\n   bind 999.999.999.999:9093\n   mode tcp\n   balance leastconn\n\n   server viavi-megafeed-kafka2 999.999.999.999:9093\n\nlisten viavi-megafeed-kafka3_ssl\n\n   bind 999.999.999.999:9093\n   mode tcp\n   balance leastconn",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 3"
        }
    },
    "296": {
        "page_content": "server viavi-megafeed-kafka2 999.999.999.999:9093\n\nlisten viavi-megafeed-kafka3_ssl\n\n   bind 999.999.999.999:9093\n   mode tcp\n   balance leastconn\n\n   server viavi-megafeed-kafka3 999.999.999.999:9093\n```\n## Procedure\n### Manage HAProxy\nStart - From incelligent node as root\n``` bash\nsystemctl start haproxy\n```\nStop - From incelligent node as root\n``` bash\nsystemctl stop haproxy\n```\nCheck - From incelligent node as root\n``` bash\nsystemctl status haproxy\nss -tulnp | grep 9093 # Check listening port\nnc -zv 999.999.999.999 9093 # Check HAProxy IP 1\nnc -zv 999.999.999.999 9093 # Check HAProxy IP 2\nnc -zv 999.999.999.999 9093 # Check HAProxy IP 3\nnc -zv 999.999.999.999 9093 # Check broker 1\nnc -zv 999.999.999.999 9093 # Check broker 2\nnc -zv 999.999.999.999 9093 # Check broker 3\n```\n### Manage DNS entries\nLogin to [IDM](https://admin.bigdata.abc.gr/ipa/ui/) with an administrative account to manage DNS entries",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 4"
        }
    },
    "297": {
        "page_content": "---\ntitle: RStudio Connect - Migrate to GROUPNET Domain\ndescription: Procedure for migrating RStudio Connect LDAP authentication from central-domain.root.def.gr to GROUPNET domain including certificate verification, configuration changes, user license management, and resolving duplicate users.\ntags:\n  - rstudio-connect\n  - ldap\n  - groupnet\n  - user-migration\n  - license-management\n  - project-transfer\n  - domain-migration\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  system: unrstudio1\n  domain_from: central-domain.root.def.gr\n  domain_to: groupnet\n  login_page: https://999.999.999.999/connect/\n---\n# RStudio - Change Domain Procedure\n## Description\nThis document describes the procedure of changing current domain from `central-domain.root.def.gr` to `groupnet` along with their users\n### Server\nPVDCAHR01.groupnet.gr\n### Useful info\nPROD\n- https://999.999.999.999/connect/\n- unrstudio1\n### Prerequisites\nVerify if GROUPNET LDAP SSL certificates are already trusted by the server\n1. Check if the ssl certificates of the groupnet have already been imported\n```bash\n[root@unrstudio1 ~]# openssl s_client -connect PVDCAHR01.groupnet.gr:636\n```\nIf they are not been imported, you should import them using formual at `admin:etc/salt/salt/tls/certificate_authority/import_ca.sls`.\n2. Customer should send an active user that belongs to the new domain for testing \n3. `/etc/hosts` file must be updated to all  BigStreamer servers with the new domain \n4. Perfom an ldap search for the given user:\n```\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -W -b \"dc=groupnet,dc=gr\" -D \"<Bind User sAMAccountName>\" '(sAMAccountName=...)'\n```\n### Backup\n1. Back up `rstudio-connect-central.gcfg`\n```bash\n[root@unrstudio1 ~]# cp -ap /etc/rstudio-connect/rstudio-connect.gcfg /etc/rstudio-connect/rstudio-connect-central.gcfg\n```\n2. Backup database directory `/var/lib/rstudio-connect/db/`\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "RStudio_Change_Domain_Procedure.md - Part 1"
        }
    },
    "298": {
        "page_content": "```bash\n[root@unrstudio1 ~]# cp -ap /etc/rstudio-connect/rstudio-connect.gcfg /etc/rstudio-connect/rstudio-connect-central.gcfg\n```\n2. Backup database directory `/var/lib/rstudio-connect/db/`\n```bash\n[root@unrstudio1 ~]# tar -zcvf var_lib_rstudioconnect_db.tar.gz /var/lib/rstudio-connect/db/\n```\n### Update configuration\nUpdate RStudio Connect config file with new LDAP bind credentials and search base DNs\n1. Stop rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n[root@unrstudio1 ~]# systemctl stop rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n2. Edit `/etc/rstudio-connect/rstudio-connect.gcfg`\nYou can find new configuration at: `[root@unrstudio1 ~]# /etc/rstudio-connect/rstudio-connect-groupnet.gcfg`\nValues that must be changed:\n- ServerAddress\n- UserSearchBaseDN\n- GroupSearchBaseDN\n- PermittedLoginGroup #This value must be set according to the ouput of previous ldap search\n- BindDN\n- BindPassword\n- PublisherRoleMapping #This value must be set according to the ouput of previous ldap search\n- ViewerRoleMapping #This value must be set according to the ouput of previous ldap search\n- AdministratorRoleMapping #This value must be set according to the ouput of previous ldap search\n3. Start rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n[root@unrstudio1 ~]# systemctl start rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n4. Login to https://999.999.999.999/connect/ with the active user.\n### Rstudio Lisence\nRStudio Connect has a limit for the number of active users it can serve. Currently, the license we have, can serve only 40 active users.\nWhat can you do though in case you want to add another user but there are not free licenses? \n**Only after getting customer's confirmation you can delete another user that it is not used**\n### Delete user\nClean up a deactivated or duplicate user from RStudio Connect user base",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "RStudio_Change_Domain_Procedure.md - Part 2"
        }
    },
    "299": {
        "page_content": "**Only after getting customer's confirmation you can delete another user that it is not used**\n### Delete user\nClean up a deactivated or duplicate user from RStudio Connect user base\n1. In order to use `/opt/rstudio-connect/bin/usermanager list --users` command you must first stop RStudio connect\n```bash\n[root@unrstudio1 ~]# systemctl stop rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n2. List existing users\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager list --users\n```\n3. Let's assume that we want to delete `dsimantir` account. Let's find his GUID.\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager list --users | grep -iv dsimantir\n```\nOutput must be something like below:\n| GUID  |  ID | Username   |  First |  Last  |  Email   |   Role |  DN  | UniqueID  |\n| ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ |\n| e633e5b9-cbc3-4fb3-8c3b-19ba4aa617b7  | 16  |  dsimantir  |   |   | dsimantir@uatdef.gr  | publisher   | CN=dsimantir,OU=def_users,DC=uatdef,DC=gr  |  EQGFgRGDt0KZ9sAipdlzhw== |\n4. Delete user\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager delete --users --user-guid e633e5b9-cbc3-4fb3-8c3b-19ba4aa617b7\n```\n5. Verify that user is deleted by re-running step 3 and make sure that there is no output.\n6. Start rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl start rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n7. You can also verify that the user is deleted by login to https://999.999.999.999/connect/ with the active user account > People \n### Transfer projects/context from one user to another in case of duplicate users\nTransfer user ownership and projects from central-domain to GROUPNET in case of duplicated accounts",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "RStudio_Change_Domain_Procedure.md - Part 3"
        }
    },
    "300": {
        "page_content": "### Transfer projects/context from one user to another in case of duplicate users\nTransfer user ownership and projects from central-domain to GROUPNET in case of duplicated accounts\nIn our case, when we changed `central-domain` to `groupnet` we noticed that when users logged in to the `groupnet` domain they were not able to see their projects.\nThat issue occurred due to the fact that name, email and other attributes where different to `central` and `groupnet`. For example:\n- Chrisostomos Charisis, ccharisis@def.gr -> central domain\n- Chrisostomos Charisis, CCHARISIS@abc.GR -> groupnet domain\n> Ndef: Login to https://999.999.999.999/connect/ with the active user account > People > Search for the specific user and check the contents of the duplicate user\nAs a result, the user was considered as different account and a different registration was created.\nSo, how can merge those two accounts? \n1. Stop rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl stop rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n2. Find id of above duplicate users:\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager list --users | grep -iv ccharisis\n```\nLet's assume that the id of ccharisis in central domain is 7 and the id of ccharisis in groupnet is 145\n3. Transfer account from `central-domain` to `groupnet` using following command:\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager transfer -source-user-id 7 -target-user-id 145\n```\n4. Start rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl start rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n5.  Login to https://999.999.999.999/connect/ with the active user account > People > Search for the specific user and check the contents of the duplicate user have been transferred\n6. Delete user that belongs to `central-domain` as described in previous section",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "RStudio_Change_Domain_Procedure.md - Part 4"
        }
    },
    "301": {
        "page_content": "---\ntitle: \"Create a Kerberos Keytab in NYMA\"\ndescription: \"Step-by-step guide to create a Kerberos keytab for a user in the NYMA environment using kadmin.local, transfer it to un2, and set ownership.\"\ntags:\n  - kerberos\n  - keytab\n  - nyma\n  - kadmin\n  - authentication\n  - principal\n  - un2\n  - linux\n  - hadoop\n  - access control\n---\n# How to create a keytab in NYMA\nThis procedure explains how to create a Kerberos keytab file for a user in the NYMA environment and deploy it to the appropriate node with correct permissions.\n## Step 1: Login to the Kerberos server (kerb1) as root\n```bash\nssh kerb1\nsudo -i\n```\n## Step 2: Open the Kerberos admin interface\n```bash\nkadmin.local\n```\n## Step 3: Check if the principal exists\n```bash\nlistprincs <username>@CNE.abc.GR\n```\n## Step 4: Create the principal (if missing)\n```bash\naddprinc <username>CNE.abc.GR\n```\n## Step 5: Generate the keytab file\n```bash\nktadd -k /tmp/<username>.keytab -norandkey <username>@CNE.abc.GR\n```\n## Step 6: Copy the keytab to un2 node\n```bash\nscp -p /tmp/<username>.keytab un2:/tmp\n```\n## Step 7: Move the keytab and set permissions\n```bash\nssh un2\nsudo -i\ncp -p /tmp/<username>.keytab /home/users/skokkoris/\nchown skokkoris. /home/users/skokkoris/<username>.keytab\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "create_keytab.md - Part 1"
        }
    },
    "302": {
        "page_content": "---\ntitle: \"abc - BigStreamer - How to Open a Ticket to DELL\"\ndescription: \"Step-by-step instructions for opening a hardware support ticket with DELL for abc BigStreamer nodes, including gathering the service tag and exporting TSR logs from iDRAC.\"\ntags:\n  - dell support\n  - hardware ticket\n  - tsr logs\n  - idrac\n  - ipmitool\n  - bigstreamer\n  - abc\n  - service tag\n  - vnc\n  - server diagnostics\n  - supportassist\n---\nThis document describes how to open a hardware support ticket to DELL for an abc BigStreamer node, including instructions to retrieve the node's iDRAC IP, collect TSR logs via iDRAC, and deliver them to DELL support.\n# abc - BigStreamer - How to open a ticket to DELL\n## Description\nBelow is a step-by-step description of the process from opening a ticket to collecting TSR logs from iDRAC.\n## Actions Taken\n1. ssh with your personal account on the issue node.\n2. Switch to root and find the iDRAC management IP:\n```bash\nsudo -i\nipmitool lan print | grep -i 'IP Address'\n# If ipmitool is missing:\nyum install ipmitool\n```\n3. Open Firefox on a VNC session and navigate to the iDRAC IP address found in Step 2.\n4. From `Server-->Overview-->Server Information` copy the `Service Tag number`\n5. Call Dell support `2108129800`. They need the `Service Tag number` from step 4\n6. A DELL engineer will create a case and send you all the necessary instructions. If not the link to collect the TSR logs is `https://www.dell.com/support/kbdoc/el-gr/000126803/export-a-supportassist-collection-via-idrac7-and-idrac8`\n7. Inform `abc` before any action on the IDRAC.\n8. Download the TSR `.zip` file locally from the iDRAC interface. If using VNC on a node like `un4`, the downloaded files will be stored under: `/home/cloudera/Downloads/`. The filename format is: `TSR<date>_<service_tag>.zip`.\n9. Send the zip file/files to DELL and wait for their response.\n## Completion\nYou have now completed the process. Await DELL\u2019s response and proceed based on their instructions.",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "how_to_create_dell_ticket.md - Part 1"
        }
    },
    "303": {
        "page_content": "---\ntitle: \"Execute Cube Indicators via Terminal\"\ndescription: \"Instructions for manually executing the Cube Indicators Spark job from terminal on un1.bigdata.abc.gr, including how to pull the latest script, modify execution date, and run the submit script.\"\ntags:\n  - cube indicators\n  - spark job\n  - pyspark\n  - hdfs\n  - brond\n  - manual execution\n  - terminal\n  - big data\n  - intra\n---\n# Execute Cube Indicators via Terminal\nThis guide explains how to manually run the Cube Indicators Spark job for missing dates from the terminal. It includes pulling the latest script from HDFS, updating the execution date, and submitting the job.\n1. SSH into `un1.bigdata.abc.gr` and switch to the `intra` user:\n```bash\nssh un1.bigdata.abc.gr\nsudo -i -u intra\n```\n2. Navigate to the working directory:\n```bash\ncd projects/cube_ind\n```\n3. Remove the old PySpark script:\n```bash\nrm Indicators_Spark_Job.py\n```\n4. Authenticate with Kerberos and fetch the updated script from HDFS:\n```bash\nkinit -kt /home/intra/intra.keytab intra\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\n```\n6. Edit submit script to change execution date. The execution date should be 2 days after the missing data date.\nFor example, to load data for 2021-01-01, set the execution date to 2021-01-03.\n```bash\nvim run_cube.sh\n```\nUpdate the relevant line:\n```bash\n...verhead=4096 Indicators_Spark_Job.py <date> brond.dsl_s...\n```\n7. Run the Spark job:\n```bash\n./run_cube.sh\n```\n---\ntags:\n  - cube indicators\n  - pyspark\n  - spark job\n  - brond\n  - manual data load\n  - hdfs\n  - intra\n---",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "execute_indicators_terminal.md - Part 1"
        }
    },
    "304": {
        "page_content": "---\ntitle: \"Streamsets - Java Heap Space Configuration and Monitoring\"\ndescription: \"Steps to increase Java Heap Memory for Streamsets via Cloudera Manager, clean up redundant configs, restart services, and verify memory settings using process inspection tools (ps, jmap, jconsole).\"\ntags:\n  - streamsets\n  - java heap\n  - memory configuration\n  - cloudera manager\n  - jmap\n  - jconsole\n  - troubleshooting\n  - performance tuning\n  - gc logs\n  - bigstreamer\n  - heap dump\n  - xmx\n  - xms\n---\n# Streamsets - Java Heap Space\nStreamsets Java Heap Memory was increased due to [this](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/102#ndef_95081) issue.\nThis guide documents the resolution of a Java Heap Space issue on the Streamsets Data Collector. It includes steps to increase heap size using Cloudera Manager, remove deprecated safety valve overrides, verify JVM options with `ps`, and inspect memory usage via `jmap` and `jconsole`. Applicable for performance tuning and troubleshooting OOM errors on Streamsets pipelines.\n## Actions Taken\nThis procedure outlines how to address Streamsets memory issues by increasing the Java heap size and verifying runtime memory settings.\n1. Configure Java Options from CLoudera Manager\n   ```bash\n   cluster -> Streamsets -> Configuration -> Java options: `-Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow`\n   ```\n2. Remove old configuration\n   ```bash\n   cluster-> Streamsets -> Configuration -> Data Collector Advanced Configuration Snippet (Safety Valve) for sdc-env.sh\n   ```\n   ```bash\n   #Remove the following line, if exists\n   export SDC_JAVA_OPTS=\"-Xmx16384m -Xms16384m -server -XX:-OmitStackTraceInFastThrow ${SDC_JAVA_OPTS}\"\n   ```\n3. Restart Streamsets\n   ```bash\n   cluster -> Streamsets -> Restart\n   ```\n4. Check Streamsets Process Options\n   ```bash\n   [root@un2 ~]# ps -ef | grep -i streamsets | grep -i xmx",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "configure_streamsets_java_heap_space.md - Part 1"
        }
    },
    "305": {
        "page_content": "sdc      24898 24873 45 12:45 ?        00:40:11 /usr/java/default/bin/java -classpath /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs/main/streamsets-datacollector-bootstrap-3.21.0.jar:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/root-lib/* -Djava.security.manager -Djava.security.policy=file:///var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR/sdc-security.policy -Xmx1024m -Xms1024m -server -XX:-OmitStackTraceInFastThrow -Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow -Dsdc.dist.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0 -Dsdc.resources.dir=/var/lib/sdc/resources -Dsdc.hostname=un2.bigdata.abc.gr -Dsdc.conf.dir=/var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR -Dsdc.data.dir=/shared/sdc/data -Dsdc.log.dir=/shared/sdc/log/ -javaagent:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs/main/streamsets-datacollector-bootstrap-3.21.0.jar -Dsdc.libraries.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs -Dsdc.librariesExtras.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs-extras -Dsdc.rootLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/root-lib -Dsdc.bootstrapLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs -Dsdc.apiLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/api-lib -Dsdc.asterClientLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/aster-client-lib -Dsdc.containerLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/container-lib -Dsdc.libsCommon.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libs-common-lib -Dsdc.userLibs.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/user-libs -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -Djdk.nio.maxCachedBufferSize=262144 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/shared/sdc/log//gc",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "configure_streamsets_java_heap_space.md - Part 2"
        }
    },
    "306": {
        "page_content": ".21.0/user-libs -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -Djdk.nio.maxCachedBufferSize=262144 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/shared/sdc/log//gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/shared/sdc/log//sdc_heapdump_1675334705.hprof -XX:ErrorFile=/shared/sdc/log//hs_err_1675334705.log com.streamsets.pipeline.BootstrapMain -mainClass com.streamsets.datacollector.main.DataCollectorMain -apiClasspath /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/api-lib/*.jar -containerClasspath /var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/container-lib/*.jar -streamsetsLibrariesDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs -userLibrariesDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/user-libs -configDir /var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR -libsCommonLibDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libs-common-lib -streamsetsLibrariesExtraDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs-extras",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "configure_streamsets_java_heap_space.md - Part 3"
        }
    },
    "307": {
        "page_content": "```\n   > Tip: When defining Java configuration options, avoid defining duplicate options. If you do define duplicates, the last option passed to the JVM usually takes precedence.\n5. Check Max Java Heap Space for Streamsets via jconsole or jmap\n### jconsole\n   > Needs additional Java Options: -Dcom.sun.management.jmxremdef -Dcom.sun.management.jmxremdef.port=3333 -Dcom.sun.management.jmxremdef.local.only=false -Dcom.sun.management.jmxremdef.authenticate=false -Dcom.sun.management.jmxremdef.ssl=false\"\n   ```bash\n   [root@un2 ~]# /usr/java/latest/bin/jconsole\n   ```\n   Select `com.streamsets.pipeline.BootstrapMain`,  `Connect` and check the metrics.\n### jmap\n   ```bash\n   jmap -heap <pid>\n   #output example\n   [root@un2 ~]# jmap -heap 24898\n   Attaching to process ID 24898, please wait...\n   Debugger attached successfully.\n   Server compiler detected.\n   JVM version is 25.181-b13\n   using parallel threads in the new generation.\n   using thread-local object allocation.\n   Concurrent Mark-Sweep GC\n   Heap Configuration:\n      MinHeapFreeRatio         = 40\n      MaxHeapFreeRatio         = 70\n      MaxHeapSize              = 34359738368 (32768.0MB)\n      NewSize                  = 2442723328 (2329.5625MB)\n      MaxNewSize               = 2442723328 (2329.5625MB)\n      OldSize                  = 31917015040 (30438.4375MB)\n      NewRatio                 = 2\n      SurvivorRatio            = 8\n      MetaspaceSize            = 21807104 (20.796875MB)\n      CompressedClassSpaceSize = 1073741824 (1024.0MB)\n      MaxMetaspaceSize         = 17592186044415 MB\n      G1HeapRegionSize         = 0 (0.0MB)\n   Heap Usage:\n   New Generation (Eden + 1 Survivor Space):\n      capacity = 2198470656 (2096.625MB)\n      used     = 1493838840 (1424.6357345581055MB)\n      free     = 704631816 (671.9892654418945MB)\n      67.94900063473942% used\n   Eden Space:\n      capacity = 1954217984 (1863.6875MB)\n      used     = 1433160568 (1366.768424987793MB)",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "configure_streamsets_java_heap_space.md - Part 4"
        }
    },
    "308": {
        "page_content": "free     = 704631816 (671.9892654418945MB)\n      67.94900063473942% used\n   Eden Space:\n      capacity = 1954217984 (1863.6875MB)\n      used     = 1433160568 (1366.768424987793MB)\n      free     = 521057416 (496.91907501220703MB)\n      73.33678124620104% used\n   From Space:\n      capacity = 244252672 (232.9375MB)\n      used     = 60678272 (57.8673095703125MB)\n      free     = 183574400 (175.0701904296875MB)\n      24.84241891937215% used\n   To Space:\n      capacity = 244252672 (232.9375MB)\n      used     = 0 (0.0MB)\n      free     = 244252672 (232.9375MB)\n      0.0% used\n   concurrent mark-sweep generation:\n      capacity = 31917015040 (30438.4375MB)\n      used     = 12194092928 (11629.193237304688MB)\n      free     = 19722922112 (18809.244262695312MB)\n      38.20561826573617% used\n   57229 interned Strings occupying 8110512 bytes.\n   ```\nReference: https://docs.streamsets.com/platform-datacollector/latest/datacollector/UserGuide/Configuration/DCEnvironmentConfig.html",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "configure_streamsets_java_heap_space.md - Part 5"
        }
    },
    "309": {
        "page_content": "---\ntitle: \"Permanent Anonymization & Retention UI Access Issue and WAR Replacement\"\ndescription: \"Troubleshooting and resolution steps for the abc BigStreamer Permanent Anonymization & Retention UI access issue due to CDN resource blocking over VPN. Involves replacing the Wildfly WAR deployment.\"\ntags:\n  - abc\n  - bigstreamer\n  - wildfly\n  - anonymization\n  - retention\n  - ui issue\n  - vpn restriction\n  - deployment\n  - war replacement\n  - haproxy\n  - trustcenter\n---\n# abc - Permanent Anonymization & Retention UI issue\nThis guide documents the root cause and resolution steps for the abc Permanent Anonymization & Retention UI failing to load over VPN due to external CDN dependency, and the WAR file replacement procedure on Wildfly.\n## Description\n```\nRegarding the problem with the Permanent Anonymization & Retention UI (https://cne.def.gr:8643/customapps)\nLet me remind you that access to this particular UI is not possible via VPN.\nThe reason is that it is trying to load a library from the internet (cdn.jsdelivr.net).\nA new war has been added which should replace the old one in wildfly.\n```\n## Actions Taken\n## Actions Taken",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "Change_war_for_Anonymization&Retention_UI.md - Part 1"
        }
    },
    "310": {
        "page_content": "1. SSH into `unc2` using your personal account.\n2. Switch to root and check HAProxy backend configuration:\n   sudo -i\n   less /etc/haproxy/haproxy.cfg\n   (Search for the `tru-backend` section.)\n3. Connect to `unekl1` and back up the existing WAR file:\n   ssh unekl1\n   cp -rp /opt/trustcenter/wf_cdef_trc/standalone/deployments/wftrust-landing-web.war /opt/trustcenter/wf_cdef_trc/standalone/deployments/wftrust-landing-web.war.bkp\n4. Set correct permissions on the new WAR file:\n   chown trustuser:trustcenter <new_war_file>\n   chmod 644 <new_war_file>\n5. Move the new WAR file to the deployments directory:\n   mv <new_war_file> /opt/trustcenter/wf_cdef_trc/standalone/deployments/\n6. No Wildfly restart is required. A .deployed marker file (wftrust-landing-web.war.deployed) will be created automatically.\n7. Verify Wildfly is running:\n   su - trustuser\n   bash\n   trust-status\n8. Repeat steps 3\u20137 on `unekl2`.\n9. Clear your browser cache and access the UI at:\n   https://cne.def.gr:8643/customapps\n## Affected Systems\nabc Bigstreamer",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "Change_war_for_Anonymization&Retention_UI.md - Part 2"
        }
    },
    "311": {
        "page_content": "---\ntitle: \"Manage IDM Replication\"\ndescription: \"Step-by-step guide for monitoring, forcing, and troubleshooting FreeIPA (IDM) LDAP replication across idm1 and idm2 nodes, including preauthentication issues and resolution procedures related to SPNs and Kerberos.\"\ntags:\n  - idm\n  - ldap\n  - freeipa\n  - kerberos\n  - spn\n  - preauthentication\n  - kdc\n  - replication\n  - ipa-replica-manage\n  - ipa\n  - gssapi\n  - ipa config\n  - krbTicketFlags\n  - kadmin\n  - hue\n  - authentication\n  - troubleshooting\n  - directory\n  - bigstreamer\n  - abc\n---\nThis document outlines the setup and operational procedures for managing FreeIPA (IDM) replication between two nodes. It also includes detailed troubleshooting steps for resolving SPN-related Kerberos preauthentication errors and replication failures across KDCs.\n# Manage IDM Replication\n## Setup\nIDM (FreeIPA) has been installed on two nodes for High Availability. Replication between the two nodes is performed on the LDAP service using GSSAPI authentication (Kerberos) using `ldap/_HOST` Service Principal Names (SPNs). It is a \"push\" replication, so each change is propagated to the other instance from the instance that it was performed.\nEach KDC uses the LDAP on the same host as backend, so since both LDAP Servers are replicated between the two instances (active-active) KDCs are up to date with between the two hosts.\n```mermaid\n  graph LR\n  A[idm1.bigdata.abc.gr<br>SPN: ldap/idm1.bigdata.abc.gr]\n  B[idm2.bigdata.abc.gr<br>SPN: ldap/idm2.bigdata.abc.gr]\n  A-->B\n  B-->A\n```\n## Procedure\n### Check replication\n``` bash\n# Assuming you are on idm1\nkinit <admin user>\nipa-replica-manage list -v # List replication targets of idm1\nipa-replica-manage list -v idm2.bigdata.abc.gr # This will connect to idm2 and show if it managed to push it's changes to the local instance (idm1)\n```\n```log\nidm1.bigdata.abc.gr: replica\n  last update status: Error (0) Replica acquired successfully: Incremental update succeeded",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "manage_idm_replication.md - Part 1"
        }
    },
    "312": {
        "page_content": "```\n```log\nidm1.bigdata.abc.gr: replica\n  last update status: Error (0) Replica acquired successfully: Incremental update succeeded\n  last update ended: 2023-12-21 12:41:17+00:00\n```\n### Force replication\n``` bash\n# Assuming you are on idm1\nkinit <admin user>\nipa-replica-manage force-sync  --from idm2.bigdata.abc.gr # This will connect to idm2 and schedule an immediate sync (push) to the local instance (idm1)\n```\n```log\nipa: INFO: Setting agreement cn=meToidm1.bigdata.abc.gr,cn=replica,cn=dc\\=bigdata\\,dc\\=abc\\,dc\\=gr,cn=mapping tree,cn=config schedule to 2358-2359 0 to force synch\nipa: INFO: Deleting schedule 2358-2359 0 from agreement cn=meToidm1.bigdata.abc.gr,cn=replica,cn=dc\\=bigdata\\,dc\\=abc\\,dc\\=gr,cn=mapping tree,cn=config\nipa: INFO: Replication Update in progress: FALSE: status: Error (0) Replica acquired successfully: Incremental update succeeded: start: 0: end: 0\n```\n## Troubleshooting\n:warning: All problems in this section have been solved and all actions mentioned below have been reverted as part of obss/oss/sysadmin-group/abc/BigStreamer/bigstreamer#53 \n### A brief history of preauthentication\nPreauthentication is authentication of the client to the KDC when requesting a Kerberos ticket. This means that instead of issuing an `AS-REQ` the client issues a `TGS-REQ` which is logged in KDCs' logs.\nWe will not include specific preauthentication protocols in this presentation. Feel free to research them.\n### Troubleshooting Preauthentication Issues\n_At the time of writing we cannot replicate the issue to get accurate log entries, but we can give you is accurate commands for the investigation_ :smile:\nWhen we failed over the Cloudera Services from `CNE.abc.GR` to `BIGDATA.abc.GR` we faced a problem with users coming from `CNE.abc.GR` not being able to login to Cloudera Services.\nTo resolve the issue we issued the following command, that disables preauthentication for SPNs:\n```bash\n# Assuming you are on idm1 and have valid admin Kerberos ticket",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "manage_idm_replication.md - Part 2"
        }
    },
    "313": {
        "page_content": "To resolve the issue we issued the following command, that disables preauthentication for SPNs:\n```bash\n# Assuming you are on idm1 and have valid admin Kerberos ticket\nipa config-mod --ipaconfigstring=\"KDC:Disable Default Preauth for SPNs\"\n```\nThis resolved our issue, but created two new problems:\n1. SPNs do not require preauthentication, but `krbtgt/BIGDATA.abc.GR` requires preauthentication. This means that if you `kinit` with an SPN `kinit -R` will refuse to run with the dreaded `NO_PREAUTH`. This is problematic for the Hue Kerberos Renewer. As a workaround we are renewing the Kerberos ticket cache of Hue via `cron`.\n2. Replication from `idm2.bigdata.abc.gr` to `idm1.bigdata.abc.gr`. The error is the error is also `NO_PREAUTH`. This means that `ldap/idm2.bigdata.abc.gr` was rejected by `ldap/idm1.bigdata.abc.gr` due to the lack of preauthentication. Let's inspect the service principals:\n```bash\nipa service-find ldap/idm1.bigdata.abc.gr --all --raw\n```\n```log\n    -----------------\n    1 service matched\n    -----------------\n      dn: krbprincipalname=ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      krbcanonicalname: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\n      krbprincipalname: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\n      ...\n      managedby: fqdn=idm1.bigdata.abc.gr,cn=computers,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      ipaKrbPrincipalAlias: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\n      ...\n      krbPwdPolicyReference: cn=Default Service Password Policy,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      krbTicketFlags: 128\n      memberof: cn=replication managers,cn=sysaccounts,cn=etc,dc=bigdata,dc=abc,dc=gr\n      objectClass: ipaobject\n      objectClass: top\n      objectClass: ipaservice\n      objectClass: pkiuser\n      objectClass: krbprincipal\n      objectClass: krbprincipalaux\n      objectClass: krbTicketPolicyAux\n      objectClass: ipakrbprincipal\n    ----------------------------",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "manage_idm_replication.md - Part 3"
        }
    },
    "314": {
        "page_content": "objectClass: pkiuser\n      objectClass: krbprincipal\n      objectClass: krbprincipalaux\n      objectClass: krbTicketPolicyAux\n      objectClass: ipakrbprincipal\n    ----------------------------\n    Number of entries returned 1\n    ----------------------------\n```\n```bash\nipa service-find ldap/idm2.bigdata.abc.gr --all --raw\n```\n```log\n    -----------------\n    1 service matched\n    -----------------\n      dn: krbprincipalname=ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      krbcanonicalname: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\n      krbprincipalname: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\n      ...\n      managedby: fqdn=idm2.bigdata.abc.gr,cn=computers,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      ipaKrbPrincipalAlias: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\n      ...\n      krbPwdPolicyReference: cn=Default Service Password Policy,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      krbTicketFlags: 0\n      memberof: cn=replication managers,cn=sysaccounts,cn=etc,dc=bigdata,dc=abc,dc=gr\n      objectClass: ipaobject\n      objectClass: top\n      objectClass: ipaservice\n      objectClass: pkiuser\n      objectClass: krbprincipal\n      objectClass: krbprincipalaux\n      objectClass: krbTicketPolicyAux\n      objectClass: ipakrbprincipal\n    ----------------------------\n    Number of entries returned 1\n    ----------------------------\n```\n`krbTicketFlags` seems to be different. Checking the same principals from `kadmin` interface:\n```bash\nkadmin.local -q \"get_principal ldap/idm1.bigdata.abc.gr\"\n```\n```log\n    Principal: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\n    Expiration date: [never]\n    ...\n    Attributes: REQUIRES_PRE_AUTH\n    Policy: [none]\n```\n```bash\nkadmin.local -q \"get_principal ldap/idm2.bigdata.abc.gr\"\n```\n```log\n    Principal: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\n    Expiration date: [never]\n    ...\n    Attributes:\n    Policy: [none]\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "manage_idm_replication.md - Part 4"
        }
    },
    "315": {
        "page_content": "kadmin.local -q \"get_principal ldap/idm2.bigdata.abc.gr\"\n```\n```log\n    Principal: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\n    Expiration date: [never]\n    ...\n    Attributes:\n    Policy: [none]\n```\nSeems that preauthentication is enforced via `krbTicketFlags`. We can remove it by issuing the following command:\n```bash\nkadmin.local -q \"modify_principal -requires_preauth ldap/idm1.bigdata.abc.gr\" # This is also replicated to idm2, since pushing from idm1 to idm2 worked\n```\nNow replication works.\n### Re-enable SPN Preauthentication (Post-Migration Cleanup)\nWhen `CNE.abc.GR` is removed we will re-enable preauthentication with the following command to resolve the issue with Hue:\n```bash\nkadmin.local -q \"modify_principal +requires_preauth ldap/idm1.bigdata.abc.gr\"\nipa config-mod --ipaconfigstring=\"\"\nipactl restart\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "manage_idm_replication.md - Part 5"
        }
    },
    "316": {
        "page_content": "---\ntitle: \"Cube Indicators Pipeline\"\ndescription: \"Overview of the Brond Cube Indicators data pipeline, including Oozie coordinators, dependency tables, and the scripts responsible for generating input data.\"\ntags:\n  - brond\n  - cube indicators\n  - oozie\n  - hadoop\n  - coordinator\n  - radius\n  - retrains\n  - fixed customers\n  - data pipeline\n  - xdsl\n---\n# Cube Indicators Pipeline\nThis document summarizes the data flow and dependencies of the `brond.cube_indicators` pipeline. It includes the Oozie coordinators involved, input tables, and the scripts or jobs that populate each dependency. The main output is the `brond.cube_indicators` table, populated for `par_date = today - 2 days`.\n* Coordinator `1011_Fixed_brond_customers`  populates table `brond.fixed_brond_customers_daily`\n* Coordinator `Coord_post_BROND_FIXED_CUSTOMERS` populates tables `brond.fixed_brond_customers_daily_unq` & `brond.fixed_customers_brond_latest`\n* Coordinator `Coord_Cube_Spark_Indicators` for `par_date=date -2 days` populates table `brond.cube_indicators`  and its dependencies are:\n  * `brond.fixed_radio_matches_unq_inp` populated by script `un2:/shared/abc/brond/bin/101_fixed_radius.sh`\n  * `brond.fixed_brond_customers_daily_unq`\n  * `radius.radacct_hist` populated by script `un2:/shared/abc/radius/DataParser/scripts/radius.pl`\n  * `brond.brond_retrains_hist` populated by script `un2:/shared/abc/brond/DataParser/scripts/brond_retrains.pl`\n  * `brond.dsl_stats_week_xdsl_hist` populated by Coordinator `coord_brond_load_dsl_daily_stats`\n---\ntags:\n  - cube_indicators\n  - brond\n  - coordinator\n  - radius\n  - retrains\n  - fixed_customers\n  - xdsl\n  - spark\n  - hadoop\n  - data_dependencies\n---",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "cube_indicators_pipeline.md - Part 1"
        }
    },
    "317": {
        "page_content": "---\ntitle: How to Change OpenLDAP Manager Password\ndescription: Step-by-step instructions for changing the OpenLDAP Manager password on `kerb1` and ensuring replication with `kerb2` is functional.\nowner: kpar\nsystem: OpenLDAP\ncluster: BigStreamer\nservices:\n  - LDAP\nnodes:\n  - kerb1.bigdata.abc.gr\n  - kerb2.bigdata.abc.gr\ntags:\n  - ldap\n  - password\n  - openldap\n  - kerb\n  - manager\n  - authentication\n  - security\nstatus: verified\nlast_updated: 2024-05-01\nrelated_docs:\n  - KnowledgeBase/prodsyspasswd.kdbx\n---\nThis procedure explains how to securely change the OpenLDAP Manager password on kerb1, update both the config and data databases via LDIF files, and verify replication and authentication using both CLI and the phpLDAPadmin web UI.\n# OpenLDAP Manager Password Change\nFor every ldasearch the password of `Manager` is [here](KnowledgeBase/prodsyspasswd.kdbx)\n## Step 1: Login to LDAP Node\nLogin into the primary LDAP node (`kerb1`) with root access.\n```bash\nssh kerb1\nsudo -i\n```\n## Step 2: Generate New SSHA Password Hash\nUse `slappasswd` to create a new SSHA-encoded password for the LDAP Manager account.\n```bash\nslappasswd -h {SSHA}\n```\n## Step 3: Store the output \nThe output will be start with something like `{SSHA}xxxxxxx` \n## Step 4: Create LDIF Files to Apply Password Change\nCreate two LDIF files: one for the config database and one for the BDB (data) database.\n### 4a. LDIF for config database\n```bash\nvi changepwconfig.ldif\ndn: olcDatabase={0}config,cn=config\nchangetype: modify\nreplace: olcRootPW\nolcRootPW: paste the output from step `3`\n```\n### 4b. LDIF for manager database\n```bash\nvi changepwmanager.ldif\ndn: olcDatabase={2}bdb,cn=config\nchangetype: modify\nreplace: olcRootPW\nolcRootPW: paste the output from step `3`\n```\n## Step 5: Backup Existing Configuration\nUse `slapcat` to export the current config and data for recovery purposes.\n```bash\nslapcat -n 0 -l config.ldif\nslapcat -n 2 -l data.ldif\n```\n## Step 6: Apply Password Changes",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "openldap_change_manager_password.md - Part 1"
        }
    },
    "318": {
        "page_content": "Use `slapcat` to export the current config and data for recovery purposes.\n```bash\nslapcat -n 0 -l config.ldif\nslapcat -n 2 -l data.ldif\n```\n## Step 6: Apply Password Changes\nUse `ldapmodify` with the generated LDIF files to apply the password change.\n```bash\nldapmodify -H ldapi:// -Y EXTERNAL -f changepwmanager.ldif\nldapmodify -H ldapi:// -Y EXTERNAL -f changepwconfig.ldif\n```\n## Step 7: Validate the New Password\nTest that the new Manager password works both via CLI and web UI on `kerb1` and `kerb2`.\n### 7a. Command-Line Verification\nFor `kerb1`. Where `uid` add a ldap user. e.g your ldap username:\n```bash\nldapsearch -H ldaps://kerb1.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=xxxx'\n```\nFor `kerb2`. Where `uid` add a ldap user. e.g your ldap username::\n```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=xxxx'\n```\n### 7b. Web UI Verification\nLogin into `admin` node as `root`:\nOpen firefox\n```bash\nfirefox\n```\nphpldapadmin link: https://kerb1.bigdata.abc.gr/phpldapadmin/\nTry to connect with the new `Manager` password",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "openldap_change_manager_password.md - Part 2"
        }
    },
    "319": {
        "page_content": "---\ntitle: Kubernetes Certificate Renewal Procedure\ndescription: Yearly renewal process for expiring Kubernetes certificates on kubemaster1, kubemaster2, and kubemaster3 including backup, kubeadm certs renewal, and container restarts.\ntags: [kubernetes, certificates, renewal, kubeadm, kubemaster, static pods, downtime, cluster-admin, tls]\ncategory: infrastructure\nproduct: BigStreamer\nplatform: kubernetes\nconfidentiality: internal\n---\n# Scope\nOnce a year the kubernetes certificates are expiring. In order to prevent that, we have a procedure below in which we are describing the steps that will help you to renew them.\n## Setup\n**Masters**: `kubemaster1`, `kubemaster2`, `kubemaster3`\n**Important ndef:** This procedure requires downtime.\n## Procedure\nLogin to kubemaster1.bigdata.abc.gr with your personal account and become root:\n- Check the certificates expiration date:\n```bash\nsudo su -write the metadata block, tags at the end, tell me what do to clean it and tell me what descriptions to write in order for the document to be properly retrieved in my RAG chatbot\nkubeadm certs check-expiration\n```\n- Keep a backup of kubernetes configuration to tmp \n```bash\ncp -ar /etc/kubernetes /tmp/\n```\n- Keep a backup of incelligent service account\n```bash\ncp -ar /home/users/incelligent/.kube/config /tmp/bckup_renew_certificates/\n```\n- Renew the certificates\n```bash\nkubeadm  certs renew all\nkubeadm certs check-expiration\n```\n- Run the following\n```bash\ncp -p /root/.kube/config /root/.kube/config_old\ncp /etc/kubernetes/admin.conf  /root/.kube/config\n```\n- From the newly create`/etc/admin/conf` make sure to copy the `client-certificate-data` and `client-key-data` data content. Edit the file /home/users/incelligent/.kube/config and replace the values of client-certificate-data and client-key-data with the ones copied from /etc/kubernetes/admin.conf.\nin order to add the new certificates.\n- Check again the certificates expiration date\n```bash\nkubeadm certs check-expiration\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "ranai_kubernetes_renew_certificates.md - Part 1"
        }
    },
    "320": {
        "page_content": "in order to add the new certificates.\n- Check again the certificates expiration date\n```bash\nkubeadm certs check-expiration\n```\n- Check the kubectl functionality\n```bash\nkubectl get pods\n```\n- When the certificates of controller, apiserver and scheduler are renewed you must also stop containers of those three:\n```bash\nctrctl ps\nCONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES\n1350c48cbfb5        b3c57ca578fb           \"kube-controller-man\u2026\"   11 minutes ago      Up 11 minutes                           k8s_kube-controller-manager_kube-controller-manager-cti-cx1_kube-system_9eb854fb973ddd6df55fb792a2fbf743_9\n1bd22e95ef01        5a84bb672db8           \"kube-scheduler --au\u2026\"   11 minutes ago      Up 11 minutes                           k8s_kube-scheduler_kube-scheduler-cti-cx1_kube-system_649aa160f1bd0840b2bb0f70b6493f99_9\ncf43799ae77d0       b6e18ffb844e6          \"kube-apiserver --au\u2026\"   11 minutes ago      Up 11 minutes                 \n```\nStop containers IDs:\n```bash\nctrctl stop 1350c48cbfb5 1bd22e95ef01 cf3cb7655b99d\n```\n> Ndef_1: Keep in mind that when the containers will be stopped, there will be downtime\n> Ndef_2: Keep in mind that those pods are static so you don't need to kill them. They will be automatically restarted\n- Also delete core-dns pod:\n```bash\nkubectl get pod -n kube-system -l k8s-app=kube-dns\nNAME                      READY   STATUS    RESTARTS      AGE\ncoredns-64897985d-7dzkl   1/1     Running   3 (84d ago)   644d\ncoredns-64897985d-rw5kc   1/1     Running   0             83d\n```\n```bash\nkubectl delete pod coredns-64897985d-7dzkl  coredns-64897985d-rw5kc  -n kube-system\n```\n- Repeat the same procedure for `kubemaster2` and `kubemaster3`\n> Note: The user `incelligent` exists only on kubemaster1. You do not need to update or copy the user config on kubemaster2 and kubemaster3.",
        "metadata": {
            "category": "procedures",
            "client": "Client_abc",
            "name": "ranai_kubernetes_renew_certificates.md - Part 2"
        }
    },
    "321": {
        "page_content": "---\ntitle: Enable Spark and YARN ACLs for Log Access\ndescription: Step-by-step guide to configure ACLs in Spark and YARN for allowing specific groups access to Spark logs and MapReduce job logs in Cloudera Manager.\ntags:\n  - spark\n  - yarn\n  - acl\n  - logs\n  - cloudera\n  - permissions\n  - jobhistory\n  - spark-ui\n  - sysadmin\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  services:\n    - Spark\n    - YARN\n  cloudera_components:\n    - Spark History Server\n    - JobHistory Server\n  acl_groups:\n    - WBDADMIN\n    - WBDOPDEV\n    - WBDOPPRO\n    - WBDOPQA\n---\n# Enable ACLs in Spark and YARN for Log Access\nThis procedure describes how to enable ACLs (Access Control Lists) in YARN and Spark using Cloudera Manager. This configuration is necessary to allow specific user groups to access Spark logs and MapReduce job logs.\n## Step 1: Configure YARN ACLs\n### a. Edit ACL Settings for Job Viewing\n1. Go to **Cloudera Manager > YARN > Configuration**.\n2. Search for **\"ACL for viewing a job\"**.\n3. Add the required groups that should have access to view MapReduce job logs.  \nExample value:\nhue WBDADMIN,WBDOPDEV,WBDOPPRO,WBDOPQA\n> **Be careful with the syntax.** Click the question mark icon in Cloudera Manager for exact formatting rules.\n### b. Enable Default Group ACLs for JobHistory Server\n1. Still under YARN > Configuration, search for:\n- **Enable Job ACLs**\n- **JobHistory Server Default Group**\n2. Enable the option and ensure the appropriate groups are assigned if needed.\n## Step 2: Configure Spark ACLs\n1. Go to **Cloudera Manager > Spark > Configuration**.\n2. Search for **Spark Client Advanced Configuration Snippet (Safety Valve) for spark-conf/spark-defaults.conf**.\n3. Add the following lines to enable ACLs and define group access:\n```properties\nspark.acls.enable=true\nspark.admin.acls.groups=WBDADMIN\nspark.history.ui.admin.acls.groups=WBDADMIN\nspark.ui.view.acls.groups=WBDOPDEV,WBDOPPRO,WBDOPQA\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "Yarn_and_Spark_ACLs_Enablement.md - Part 1"
        }
    },
    "322": {
        "page_content": "```properties\nspark.acls.enable=true\nspark.admin.acls.groups=WBDADMIN\nspark.history.ui.admin.acls.groups=WBDADMIN\nspark.ui.view.acls.groups=WBDOPDEV,WBDOPPRO,WBDOPQA\n```\nThese settings control who can view Spark UI logs and access Spark History Server in the cluster.",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "Yarn_and_Spark_ACLs_Enablement.md - Part 2"
        }
    },
    "323": {
        "page_content": "---\ntitle: Oracle Linux 7.9 Edge Node OS Upgrade Procedure\ndescription: Procedure for performing minor version updates of Oracle Linux 7.9 on BigStreamer PR and DR edge nodes using Nexus-sourced YUM repositories, including rollback and repository configuration.\ntags:\n  - os-upgrade\n  - oracle-linux\n  - yum\n  - nexus\n  - edge-nodes\n  - rollback\n  - security\n  - bigstreamer\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  os_version: Oracle Linux 7.9\n  nodes:\n    - pr1edge01\n    - pr1edge02\n    - dr1edge01\n    - dr1edge02\n  repositories:\n    - el7_uek_latest\n    - uek_release_4_packages\n    - ol7_9_latest\n    - ol7_9_epel\n  nexus_url: http://999.999.999.999:8081\n  internal_links:\n    - https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx\n---\n# OS Upgrade\nThis procedure details how to upgrade Oracle Linux 7.9 on PR/DR edge nodes using Nexus YUM repositories. It includes package preparation, clean upgrade commands, repository setup, and rollback steps, along with a reference to cluster resource switchover guidelines.\nAll procedures pertain to PR and DR edge nodes:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Minor OS Version Update on Edge Nodes\nOS packages are sourced from the mno Nexus Repository, which in itself is a yum proxy\nto the official oracle repositories for Oracle Linux 7.9. As such updating them requires only putting\nan edge node on standby and updating through **YUM**:\n```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo -i\n```\nFollow the procedures described in the **Switchover of Cluster Resources** chapter\nof the **Security Vulnerabilities** MOP [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx).\n```bash\n# yum clean all\n# yum check-update\n```\nAfter reviewing the packages that will be updated continue with the update and after it is",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "os_upgrade.md - Part 1"
        }
    },
    "324": {
        "page_content": "```bash\n# yum clean all\n# yum check-update\n```\nAfter reviewing the packages that will be updated continue with the update and after it is\ncomplete unstandby the node:\n```bash\n# yum update\n# systemctl reboot\n# cat /etc/oracle-release\n```\n## Rollback to Previous Packages\nLogin to each edge node and downgrade using **YUM**:\n```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo -i\n# yum clean all\n# yum downgrade\n# reboot\n# cat /etc/oracle-release\n```\n## Configure Nexus YUM Repositories\nMake sure that OS packages are sourced from the already setup Nexus repository.\nLogin to each edge node and edit/create the following repositories accordingly:\n```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo \u2013i\n# cd /etc/yum.repos.d\n# vi el7_uek_latest.repo\n[el7_uek_latest]\nname = el7_uek_latest\nbaseurl = http://999.999.999.999:8081/repository/el7_uek_latest/\nenabled = 1\ngpgcheck = 0\nexclude=postgresql*\n# vi uek_release_4_packages.repo\n[uek_release_4_packages]\nname = uek_release_4_packages\nbaseurl = http://999.999.999.999:8081/repository/uek_release_4_packages/\nenabled = 1\ngpgcheck = 0\nexclude=postgresql*\n# vi ol7_9_latest.repo\n[ol7_9_latest]\nname = ol7_9_latest\nbaseurl = http://999.999.999.999:8081/repository/latest_packages/\nenabled = 1\ngpgcheck = 0\nexclude=postgresql*\n# vi ol7_9_epel.repo\n[ol7_9_epel]\nname = ol7_9_epel\nbaseurl = http://999.999.999.999:8081/repository/latest_epel_packages/\nenabled = 1\ngpgcheck = 0\nexclude=postgresql*\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "os_upgrade.md - Part 2"
        }
    },
    "325": {
        "page_content": "---\ntitle: Fix MySQL Replication on Slave Node\ndescription: Step-by-step procedure for identifying and repairing broken MySQL replication on slave nodes in the PR and DR environments, including status checks, backup restoration, and restarting replication.\ntags:\n  - mysql\n  - replication\n  - slave-node\n  - backup\n  - database-recovery\n  - sysadmin\n  - cloudera\n  - bigstreamer\n  - dr\n  - pr\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  mysql_clusters:\n    - PR\n    - DR\n  hosts:\n    - pr1node02.mno.gr\n    - pr1node03.mno.gr\n    - dr1node02.mno.gr\n    - dr1node03.mno.gr\n  users:\n    - mysql\n  ports:\n    - 3306\n  backup_format: CLUSTER_YYYY-MM-DD.tar.gz\n  backup_location: /backup\n  config_files:\n    - /etc/my.cnf\n    - /var/lib/mysql/\n    - /var/log/mysqld.log\n    - /var/log/mysqld_error.log\n---\n# Repair MySQL Replication on Slave Node\nThis document details how to detect and recover from broken MySQL replication by stopping the slave, restoring the latest MySQL backup, and restarting replication. It applies to both PR and DR environments within the BigStreamer setup.\n## Scope\nSometimes there are invalid MySQL queries which cause the replication to not work anymore. In this short guide, it explained how you can repair the replication on the MySQL slave. This guide is for MySQL.\n### Glossary\n- MySQL replication: It is a process that enables data from one MySQL database server (the master) to be copied automatically to one or more MySQL database servers (the slaves)\n## Setup\n### Mysql Instances\n#### PR\nMysql supported by Oracle and if any other issue occurred a critical ticket should created on Oracle Support. **This instance is not supported by jkl Telecom S.A.**\n**User**: `mysql`\n**Port**: `3306`\n**Password**: [prodsyspasswd.kbdx](/KnowledgeBase/prodsyspasswd.kdbx)\n**Master Mysql Host**: `pr1node03.mno.gr`\n**Slave Mysql Host**: `pr1node02.ngr.gr`\n**Mysql Configuration**: `/etc/my.cnf`\n**Mysql Data Path**: `/var/lib/mysql/`",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "sync_mysql.md - Part 1"
        }
    },
    "326": {
        "page_content": "**Master Mysql Host**: `pr1node03.mno.gr`\n**Slave Mysql Host**: `pr1node02.ngr.gr`\n**Mysql Configuration**: `/etc/my.cnf`\n**Mysql Data Path**: `/var/lib/mysql/`\n**Mysql General Log File**: `/var/log/mysqld.log`\n**Mysql Error Log File**: `/var/log/mysqld_error.log`\n#### DR\n**User**: `mysql`\n**Port**: `3306`\n**Password**: [prodsyspasswd.kbdx](/KnowledgeBase/prodsyspasswd.kdbx)\n**Master Mysql Host**: `dr1node03.mno.gr`\n**Slave Mysql Host**: `dr1node02.mno.gr`\n**Mysql Configuration**: `/etc/my.cnf`\n**Mysql Data Path**: `/var/lib/mysql/`\n**Mysql General Log File**: `/var/log/mysqld.log`\n**Mysql Error Log File**: `/var/log/mysqld_error.log`\n## Procedure\n### Identify the problem\n1. From **Slave Mysql Host** as `root`:\n```bash\nmysql -u root -p\nSHOW SLAVE STATUS\\G;\n```\n2. If one of `Slave_IO_Running` or `Slave_SQL_Running` is set to `No`, then the replication is broken\n### Repair MySQL Replication\n1. From **Slave Mysql Host** as `root`:\n```bash\nmysql -u root -p\nSTOP SLAVE;\n```\n- To ensure the slave is stopped, run:\n``` bash\nSHOW SLAVE STATUS\\G\n```\n- Now both `Slave_IO_Running` & `Slave_SQL_Running` is set to `No`.\n2. Restore from latest mysqldump backup:\n- From **Slave Mysql Host** as `root`:\n```bash\ncd /backup\nls -ltr\ntar -ztvf /backup/DRBDA_year-month-day.tar.gz | grep -i mysql_backup # List contents of the tar.gz file.Under backup folder stored tar.gz files from daily backup procedure,for both sites, with the format CLUSTER_year-month-day.tar.gz (e.g DRBDA_2022-03-21.tar.gz). This files contains several gz files combined in a tar.gz. Now we need to find the exact name of the gz backup file for mysql backup to proceed at next step.\ntar -zxvf /backup/DRBDA_year-month-day.tar.gz mysql_backup_yearmonthday.sql.gz # Untar from the tar.gz file the exact gz backup file for mysql backup that found from previous step. The exact name would be placed on mysql_backup_yearmonthday.sql.gz position",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "sync_mysql.md - Part 2"
        }
    },
    "327": {
        "page_content": "gunzip mysql_backup_yearmonthday.sql.gz # Decompress the file that untared from previous step\nmysql -uroot -p < mysql_backup_yearmonthday.sql\n```\n3. After successfully restoration on **Slave Mysql Host** start slave:\n```bash\nmysql -u root -p\nSHOW SLAVE STATUS\\G\n```\n- No error should exist on `Last_Error`\n- If no error appeared then `START SLAVE`\n```bash\nSTART SLAVE;\n```  \n4. Check if replication is working again\n```bash\nSHOW SLAVE STATUS\\G\n```\n- Both Slave_IO_Running and Slave_SQL_Running are set to `Yes` now. And the replication is running without any error.\n- `Seconds_Behind_Master` should be 0 after some minutes",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "sync_mysql.md - Part 3"
        }
    },
    "328": {
        "page_content": "---\ntitle: InfiniBand Card Replacement Procedure\ndescription: Step-by-step procedure for safely replacing an InfiniBand (IB) card in a BigStreamer cluster node, including pre-checks, interface cleanup, and recommissioning in Cloudera Manager.\ntags:\n  - infiniband\n  - ib\n  - hardware-replacement\n  - cloudera\n  - maintenance\n  - hca\n  - smpartition\n  - arp\n  - kudu\n  - recommission\n  - network\n  - node-replacement\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  services:\n    - Cloudera Manager\n    - Kudu\n  nodes:\n    - bda01node05\n    - bda01sw-ib1\n  network_interfaces:\n    - bondeth1\n    - bondeth2\n  monitoring_tools:\n    - sminfo\n    - ibnetdiscover\n    - smpartition\n    - ip\n    - arp\n    - ksck\n  reference:\n    - MOS Doc ID 1985159.1\n---\n# InfiniBand HCA Card Replacement Procedure\nReplacing an IB card require powering off the host. Before doing so some checks must be performed and the host must be decommissioned. After replacing, some configurations must be performed in the interfaces and the roles recommissioned.\nThis document outlines the required steps for replacing a failed InfiniBand HCA (Host Channel Adapter) card on a BigStreamer cluster node. It covers node decommissioning from Cloudera Manager, checking for non-default IB partitions, post-replacement interface configuration cleanup, and recommissioning of the node and roles. It also links to the relevant Oracle MOS document in case IB partitions need manual GUID replacement. Use this guide whenever you are performing planned maintenance involving InfiniBand hardware replacement.\n## Step 1: Decommission the Host\nTo decommission the node from Cloudera Manger, select the specific host and:\n1. Enter maintenance mode\n2. Select decommission roles\n3. If a datanode role is present on this host, take it offline for at least 4 hours\n## Step 2: Check for Non-Default IB Partitions\nMost probably only the default IB partitions are present. To check this perform the following steps provided",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "ib_card_replacement.md - Part 1"
        }
    },
    "329": {
        "page_content": "## Step 2: Check for Non-Default IB Partitions\nMost probably only the default IB partitions are present. To check this perform the following steps provided\nby Oracle:\n```\nNdef:\nIf a system uses custom non-default InfiniBand partitions [e.g., Exalogic (virtual/physical/hybrid), Exadata (virtual/physical), SuperCluster, BDA] then the HCA Port GUIDs might need to be updated in the InfiniBand partition(s) after replacing an HCA.Determine the switch running as Primary. From it, check for any custom, non-default IP partitions.\n[root@bda01node05 ~]# sminfo\nsminfo: sm lid 15 sm guid 0x10e0406d5aa0a0, activity count 26263191 priority 14 state 3 SMINFO_MASTER\n[root@bda01node05 ~]# ibnetdiscover | grep 10e0406d5aa0a0\nswitchguid=0x10e0406d5aa0a0(10e0406d5aa0a0)\nSwitch 36 \"S-0010e0406d5aa0a0\" # \"SUN DCS 36P QDR bdax01sw-ib1 xxx.xxx.171.24\" enhanced port 0 lid 15 lmc 0\n[root@bda01node05 ~]# ssh root@xxx.xxx.171.24\n[root@bda01sw-ib1 ~]# smpartition list active\n# Sun DCS IB partition config file\n# This file is generated, do not edit\n#! version_number : 0\nDefault=0x7fff, ipoib : ALL_CAS=full, ALL_SWITCHES=full, SELF=full;\nSUN_DCS=0x0001, ipoib : ALL_SWITCHES=full;\nIf there are IB partitions other than default partitions, then refer to MOS ndef 1985159.1 for additional steps that will need to be taken before the old HCA is removed.\n```\n- [MOS ndef 1985159.1](https://support.oracle.com/epmos/faces/DocumentDisplay?parent=SrDetailText&sourceId=3-37179888534&id=1985159.1)\nIf `smpartition list active` shows output similar to the above, no actions are needed. If not the attached procedure must followed in order to replace the UUIDs.\n## Step 3: Fix Network Interface ARP Config\nOracle runs an automated configuration scripts that sets arp checking of the gateway in certain interfaces. If\nthe interfaces are non-routable, as is the case for bondeth1 and bondeth2, these options must be removed.\n1. Check for any interfaces that should not be in the DOWN state with `ip a`",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "ib_card_replacement.md - Part 2"
        }
    },
    "330": {
        "page_content": "the interfaces are non-routable, as is the case for bondeth1 and bondeth2, these options must be removed.\n1. Check for any interfaces that should not be in the DOWN state with `ip a`\n2. ssh into another known with known good configs\n3. compare the interfaces in question with the interfaces in the changed hosts and remove any option not present in the known-good node. Generally these would be options referring to ARP.\n4. Bring the interfaces down with `ifdown <IFCACE_NAME>`\n5. Bring it back up with `ifup <IFCACE_NAME>`\n6. Check if the interfaces are in the UP state with `ip a`\n7. Check that ARP entries are complete with `arp`\n## Step 4: Recommission the Node in Cloudera Manager\nRecommission the node through cloudera manager. Recommissioning and starting roles in the same step might fail so it is best to recommission first without starting roles.\n1. Recommission without starting roles\n2. Start roles\nAfter everything is back online the kudu tablet on the host might not have taken on any tablets. This\nis normal as all tablets have been replicated to the other hosts. With time it will take on tablets as\nwell.\nThis can be verified using `ksck` as the kudu user.",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "ib_card_replacement.md - Part 3"
        }
    },
    "331": {
        "page_content": "# Manage Wildfly---\ntitle: Manage Wildfly Application Servers in BigStreamer\ndescription: Step-by-step guide for managing Wildfly instances on BigStreamer edge nodes, including service control, networking details, load balancer routing, and environment-specific configurations for production, QA, and development.\ntags:\n  - wildfly\n  - bigstreamer\n  - loadbalancer\n  - netscaler\n  - internet-banking\n  - online\n  - rest-apis\n  - supervisorctl\n  - edge-nodes\n  - application-server\n  - deployment\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  application_servers:\n    - prodrestib\n    - prodreston\n    - prodrestibmetrics\n    - prodrestintapps\n  users:\n    - PRODREST\n  loadbalancers:\n    - Netscaler\n  sites:\n    - PR\n    - DR\n  virtual_ips:\n    - prodrestibank.mno.gr\n    - prodrestonline.mno.gr\n    - devrestibank.mno.gr\n    - devrestonline.mno.gr\n    - qarestibank.mno.gr\n    - qarestonline.mno.gr\n  ports:\n    - 8080\n    - 8081\n---\n## Scope\nIntegration between the Big Data clusters and the backend servers of mno is done over REST APIs. The applications that handle the HTTP calls are installed on the edge servers of both sites. At normal operation only one site is active. These applications are deployed on top of Wildfly instances. There are four sets of Wildfly installations one for the `ibank` flow and one for the `online` flow and two others for applications developed by mno. All application servers are managed by `supervisord` owned by `root` user.\n**DEV/QA Information**:\nThe information below are written for the Production enviroment. There is a development/QA environment that runs only on DR site. In case of a problem the paths are exactly the same if you substitute the `prod` with the `dev` prefix. For the networking part of the DEV/QA environment use the [Network Information](#network-information) chapter\n### Glossary\n- NetScaler: Loadbalancer managed by mno. It handles SSL offloading\n- VIP: Virtual IP of the Loadbalancer",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 1"
        }
    },
    "332": {
        "page_content": "### Glossary\n- NetScaler: Loadbalancer managed by mno. It handles SSL offloading\n- VIP: Virtual IP of the Loadbalancer\n- SNIP: IP of the Loadbalancer that initiates the connection to Wildfly instances\n- Health check: Endpoint that the Loadbalancer uses to determine if a specific Wildfly instance is active. It expects a `HTTP 200/OK` response\n## Setup\n### Internet Banking Wildfly Instances\n#### prodrestib\nHandles ingestion and queries for the Internet Banking (`ibank`) flow.\n**User**: `PRODREST`\n**Port**: `8080`\n**Health Check Endpoint**: `/trlogibank/app`\n**Supervisor Configuration**: `/etc/supervisor.d/wildfly-prodrestib.ini`\n**Installation Path**: `/opt/wildfly/default/prodrestib`\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodrestib/server.log`\n**Access Log**: `/var/log/wildfly/prodrestib/access.log`\n#### prodrestibmetrics\nHosts applications developed by mno and accessed by the Internet Banking backend servers. **This instance is not supported by jkl Telecom S.A.**\n**User**: `PRODREST`\n**Port**: `8081`\n**Health Check Endpoint**: `/ibankmetrics/app`\n**Supervisor Configuration**: `/etc/supervisor.d/wildfly-prodrestibmetrics.ini`\n**Installation Path**: `/opt/wildfly/default/prodrestibmetrics`\n**Deployments Path**: `/opt/wildfly/default/prodrestibmetrics/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestibmetrics/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `Managed by mno`\n**Application Logs**: `/var/log/wildfly/prodrestibmetrics/server.log`\n**Access Log**: `/var/log/wildfly/prodrestibmetrics/access.log`\n### Internet Banking Loadbalancer farms",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 2"
        }
    },
    "333": {
        "page_content": "**Application Logs**: `/var/log/wildfly/prodrestibmetrics/server.log`\n**Access Log**: `/var/log/wildfly/prodrestibmetrics/access.log`\n### Internet Banking Loadbalancer farms\nThere are two active Loadbalancers for Internet Banking. The original setup routes all traffic to `prodrestib`, while the later one routes conditionaly traffic between `prodrestib` and `prodrestibmetrics`\n#### Original setup for Internet Banking\n```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|prodrestibank.mno.gr:443 <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta12.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta21.mno.gr]\n```\n#### Setup with routing for Internet Banking\nIf the request from `ibank` starts with `/trlogibank`:\n```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|prodrestibank.mno.gr:443 <br> Request that starts with /trlogibank <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta12.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta21.mno.gr]\n```\nIf the request from `ibank` does not start with `/trlogibank`:\n```mermaid\n  graph TD",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 3"
        }
    },
    "334": {
        "page_content": "```\nIf the request from `ibank` does not start with `/trlogibank`:\n```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|prodrestibank.mno.gr:443 <br> Request that does not start with /trlogibank <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodresta11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodresta12.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodresta21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodresta21.mno.gr]\n```\n### Online Wildfly Instances\n#### prodreston\nHandles ingestion and queries for the Online (`online`) flow.\n**User**: `PRODREST`\n**Port**: `8080`\n**Health Check Endpoint**: `/trlogonline/app`\n**Supervisor Configuration**: `/etc/supervisor.d/wildfly-prodreston.ini`\n**Installation Path**: `/opt/wildfly/default/prodreston`\n**Deployments Path**: `/opt/wildfly/default/prodreston/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodreston/server.log`\n**Access Log**: `/var/log/wildfly/prodreston/access.log`\n#### prodrestintapps\nHosts applications developed by mno and accessed by the Online backend servers. **This instance is not supported by jkl Telecom S.A.**\n**User**: `PRODREST`\n**Port**: `8081`\n**Health Check Endpoint**: `/intapps/app`\n**Supervisor Configuration**: `/etc/supervisor.d/wildfly-prodrestintapps.ini`\n**Installation Path**: `/opt/wildfly/default/prodrestintapps`\n**Deployments Path**: `/opt/wildfly/default/prodrestintapps/standalone/deployments`",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 4"
        }
    },
    "335": {
        "page_content": "**Installation Path**: `/opt/wildfly/default/prodrestintapps`\n**Deployments Path**: `/opt/wildfly/default/prodrestintapps/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestintapps/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `Managed by mno`\n**Application Logs**: `/var/log/wildfly/prodrestintapps/server.log`\n**Access Log**: `/var/log/wildfly/prodrestintapps/access.log`\n### Online Loadbalancer farms\nThere are two active Loadbalancers for Online. The original setup routes all traffic to `prodreston`, while the later one routes conditionaly traffic between `prodreston` and `prodrestintapps`\n#### Original setup for Online\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|prodrestonline.mno.gr:443 <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb12.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb21.mno.gr]\n```\n#### Setup with routing for Online\nIf the request from `online` starts with `/trlogonline`:\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|prodrestonline.mno.gr:443 <br> Request that starts with /trlogonline <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb12.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb21.mno.gr]",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 5"
        }
    },
    "336": {
        "page_content": "B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb21.mno.gr]\n```\nIf the request from `online` does not start with `/trlogonline`:\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|prodrestonline.mno.gr:443 <br> Request that does not start with /trlogonline <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodrestb11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodrestb12.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodrestb21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodrestb21.mno.gr]\n```\n### Consolidated Network Information\n#### Production\n|   #   |     Hostname     | Prod-Rest-1 Hostname | Prod-Rest-1 IP | Prod-Rest-2 Hostname | Prod-Rest-2  IP |\n| :---: | :--------------: | :------------------: | :------------: | :------------------: | :-------------: |\n|  001  | pr1edge01.mno.gr |  prodresta11.mno.gr  |  999.999.999.999  |  prodrestb11.mno.gr  |  999.999.999.999   |\n|  002  | pr1edge02.mno.gr |  prodresta12.mno.gr  |  999.999.999.999  |  prodrestb12.mno.gr  |  999.999.999.999   |\n|  003  | dr1edge01.mno.gr |  prodresta21.mno.gr  |  999.999.999.999  |  prodrestb21.mno.gr  |  999.999.999.999   |\n|  004  | dr1edge02.mno.gr |  prodresta22.mno.gr  |  999.999.999.999  |  prodrestb22.mno.gr  |  999.999.999.999   |\n#### Development\n|   #   |     Hostname     | Dev-Rest-1 Hostname | Dev-Rest-1 IP | Dev-Rest-2 Hostname | Dev-Rest-2 IP |\n| :---: | :--------------: | :-----------------: | :-----------: | :-----------------: | :-----------: |",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 6"
        }
    },
    "337": {
        "page_content": "| :---: | :--------------: | :-----------------: | :-----------: | :-----------------: | :-----------: |\n|  001  | pr1edge01.mno.gr |  devresta11.mno.gr  | 999.999.999.999  |  devrestb11.mno.gr  | 999.999.999.999  |\n|  002  | pr1edge02.mno.gr |  devresta12.mno.gr  | 999.999.999.999  |  devrestb12.mno.gr  | 999.999.999.999  |\n|  003  | dr1edge01.mno.gr |  devresta21.mno.gr  | 999.999.999.999  |  devrestb21.mno.gr  | 999.999.999.999  |\n|  004  | dr1edge02.mno.gr |  devresta22.mno.gr  | 999.999.999.999  |  devrestb22.mno.gr  | 999.999.999.999  |\n#### QA\n|   #   |     Hostname     | QA-Rest-1 Hostname | QA-Rest-1 IP  | QA-Rest-2 Hostname | QA-Rest-2 IP  |\n| :---: | :--------------: | :----------------: | :-----------: | :----------------: | :-----------: |\n|  001  | pr1edge01.mno.gr |  qaresta11.mno.gr  | 999.999.999.999 |  qarestb11.mno.gr  | 999.999.999.999 |\n|  002  | pr1edge02.mno.gr |  qaresta12.mno.gr  | 999.999.999.999 |  qarestb12.mno.gr  | 999.999.999.999 |\n|  003  | dr1edge01.mno.gr |  qaresta21.mno.gr  | 999.999.999.999 |  qarestb21.mno.gr  | 999.999.999.999 |\n|  004  | dr1edge02.mno.gr |  qaresta22.mno.gr  | 999.999.999.999 |  qarestb22.mno.gr  | 999.999.999.999 |\n#### Virtual IPs\n|   #   |       Hostname        |               IP               |                                      Servers                                       |                                              Comment                                              |\n| :---: | :-------------------: | :----------------------------: | :--------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: |\n|  001  | prodrestibank.mno.gr  | 999.999.999.999 <br> 999.999.999.999 |   mno's Loadbalancer (Netscaler) <br> Source IP for the cluster:  999.999.999.999    |                        Used for the Production servers of Internet Banking                        |",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 7"
        }
    },
    "338": {
        "page_content": "|  002  | prodrestonline.mno.gr | 999.999.999.999  <br> 999.999.999.999 |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |                             Used for the Production servers of Online                             |\n|  003  |  devrestibank.mno.gr  |          999.999.999.999           |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |   Used for the QA servers of Internet Banking  <br> Accessible to all developers' workstations    |\n|  004  | devrestonline.mno.gr  |          999.999.999.999           |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |        Used for the QA servers of Online  <br> Accessible to all developers' workstations         |\n|  005  |  qarestibank.mno.gr   |          999.999.999.999           |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |                                            **UNUSED**                                             |\n|  006  |  qarestonline.mno.gr  |          999.999.999.999           |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |                                            **UNUSED**                                             |",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 8"
        }
    },
    "339": {
        "page_content": "## Procedure\n### Stop a Wildfly instance - prodrestib\nThis procedure outlines how to gracefully stop the Wildfly instance for the Internet Banking flow (prodrestib), ensuring no client traffic is disrupted.\n1. Shutdown the Health Check endpoint:\n- If you are in a call with mno, ask for a Network administrator to join the call\n- Ask them to disable **gracefully** the server (mention the two Loadbalancer IPs and the IP you want them to disable)\n- If there is no one from mno's networking team and **only after communicating with mno**, use the following command:\nFrom the server as `PRODREST`:\n```bash\ncurl -XPUT https://<hostname>:8080/trlogibank/app/app-disable\n```\n- Check access logs to ensure no traffic is sent to the Wildfly\n2. Stop the Wildfly instance\nFrom the server as `root`:\n```bash\nsupervisorctl stop wildfly-prodrestib\n```\n3. Ensure that Wildfly is down\nFrom the server as `root`:\n```bash\nps -ef | grep 'prodrestib/'\nsupervisorctl status wildfly-prodrestib\ntail -f /var/log/wildfly/prodrestib/server.log\ntail -f /var/log/wildfly/prodrestib/access.log\n```\n### Stop a Wildfly instance - prodreston\n1. Shutdown the Health Check endpoint:\n- If you are in a call with mno, ask for a Network administrator to join the call\n- Ask them to disable **gracefully** the server (mention the two Loadbalancer IPs and the IP you want them to disable)\n- If there is no one from mno's networking team and **only after communicating with mno**, use the following command:\nFrom the server as `PRODREST`:\n```bash\ncurl -XPUT https://<hostname>:8080/trlogonline/app/app-disable\n```\n- Check access logs to ensure no traffic is sent to the Wildfly\n2. Stop the Wildfly instance\nFrom the server as `root`:\n```bash\nsupervisorctl stop wildfly-prodreston\n```\n3. Ensure that Wildfly is down\nFrom the server as `root`:\n```bash\nps -ef | grep 'prodrestib/'\nsupervisorctl status wildfly-prodreston\ntail -f /var/log/wildfly/prodreston/server.log\ntail -f /var/log/wildfly/prodreston/access.log\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 9"
        }
    },
    "340": {
        "page_content": "```bash\nps -ef | grep 'prodrestib/'\nsupervisorctl status wildfly-prodreston\ntail -f /var/log/wildfly/prodreston/server.log\ntail -f /var/log/wildfly/prodreston/access.log\n```\n### Start a Wildfly instance - prodrestib\nThis section explains how to restart the prodrestib Wildfly application server and verify it is serving traffic as expected.\n1. Check configuration:\n- If the server is in the DR site, check `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml` against PR site servers. The two sites are identical so the only change should be in hostnames.\n2. Start the Wildfly instance\nFrom the server as `root`:\n```bash\nsupervisorctl start wildfly-prodrestib\n```\n3. Ensure that Wildfly is up and has traffic\nFrom the server as `root`:\n```bash\nps -ef | grep 'prodrestib/'\nsupervisorctl status wildfly-prodrestib\ntail -f /var/log/wildfly/prodrestib/server.log\ntail -f /var/log/wildfly/prodrestib/access.log\n```\n### Start a Wildfly instance - prodreston\n1. Check configuration:\n- If the server is in the DR site, check `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml` against PR site servers. The two sites are identical so the only change should be in hostnames.\n2. Start the Wildfly instance\nFrom the server as `root`:\n```bash\nsupervisorctl start wildfly-prodreston\n```\n3. Ensure that Wildfly is up and has traffic\nFrom the server as `root`:\n```bash\nps -ef | grep 'prodreston/'\nsupervisorctl status wildfly-prodreston\ntail -f /var/log/wildfly/prodreston/server.log\ntail -f /var/log/wildfly/prodreston/access.log\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_wildfly.md - Part 10"
        }
    },
    "341": {
        "page_content": "---\ntitle: Benchmarking HBase Performance with YCSB on Lab\ndescription: Procedure to benchmark HBase performance using YCSB on the lab environment with and without quotas applied, including setup, workload execution, and quota testing for read and write limits.\ntags:\n  - hbase\n  - ycsb\n  - benchmarking\n  - quotas\n  - read-quota\n  - write-quota\n  - lab\n  - performance-testing\n  - bigstreamer\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer Lab\n  system: jakarta\n  host_ip: 999.999.999.999\n  tool: YCSB 0.17.0\n  table_name: usertable\n  namespaces:\n    - default\n    - quotas_test\n  quotas:\n    - read: 20req/sec\n    - write: 20req/sec\n  workloads: [workloada, workloadb, workloadc, workloadd, workloade, workloadf]\n  hbase_config_path: /HBASE-HOME-DIR/conf\n---\n# Benchmarking HBASE on Lab with YCSB Tool\n## Introduction\nOur purpose was to run performance tests on a created Hbase table on Lab environment and document the results which will be used as a point of reference to evaluate the efficacy of quotas that will be applied. After running tests with no quotas, we run the same tests after setting firstly read throtttle quotas and secondly write quotas. We implemented the following procedure on jakarta node (999.999.999.999). Kinit with hbase keytab was a prerequisite.\n## Create and Pre-Split HBase Table\nThis section creates the usertable using pre-splitting to distribute load evenly across regionservers.\n- Created an hbase table using pre-splitting strategy to ensure write operations target region servers uniformly\n```bash\nhbase shell\nn_splits = 300 # HBase recommends (10 * number of regionservers, in our case 3 regionservers)\ncreate 'usertable', 'family', { SPLITS => (1..n_splits).map { |i| \"user#{1000 + i * (9999 - 1000) / n_splits}\" } }\n```\n## Install and Configure YCSB\nThis section describes downloading YCSB, extracting it, and pointing it to the HBase configuration directory.",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "hbase_benchmarking.md - Part 1"
        }
    },
    "342": {
        "page_content": "```\n## Install and Configure YCSB\nThis section describes downloading YCSB, extracting it, and pointing it to the HBase configuration directory.\n- Get latest release of ycsb-0.17.0.tar.gz from https://github.com/brianfrankcooper/YCSB/releases/tag/0.17.0 on jakarta\n- Unzip file\n```bash\ntar xfvz ycsb-0.17.0.tar.gz\n```\n- Specify a HBase config directory containing  hbase-site.xml\n```bash\nmkdir -p  /HBASE-HOME-DIR/conf\ncd /HBASE-HOME-DIR/conf\ncp /etc/hbase/conf/hbase-site.xml .\n```\n- Get to YCSB directory\n```bash\ncd ycsb-0.17.0\n```\n## Baseline HBase Performance Without Quotas\nRun workloads A\u2013F with YCSB against HBase before applying any read/write quotas to establish baseline metrics.\n- Load the data\n```bash\nbin/ycsb load hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table=usertable -p columnfamily=family\n```\n- Execute the workload requesting a time series with 1 sec granularity and directing output to a datafile\n```bash\nbin/ycsb run hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table=usertable -p columnfamily=family -p measurementtype=timeseries -p timeseries.granularity=1000 -s > workloada.dat\n```\n- Delete and recreate table to repeat the aforementioned steps with workload{b-f} changing workload and datafiles respectively\n```bash\nhbase shell\ndisable 'usertable'\ndrop 'usertable'\nexists 'usertable'\nn_splits = 300\ncreate 'usertable', 'family', { SPLITS => (1..n_splits).map { |i| \"user#{1000 + i * (9999 - 1000) / n_splits}\" } }\n```\n## Apply Read Quotas on HBase Namespace\nCreate a new HBase namespace and apply read throttle quotas to test impact on performance.\n- Create namespace\n```bash\nhbase shell\ncreate_namespace 'quotas_test'\nlist_namespace\n```\n- Create table in the namespace\n```bash\nhbase shell\nn_splits = 300 # HBase recommends (10 * number of regionservers, in our case 3 regionservers)\ncreate 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n```\n- Set throttle quotas of type 'read'",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "hbase_benchmarking.md - Part 2"
        }
    },
    "343": {
        "page_content": "create 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n```\n- Set throttle quotas of type 'read'\n```bash\nhbase shell\nset_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => 'quotas_test', LIMIT => '20req/sec'\nlist_quotas\n```\n## Test Read Performance with Quotas Applied\n- Load the data\n```bash\nbin/ycsb load hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table='quotas_test:usertable' -p columnfamily=family\n```\n- Execute the workload requesting a time series with 1 sec granularity and directing output to a datafile\n```bash\nbin/ycsb run hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table='quotas_test:usertable' -p columnfamily=family -p measurementtype=timeseries -p timeseries.granularity=1000 -s > workloada_read_quotas.dat\n```\n- Delete and recreate table to repeat the aforementioned steps with workload{b-f} changing workload and datafiles respectively\n```bash\nhbase shell\ndisable 'quotas_test:usertable'\ndrop 'quotas_test:usertable'\nexists 'quotas_test:usertable'\nn_splits = 300\ncreate 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n```\n## Switch from Read to Write Quotas\nRemove read quotas and apply write quotas to the same namespace for comparative benchmarking.\n- Remove read quotas\n```bash\nhbase shell\nset_quota TYPE => THROTTLE, NAMESPACE => 'quotas_test', LIMIT => NONE\nlist_quotas\n```\n- Set write quotas\n```bash\nhbase shell\nset_quota TYPE => THROTTLE, THROTTLE_TYPE => WRITE, NAMESPACE => 'quotas_test', LIMIT => '20req/sec'\nlist_quotas\n```\n-  Delete and recreate table to repeat to run tests with write quotas\n```bash\nhbase shell\ndisable 'quotas_test:usertable'\ndrop 'quotas_test:usertable'\nexists 'quotas_test:usertable'\nn_splits = 300\ncreate 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n```\n## Test Write Performance with Quotas Applied\n- Load the data\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "hbase_benchmarking.md - Part 3"
        }
    },
    "344": {
        "page_content": "create 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n```\n## Test Write Performance with Quotas Applied\n- Load the data\n```bash\nbin/ycsb load hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table='quotas_test:usertable' -p columnfamily=family\n```\n- Execute the workload requesting a time series with 1 sec granularity and directing output to a datafile\n```bash\nbin/ycsb run hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table='quotas_test:usertable' -p columnfamily=family -p measurementtype=timeseries -p timeseries.granularity=1000 -s > workloada_write_quotas.dat\n```\n- Delete and recreate table to repeat the aforementioned steps with workload{b-f} changing workload and datafiles respectively\n```bash\nhbase shell\ndisable 'quotas_test:usertable'\ndrop 'quotas_test:usertable'\nexists 'quotas_test:usertable'\nn_splits = 300\ncreate 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n```\n**References:**\nhttps://github.com/brianfrankcooper/YCSB#ycsb\nhttps://github.com/brianfrankcooper/YCSB/tree/master/hbase2\nhttps://github.com/brianfrankcooper/YCSB/wiki/Running-a-Workload\nhttps://github.com/brianfrankcooper/YCSB/wiki/Core-Workloads\nhttps://blog.cloudera.com/hbase-performance-testing-using-ycsb/",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "hbase_benchmarking.md - Part 4"
        }
    },
    "345": {
        "page_content": "---\ntitle: Decrypt Encrypted Disk on BigStreamer Node\ndescription: Step-by-step instructions for decrypting an encrypted disk used in BigStreamer nodes, including optional backup, disk preparation, fstab updates, and service reactivation.\ntags:\n  - disk\n  - decryption\n  - encrypted-disk\n  - navencrypt\n  - fstab\n  - kafka\n  - kudu\n  - cloudera\n  - bigstreamer\n  - storage\n  - keytrustee\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  services:\n    - Kafka\n    - Kudu\n    - Key Trustee Server\n  disk_paths:\n    - /data/1\n  tools:\n    - navencrypt\n    - ztab\n    - tar\n    - fstab\n---\n# Below procedure describes how to decrypt an encrypted disk\n## Optional: Backup Encrypted Disk\nThis step creates a backup of the encrypted disk contents, only if the partition contains data.\nBackup data of encrypted disk\n>Ndef_1: In our case we don't have data in this partition. So, we don't have to backup\n```bash\ntar zcvf /backup/data_1-$(date +%Y-%m-%d).tar.gz /data/1\n```\n## Decryption Procedure\nThese steps guide the decryption process, including service validation, disk prep, and post-checks.\n1. Make sure that Kafka and Kudu services are down\n>Ndef_2: You should stop kafka and kudu in case we have data at `/data/1` partition. In our case we don't have data so we skip this step\n- From Cloudera Manager > Kafka > Stop\n- From Cloudera Manager > Kudu > Stop\n2. Check that KTS is up and running\nFrom Cloudera Manager with admin account:\n- Go to Keytrustee > Key Trustee Server  \n3. Remove /data/1 mountpoint that is no longer in use\n```bash\nnavencrypt-prepare --undo-force /data/1\n```\n4. Check ztab:\n```bash\ncat /etc/navencrypt/ztab | grep /data/1\n```\nThe output should be commented\n5. List the mountpoints\n```bash\nmount -l\n```\n6. Uncomment the decrypted mount points on fstab\nUncomment line for `/dev/mapper/ol_pr1edge01-data_1 /data/1 xfs defaults 0 0` at `/etc/fstab`\n7. Check if disk is mounted with below command\n```bash\nmount -a\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "navencrypt_disk_decryption.md - Part 1"
        }
    },
    "346": {
        "page_content": "Uncomment line for `/dev/mapper/ol_pr1edge01-data_1 /data/1 xfs defaults 0 0` at `/etc/fstab`\n7. Check if disk is mounted with below command\n```bash\nmount -a\n```\n8. Move data from backup directory back to decrypted disk\n>Ndef_3: Occurs only if step 1 is performed\n```bash\ntar -xvf /backup/data_1.tar.gz -C /data/1\n```\n9. Start kudu and kafka\nIf services were stopped earlier due to active disk usage, they must now be restarted via Cloudera Manager.\n>Ndef_4: Occurs only if step 1 is performed \n- From Cloudera Manager > Kafka > Start\n- From Cloudera Manager > Kudu > Start",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "navencrypt_disk_decryption.md - Part 2"
        }
    },
    "347": {
        "page_content": "---\ntitle: Oracle Java 1.8 Minor Upgrade on Edge Nodes\ndescription: Procedure for upgrading Oracle Java 1.8 to a newer minor version on BigStreamer edge nodes, including local RPM repository setup, edge node preparation, execution, certificate handling, update-alternatives configuration, and rollback instructions.\ntags:\n  - java\n  - oracle-java\n  - upgrade\n  - edge-nodes\n  - yum\n  - rpm\n  - certificates\n  - update-alternatives\n  - rollback\n  - cloudera\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  systems:\n    - pr1edge01\n    - pr1edge02\n    - dr1edge01\n    - dr1edge02\n    - pr1node01\n  tools:\n    - Oracle Java 8\n    - YUM\n    - update-alternatives\n    - Wildfly\n    - jssecacerts\n  repositories:\n    - /var/www/html/oracle_java/Packages\n---\n# Oracle Java 1.8 Upgrade Procedure on Edge Nodes\nThis document describes the controlled upgrade of Oracle Java 1.8 minor versions on BigStreamer edge nodes. It covers the creation and maintenance of a local RPM repository on pr1node01, edge node backup and update procedures, handling of security certificates, switching Java versions using update-alternatives, and guidance for validating application behavior post-upgrade. Rollback steps are also provided.\nThis document outlines the upgrade process of minor java version for Oracle Java 1.8\non mno's edge nodes. All procedures pertain to PR and DR edge nodes, except the RPM repository\ncreation which is performed on pr1node1:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Step 1: Create Local RPM Repository\nThis step only needs to be performed once as all subsequent RPM's will be placed inside this\nrepository. SSH into **p1node01** and as root create the repository directories:\n```bash\n$ ssh Exxxx@pr1node01\n$ sudo -i\n# mkdir -p /var/www/html/oracle_java/Packages\n```\nDownload the desired RPMs from [Oracle Java SE Archive Downloads](https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html), place them inside",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "java_upgrade.md - Part 1"
        }
    },
    "348": {
        "page_content": "```\nDownload the desired RPMs from [Oracle Java SE Archive Downloads](https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html), place them inside\n`/var/www/html/oracle_java/Packages` and create the repository:\n```bash\n# cd /var/www/html/oracle_java\n# createrepo .\n```\nSSH into one of the edge nodes, create the corresponding yum repo file and **scp** it into\nall other edge nodes:\n```bash\n$ ssh Exxx@pr1edge01\n$ sudo -i\n# vi /etc/yum.repos.d/oracle_java.repo\n[oracle_java]\nname = oracle_java\nbaseurl =  http://p1node01.mno.gr/oracle_java\nenabled = 1\ngpgcheck = 0\n# scp /etc/yum.repos.d/oracle_java.repo XXXedgeXX:/etc/yum.repos.d/\n```\nFinally on each edge node install the above packages:\n```bash\n# yum clean all\n# yum install jdk-1.8\n```\n## Step 2: Update the Repository with New RPMs\nDownload the desired version of Oracle Java 8 from\n[Oracle Java SE Archive Downloads](https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html) or\n[Oracle Java SE Downloads](https://www.oracle.com/java/technologies/downloads/) and\nplace the RPMs inside `/var/www/html/oracle_java/Packages` of **pr1node01**. Login to\n**pr1node01** and update the repository with the new packages:\n```bash\n$ ssh Exxxx@pr1node01\n$ sudo -i\n# cd /var/www/html/oracle_java\n# createrepo --update .\n```\n## Step 3: Upgrade Java on Edge Hosts\n### Preparation\nBefore upgrading the edge nodes, their resources must be moved to other nodes and a backup\nof the old java be made. Login to each edge node:\n```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo -i\n# cp -rap /usr/java/jdk1.8.0_<old version>-amd64/  /usr/java/jdk1.8.0_<old version>-amd64.bak/\n```\nAnd follow the procedures described in the **Switchover of Cluster Resources** chapter\nof the **Security Vulnerabilities** MOP [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx).\n### Execution",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "java_upgrade.md - Part 2"
        }
    },
    "349": {
        "page_content": "### Execution\nInside each edge node, update the java package using **YUM**:\n```bash\n# yum clean all\n# yum update java-1.8\n```\nCopy the old certificates into the new installation directory and run the update alternatives\ntool where you input the new version when prompted:\n```bash\n# cp -ap /usr/java/jdk1.8.0_<old version>-amd64.bak/jre/lib/security/jssecacerts \\\n/usr/java/jdk1.8.0_<new version>-amd64/jre/lib/security/\n# update alternatives --config java * javac\n# java -version\n```\nIf everything is OK unstandby the node and check each wildfly instance's access and\nserver logs for the following:\n- `/var/log/wildfly/*/server.log`: There are no undeployed WARs\n- `/var/log/wildfly/*/access.log`: Everything is reporting HTTP 200 status\nDetailed wildfly information and management instructions can be found\n[here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/manage_wildfly.md).\n## Step 4: Rollback to Previous Java Version\nLogin to each edge node and downgrade using the update-alternatives and inputting the previous version:\n```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo -i\n# update alternatives --config java * javac\n# java -version\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "java_upgrade.md - Part 3"
        }
    },
    "350": {
        "page_content": "---\ntitle: BigStreamer PR-DR Failover Procedure\ndescription: Step-by-step failover procedure for BigStreamer cluster environments from production (PR) to disaster recovery (DR) site, including stopping streaming/batch jobs, migrating UC4 agents, switching Wildfly traffic, and updating external flows.\ntags:\n  - failover\n  - dr\n  - disaster-recovery\n  - uc4\n  - spark\n  - streaming\n  - batch\n  - wildfly\n  - bigstreamer\n  - hdfs\n  - yarn\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  clusters:\n    - production\n    - disaster recovery\n  services:\n    - Spark Streaming\n    - Wildfly\n    - UC4\n    - HDFS\n    - Yarn\n  users:\n    - PRODREST\n    - DEVREST\n    - PRODUSER\n  systems:\n    - dr1edge01\n    - pr1edge01\n    - edge nodes\n    - cluster load balancer\n---\n# Failover\n## Scope\nIn case that the active site faces multiple issues that cannot be resolved in a small amount of time, we need to failover applications and procedures to the standby one. \n## Setup\nTwo symmetrical clusters have been setup named production (PR) and disaster (DR). Streaming and batch procedures are running in both sites. External traffic and UC4 flows however are only active in one of them. \n## Procedure\n### Stop Spark Streaming Topologies (PROD & DEV)\nThis section describes how to gracefully shut down Spark Streaming topologies on the currently active site (PROD or DR), by disabling crontab restarts and creating shutdown markers in HDFS.\n1. Stop production IBank, Online Spark topologies:\n- Login with your personal account at `dr1edge01` or `pr1edge01`, based on the site that will be stopped.\n- Switch user to `PRODREST`.\n- Comment lines in crontab that run `/opt/ingestion/PRODREST/common/scripts/restart_topology_STABLE.sh` and `/opt/ingestion/PRODREST/common/scripts/restart_visible_topology.sh`.\n- Create `SHUTDOWN` markers for the Spark topologies.\n```bash\n[PRODREST@Xr1edge01]# touch SHUTDOWN",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "failover.md - Part 1"
        }
    },
    "351": {
        "page_content": "- Create `SHUTDOWN` markers for the Spark topologies.\n```bash\n[PRODREST@Xr1edge01]# touch SHUTDOWN\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown_marker/\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\n```\n- Wait for 5 minutes and check that the above applications are no longer running.\n```bash\n[PRODREST@Xr1edge01]# yarn application -list | grep PRODUSER\n```\n1. Stop development IBank, Online Spark topologies:\n- Login with your personal account at `dr1edge01`. **This is done only on DR site**\n- Switch user to `DEVREST`.\n- Comment line in crontab that run `/opt/ingestion/DEVREST/common/scripts/restart_topology.sh`.\n- Create `SHUTDOWN` markers for the Spark topologies. \n```bash\n[DEVREST@dr1edge01]# touch SHUTDOWN\n[DEVREST@dr1edge01]# hdfs dfs -DEV_IBank_Ingest/topology_shutdown_marker/\n[DEVREST@dr1edge01]# hdfs dfs -put SHUTDOWN /user/DEVREST/service/DEV_Online_Ingest/topology_shutdown_marker/\n```\n- Wait for 5 minutes and check that the above applications are no longer running.\n``` bash\n[DEVREST@dr1edge01]# yarn application -list | grep DEVREST\n```\n### Stop Batch Jobs (PROD & DEV)\nThis section explains how to disable hourly and daily batch jobs for IBank and Online applications in both production and development environments, by commenting crontab lines and checking for active processes.\n1. Disable daily and hourly IBank production batch jobs\n- Login with your personal account at `dr1edge01` or `pr1edge01`, based on the site that will be stopped.\n- Switch user to `PRODREST`.\n- Comment lines in crontab that run `/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh` and `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh`.",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "failover.md - Part 2"
        }
    },
    "352": {
        "page_content": "- Check that batch jobs are not already running.\n```bash\n[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh'\n[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh'\n```\n- If they are already running wait for them to stop.\n2. Disable daily and hourly Online production batch jobs:\n- Login with your personal account at `dr1edge01` or `pr1edge01`, based on the site that will be stopped.\n- Switch user to `PRODREST`.\n- Comment lines in crontab that run `/opt/ingestion/PRODREST/common/scripts/online_daily_batch_jobs_STABLE.sh` and `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh`.\n- Check that batch job is not already running.\n```bash\n[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/common/scripts/online_daily_batch_jobs_STABLE.sh'\n[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh'\n```\n- If they are already running wait for them to stop.\n3. Disable daily IBank, Online development batch jobs:\n- Login with your personal account at `dr1edge01`. **This is done only on DR site**\n- Switch user to `DEVREST`.\n- Comment lines that run `/opt/ingestion/DEVREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs.sh` in crontab.\n- Check that batch jobs are not already running.\n```bash\n[DEVREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/DEVREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs.sh'\n```\n- If they are already running wait for them to stop.\n### Migrate Wildfly Traffic to DR Site\nThis section covers how to shift Wildfly application traffic (IBank and Online) from the active to the standby site, by launching Wildfly on the DR edge nodes and coordinating with network administrators for load balancer changes.",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "failover.md - Part 3"
        }
    },
    "353": {
        "page_content": "1. Start `prodrestib` Wildfly instances at both edge nodes of the other site using this [procedure](manage_wildfly.md#start-a-wildfly-instance-prodrestib).\n2. Start `prodreston` Wildfly instances at both edge nodes of the other site using this [procedure](manage_wildfly.md#start-a-wildfly-instance-prodreston).\n3. Ask for a mno Network administrator to make a call.\n4. Ask them to enable the new servers (mention the Loadbalancer IPs and the IP you want them to enable as explained [here](manage_wildfly.md#consolidated-network-information)).\n5. Check logs for both Wildfly instances at both servers to ensure everything works.\n6. When you are certain everything is OK, ask the mno Network administrators to disable the prexisting servers (mention the Loadbalancer IPs and the IP you want them to disable).\n7. From the access logs of the prexisting Wildfly instances check that no traffic is received. \n8. Stop these Wildfly instances as described in the procedures [here](manage_wildfly.md#stop-a-wildfly-instance-prodrestib) and [here](manage_wildfly.md#stop-a-wildfly-instance-prodreston).\n### Migrate UC4 Agent and External Trigger Handling\nThis section outlines the process of moving the UC4 job scheduler and external trigger file creation logic from the primary site to the DR site, including enabling UC4 agents, updating job scripts, and ensuring data warehouse monitoring continues without interruption.\n1. Login to the edge servers of the active and passive site using your personal account and become `root`.\n2. Stop UC4 agent at the edge nodes of the active site.\n```bash\nsystemctl stop uc4agent\n```\n3. Start service for UC4 agent at the edge servers of the passive site.\n```bash\nsystemctl start uc4agent\n```\n4. Add entries for last successful execution of IBank DataWarehouse at the edge servers of the passive site.\n```bash\nsudo -u PRODUSER /opt/ingestion/PRODUSER/datawarehouse-ibank/insert_rows_dwh_monitoring.sh <date> \n# Use the previous day's date in YYYYMMdd format.",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "failover.md - Part 4"
        }
    },
    "354": {
        "page_content": "```bash\nsudo -u PRODUSER /opt/ingestion/PRODUSER/datawarehouse-ibank/insert_rows_dwh_monitoring.sh <date> \n# Use the previous day's date in YYYYMMdd format.\n# If today is Sunday or Monday, use the date of the last Friday instead.\n```\n5. Migrate the creation of trigger files for external jobs\n- On the active site:\n```bash\nvi /opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh\n# Comment the followin lines along with the assosiated checks\n# touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY\n# touch /opt/applications/landing_zone/PRODUSER/triggers/IBANK_SA_`date '+%Y%m%d'`.READY\n```  \n- On the passive site:\n```bash\nvi /opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh\n# Uncomment the followin lines along with the assosiated checks\n# touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY\n# touch /opt/applications/landing_zone/PRODUSER/triggers/IBANK_SA_`date '+%Y%m%d'`.READY\n```\n### Revert Failover to PR\n>To revert the failover and restore traffic back to PR, repeat the above steps in reverse order, starting by stopping all workloads and UC4 agents in DR and reactivating them in PR.",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "failover.md - Part 5"
        }
    },
    "355": {
        "page_content": "---\ntitle: Certificate Renewal Procedure for BigStreamer\ndescription: Step-by-step guide to renew and validate Cloudera and HAProxy certificates across PR and DR environments, including certificate checks, backups, distribution, HAProxy replacement, and application restarts.\ntags:\n  - certificates\n  - cloudera\n  - haproxy\n  - ssl\n  - openssl\n  - pem\n  - jks\n  - kudu\n  - flows\n  - cluster-maintenance\n  - bigstreamer\n  - edge-nodes\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  sites:\n    - PR\n    - DR\n  systems:\n    - node01\n    - dr1edge01\n    - pr1edge01\n    - Xr1node03\n    - un1\n    - edge nodes\n  backup_paths:\n    - /backup/new_certs/\n    - /backup/haproxy_certs/\n    - /opt/cloudera/security/\n    - /opt/haproxy/security/\n  services:\n    - haproxy\n    - kudu\n    - spark flows\n    - cloudera-scm-agent\n    - cloudera-scm-server\n    - bigdatamanager\n---\n# Certificate Renewal Procedure\nBack up every certificate before doing any action\n### Backup Procedure\n- From node1 as root:\n``` bash\ndcli -C \"cp -r /opt/cloudera/security /opt/cloudera/security.BAK_`date +%d%b%Y%H%M%S`\" \n```\n- From edge nodes as root:\n```bash\ncp -r /opt/haproxy/security /opt/haproxy/security.BAK_`date +%d%b%Y%H%M%S`\n```\n## Node and VIP Certificates check\nThis section explains how to verify unsigned and signed certificates for Cloudera and edge nodes using OpenSSL. Ensures certificate integrity before replacement.\n### Check unsigned certificates\n- In this step we checked the presigned certificates in `backup/new_certs/cert_requests` if they have correct subject with the following command: \n`openssl req -in new_certs/cert_requests/dr1edge.mno.gr-ert-file -noout -text`\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/certreq.JPG)\nand also we check the modulus if it is the same. Basically we check the output of the following commands:\n`openssl rsa -in /opt/cloudera/security/x509/node.key -noout -text modulus`",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 1"
        }
    },
    "356": {
        "page_content": "and also we check the modulus if it is the same. Basically we check the output of the following commands:\n`openssl rsa -in /opt/cloudera/security/x509/node.key -noout -text modulus`\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/modulus1.JPG)\n`openssl req -in new_certs/cert_requests/dr1edge.mno.gr-ert-file -noout -text -modulus`\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/modulus2.JPG)\nThen we provide to the customer the certificates located in `backup/new_certs/cert_requests` in order to be signed\n### Check signed certificates from mno\nIn the following folder are located the signed certificates\nBackup NFS Folder: `/backup/new_certs/certificates`\nCheck the certificates in the above mentioned folder for issuer, subject, TLS Web, date.\nThe `'ln -1'` feature prints all files in the for loop per line\n- Check the issuer\n`for i in 'ln -1'; do echo $i; openssl x509 -noout -text -in $i | grep -i issuer ; done` \nIn the above command we wait a return such as this: \n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/issuer.JPG)\n- Check the subject\n`for i in 'ln -1'; do echo $i; openssl x509 -noout -text -in $i | grep -i subject ; done`\nIn the above command we wait a return such as this:\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/subject.JPG)\n- Check the TLS Web\n`for i in 'ln -1'; do echo $i; openssl x509 -noout -text -in $i | grep -i 'TLS Web' ; done` \nIn the above command we wait a return such as this: \n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/tls.JPG)\n- Check the dates\n`openssl x509 -noout -text -in 'cert_file' - dates`\nIn the above command we wait a return such as this: \n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/dates.JPG)\n- Or with a for loop for all the files\n`for i in 'ln -1'; do openssl x509 -noout -text -in $i | grep -i 'ndef after'; done`",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 2"
        }
    },
    "357": {
        "page_content": "- Or with a for loop for all the files\n`for i in 'ln -1'; do openssl x509 -noout -text -in $i | grep -i 'ndef after'; done`\nIn the above command we wait a return such as this: \n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/notafter.JPG)\n### Haproxy certificates check and replacement\nBackup NFS Folder: `/backup/haproxy_certs`\n`ssh root@pr1edge01`\nIn order to set the new haproxy certificates we need to have 9 certificates\nCheck the haproxy security folder: `/opt/haproxy/security/x509/`\n```\ndevsqla_mno_gr.haproxy.pem\npr1edge_mno_gr.haproxy.pem\ndr1edge_mno_gr.haproxy.pem\nqasqla_mno_gr.haproxy.pem\nprodsqla_mno_gr.haproxy.pem\n```\nand the node certifate for PR and DR in the following format \n`node.haproxy.pem`\n- Now in the NFS mentioned files we need to replace the second certificate with the one that is located in the signed cert files that the customer has send to us\n`vi /backup/haproxy_certs/devsqla_mno_gr.haproxy.pem` and replace the \n```\n--- BEGIN CERTIFICATE ---\n... \n--- END CERTIFICATE ---\n```\nwith the one located in `/backup/new_certs/certificates/devsqla_mno_gr-cert-file.cer`\n- Moreover, as root replace the CERTIFICATE to the\n`vi /backup/haproxy_certs/devsqla_mno_gr.haproxy.pem`\nwith the certificate from \n`cat /backup/new_certs/certificates/devsql_mno_gr-cert-file.cer` \nand copy the section:\n```\n\u00a0\u00a0\u00a0 ---BEGIN CERTIFICATE---",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 3"
        }
    },
    "358": {
        "page_content": ".....",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 4"
        }
    },
    "359": {
        "page_content": "---END CERTIFICATE---\n```\nand replace it with the certificate on the pem file `/backup/haproxy_certs/devsqla_mno_gr.haproxy.pem`\nFor all the other `pem` files we need to do the same procedure accordingly. **EVERY PEM HAS A UNIQUE CER FILE**\nWe need to specify in more detail the above steps especially for the 4 edge nodes we have on PR & DR sites.\n- Firstly, under `/opt/haproxy/security/x509/` folder there is the `node.haproxy.pem` certificate as mentioned before. We must copy this file under the NFS mentioned folder `/backup/haproxy_certs/`.\nFor example:\n```bash\ncp /opt/haproxy/security/x509/node.haproxy.pem /backup/haproxy_certs/Xredge0X.node.haproxy.pem\n```\n- Then, from `/backup/haproxy_certs/Xredge0X.node.haproxy.pem` we must keep only the private key certificate section and replace the rest of the certificates with the ones that are located in the signed cert file `/backup/new_certs/certificates/Xr1edge0X_mno_gr-cert-file.cer` that the customer has sent to us.\n- We must follow the same procedure for all edge nodes certificates.\n#### Checks\n- Check the issuer on previous certificates located in `/etc/pki/ca-trust/source/anchors/`\n```\nca1.crt\nca.crt\nca3.crt\n```\n- Check the issuer in the above mentioned crt\n`for i in ca1.crt ca3.crt ca.crt; do echo $i; openssl x509 -noout -text -in $i | grep -i issuer; done`\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/pki.JPG)\nFrom the above image we excepted to see the issuer as `mnoInternalRoot` which is correct in `ca3.crt`.\n- In order to check if the certificates has been signed from the same issuer. Run the following commands and check the output. It should be the same. If not, the certificate is wrong\n```\nopenssl x509 -noout -modulus -in 'cert_file'\nopenssl rsa -noout -modulus -in 'cert_file'\n```\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/x509.JPG)\n### Actions Before Distributing the certificates",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 5"
        }
    },
    "360": {
        "page_content": "openssl rsa -noout -modulus -in 'cert_file'\n```\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/x509.JPG)\n### Actions Before Distributing the certificates\nExplains how to safely stop Spark flows and prepare systems for certificate changes.\nmno is obliged to move the traffic from PR site to DR site.\nStop the flows, as user PRODREST:\n```\n# Signal Spark flows to shut down safely before cert replacement\n[PRODREST@Xr1edge01]# touch SHUTDOWN\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown_marker/\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\n```\nCheck that flows stopped.\n```\n[PRODREST@Xr1edge01]# yarn application -list | grep -i PROD_\n```\nWhen executing the same procedure on the DR site, we should **additionally** stop the following flows as user DEVREST:\n```\n[DEVREST@dr1edge01]# touch SHUTDOWN\n[DEVREST@dr1edge01]# hdfs dfs -put SHUTDOWN /user/DEVREST/service/DEV_IBank_Ingest/topology_shutdown_marker/\n[DEVREST@dr1edge01]# hdfs dfs -put SHUTDOWN /user/DEVREST/service/DEV_Online_Ingest/topology_shutdown_marker/\n```\nCheck that flows stopped.\n```\n[DEVREST@dr1edge01]# yarn application -list | grep DEVREST\n```\n## Distribute the certificates\nCovers how to copy, import, and activate the new signed certificates across all cluster nodes.\n### Generate the keystore password (It's not the same for both sites)\n`bdacli getinfo cluster_https_keystore_password`\nFrom node01:\n#### Node certificates\nFor internal nodes:\n```\ndcli -C cp /backup/new_certs/certificates/\\$HOSTNAME-cert-file.cer /opt/cloudera/security/x509/node.cert\n```\nFor edge nodes:\n```\ncp /backup/new_certs/cert_2024/$HOSTNAME-cert-file.cer /opt/cloudera/security/x509/node.cert\n```\n#### JKS certificates\nFor internal nodes:\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 6"
        }
    },
    "361": {
        "page_content": "```\nFor edge nodes:\n```\ncp /backup/new_certs/cert_2024/$HOSTNAME-cert-file.cer /opt/cloudera/security/x509/node.cert\n```\n#### JKS certificates\nFor internal nodes:\n```\n# Import signed certificate into Cloudera's Java Keystore (JKS) on internal nodes\ndcli -C keytool -import -file /opt/cloudera/security/x509/node.cert -alias \\$HOSTNAME -keystore /opt/cloudera/security/jks/node.jks -storepass KEYSTORE_PASS_FROM_ABOVE -keypass KEYSTORE_PASS_FROM_ABOVE -noprompt\n```\nFor edge nodes:\n```\nkeytool -import -file /opt/cloudera/security/x509/node.cert -alias $HOSTNAME -keystore /opt/cloudera/security/jks/node.jks -storepass KEYSTORE_PASS_FROM_ABOVE -keypass KEYSTORE_PASS_FROM_ABOVE -noprompt\n```\n#### Check new certificates\nFor internal nodes:",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 7"
        }
    },
    "362": {
        "page_content": "```\ndcli -C \"keytool -list -v -keystore /opt/cloudera/security/jks/node.jks -alias \\$HOSTNAME\"\n```\nFor edge nodes:\n```\nkeytool -list -v -keystore /opt/cloudera/security/jks/node.jks -alias $HOSTNAME\n```\n#### Haproxy certificates\nCopy the files from `/backup/haproxy_certs/` to `/opt/haproxy/security/x509/` and replace the existing ones.\n**Special caution**:\nMust copy `Xr1edge0X_mno_gr.node.haproxy.pem` that we created in the previous steps to the `node.haproxy.pem` certificate existing on the edge nodes\n```\n# Replace haproxy node certificate with newly signed one\ncp /backup/haproxy_certs/Xr1edge0X_mno_gr.node.haproxy.pem /opt/haproxy/security/x509/node.haproxy.pem\n```\n**Do not copy root.inter.pem**\nAfter copying the certificates, restart the haproxy service on both edge nodes\n```\nsystemctl reload haproxy \nsystemctl status haproxy\npcs resource cleanup haproxy-clone\n```\nIf after restarting HAProxy the service fails due to missing chain or improper concatenation, rebuild the node certificate manually like this:\n```\ncd /opt/cloudera/security/x509\ncat node.hue.key node.cert > /opt/haproxy/security/x509/node.haproxy.pem\n```\n### Actions After Distributing the certificates\nSteps to restart agents and verify successful service recovery after new certificates are in place.\nWhen the new certificates replace the old ones, the services of the cluster will become healthy. Restart is required. [Official Guide](https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cm_mc_start_stop_service.html)\nAll services except zookeeper need restart. The service `Bigdatamanager` should always remain stopped \nWe prefer to start with Kudu because it takes longer to synchronize. Also, for KMS service perform restart 1 by 1. For KMS server perform restart on both passive services and then on both active.  \nLastlty, after Kudu syncs start the flows.\nWhen the cluster be stopped then:\nFor edge nodes:\n```\nsystemctl status cloudera-scm-agent\nsystemctl restart cloudera-scm-agent \n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 8"
        }
    },
    "363": {
        "page_content": "Lastlty, after Kudu syncs start the flows.\nWhen the cluster be stopped then:\nFor edge nodes:\n```\nsystemctl status cloudera-scm-agent\nsystemctl restart cloudera-scm-agent \n```\nFor internal nodes:\n```\ndcli -C \"systemctl status cloudera-scm-agent | grep -i Active\" \n# Restart Cloudera agents across all nodes to load new certificates\ndcli -C \"systemctl restart cloudera-scm-agent\" \ndcli -C \"systemctl status cloudera-scm-agent | grep -i Active\"\n```\n```\ndcli -c Xr1node03 \"systemctl restart cloudera-scm-server\" \ndcli -c Xr1node03 \"systemctl status cloudera-scm-server\"\n```\n### Kudu Checks\nOpen UIs from masters and tablets from internal firefox from edge nodes. Firefox launced with your personal Exxx account.\nLogs from kudu logs on every node:\n`tail -f /var/log/kudu/kudu-tserver.INFO` checks that the number will be equal to this number => `ls /u12/kudu/tablet/data/data/ | grep metadata | wc -l`, when the synchronization is successfully completed.\n#### Start flows\nStart ibank from edge Node as PRODREST\n```\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\n```\nStart ibank visible from edge Node as PRODREST\n```\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_VISIBLE_STABLE.sh\n```\nStart online from edge Node as PRODREST\n```\n/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\n```\nSimilarly from a DR edge node as DEVREST:\nStart ibank\n```\n/opt/ingestion/DEVREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno.sh\n```\nStart online\n```\n/opt/ingestion/DEVREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno.sh\n```\n### Applications checks\nWhen all Kudu Tablets are synchronized and **all flows are on RUNNING status** then:\nas user PRODREST from an edge node:\n```\nimpala-shell xr1edge.mno.gr -k -ssl\n```\nExecute the following query:\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 9"
        }
    },
    "364": {
        "page_content": "When all Kudu Tablets are synchronized and **all flows are on RUNNING status** then:\nas user PRODREST from an edge node:\n```\nimpala-shell xr1edge.mno.gr -k -ssl\n```\nExecute the following query:\n```\nselect max(timestamp) as time, 'ibank' as application from prod_trlog_ibank.service_audit_stream union select max(timestamp) as time, 'online' as application from prod_trlog_online.service_audit_stream;\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "certificate_renewal_procedure.md - Part 10"
        }
    },
    "365": {
        "page_content": "---\ntitle: Manage HBase Quotas on BigStreamer\ndescription: Procedure for enabling, setting, and removing HBase namespace-level read and write quotas in a Cloudera-managed environment on BigStreamer using Cloudera Manager and HBase shell.\ntags:\n  - hbase\n  - quotas\n  - cloudera\n  - throttling\n  - hbase-shell\n  - namespace\n  - read-quota\n  - write-quota\n  - bigstreamer\n  - cm-safety-valve\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  components:\n    - HBase\n    - Cloudera Manager\n  systems:\n    - edge nodes\n  commands:\n    - hbase shell\n    - kinit\n    - Cloudera Safety Valve\n---\n# Manage HBase Quotas\nThis document describes how to manage HBase quotas in the BigStreamer environment. It explains how to enable HBase throttling via Cloudera Manager, configure namespace-specific read and write request limits using the HBase shell, and cleanly remove quotas when no longer needed. Steps include using kinit, navigating HBase processes, and verifying changes through list_quotas.\n## Step 1: Enable Global HBase Quotas via Cloudera Manager\n1. Go to ```Cloudera Manager => HBase => Configuration => HBase Service Advanced configuration Snippet (Safety Valve) for hbase-site.xml```\n2. Add the following configuration:\n```\nName: hbase.quota.enabled\nValue: true\nDescription: enable hbase quotas\n```\n3. Restart HBase service\n## Step 2: Set Namespace-Level HBase Quotas\n1. ssh to an edge node\n2. kinit as hbase\n```bash\ncd /var/run/cloudera-scm-agent/processes\nls \u2013ltr HBASE\ncd <latest hbase process folder>\nkinit -kt hbase.keytab `hostname`\n```\n3. Get list of namespaces\n```bash\nhbase shell\nlist_namespace\n```\n4. Set throttle READ quotas \n```bash\nhbase shell\nset_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => 'namespace', LIMIT => 'Xreq/sec'\n```\n5. Set throttle WRITE quotas\n```bash\nhbase shell\nset_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => 'namespace', LIMIT => 'Xreq/sec'\n```\n6. Show all quotas\n```bash\nhbase shell\nlist_quotas\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_hbase_quotas.md - Part 1"
        }
    },
    "366": {
        "page_content": "```bash\nhbase shell\nset_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => 'namespace', LIMIT => 'Xreq/sec'\n```\n6. Show all quotas\n```bash\nhbase shell\nlist_quotas\n```\n## Step 3: Remove Namespace-Level Quotas\n1. ssh to an edge node\n2. kinit as hbase\n```bash\ncd /var/run/cloudera-scm-agent/processes\nls \u2013ltr HBASE\ncd <latest hbase process folder>\nkinit -kt hbase.keytab `hostname`\n```\n3. Get list of namespaces and list of quotas already set\n```bash\nhbase shell\nlist_namespace\nlist_quotas\n```\n4. Delete throttle quotas\n```bash\nhbase shell\nset_quota TYPE => THROTTLE, NAMESPACE => 'namespace', LIMIT => NONE\n```\n5. Verify that quotas have been removed\n```bash\nhbase shell\nlist_quotas\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_hbase_quotas.md - Part 2"
        }
    },
    "367": {
        "page_content": "---\ntitle: Postgres 14 Upgrade Procedure\ndescription: Detailed instructions for upgrading PostgreSQL from version 9.5 to 14 on PR and DR edge nodes in the BigStreamer environment, including data backup, repository setup, YUM installation, and rollback steps.\ntags:\n  - postgres\n  - postgresql\n  - yum\n  - upgrade\n  - rollback\n  - nexus\n  - repository\n  - pr\n  - dr\n  - edge-nodes\n  - cloudera\n  - bigstreamer\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  edge_nodes:\n    - pr1edge01\n    - pr1edge02\n    - dr1edge01\n    - dr1edge02\n  postgres_versions:\n    - 9.5\n    - 14\n  nexus_repo_node: pr1node01\n  repository_url: http://pr1node01.mno.gr/postgres14/\n  yum_repo_file: /etc/yum.repos.d/postgres14.repo\n  backup_paths:\n    - /var/lib/psql/9.5/data/pg_hba.conf\n    - /var/lib/psql/9.5/data/postgresql.conf\n    - edgeXX_postgres_backup\n---\n# PostgreSQL 14 Upgrade from 9.5\nAll procedures pertain to PR and DR edge nodes, except the RPM repository creation\nwhich is performed on pr1node1:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Step 1: Put Node in Standby and Stop Services\nStandby and backup steps before upgrading PostgreSQL on BigStreamer edge nodes.\nBefore continuing with the changes it is best to put the edge node you are\nworking on in standby mode, so as to not disrupt services:\n```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo -i\n```\nAnd follow the procedures described in the **Switchover of Cluster Resources** chapter\nof the **Security Vulnerabilities** MOP [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx).\nStop the running postgres service:\n```bash\n# sudo -iu postgres\n$ systemctl stop postgresql-9.5.service\n$ systemctl disable postgresql-9-5.service\n$ systemctl status postgresql-9.5.service\n```\nBackup data on each edge server:\n- edge01: `$ pg_dumpall > edge01_postgres_backup`\n- edge02: `$ pg_dumpall > edge02_postgres_backup`",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "postgres_upgrade.md - Part 1"
        }
    },
    "368": {
        "page_content": "$ systemctl status postgresql-9.5.service\n```\nBackup data on each edge server:\n- edge01: `$ pg_dumpall > edge01_postgres_backup`\n- edge02: `$ pg_dumpall > edge02_postgres_backup`\nBackup **pg_hba.conf** and **postgresql.conf**:\n```bash\n# cp -ap /var/lib/psql/9.5/data/pg_hba.conf /var/lib/psql/9.5/data/pg_hba.conf.bak\n# cp -ap /var/lib/psql/9.5/data/postgresql.conf /var/lib/psql/9.5/data/postgresql.conf.bak\n```\n## Step 2: Create Nexus Repository for PostgreSQL 14 RPMs\nInstructions for setting up a Nexus YUM repository for PostgreSQL 14 RPMs.\nDownload rpms for Postgres 14 from the Postgres site\nhttps://download.postgresql.org/pub/repos/yum/14/redhat/rhel-7.9-x86_64/.....\nand prepare the new postgres repository on pr1node01:\n```bash\n$ ssh Exxxx@pr1node01\n$ sudo -i\n# mkdir -p /var/www/postgres14/Packages/\n```\nMove all the rpm files of Postgres14 under `/var/www/html/postgres14/Packages` and\ncreate the **YUM** repository:\n```bash\n# cd /var/www/postgres14/\n# createrepo .\n```\nor if the repository existed:\n```bash\n# createrepo --update .\n```\nCreate the repository file on one of the edge nodes and copy it to all others:\n```bash\n$ ssh Exxx@pr1edge01\n$ sudo -i\n# vi /etc/yum.repos.d/postgres14.repo\n[postgres14]\nname = Postgres14\nbaseurl =  http://pr1node01.mno.gr/postgres14/\nenabled = 1\ngpgcheck = 0\n# scp /etc/yum.repos.d/postgres14.repo XXXedgeXX:/etc/yum.repos.d/\n```\nOn each edge node disable the old postgres repository by setting `enabled = 0` inside its repo file under `/etc/yum.repos.d/`.\n## Step 3: Install and Initialize PostgreSQL 14\nSteps to install PostgreSQL 14, configure the data directory, and restore from backup.\nPerform the update using **YUM**, while enabling the repository for the new Postgres and disabling the previous repository if exists on each edge node:\n```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo -i\n# yum clean all\n# yum install --disablerepo=* --enablerepo=postgres14 postgresql14 postgresql14-server postgresql14-contrib postgresql14-libs\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "postgres_upgrade.md - Part 2"
        }
    },
    "369": {
        "page_content": "```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo -i\n# yum clean all\n# yum install --disablerepo=* --enablerepo=postgres14 postgresql14 postgresql14-server postgresql14-contrib postgresql14-libs\n```\nChange the data directory and setup the newly updated PostgreSQL:\n```bash\n# vi usr/lib/systemd/system/postgresql-14.service\nEnvironment=PGDATA=/var/lib/pgsql/9.14/data\n# /usr/pgsql-14/bin/postgresql-14-setup initdb\n# systemctl enable --now postgresql-14\n```\nLogin to each edge node and restore data from backup:\n```bash\n$ ssh Exxx@XXXedgeXX:\n$ sudo -iu postgres\n$ psql -f edgeXX_postgres_backup postgres\n$ systemctl restart postgresql-14.service\n$ systemctl status postgresql-14.service\n```\nCheck **pg_hba.conf** and **postgresql.conf** for differencies between versions:\n```bash\n$ sdiff /var/lib/pgsql/9.14/data/pg_hba.conf /var/lib/psql/9.5/data/pg_hba.conf\n$ sdiff /var/lib/pgsql/9.14/data/postgresql.conf /var/lib/psql/9.5/data/postgresql.conf\n```\nIf everything is ok, unstandby the node.\n## Step 4: Rollback to PostgreSQL 9.5 (if needed)\nReverting back to PostgreSQL 9.5 in case of failure, with repository and service configuration.\nLogin to each edge node, stop the postgres service and downgrade using **YUM**:\n```bash\n$ ssh Exxx@XXXedgeXX:\n$ sudo -iu postgres\n$ systemctl disable --now postgresql-14.service\n$ systemctl status postgresql-14.service\n$ sudo -i\n# yum clean all\n# yum downgrade --disablerepo=* --enablerepo=postgres9 postgresql\n# systemctl enable --now postgresql-9-5.service\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "postgres_upgrade.md - Part 3"
        }
    },
    "370": {
        "page_content": "---\ntitle: SSL Configuration Hardening for Edge Nodes\ndescription: Procedure for updating SSL configurations for httpd, nginx, haproxy, and sshd on PR and DR edge nodes to enforce TLSv1.2, disable weak ciphers, and enhance cryptographic security.\ntags:\n  - ssl\n  - tls\n  - httpd\n  - nginx\n  - haproxy\n  - sshd\n  - tls1.2\n  - edge-nodes\n  - security-hardening\n  - bigstreamer\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  nodes:\n    - pr1edge01\n    - pr1edge02\n    - dr1edge01\n    - dr1edge02\n  services:\n    - httpd\n    - nginx\n    - haproxy\n    - sshd\n  ssl_protocol: TLSv1.2\n  backup_paths:\n    - /etc/httpd/conf.d/ssl.conf\n    - /etc/httpd/conf/httpd.conf\n    - /etc/httpd/conf.d/graphite-web.conf\n    - /etc/nginx/nginx.conf\n    - /etc/haproxy/haproxy.cfg\n    - /etc/ssh/sshd_config\n---\n# SSL Hardening Procedure for Edge Services\nAll procedures pertain to PR and DR edge nodes:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Preparation\nBefore continuing with the changes it is best to put the edge node you are\nworking on in standby mode, so as to not disrupt services:\n```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo -i\n```\nAnd follow the procedures described in the **Switchover of Cluster Resources** chapter\nof the **Security Vulnerabilities** MOP [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx).\n## Step 1: Harden Apache httpd SSL Configuration\nEnforce TLSv1.2 and disable weak ciphers in Apache httpd.\nBackup the old httpd configs:\n```bash\n# cp -ap /etc/httpd/conf.d/ssl.conf  \"/etc/httpd/conf.d/ssl.conf.bak.$(date +%Y%m%d)\"\n# cp -ap /etc/httpd/conf/httpd.conf  \"/etc/httpd/conf/httpd.conf.bak.$(date +%Y%m%d)\"\n# cp -ap /etc/httpd/conf.d/graphite-web.conf \"/etc/httpd/conf.d/graphite-web.conf.bak.$(date +%Y%m%d)\"\n```\nAdd the following line in `/etc/httpd/conf/httpd.conf`:\n```\nTraceEnable Off\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "ssl_configuration_changes.md - Part 1"
        }
    },
    "371": {
        "page_content": "# cp -ap /etc/httpd/conf.d/graphite-web.conf \"/etc/httpd/conf.d/graphite-web.conf.bak.$(date +%Y%m%d)\"\n```\nAdd the following line in `/etc/httpd/conf/httpd.conf`:\n```\nTraceEnable Off\n```\nAdd the following line in `/etc/httpd/conf/httpd.conf`, `/etc/httpd/conf.d/ssl.conf`\nand `/etc/httpd/conf.d/graphite-web.conf`:\n```\nSSLProtocol +TLSv1.2\n```\nEdit `/etc/httpd/conf/httpd.conf`, `/etc/httpd/conf/ssl.conf` and\n`/etc/httpd/conf/graphite-web.conf` and remove the following lines:\n```\nSSLHonorCipherOrder Off\nSSLCipherSuite ECDH+AESGCM:ECDH+CHACHA20:ECDH+AES256:ECDH+AES128:!aNULL:!SHA1:!AESCCM:!MD5:!3DES:!DES:!IDEA\n```\nRestart the **httpd** service:\n```bash\n# systemctl restart httpd\n```\n## Step 2: Harden nginx SSL Configuration\nHarden nginx SSL config by restricting protocols.\nBackup the old **nginx.conf**:\n```bash\n# cp -ap /etc/nginx/nginx.conf \"/etc/nginx/nginx.conf.bak.$(date +%Y%m%d)\"\n```\nAdd the following line in `/etc/nginx/nginx.conf`:\n```\nssl_protocols TLSv1.2;\n```\nDisable and restart the **nginx** service:\n```bash\n# systemctl disable --now nginx\n# systemctl start nginx\n```\n## Step 3: Update haproxy SSL Bindings\nAdd TLSv1.2 bindings and update HAProxy certificate paths.\nBackup the old **haproxy.cfg**:\n```bash\n# cp -ap /etc/haproxy/haproxy.cfg \"/etc/haproxy/haproxy.cfg.bak.$(date +%Y%m%d)\"\n```\nAdd options for 8889 and 25002 port and repeat for **hue_vip**:\n```\nbind 999.999.999.999:25002 ssl crt no-sslv3 /opt/haproxy/security/x509/node.haproxy.pem\n```\nRestart the **haproxy** service:\n```bash\n# systemctl restart haproxy\n```\n## Step 4: Strengthen SSH Daemon Cryptographic Settings\nStrengthen SSH security by configuring allowed ciphers and key exchanges.\nBackup the old **sshd_config**:\n```bash\n# cp -ap /etc/ssh/sshd_config \"/etc/ssh/sshd_config.bak.$(date +%Y%m%d)\"\n```\nEdit the sshd config `/etc/ssh/sshd_config` and add the following:\n```\nCiphers aes256-ctr,aes192-ctr,aes128-ctr # 5.2.11",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "ssl_configuration_changes.md - Part 2"
        }
    },
    "372": {
        "page_content": "# cp -ap /etc/ssh/sshd_config \"/etc/ssh/sshd_config.bak.$(date +%Y%m%d)\"\n```\nEdit the sshd config `/etc/ssh/sshd_config` and add the following:\n```\nCiphers aes256-ctr,aes192-ctr,aes128-ctr # 5.2.11\nKexAlgorithms ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group14-sha1,diffie-hellman-group-exchange-sha256\n```\nRestart the **sshd** service:\n```bash\n# systemctl restart sshd\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "ssl_configuration_changes.md - Part 3"
        }
    },
    "373": {
        "page_content": "---\ntitle: Grafana Upgrade Procedure for PR/DR Edge Nodes\ndescription: Step-by-step procedure to upgrade Grafana OSS on PR and DR edge nodes in BigStreamer, including plugin and dashboard backup, RPM repository setup, execution using YUM, and rollback instructions.\ntags:\n  - grafana\n  - monitoring\n  - upgrade\n  - rollback\n  - dashboards\n  - plugins\n  - bigstreamer\n  - rpm\n  - yum\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  systems:\n    - pr1edge01\n    - pr1edge02\n    - dr1edge01\n    - dr1edge02\n    - pr1node01\n  components:\n    - grafana\n    - grafana-server\n    - grafana.ini\n    - datasources\n    - dashboards\n  backup_targets:\n    - /var/lib/grafana/plugins\n    - /etc/grafana/grafana.ini\n    - API backups of dashboards/datasources\n  rpm_repository_host: pr1node01\n---\n# Grafana Upgrade\nAll procedures pertain to PR and DR edge nodes, except the RPM repository creation\nwhich is performed on pr1node1:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Pre-Upgrade Preparation and Backups\nThis section includes backing up Grafana plugins, INI configuration, dashboards, and datasources, and preparing RPM repositories for PR/DR edge nodes.\nBefore continuing with the changes it is best to inform the monitoring team\nthat there will be an outage on the monitoring service.\nLogin to each edge node and get a root shell:\n```bash\n$ ssh Exxxx@XXXedgeXX\n$ sudo -i\n```\n### Backup\nBackup Installed plugins before you upgrade them in case you want to rollback the\nGrafana version and want to get the exact same versions you were running before the\nupgrade or in case you add new configuration options after upgrade and then rollback.\n- pr1edge01:\n  - Plugins: `# tar -zcvf grafana_plugins_edge01.tar.gz /var/lib/grafana/plugins`\n  - INI file: `# tar -zcvf grafana_ini_edge01.tar.gz /etc/grafana/grafana.ini`\nBackup Grafana Datasources and Dashboards",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "grafana_upgrade.md - Part 1"
        }
    },
    "374": {
        "page_content": "- INI file: `# tar -zcvf grafana_ini_edge01.tar.gz /etc/grafana/grafana.ini`\nBackup Grafana Datasources and Dashboards\nWith an admin account login to Grafana and go to Configuration > API keys. Create a new key by clicking on \u201cAdd API key\u201d with role admin. Copy the authorization token for the key.\nLogin to an edge node and use the API to back up the datasources and dashboards:\n```bash\n# curl -H \"Authorization: Bearer <insert token>\"  https://<grafana host>:3000/api/datasources > grafana_datasources.json\n# curl -H \"Authorization: Bearer <insert token>\"  https://<grafana host>:3000/api/search | grep -o -E '\"uid\":\"[a-zA-Z0-9_-]+\"' | sed 's/\"uid\":\"//g' | sed 's/\"//g' > grafana_dashboards_uids\n# for uid in `cat grafana_dashboards_uids`; do curl -H \"Authorization: Bearer <insert token>\"  https://<grafana host>:3000/api/dashboards/uid/${uid}; done > /tmp/grafana_dashboard_${uid}.json\n```\n### Repositories\nDownload Grafana RPMs from [Grafana Downloads](https://grafana.com/grafana/download?edition=oss)\nand prepare their repositories on pr1node01, from where PR and DR edge nodes will download them:\n```bash\n$ ssh Exxxx@pr1node01\n$ sudo -i\n# mkdir -p /var/www/grafana8/Packages/\n```\nMove all the downloaded RPMs under `/var/www/html/grafana8/Packages` and create the\nrepository:\n```bash\n# cd /var/www/grafana8\n# createrepo .\n```\nIf the repository already exists, issue:\n```bash    \n# createrepo --update .\n```\nLogin to an edge node, create the repository file and copy it to all other\nedge nodes appropriately:\n```bash\n$ ssh Exxx@XXXedgeXX\n$ sudo -i\n# vi /etc/yum.repos.d/grafana8.repo\n[grafana8]\nname = Grafana8\nbaseurl =  http://pr1node01.mno.gr/grafana8/\nenabled = 1\ngpgcheck = 0\n# scp /etc/yum.repos.d/grafana8.repo repo XXXedgeXX:/etc/yum.repos.d/\n```\n## Upgrade Execution on All Edge Nodes\nThis section explains how to stop Grafana, perform the YUM upgrade, and verify the updated Grafana server and configuration.",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "grafana_upgrade.md - Part 2"
        }
    },
    "375": {
        "page_content": "```\n## Upgrade Execution on All Edge Nodes\nThis section explains how to stop Grafana, perform the YUM upgrade, and verify the updated Grafana server and configuration.\nLogin to each edge node, stop the **grafana-server**, and update it using **YUM**:\n```bash\n$ ssh Exxx@XXXedgeXX\n$ sudo -i\n# systemctl stop grafana-server\n# systemctl status grafana-server\n# yum clean all\n# yum update grafana\n# systemctl start grafana-server\n# systemctl  status grafana-server\n```\nCheck Grafana UI, Dashboards and compare new and old configs with **diff** for any discrepancies:\n```bash\n# sdiff /etc/grafana/grafana.ini <path/to/old/grafana.ini>`\n```\n## Rollback Grafana to Previous Version\nThis section covers how to revert Grafana to the previous version, restore configuration and plugins, and verify dashboard functionality.\nLogin to each edge node to stop the grafana service and downgrade the package using **YUM**:\n```bash\n$ ssh Exxx@XXXedgeXX\n$ sudo -i\n# systemctl stop grafana-server\n# systemctl status grafana-server\n# yum clean all\n# yum downgrade grafana\n```\nRestore plugins and INI files from the backups previously created at `grafana_plugins_edge0X.tar.gz`\nand `grafana_ini_edge0X.tar.gz`, start the grafana service and check dashboards:\n```bash\n# systemctl start grafana-server\n# systemctl status grafana-server\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "grafana_upgrade.md - Part 3"
        }
    },
    "376": {
        "page_content": "---\ntitle: Kafka MirrorMaker Offset Management Procedure\ndescription: Step-by-step instructions for stopping MirrorMakers, committing consumer group offsets, and restarting MirrorMakers on PR and DR Kafka clusters to avoid offset resets and message replay in Spark streaming topologies.\ntags:\n  - kafka\n  - mirrormaker\n  - consumer-groups\n  - offsets\n  - cloudera\n  - spark-streaming\n  - hdfs\n  - kerberos\n  - bigstreamer\n  - kafka-admin\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  kafka_clusters:\n    - PRBDA\n    - DRBDA\n  consumer_groups:\n    - mir-trlog-ingest-stream-con-001\n    - mir-trlog-ingest-stream-con-002\n  kafka_nodes:\n    - pr1node01\n    - pr1node02\n    - pr1node04\n    - pr1node05\n    - pr1node06\n    - dr1node01\n    - dr1node02\n    - dr1node04\n    - dr1node05\n    - dr1node06\n  kerberos_principals:\n    - kafka@BDAP.mno.GR\n    - kafka@BDAD.mno.GR\n---\n# Manage Kafka MirrorMaker\nThis guide documents how to safely commit Kafka consumer group offsets in BigStreamer environments where Kafka MirrorMaker is used. It avoids offset resets by controlling the stop/commit/start sequence of MirrorMakers on PR and DR Kafka clusters using Cloudera Manager, Kerberos-authenticated CLI tools, and timestamp-based offset commits.\n## Scope\nKafka MirrorMaker has been configured with auto.offsets.reset=false. This means that the MirrorMaker consumers never commit the offsets.\nIf the MirrorMakers' consumer group goes inactive(both MirrorMakers are offline at the same time), their consumer group will reset to the earliest offset available.\nThis means that the last week will be mirrored to the final topic and will be replayed by the Spark streaming topology.\nThis document describes how to commit the offsets for the MirrorMaker consumer groups, in order to avoid this issue.\n## Setup\n1. MirrorMakers on nodes pr1node01 and pr1node04:\n- Replicate the traffic from the **Primary Site Mirror Topics** to the **Primary Site Final Topics**.",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_mirrormaker.md - Part 1"
        }
    },
    "377": {
        "page_content": "## Setup\n1. MirrorMakers on nodes pr1node01 and pr1node04:\n- Replicate the traffic from the **Primary Site Mirror Topics** to the **Primary Site Final Topics**.\n- Replicate Production Topics for both Internet Banking and Online Applications.\n- Use **mir-trlog-ingest-stream-con-002** consumer group.\n- Offsets are committed to the **Primary Site Kafka cluster**.\n2. MirrorMakers on nodes pr1node05 and pr1node06:\n- Replicate the traffic from the **Disaster Site Mirror Topics** to the **Primary Site Final Topics**.\n- Replicate Production Topics for both Internet Banking and Online Applications.\n- Use **mir-trlog-ingest-stream-con-001** consumer group.\n- Offsets are committed to the **Disaster Site Kafka cluster**.\n3. MirrorMakers on nodes dr1node01 and dr1node04:\n- Replicate the traffic from the **Disaster Site Mirror Topics** to the **Disaster Site Final Topics**.\n- Replicate Production Topics for both Internet Banking and Online Applications.\n- Use **mir-trlog-ingest-stream-con-002** consumer group.\n- Offsets are committed to the **Disaster Site Kafka cluster**.\n4. MirrorMakers on nodes dr1node05 and dr1node06:\n- Replicate the traffic from the **Primary Site Mirror Topics** to the **Disaster Site Final Topics**.\n- Replicate Production Topics for both Internet Banking and Online Applications.\n- Use **mir-trlog-ingest-stream-con-001** consumer group.\n- Offsets are committed to the **Primary Site Kafka cluster**.\n## Procedure\n### Stop All Kafka MirrorMakers Affecting PR Site\n1. Stop Primary Site MirrorMakers:\nFrom the Primary Site Cloudera Manager with a user that has administrator privileges:\n- PRBDA > Kafka > Instances\n- Select the MirrorMakers running on nodes pr1node01,pr1node04,pr1node05 and pr1node06\n- Stop\n2. Stop Disaster Site MirrorMakers:\nFrom the Disaster Site Cloudera Manager with a user that has administrator privileges:\n- DRBDA > Kafka > Instances\n- Select the MirrorMakers running on nodes dr1node05 and dr1node06\n- Stop",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_mirrormaker.md - Part 2"
        }
    },
    "378": {
        "page_content": "From the Disaster Site Cloudera Manager with a user that has administrator privileges:\n- DRBDA > Kafka > Instances\n- Select the MirrorMakers running on nodes dr1node05 and dr1node06\n- Stop\n### Stop All Kafka MirrorMakers Affecting DR Site\n1. Stop Primary Site MirrorMakers:\nFrom the Primary Site Cloudera Manager with a user that has administrator privileges:\n- DRBDA > Kafka > Instances\n- Select the MirrorMakers running on nodes dr1node01,dr1node04,dr1node05 and dr1node06\n- Stop\n2. Stop Disaster Site MirrorMakers:\nFrom the Disaster Site Cloudera Manager with a user that has administrator privileges:\n- PRBDA > Kafka > Instances\n- Select the MirrorMakers running on nodes pr1node05 and pr1node06\n- Stop\n### Commit Consumer Group Offsets on PR Kafka Cluster\nThe following steps can be performed at any node of the Primary Site cluster. The consumer groups need to be **inactive** for these action to be performed.\n1. Create a file named group.properties:\n```conf\nsecurity.protocol=SASL_SSL\nsasl.kerberos.service.name=kafka\n```\n2. Create a file named jaas.conf:\n```conf\nClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=false\n    useTicketCache=true\n    doNotPrompt=true\n    principal=\"kafka@BDAP.mno.GR\";\n};\nKafkaClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=false\n    useTicketCache=true\n    doNotPrompt=true\n    principal=\"kafka@BDAP.mno.GR\"\n    service=\"kafka\";\n};\n```\n3. Log in to kerberos as the **_kafka@BDAP.mno.GR_** principal and configure security:\n```bash\nkinit kafka@BDAP.mno.GR\nexport KAFKA_JVM_PERFORMANCE_OPTS=\"-Djava.security.auth.login.config=./jaas.conf\"\n```\n4. Commit the offsets for all relevant consumer groups:\n```bash\nexport DATETIME=1970-01-01T00:00:00.000Z #UTC time. See ndefs\nkafka-consumer-groups --bootstrap-server pr1node01.mno.gr:9093,pr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --reset-offsets --to-datetime $DATETIME --execute",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_mirrormaker.md - Part 3"
        }
    },
    "379": {
        "page_content": "kafka-consumer-groups --bootstrap-server pr1node01.mno.gr:9093,pr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\n```\n### Commit Consumer Group Offsets on DR Kafka Cluster\nThe following steps can be performed at any node of the Disaster Site cluster. The consumer groups need to be **inactive** for these action to be performed.\n1. Create a file named group.properties:\n```conf\nsecurity.protocol=SASL_SSL\nsasl.kerberos.service.name=kafka\n```\n2. Create a file named jaas.conf:\n```conf\nClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=false\n    useTicketCache=true\n    doNotPrompt=true\n    principal=\"kafka@BDAD.mno.GR\";\n};\nKafkaClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=false\n    useTicketCache=true\n    doNotPrompt=true\n    principal=\"kafka@BDAD.mno.GR\"\n    service=\"kafka\";\n};\n```\n3. Log in to kerberos as the **_kafka@BDAD.mno.GR_** principal and configure security:\n```bash\nkinit kafka@BDAD.mno.GR\nexport KAFKA_JVM_PERFORMANCE_OPTS=\"-Djava.security.auth.login.config=./jaas.conf\"\n```\n4. Commit the offsets for all relevant consumer groups:\n```bash\nexport DATETIME=1970-01-01T00:00:00.000Z #UTC time. See ndefs\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --reset-offsets --to-datetime $DATETIME --execute\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_mirrormaker.md - Part 4"
        }
    },
    "380": {
        "page_content": "kafka-consumer-groups --bootstrap-server pr1node01.mno.gr:9093,pr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\n```\n### Restart MirrorMakers Serving PR Site\n1. Start Primary Site MirrorMakers:\nFrom the Primary Site Cloudera Manager with a user that has administrator privileges:\n- PRBDA > Kafka > Instances\n- Select the MirrorMakers running on nodes pr1node01,pr1node04,pr1node05 and pr1node06\n- Start\nAll messages should be consumed in about one to two minutes.\n2. Start Disaster Site MirrorMakers:\nFrom the Disaster Site Cloudera Manager with a user that has administrator privileges:\n- DRBDA > Kafka > Instances\n- Select the MirrorMakers running on nodes dr1node05 and dr1node06\n- Start\nWait for traffic on all topics to get back to normal values before any changes.\n### Restart MirrorMakers Serving DR Site\n1. Start Primary Site MirrorMakers:\nFrom the Disaster Site Cloudera Manager with a user that has administrator privileges:\n- DRBDA > Kafka > Instances\n- Select the MirrorMakers running on nodes dr1node01,dr1node04,dr1node05 and dr1node06\n- Start\nAll messages should be consumed in about one to two minutes.\n2. Start Disaster Site MirrorMakers:\nFrom the Primary Site Cloudera Manager with a user that has administrator privileges:\n- PRBDA > Kafka > Instances\n- Select the MirrorMakers running on nodes pr1node05 and pr1node06\n- Start\nWait for traffic on all topics to get back to normal values before any changes.\n## Ndefs\n- The result from the following queries can be useful during startup:\n```sql\nSELECT min(`timestamp`),max(`timestamp`) FROM prod_trlog_ibank.service_audit_stream\nSELECT min(`timestamp`),max(`timestamp`) FROM prod_trlog_online.service_audit_stream\n```\n- Consider committing offsets at a time 5 minutes prior to max timestamp\n- Time should be in UTC e.g. 2019-09-02T12:30:00.000Z = 2019-09-02T15:30:00.000 EEST",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_mirrormaker.md - Part 5"
        }
    },
    "381": {
        "page_content": "```\n- Consider committing offsets at a time 5 minutes prior to max timestamp\n- Time should be in UTC e.g. 2019-09-02T12:30:00.000Z = 2019-09-02T15:30:00.000 EEST\n- The option _--all-topics_ commits the offsets **only** for the topics this consumer group has ever subscribed and not all the topics of the Kafka cluster\n- These commands are only for consumers that use the new API (version 0.10 and later)\n- The following commands can be useful:\n```bash\nexport DATETIME=1970-01-01T00:00:00.000Z\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --describe # List offsets\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --reset-offsets --to-datetime $DATETIME # Dry run\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "manage_mirrormaker.md - Part 6"
        }
    },
    "382": {
        "page_content": "---\ntitle: Add New Yum Repository on Nexus\ndescription: Procedure for creating a new YUM (proxy) repository in Nexus, configuring repository metadata, and registering the repo on edge nodes using a custom `.repo` file.\ntags:\n  - nexus\n  - yum\n  - proxy-repo\n  - edge-nodes\n  - firefox\n  - repository-management\n  - sysadmin\n  - bigstreamer\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  tools:\n    - Nexus Repository Manager\n    - YUM\n    - Firefox\n  nodes:\n    - xedge0x\n    - edge nodes\n  nexus_url: https://999.999.999.999:8081/\n  internal_links:\n    - https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/passwords.kdbx\n---\n# Below procedure describes how to add a new repository on Nexus.\nThis guide explains how to add and register a new YUM (proxy) repository using Nexus Repository Manager and configure it on BigStreamer edge nodes.\n## Prerequisites\n- Access to an edge node with GUI (X11 forwarding)\n- Nexus credentials from [passwords.kdbx](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/passwords.kdbx)\n## Open Nexus Web Interface\n1. Login with your personal account to an edge node and open firefox\nStart Firefox with GUI support from an edge node:\n```bash\nssh -X xedge0x\nfirefox\n```\n## Create Yum Proxy Repository\n2. When firefox window pops up login to `https://999.999.999.999:8081/` with Nexus creds.\n[Click me for the credentials](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/passwords.kdbx)\n## Add Repository to Edge Node\n3. Click on the gear icon and then **repositories** button and **Create repository** and select **yum (proxy)**.\nAdd below values:\n- **Name**: name_of_repo\n- **Remdef storage**: remdef_storage_url \n- **Maximum Component age**: 20\n- **Minimum Component age**: 20\n- **Clean up policies**: daily_proxy_clean\nLeave the rest of the settings as default\n4. Click on **Create repository**",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "nexus_new_repo_procedure.md - Part 1"
        }
    },
    "383": {
        "page_content": "- **Maximum Component age**: 20\n- **Minimum Component age**: 20\n- **Clean up policies**: daily_proxy_clean\nLeave the rest of the settings as default\n4. Click on **Create repository**\n## Verify Repository Access\n5. Login with your personal account at node and add the following repos:\nCreate the new YUM repository definition file:\n```bash\nvi /etc/yum.repos.d/name_of_repo.repo\n[name_of_repo]\nname = name_of_repo\nbaseurl = http://999.999.999.999:8081/repository/name_of_repo.repo\nenabled = 1\ngpgcheck = 0\n```\n6. Check and add new repo\nClean cache, check for updates, and verify repo registration:\n```bash\nssh to_node\nyum clean all\nyum check-update > /tmp/test-repo.txt\nyum repolist\n```",
        "metadata": {
            "category": "procedures",
            "client": "Client_mno",
            "name": "nexus_new_repo_procedure.md - Part 2"
        }
    },
    "384": {
        "page_content": "---\ntitle: RStudio Connect User Application Failure Due to Expired LDAP Bind Password\ndescription: Troubleshooting of failed RStudio Connect applications for user `kmpoletis` due to expired `t1-svc-cnebind` LDAP bind account password. Steps include LDAP connection tests, error diagnostics, and password reconfiguration in the RStudio Connect configuration file.\ntags:\n  - bigstreamer\n  - rstudio\n  - rstudio connect\n  - ldap\n  - unrstudio1\n  - kmpoletis\n  - t1-svc-cnebind\n  - ldapsearch\n  - bind account\n  - expired password\n  - authentication failure\n  - rstudio applications not loading\n  - user login issue\n  - abc admin\n  - password reset\n  - service account credentials\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IMXXXXXXX\n  system: abc BigStreamer RStudio Connect\n  user_affected: kmpoletis\n  unaffected_users:\n    - charisis\n    - ploskas\n  hostname: unrstudio1\n  ldap_bind_account: t1-svc-cnebind\n  ldap_server: ldaps://PVDCAHR01.groupnet.gr\n  config_file: /etc/rstudio-connect/rstudio-connect.gcfg\n  root_cause: Expired bind account password\n  resolution: Manual password update by abc Admin\n---\n# abc - IM1962926 - RStudio Connect User App Failure (kmpoletis)\n## Description\nRStudio user applications not working.\nThe applications of the user kmpoletis are not running showing the following error. This morning we reset the user's password because it had expired. It can connect normally with the credentials. Also let me know that the applications of the other users (Charisis, Ploskas) are running normally.\n## Actions Taken\n### 1. Validate LDAP Bind Account (t1-svc-cnebind)\n- Ssh to **unrstudio1**\n- Do an ldap search using `t1-svc-cnebind` as the following in order to check if `t1-svc-cnebind` password is still valid:\n```bash\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D \"t1-svc-cnebind\" -W -b \"dc=groupnet,dc=gr\" '(sAMAccountName=t1-svc-cnebind)'\n```\n-  In the above case, if you get an error that you can't connect with the ldap server, that means it has an expired `t1-svc-cnebind` password, so you will need to replace it with a new one.\n### 2. Update LDAP Bind Password in RStudio Connect Config\n- Check the configuration file below for the current ldap t1-svc-cnebind password & replace it with new **(Responsible for that abc Admin)**:\n```bash\nssh unrstudio1\nvi /etc/rstudio-connect/rstudio-connect.gcfg\n```\n### Conclusion\nRStudio Connect applications for user `kmpoletis` failed due to expired `t1-svc-cnebind` LDAP credentials. Issue resolved by updating the bind password in the system configuration file. Other users remained unaffected.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20220909-IM1962926.md"
        }
    },
    "385": {
        "page_content": "---\ntitle: CDSW Unavailability Due to Resource Exhaustion on All Worker Nodes\ndescription: CDSW became unavailable due to all underlying nodes (mncdsw1, wrkcdsw1-6) reaching resource capacity, causing critical pods (web, tcp-ingress-controller, ds-reconciler) to become unready. No manual intervention was needed; services recovered once load decreased.\ntags:\n  - bigstreamer\n  - cdsw\n  - pod unavailability\n  - cluster load\n  - wrkcdsw nodes\n  - mncdsw1\n  - pod health\n  - resource saturation\n  - cloudera manager\n  - web pod\n  - tcp-ingress\n  - ds-reconciler\n  - service downtime\n  - kubectl\n  - memory exhaustion\n  - high load\n  - cdsw status\n  - outage\n  - auto recovery\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2178255\n  system: abc BigStreamer CDSW\n  date: 2023-06-29\n  root_cause: All CDSW nodes reached maximum resource utilization, causing pods to fail readiness checks\n  nodes_affected:\n    - mncdsw1\n    - wrkcdsw1\n    - wrkcdsw2\n    - wrkcdsw3\n    - wrkcdsw4\n    - wrkcdsw5\n    - wrkcdsw6\n  failed_pods:\n    - role/web\n    - role/tcp-ingress-controller\n    - role/ds-reconciler\n  recovery: Automatic when load normalized\n---\n# abc - BDC - IM2178255 - CDSW Downtime Due to Host Resource Exhaustion\nCDSW is not available. From the Cloudera manager we see the following:\nPods not ready in cluster default ['role/ds-reconciler', 'role/tcp-ingress-controller', 'role/web']. * Web is not up yet.\nThe nodes are all started and in good health.\n## Actions Taken\nAfter investigation we found out that in the time the issue has been assigned to our team **the resources of the CDSW hosts(mncdsw1,wrkcdsw1,wrkcdsw2,wrkcdsw3,wrkcdsw4,wrkcdsw5,wrkcdsw6) were maxed out** as we can see in the following pictures. The results of the following screenshot has been aggregated. \nAlso, another step is the check @ mncdsw1 the cdsw status:\n`cdsw status` and the health of the pods: `kubectl get pods -A` . \nAt the time that we check the above mentioned status, all services and pods were healthy. \n![cfgh_cdsw_host_memory](.media/cfgh_cdsw.JPG)\n![cfgh_cdsw_host_memory](.media/cfgh_cdsw_host_memory.JPG)\n![cfgh_cdsw_host_load](.media/cfgh_cdsw_host_load.JPG)\nDue to increased load in the above mentioned time frame the service was down.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230629-IM2178255.md"
        }
    },
    "386": {
        "page_content": "---\ntitle: Location Mobility Export Failure Due to Memory Exhaustion and Missing HDFS File\ndescription: The Location Mobility LM_02_LTE file failed to export due to memory exhaustion on sn102 caused by a heavy user query, followed by missing HDFS files triggering I/O errors on other nodes. Issue was resolved via retention cleanup and table refresh, without flow restarts.\ntags:\n  - bigstreamer\n  - location mobility\n  - lm_02_lte\n  - sn102\n  - impala\n  - memory exhaustion\n  - out of memory\n  - disk io error\n  - missing hdfs file\n  - ranai-geo\n  - npce.eea_hour\n  - impala daemon\n  - export failure\n  - trustcenter\n  - hdfs\n  - retention\n  - refresh table\n  - root cause analysis\n  - lte\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2131290\n  system: abc BigStreamer TrustCenter LTE\n  failed_flow: Location Mobility LM_02_LTE export\n  node_with_issue: sn102.bigdata.abc.gr\n  primary_error: Out-of-memory error caused by impala query from ranai-geo\n  secondary_error: Disk I/O error due to missing HDFS file on sn111\n  recovery_method: Automatic retention and table refresh; dev config correction\n  impala_query_user: ranai-geo\n  tables_involved:\n    - npce.eea_hour\n    - refdata.mediation_loc_mobility_load_info\n---\n# abc - BigStreamer - IM2131290 - Location Mobility LM_02_LTE Export Failure\n## Description\nThe LM_02_LTE export from the Location Mobility workflow failed starting April 19 at 15:00. Below is the initial user-reported issue:\nAs of yesterday noon at 15:00 the creation of the LM_02_LTE file of the location mobility stream fails. From the HUE jobs you can see that the workflow (export_Location_Mobility_files_to_mediat...) is killed.\nWe also saw from the logs (lm_export_lte_v2_mon.cron.20230419.log) that at 15:00 when the problem starts we have the following error:\nQuery submitted at: 2023-04-19 15:00:31 (Coordinator: http://sn72.bigdata.abc.gr:25000)\nQuery progress can be monitored at: http://sn72.bigdata.abc.gr:25000/query_plan?query_id=c74df6d614d535ea:4de432ac00000000\nERROR: Failed due to unreachable impalad(s): sn102.bigdata.abc.gr:22000\nCould not execute command: SELECT\nachievable_thr_bytes_down_1,\nachievable_thr_bytes_up_1,\nachievable_thr_time_down_1,\n..................................................................\n[2023/04/19 15:19:13] - ERROR: Impala shell command for par_msisdn= failed.\n[2023/04/19 15:19:13] - ERROR: Clean up and exit.\n% Total % Received % Xferd Average Speed Time Time Time Current\nFrom there onwards we observe errors of the form:\nQuery submitted at: 2023-04-19 17:00:31 (Coordinator: http://sn64.bigdata.abc.gr:25000)\nQuery progress can be monitored at: http://sn64.bigdata.abc.gr:25000/query_plan?query_id=094fdeda997b8d44:5826172200000000\nERROR: Disk I/O error on sn62.bigdata.abc.gr:22000: Failed to open HDFS file hdfs://nameservice1/ez/warehouse/npce.db/eea_hour/pardt=2023041910/ef4d08b3073e8531-d909e37b0000010\ne_1525512597_data.0.txt\nError(2): No such file or directory\nRoot cause: RemfghException: File does not exist: /ez/warehouse/npce.db/eea_hour/pardt=2023041910/ef4d08b3073e8531-d909e37b0000010e_1525512597_data.0.txt\nat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:85)\nat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)\nat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1909)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:736)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:415)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)\nat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:869)\nWe have similar errors in today's log file lm_export_lte_v2_mon.cron.20230420.log\n## Root Cause Analysis\nFirst thing that we have checked were the comments on the [md](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/trustcenter_flows.md) that exists for this flow.\nAfter checking the logs of the flow we saw that the flow was running successfully after it had failed for half day. The reason that it was able to run  was the retention that had already taken place the day that we received the ticket.\nWe were able to identify the issue that caused the problem. After checking the logs of the host we saw that the host didn't have enough memory at the time.\n![sn102_memory](.media/sn102_memory.JPG) because of the user `ranai-geo` that had run an impala query \n![query](.media/query.JPG) that took all the resources of the impala deamon on sn102.\n![query_details](.media/q_details.JPG) \n## Actions Taken\nNo restart needed of the flow. There were some adjustments from dev team that took place.\n### Development Intervention\n```\nIt was necessary to make some corrections in the configuration table of the refdata.mediation_loc_mobility_load_info flow to synchronize the data sets that will be exported.\nAdditionally, due to the following exception a refresh table was added before the select in npce.eea_hour.\nERROR: Disk I/O error on sn111.bigdata.abc.gr:22000: Failed to open HDFS file hdfs://nameservice1/ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\nError(2): No such file or directory\nRoot cause: RemfghException: File does not exist: /ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230420-IM2131290.md"
        }
    },
    "387": {
        "page_content": "---\ntitle: CPU_LOAD and MEMORY_USAGE Export Issues Due to Misaligned Timestamps\ndescription: Root cause analysis and remediation steps for missing or misaligned CPU_LOAD and MEMORY_USAGE export files in abc BigStreamer, including Flume Morphline rounding config fix, field_min_5 correction, and IO bottleneck investigation on sdc device.\ntags:\n  - bigstreamer\n  - abc\n  - flume\n  - morphline\n  - component-metrics\n  - cpu_load\n  - memory_usage\n  - nnm\n  - ip_vpn\n  - timestamp-rounding\n  - impala\n  - logs\n  - data-export\n  - par_dt\n  - field_min_5\n  - sdc\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1333238\n  cluster: abc\n  nodes:\n    - un2.bigdata.abc.gr\n  user: ipvpn\n  metrics_files:\n    - CPU_LOAD_YYYY-MM-DD_HH.MM.00.csv\n    - MEM_USAGE_YYYY-MM-DD_HH.MM.00.csv\n  root_cause:\n    - wrong field_min_5 due to time bucket rounding\n    - empty export files due to no matched records\n  logs_checked:\n    - initiate_export_components.cron.YYYYMMDD.log\n    - compute_cpu_kpis.YYYYMMDD.log\n    - compute_memory_kpis.YYYYMMDD.log\n    - nnm_component_metrics.cron.YYYYMMDD.log\n    - flume-cmf-flume5-AGENT-un2.bigdata.abc.gr.log\n  tools:\n    - impala-shell\n    - flume\n    - Cloudera Manager\n    - Morphline\n  config_files_updated:\n    - morphline_nnmMetricsCsvToRecord_ipvpn_user.conf\n---\n# abc - BigStreamer - IM1333238 - abc BigStreamer SM-MISSING DATA\n## Description\nFiles CPU_LOAD and MEMORY_USAGE are exported in the wrong order or not at all Missing files for 4:25 and 5:05 a.m.\n## Actions Taken\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `ipvpn` with sudo\n### Check export log errors\n2. Inspect logs of *export component files* flow\n``` bash\ncd /shared/abc/ip_vpn/log/\nless initiate_export_components.cron.20201025.log\n```\n### Identify missing timestamps\n3. Check messages for files with 04:25 and 05:05 metrics\n``` bash\n[2020/10/25 04:32:56] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-25_04.25.00.csv is empty.\n[2020/10/25 04:32:56] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-25_04.25.00.csv is empty.\n...\n[2020/10/25 05:12:57] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-25_05.05.00.csv is empty.\n[2020/10/25 05:12:57] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-25_05.05.00.csv is empty.\n```\n### Validate Impala queries for metrics data\n4. Check Impala queries execution for that files\nFor `CPU_LOAD`:\n``` bash\nless compute_cpu_kpis.20201025.log\n...\nQuery: SELECT straight_join a.min_5 AS time,\n...\n    a.component_type='CPU' AND\n    a.min_5='2020-10-25 04:25:00' AND\n    a.par_dt='20201025'\n...\nFetched 0 row(s) in 12.20s\nINFO: CPU file exported.\n```\nFor `MEM_USAGE`:\n``` bash\nless compute_memory_kpis.20201025.log\n...\nQuery: SELECT straight_join a.min_5 AS time,\n...\na.component_type='MEMORY' AND\na.min_5='2020-10-25 04:25:00' AND\na.par_dt='20201025'\n...\nFetched 0 row(s) in 12.03s\nINFO: Memory file exported.\n```\n### Confirm input table row availability\n5. Check input metrics table\nExecute the Impala query either from Hue or impala-shell\n```bash\n# Confirm if metrics exist for the missing par_dt in the input table\nSELECT count(*)\nFROM bigcust.nnm_ipvpn_componentmetrics_hist a\nWHERE        \n    a.min_5='2020-10-25 04:25:00' AND\n    a.par_dt='20201025';\nResult = 0\n```\n### Analyze input file ingestion process\n6. Inspect logs of input metrics ingestion\nFiles are transferred from NNM node to a local spool directory every 5 minutes.\n```bash\nless /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20201025.log\n...\n[2020/10/25 04:31:05] - INFO - Checking file: /data/1/nnm_components_LZ/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.csv.gz\n...\n[2020/10/25 04:31:05] - INFO - /bin/mv /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv.tmp /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv\n```\n### Detect timestamp misalignment in Flume logs\n7. Inspect logs of Flume agent\n```bash\nless /var/log/flume-ng/flume-cmf-flume5-AGENT-un2.bigdata.abc.gr.log\n...\n2020-10-28 22:46:05,308 INFO org.kitesdk.morphline.stdlib.LogInfoBuilder$LogInfo: After Split record:[{IPAddress=[87.203.132.214], ..., file=[/data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv]}]\n2020-10-28 22:46:05,308 INFO org.kitesdk.morphline.stdlib.LogInfoBuilder$LogInfo: After extractTimeBucket record:[{IPAddress=[87.203.132.214], ..., field_min_5=[2020-10-25 04:30:00], field_par_dt=[20201025], file=[/data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv]}]\n```\nThe issue is that field_min_5 should have been `2020-10-25 04:25:00`, not `2020-10-25 04:30:00`.\n### Fix Morphline configuration for rounding\n8. Alter Morphline configuration\nChange rounding to `mathematical` so field_min_5 is the timestamp of the metric rounded to the nearest 5-minute interval. \n```bash\n# Fix Morphline to use correct rounding logic (nearest 5-minute interval)\nvim /shared/abc/ip_vpn/flume/nnm_component_metrics/morphline_nnmMetricsCsvToRecord_ipvpn_user.conf\n...\n          { extractTimeBucket { field : file, bucket:5, rounding:mathematical } }\n```\n### Restart Flume agent and validate behavior\n9. Restart Flume Agent\nWhen the agent is not processing any files, restart `FLUME-IPVPN` at `un2.bigdata.abc.gr` from Cloudera Manager. Monitor that field_min_5 is rounded to the nearest 5-minute interval.\n```bash\n# Monitor log to verify that field_min_5 is now correctly aligned\ntail -f less /var/log/flume-ng/flume-cmf-flume5-AGENT-un2.bigdata.abc.gr.log\n```\n### Analyze export job timing issues\n10. Investigate delays in export\n```bash\nless initiate_export_components.cron.20201025.log\n...\n[2020/10/25 03:12:56] - INFO: Searching for output files..\n[2020/10/25 03:13:04] - INFO: cpu_output_file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-25_03.05.00.csv\n[2020/10/25 03:13:04] - INFO: mem_output_file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-25_03.05.00.csv\n```\nSearching for the files is taking too long.\n### Investigate disk performance anomalies\n11. Inspect host resources at that time\nLogin to Cloudera Manager. CPU, memory and network did not show anomalies, however disk queue was increased for sdc device. Peaks were nfghd every 2 hours.\n### Identify IO-heavy processes\n12. Find processes with heavy disk i/o\nBased on a previous investigation the processes that write to sdc run for Location Mobility. Communication with the development team to change disk/device.\n## Affected Systems\nabc Bigstreamer Backend\n## Summary of Fix\n- Root cause: Incorrect rounding in Morphline config caused misaligned `field_min_5`, leading to empty exports for CPU and memory metrics at 04:25 and 05:05.\n- Fix applied: Morphline config updated to use `rounding: mathematical`; Flume agent restarted.\n- Additional findings: High disk queue on sdc caused delays in export jobs. Development team informed to rebalance disk-intensive processes.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201029-IM1332709.md"
        }
    },
    "388": {
        "page_content": "---\ntitle: Cloudera AlertPublisher Reported No Email Alerts During Vacation Period\ndescription: Investigation into missing Cloudera Manager alert emails. AlertPublisher logs and system email logs reviewed on node un5. No system error was found; the drop in alert volume was attributed to reduced system activity during a vacation period.\ntags:\n  - bigstreamer\n  - cloudera\n  - alertpublisher\n  - email alerts\n  - no alerts received\n  - alerting system\n  - un5\n  - mail.err\n  - mail.info\n  - cmf-mgmt\n  - logs\n  - alert publisher restart\n  - cloudera manager\n  - alert emails stopped\n  - log grep\n  - vacation period\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IMXXXXXXX\n  system: abc BigStreamer Alerting\n  service: Cloudera Manager AlertPublisher\n  alert_node: un5.bigdata.abc.gr\n  investigation_logs:\n    - /var/log/messages\n    - /var/log/mail.err\n    - /var/log/mail.info\n    - /var/log/cloudera-scm-alertpublisher\n    - mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out\n  root_cause: Drop in alerts was due to vacation period, not a system error\n  resolution: Verified system functionality; restarted AlertPublisher as precaution\n---\n# abc - IM1957832 - Cloudera AlertPublisher - No Email Alerts Observed\n## Description\nNo alerts from cloudera. We noticed that since yesterday morning, alerts have stopped coming from the cloudera manager.\nA test mail was sent today from the cloudera manager which came normally but the automatic mails we receive every day have stopped.\n## Actions Taken\n### 1. Log Inspection on un5 Node\n- Ssh to **un5**  Alert Publisher Node and check the following files for error:\n```\nless /var/log/messages\nless /var/log/mail.err \nless /var/log/mail.info \nless /var/log/cloudera-scm-alertpublisher\nless mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out\n```\n### 2. Historical Alert Frequency Analysis\n- Use the following commands as example to check the above log file in order to count how many alerts are in the logs and you can grep depending on month,date and the message. \n```bash\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Generated subject [Cloudera Alert]' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Cloudera Alert' | wc -l;done\nfor i in {20..31}; do echo 2022-08-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-08-${i} | grep 'Cloudera Alert' | wc -l;done\nfor i in {20..31}; do echo 2022-08-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-08-${i} | grep 'has become bad' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Collected new alert' | wc -l;done\nfor i in {20..31}; do echo 2022-08-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-08-${i} | grep 'Collected new alert' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'has become bad' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Cloudera Alert' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Collected new alert' | wc -l;done\n```\n### 3. Preventive Restart\n- _Preventively step_ Restart Alert Publisher through Cloudera Manager\n### Conclusion\nAlertPublisher was functioning normally. The temporary absence of automated email alerts was due to a drop in alert volume during a national vacation period. No system misconfiguration or failure was detected.\nFrom the above investigation we came up to the result that no issues were found with the email alerting system; the absence of alerts was due to reduced system activity during the vacation period.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20220901-IM1957832.md"
        }
    },
    "389": {
        "page_content": "---\ntitle: Missing Partition in refdata.rd_cells_load Table on 2022-01-20\ndescription: Root cause analysis and resolution for missing data partition in the `refdata.rd_cells_load` table for 2022-01-20. Covers partition inspection, file availability check in HDFS and SFTP, manual invocation of the ingestion script, and validation of successful load.\ntags:\n  - bigstreamer\n  - refdata\n  - rd_cells\n  - rd_cells_load\n  - partition missing\n  - manual reload\n  - ingestion\n  - file not loaded\n  - impala\n  - hdfs\n  - crontab\n  - refresh\n  - shell script\n  - 010_refData_Load.sh\n  - abc\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1773928\n  system: abc BigStreamer RefData ingestion\n  target_table: refdata.rd_cells_load\n  missing_partition: 20220120\n  script_used: /shared/abc/refdata/bin/010_refData_Load.sh\n  logs_path: /shared/abc/refdata/log/\n  source_file_path: /shared/vantage_ref-data/REF-DATA/\n  file_status:\n    - cells_20220120.csv.gz NOT_LOADED\n  resolution_status: partition 20220120 manually loaded via script\n---\n# abc - BigStreamer - IM1773928  - abc BigStreamer (refdata.rd_cells)\n## Description\nyesterday's file (2022-01-24) uploaded to the REF_DATA folder was not loaded\n## Actions Taken\n1. Check the size of current partition from Impala-Shell\n```bash\n[un-vip.bigdata.abc.gr:22222] default> show files in refdata.rd_cells_load partition (par_dt>='20220118');\n```\nexample output:\n```bash\n+----------------------------------------------------------------------------------------------+---------+-----------------+\n| Path                                                                                         | Size    | Partition       |\n+----------------------------------------------------------------------------------------------+---------+-----------------+\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220118/cells_20220118.csv | 44.00MB | par_dt=20220118 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220119/cells_20220119.csv | 44.12MB | par_dt=20220119 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220121/cells_20220121.csv | 43.72MB | par_dt=20220121 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220122/cells_20220122.csv | 43.64MB | par_dt=20220122 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220123/cells_20220123.csv | 43.59MB | par_dt=20220123 |\n+----------------------------------------------------------------------------------------------+---------+-----------------+\nFetched 5 row(s) in 0.02s\n```\nAs you can see there is a missing partition for 20220120.\n2. At first, lets try to reload the:\n```bash\n[un-vip.bigdata.abc.gr:22222] default> refresh refdata.rd_cells_load;\n```\n3. Repeat step #2 in ordet to check that the missing partition is fixed. If not keep on with the follwing steps:\n4. Check logs \n``` bash\n[intra@un2]$ less /shared/abc/refdata/log\n```\nNotice that there is nothing for partition 20220120.\nFirst lines of this log must be something like:\n```bash\n===[Sat Jan  1 00:05:01 EET 2022, 20211231 --> 20211231, 010_refData_Load.sh]===\nKINIT_INFO: 2022-01-01 00:05:01, check for valid kerberos ticket\n```\nAt /shared/vantage_ref-data/REF-DATA/ you will see the following lines:\n``` bash\n-rw-r--r--   1 vantagerd external  46258798 Jan 19 17:50 cells_20220119.csv.20220120.LOADED\n-rw-r--r--   1 vantagerd external  46289460 Jan 20 17:50 cells_20220120.csv.gz NOT_LOADED\n-rw-r--r--   1 vantagerd external  46258798 Jan 19 17:50 cells_20220121.csv.20220122.LOADED\n```\n5. So lets check 10_refData_Load.sh script and run this script for the missing partition \n```bash\n[intra@un2 bin]$ /shared/abc/refdata/bin/010_refData_Load.sh 20220120\n```\n6. Verify that the missing partion is loaded: \n```bash\n[un-vip.bigdata.abc.gr:22222] default> show files in refdata.rd_cells_load partition (par_dt>='20220118');\nQuery: show files in refdata.rd_cells_load partition (par_dt>='20220118')\n+----------------------------------------------------------------------------------------------+---------+-----------------+\n| Path                                                                                         | Size    | Partition       |\n+----------------------------------------------------------------------------------------------+---------+-----------------+\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220118/cells_20220118.csv | 44.00MB | par_dt=20220118 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220119/cells_20220119.csv | 44.12MB | par_dt=20220119 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220120/cells_20220120.csv | 44.15MB | par_dt=20220120 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220121/cells_20220121.csv | 43.72MB | par_dt=20220121 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220122/cells_20220122.csv | 43.64MB | par_dt=20220122 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220123/cells_20220123.csv | 43.59MB | par_dt=20220123 |\n+----------------------------------------------------------------------------------------------+---------+-----------------+\nFetched 6 row(s) in 0.02s\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20220121-\u0399\u039c1773928.md"
        }
    },
    "390": {
        "page_content": "---\ntitle: brond_adsl/vdsl_stats_week Tables Missing Data Due to Filename Format Issue and SFTP Gaps\ndescription: Root cause analysis and recovery steps for missing partitions in `brond.brond_adsl_stats_week` and `brond.brond_vdsl_stats_week` on 01/01 and 02/01. Covers verification of upstream source tables, incorrect file patterns, log inspection, and communication with abc regarding missing VDSL files on SFTP.\ntags:\n  - bigstreamer\n  - brond\n  - adsl_stats_week\n  - vdsl_stats_week\n  - missing data\n  - partition missing\n  - file pattern\n  - gz files\n  - sftp\n  - filename format\n  - log check\n  - dsl\n  - automated load\n  - crontab\n  - abc\n  - DataParser\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD1811951\n  system: abc BigStreamer DSL Weekly Stats\n  target_tables:\n    - brond.brond_adsl_stats_week\n    - brond.brond_vdsl_stats_week\n  missing_partitions:\n    - 20230101\n    - 20230102\n  file_naming_expected: DWH_ADSL.<number>_YYYY_MM_DD.csv.gz\n  file_naming_received: DWH_ADSL_YYYY_MM_DD.csv.gz\n  file_location: /ez/landingzone/brond_dsl_stats/\n  ingestion_logs: /shared/abc/brond_dsl_stats/DataParser/scripts/log/\n  resolution_status: awaiting correct SFTP file upload from abc\n---\n# abc - Bigstreamer - SD1811951 - brond.brond_adsl/vdsl_stats_week \n## Description\nPlease load the tables for 01/01 and 02/01 with today's data available\nbrond.brond_adsl_stats_week\nbrond.brond_vdsl_stats_week\n## Actions Taken\n### 1. Verify Data Presence in Weekly Stats Tables\n1. ssh un2;\n2. sudo -iu intra; secimp\n3. Check that source tables brond.brond_vdsl_stats_week and brond.brond_adsl_stats_week have data for these partitions.\n```sql\nselect count(*), par_dt from brond.brond_vdsl_stats_week where par_dt >= 'xxxxx' group by 2 order by 2;\nselect count(*), par_dt from brond.brond_adsl_stats_week where par_dt >= 'xxxxx' group by 2 order by 2;\nexit;\n```\n### 2. Inspect Ingestion Logs and Filename Formats\n4. cd /shared/abc/brond_dsl_stats/DataParser/scripts/log/ ; check logs of the missing date\n5. less load_dsl_stats.missing_date.log\n### 3. Identify Filename Pattern Issues and Missing SFTP Files\n6. The filename should be like DWH_ADSL.number_year_month_day.csv.gz but for the missing data was DWH_ADSL_year_month_day.csv.gz. Furthermore, VDSL files did missing on the sftp server.\n### 4. Notify abc and Wait for Correct File Upload\n7. Inform abc for missing files on sftp server.\n### 5. Prepare for Auto-Recovery via Workflow Re-run\n8. The next execution will be automated get the files and create the missing partitions. If not the check if any DWH_ file with wrong pattern exist under /ez/landingzone/brond_dsl_stats/ . If yes remove it and re-run the workflow.\n9. When step 7 completed continue with https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/20210122-IM1421557.md\n## Affected Systems\nabc Bigstreamer\nAction points\nhttps://metis.xyztel.com/obss/oss/sysadmin-group/support/-/issues/43",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20220103-SD1811951.md"
        }
    },
    "391": {
        "page_content": "---\ntitle: StreamSets \u2013 Missing Data in open_weather_map.openweathermap_final\ndescription: Procedure to investigate and fix reduced daily ingestion in the `open_weather_map.openweathermap_final` table via StreamSets pipeline tuning. Includes parameter updates in SFTP and Hadoop FS processors.\ntags:\n  - streamsets\n  - open_weather_map\n  - openweathermap_final\n  - missing data\n  - ingestion\n  - sftp\n  - hadoop fs\n  - batch size\n  - idle timeout\n  - pipeline tuning\n  - abc bigstreamer\n  - weather pipeline\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1599907\n  system: abc BigStreamer StreamSets\n  detection_target: openweathermap_final record count drop\n  pipeline_components:\n    - SFTP FTP Client\n    - Hadoop FS 1\n    - Hadoop FS 2\n  pipeline_path: streamsets > open_weather_map pipeline\n  observed_issue: daily row count drop after 2024-06-24\n---\n# abc - BigStreamer - IM1599907 - Streamsets : Missing Data - open_weather_map.openweathermap_final\n## Description\nAs of 6/24 open_weather_map.openweathermap_final has very few entries daily\n- Low or zero daily records in `open_weather_map.openweathermap_final`\n- Weather data ingestion drop\n- StreamSets pipeline SFTP misconfiguration\n- HDFS writing timeout issues in weather flow\n## Actions Taken\n1. Changed on the `SFTP FTP Client bullet(Tab)` `Max Batch Size(records)` from `1000` to `100000` and `Batch Wait Time(ms)` from `1000` to `30000`\n2. On the `Hadoop-FS 1` and `Hadoop-FS 2` bullets changed the `Idle Timeout` from `${1 * MINUTES}` to `${5 * MINUTES}`\n## Affected Systems\nabc Bigstreamer Streamsets\n## Action Points\nReference from devs:\nhttps://metis.xyztel.com/obss/bigdata/abc/devops/devops-projects/-/issues/58#nfgh_44105",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210804-IM1599907.md"
        }
    },
    "392": {
        "page_content": "---\ntitle: CDSW Outage Due to Disk I/O Saturation on mncdsw1 Node\ndescription: Investigation into recurring CDSW service failures caused by high disk I/O on `/var/lib/cdsw` (dm-7) resulting in etcd timeouts and failure of Kubernetes control plane pods. Logs show loss of apiserver connectivity and persistent Sunday morning crashes.\ntags:\n  - bigstreamer\n  - cdsw\n  - mncdsw1\n  - etcd\n  - disk io\n  - kubelet\n  - control plane\n  - kubernetes\n  - cdsw pods\n  - grpc errors\n  - liveness probe\n  - unexpected error\n  - /var/lib/cdsw\n  - pv nfs\n  - project storage\n  - sunday crash\n  - root cause analysis\n  - cluster instability\n  - application unresponsive\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2073052\n  system: abc BigStreamer CDSW\n  root_cause: High disk I/O on /var/lib/cdsw (dm-7) leading to etcd timeout and Kubernetes control plane failure\n  node_affected: mncdsw1.bigdata.abc.gr\n  storage_path: /var/lib/cdsw\n  symptom: \"Unexpected error\" in UI and CDSW pods check failure\n  recurrence: Every Sunday ~06:00 based on `/var/log/warn` and `stderr.log`\n  affected_component: Kubernetes etcd / apiserver / control plane pods\n  action_taken: Restart CDSW, check logs, raise architecture issue\n---\n# abc - BigStreamer - IM2073052 - CDSW Crash Due to Disk I/O on mncdsw1\n## Description\nThe error \"Unexpected Error\" presented to users was traced to an etcd failure caused by high disk I/O on the CDSW storage volume. Despite the node mncdsw1 restarting and appearing healthy, CDSW services were unable to synchronize due to control plane issues.\nIn CDSW we get error \"Unexpected Error. An unexpected error occurred\" when connecting. We saw that the node mncdsw1.bigdata.abc.gr was down. We did a restrart, just that, and it now appears to be in good health status.\nHowever, we still get the same error.\nIn CDSW status it has the following message:\n```\nFailed to run CDSW Nodes Check. * Failed to run CDSW system pods check. * Failed to run CDSW application pods check. * Failed to run CDSW services check. * Failed to run CDSW secrets check. * Failed to run CDSW persistent volumes check. * Failed to run...\n```\n## Actions Taken\n### 1. Restart CDSW via Cloudera Manager\n1. Restart CDSW\nThe customer had already restarted CDSW, so we tried it once more in order to live monitor it.\n```bash\nCloudera Manager -> CDSW -> Restart\n```\n### 2. Monitor Startup Logs with cdsw status\n2. Check status\nWe followed the logs until CDSW was available again.\n```bash\n#from mncdsw1\ncdsw status\n...\nCloudera Data Science Workbench is ready!\n```\nSince CDSW was up and running, we continued with root cause analysis.\n### 3. Inspect /var/log/cdsw/cdsw_health.log for Errors\n3. Check logs\n```bash\nless /var/log/cdsw/cdsw_health.log\n```\nFirstly, we noticed an abnormal behavior with some of the control plane pods:\n```bash\n2023-01-29 05:50:53,868 ERROR cdsw.status:Pods not ready in cluster kube-system ['component/kube-controller-manager', 'component/kube-scheduler'].\n```\nAnd after that, CDSW lost connection with apiserver pod completely:\n```bash\n2023-01-29 05:51:42,392 WARNING urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549bb50>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\n2023-01-29 05:51:42,735 WARNING urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b710>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\n2023-01-29 05:51:43,065 WARNING urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b050>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\n2023-01-29 05:51:43,371 ERROR cdsw.status:Failed to run CDSW Nodes Check.\n```\n### 4. Investigate Disk I/O Saturation on /var/lib/cdsw\n4. Check node resources\nFrom Cloudera Manager we saw that CPU and Memory were not increased but Disk I/O reached 100%.\n![IM2073052_diskio](.media/IM2073052_diskio.png)\nFrom the image above we noticed that the issue occured on dm-7.\n```bash\n[root@mncdsw1 ~]# ll /dev/mapper/cdsw-var_lib_cdsw\nlrwxrwxrwx 1 root root 7 Dec 16  2021 /dev/mapper/cdsw-var_lib_cdsw -> ../dm-7\n```\n```bash\n[root@mncdsw1 ~]# lsblk | grep cdsw-var\n\u2514\u2500cdsw-var_lib_cdsw                                                                         253:7    0   931G  0 lvm  /var/lib/cdsw\n```\n```bash\n[root@mncdsw1 ~]# kubectl get pv\nNAME\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CAPACITY\u00a0\u00a0 ACCESS MODES\u00a0\u00a0 RECLAIM POLICY\u00a0\u00a0 STATUS\u00a0\u00a0 CLAIM\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 STORAGECLASS\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 REASON\u00a0\u00a0 AGE\n0c9df8bb\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1Ti\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RWX\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Retain\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Bound\u00a0\u00a0\u00a0 default-user-120/b128af5f\u00a0\u00a0 cdsw-storageclass-whiteout\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 83m\n1214923b\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1Ti\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RWX\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Retain\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Bound\u00a0\u00a0\u00a0 default-user-98/1ec1e99a\u00a0\u00a0\u00a0 cdsw-storageclass-whiteout\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 11m\n1297834a\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1Ti\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RWX\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Retain\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Bound\u00a0\u00a0\u00a0 default-user-9/740094c3\u00a0\u00a0\u00a0\u00a0 cdsw-storageclass-whiteout\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 54s\n1a2f7a8a\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1Ti\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RWX\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Retain\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Bound\u00a0\u00a0\u00a0 default-user-9/92acb87f\u00a0\u00a0\u00a0\u00a0 cdsw-storageclass-whiteout\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 55s\n1f498fe8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1Ti\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RWX\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Retain\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Bound\u00a0\u00a0\u00a0 default-user-120/588500de\u00a0\u00a0 cdsw-storageclass-whiteout\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 106s \n```\n```bash\n[root@mncdsw1 ~]# kubectl get pv 0c9df8bb -o yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n   name: 0c9df8bb\nspec:\n   accessModes:\n   - ReadWriteMany\n   capacity:\n     storage: 1Ti\n   mountOptions:\n   - nfsvers=4.1\n   nfs:\n      path: /var/lib/cdsw/current/projects/cdn/4xsyzsv0lnij00ob\n      server: 10.255.241.130\n   persistentVolumeReclaimPolicy: Retain\n     storageClassName: cdsw-storageclass-whiteout\n     volumeMode: Filesystem \n```\nIt seems that every CDSW project uses mncdsw1:/var/lib/cdsw for storage.\n5. Check kubelet logs\n```bash\nll /run/cloudera-scm-agent/process/ | grep -i master\n```\n```bash\n[root@mncdsw1 ~]# ll /run/cloudera-scm-agent/process/145081-cdsw-CDSW_MASTER/logs/\ntotal 111880\n-rw-r--r-- 1 root root  9658036 Jan 30 10:24 stderr.log\n-rw-r--r-- 1 root root 10485841 Jan 30 05:42 stderr.log.1\n-rw-r--r-- 1 root root 10485989 Jan  4 19:40 stderr.log.10\n-rw-r--r-- 1 root root 10485928 Jan 30 00:20 stderr.log.2\n-rw-r--r-- 1 root root 10486166 Jan 29 18:58 stderr.log.3\n-rw-r--r-- 1 root root 10485841 Jan 29 13:36 stderr.log.4\n-rw-r--r-- 1 root root 10485790 Jan 29 08:06 stderr.log.5\n-rw-r--r-- 1 root root 10485858 Jan 25 16:41 stderr.log.6\n-rw-r--r-- 1 root root 10485835 Jan 21 08:56 stderr.log.7\n-rw-r--r-- 1 root root 10485760 Jan 15 14:47 stderr.log.8\n-rw-r--r-- 1 root root 10485805 Jan 10 11:57 stderr.log.9\n-rw-r--r-- 1 root root    12055 Nov 21 14:58 stdout.log\n```\n### 5. Analyze kubelet Logs and etcd Failures\nIn stderr.log.5 file there were many log entries indicating a problem with etcd.\n```bash\nI0129 05:50:11.022246   89953 prober.go:117] Liveness probe for \"etcd-mncdsw1.bigdata.abc.gr_kube-system(ef618d8c591c98ed7bd7d66b177d34f7):etcd\" failed (failure): HTTP probe failed with statuscode: 503\n```\n```bash\nE0129 05:51:22.881553   89953 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"etcd-mncdsw1.bigdata.abc.gr.17299b09446544c4\", GenerateName:\"\", Namespace:\"kube-system\", SelfLink:\"\", UID:\"\", ResourceVersion:\"27938507\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"etcd-mncdsw1.bigdata.abc.gr\", UID:\"ef618d8c591c98ed7bd7d66b177d34f7\", APIVersion:\"v1\", ResourceVersion:\"\", FieldPath:\"spec.containers{etcd}\"}, Reason:\"Unhealthy\", Message:\"Liveness probe failed: HTTP probe failed with statuscode: 503\", Source:v1.EventSource{Component:\"kubelet\", Host:\"mncdsw1.bigdata.abc.gr\"}, FirstTimestamp:time.Date(2022, time.November, 21, 15, 0, 1, 0, time.Local), LastTimestamp:time.Date(2023, time.January, 29, 5, 50, 41, 21788692, time.Local), Count:700, Type:\"Warning\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'rpc error: code = Unknown desc = OK: HTTP status code 200; transport: missing content-type field' (will not retry!)\n```\nApparently the high DiskI/O was affecting the etcd server.\n6. Check warn file logs\n```bash\nless /var/log/warn\n...\nJan 29 05:51:22 mncdsw1.bigdata.abc.gr dockerd[86247]: W0129 03:51:22.867126\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {https://127.0.0.1:2379\u00a0 <nil> 0 <nil>}. E\nrr :connection error: desc = \"transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused\". Reconnecting..\n```\n### 6. Review /var/log/warn for Recurring Errors\nRun the following to check if this error occured in the past.\n```bash\ncat /var/log/warn | grep\u00a0 -e \"Jan.*29.*grpc: addrConn.createTransport failed to connect to\" | less\n```\n![IM2073052_warn_logs1](.media/IM2073052_warn_logs1.png)\n![IM2073052_warn_logs2](.media/IM2073052_warn_logs2.png)\nThe errors appear every Sunday morning.\n## Our Ticket Response\nfrom the analysis we saw that CDSW crashed as there was a problem in the Control Plane pods of the Kubernetes Cluster in which CDSW is deployed.  We notice in the logs that the problem started with timeouts in requests to the etcd of the cluster, which seem to be due to a high Disk I/O of the sdb disk of mncdsw1 at that moment. As a result we have the inability to synchronize the control plane pods with the base of the cluster, which led to their termination and by extension the entire service. Attached you will also find the screenshot that describes the high Disk I/O at that time.\nContinuing the analysis we noticed that this behavior is periodic, and more specifically it happens every Sunday at 6 am. Attached you will also find the screenshots that show that there was the same problem on January 15, 22 and 29. Before January the logs are clean. Could you tell us if you have any job set up every Sunday morning that needs a lot of Disk I/O?\n## Action Points\nWe opened [this](https://metis.xyztel.com/obss/oss/sysadmin-group/abc/BigStreamer/bigstreamer/-/issues/25) issue to re-evaluate CDSW disk architecture.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230130-IM2073052.md"
        }
    },
    "393": {
        "page_content": "---\ntitle: Investigating Missing Syslog Logs from abc Server\ndescription: Troubleshooting procedure for missing or irregular syslog logs received from server 172.25.37.236 (abc) on BigStreamer un2 node during July 23\u201327. Includes configuration and logrotate checks.\ntags:\n  - syslog\n  - rsyslog\n  - logrotate\n  - bigstreamer\n  - missing-logs\n  - incident\n  - un2\n  - abc\n  - monitoring\nlast_updated: 2025-05-01\nauthor: u27\ncontext:\n  environment: BigStreamer\n  node: un2\n  server_ip: 172.25.37.236\n  log_receiver: abc syslog\n  issue_id: IM1299104\n  timeframe: 2020-07-23 to 2020-07-27\n  reported_by: abc syslog administrators\n  status: Open\n---\n# Missing Logs from abc Syslog (un2)\n## Description\nThe abc syslog administrators reported that server `172.25.37.236` showed significantly lower and irregular log reception from July 23 to July 27. This procedure outlines the steps taken to investigate and identify potential causes for the missing logs.\n## Actions Taken\n1. SSH into the `un2` node as `root`:\n```bash\nssh root@un2\n```\n2. Check rsyslog configuration to verify the destination and filtering rules:\n```bash\ncat /etc/rsyslog.conf | more\n```\n- Only `abc` servers are listed in this configuration.\n3. Check log rotation settings that may have affected log visibility:\n```bash\ncat /etc/logrotate.conf | more\n```\n- Verify if logs were rotated or deleted.\n## Keywords\n- logs\n- rsyslog\n- missing logs\n- abc server\n- logrotate\n- BigStreamer",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201001-IM1299104.md"
        }
    },
    "394": {
        "page_content": "---\ntitle: Prometheus Table `dwh22_last` Empty \u2013 Cron Job Debug & Reload\ndescription: Investigation and resolution steps for the empty `prometheus.dwh22_last` table and downstream view `prometheus.prom_total_subscrs`. Includes cron validation, log inspection, manual script rerun with parameter substitution, and data verification queries.\ntags:\n  - bigstreamer\n  - prometheus\n  - dwh22_last\n  - prom_total_subscrs\n  - empty table\n  - missing data\n  - cronjob\n  - hive\n  - impala\n  - data pipeline\n  - historical table\n  - last table\n  - reload\n  - manual rerun\n  - script override\n  - log inspection\n  - daily partition\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: N/A\n  system: abc BigStreamer Prometheus Load\n  failure_target: prometheus.dwh22_last (and prom_total_subscrs view)\n  trigger: empty partition on `dwh22_last`\n  script: /shared/abc/prometheus/bin/Cron_Prometheus_Load.sh\n  log_dir: /shared/abc/prometheus/log/\n  root_cause: cron failed or did not run for target date\n  recovery_method: manual override of yesterday_dt and re-run of load script\n  verification_query: select count(*), par_dt from prometheus.dwh22_last group by par_dt;\n---\n# abc - BigStreamer/BackEnd  - prometheus.dwh22_last empty \nThis document outlines how to resolve an empty prometheus.dwh22_last table due to a missed or failed cron execution, including log investigation, manual script rerun with date override, and post-load verification.\n## Actions Taken\n### Step 1 \u2013 Identify and Validate Cron Schedule\n1. ssh un2 with your personal account; sudo -iu intra\n### Step 2 \u2013 Check Script Execution Log\n2. crontab -l | grep prometheus\n```bash\n0 6 * * * /shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.`date '+\\%Y\\%m\\%d'`.log 2>&1\n```\n### Step 3 \u2013 Rerun Cron Manually with Date Override (If Needed)\n3. Check the latest log file to find the root cause `/shared/abc/prometheus/log/Cron_Prometheus_Load.date_of_issue.log`\n### Step 4 \u2013 Revert Temporary `yesterday_dt` Override\n4. If the issue date is today (i.e., partition not yet generated), simply re-run the script as is: `/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.issue_date.log`\nIf the issue date passed then comment the `yesterday_dt=` and replace it with `yesterday_dt=<issue date -1>`. Then run the script ``/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.issue_date.log``\n### Step 5 \u2013 Validate Load via SQL\n5. When the script finished replace the old value of `yestarday_dt` on script.\n6. Checks:\n```sql\nselect count(*),par_dt from prometheus.table where par_dt >= 'issue_date -1' group by 2;\n```\n## Affected Systems\nabc Bigstreamer Prometheus Tables\n```\nprometheus.DWH22_hist - IMPALA\nprometheus.DWH22_last - IMPALA\nrometheus.dwh3_hist - HIVE\nprometheus.DWH3_hist - IMPALA\nprometheus.DWH3_last - IMPALA\nprometheus.dwh4_hist - HIVE\nprometheus.DWH4_hist  - IMPALA\nprometheus.DWH4_last - IMPALA\nprometheus.dwh9_hist - HIVE\nprometheus.DWH9_hist - IMPALA\nprometheus.DWH9_last - IMPALA\nprometheus.dwh11_hist - HIVE\nprometheus.DWH11_hist - IMPALA\nprometheus.DWH11_last - IMPALA\nprometheus.dwh14_hist - HIVE\nprometheus.DWH14_hist - IMPALA\nprometheus.DWH14_last - IMPALA\nprometheus.dwh17_hist - HIVE\nprometheus.DWH17_hist - IMPALA\nprometheus.DWH17_last - IMPALA\nprometheus.dwh2_hist - HIVE\nprometheus.DWH2_hist - IMPALA\nprometheus.DWH2_last - IMPALA\nprometheus.dwh43_hist - HIVE\nprometheus.DWH43_hist - IMPALA\nprometheus.DWH43_last - IMPALA\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20211215-IM1742741.md"
        }
    },
    "395": {
        "page_content": "---\ntitle: Disable Root SSH Login and Migrate Backup Scripts to Non-Root User\ndescription: Step-by-step security hardening guide for BigStreamer environments to disable root SSH login, migrate backup and sync scripts to a dedicated backup user, configure SSH permissions via SaltStack, and assess existing SSH port forwarding usage across all nodes.\ntags:\n  - ssh\n  - saltstack\n  - sysadmin\n  - sshd_config\n  - root-login\n  - security-hardening\n  - acls\n  - backup-user\n  - port-forwarding\n  - bigstreamer\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  hosts:\n    - admin.bigdata.abc.gr\n    - un1.bigdata.abc.gr\n    - wrkmncdsw1.bigdata.abc.gr\n  user:\n    root: disallowed\n    backup_user: created\n  files_updated:\n    - /etc/ssh/sshd_config\n    - various cron-based backup scripts\n  tools:\n    - saltstack\n    - rsync\n    - sshd\n  scripts_migrated:\n    - CM_Config_Backup.sh\n    - MySQL_Dump_All_DBs.sh\n    - dfs-backup.sh\n    - cdsw_rsync_backup.sh\n    - MySQL_Dump_spec_DB.sh\n---\n# Disabling Root SSH Login on BigStreamer Nodes - GI7 Ticket - abc Security Requirement Implementation\n## Description\nSecurity requirements from abc mandates that we should change the ssh configuration in all servers in order not to permit root ssh login\n## Actions Taken\n1. Login to `admin.bigdata.abc.gr` with personal account and change to root with sudo\n2. Inspect the status of sshd on all nodes\n```bash\nsalt '*' cmd.run 'grep \"^PermitRootLogin\" /etc/ssh/sshd_config'\n```\n3. Inspect cronjobs of root user on all nodes\n```bash\nsalt '*' cmd.run 'cronjob -l'\n```\n4. After collecting the scripts executed by root user, we checked the scripts that contain the words `ssh`, `scp` and `rsync`\n```bash\nfor i in {'/root/disk_balance_with_check.sh' '/usr/local/bin/CM_Config_Backup.sh' '/etc/elasticsearch/elasticsearch_monitoring.sh' '/etc/keepalived/scripts/mysql_check_crontab.sh' '/home/intra/scripts/MySQL_Dump_All_DBs.sh' '/usr/local/bin/krb5prop.sh' '/root/scripts/ldap_check.sh' '/root/send_haproxy_statistics.sh' '/root/send_haproxy_weekly_statistics.sh' '/home/intra/MySQL_Dump_All_DBs.sh' '/home/intra/dfs-backup.sh' '/usr/lib/icom/scripts/cdsw_rsync_backup.sh' '/usr/lib/icom/scripts/cdsw_tar_backup.sh' '/home/intra/MySQL_Dump_spec_DB.sh' '/home/intra/scripts/hue_workflows_all.sh' '/root/hive_logs_retention.sh'}\ndo\nsalt \"*\" cmd.run \"if [ -f $i ]; then grep -e ssh -e scp -e rsync $i; fi\" | grep -B1 -e ssh -e scp -e rsync\ndone\n```\n5. Configure ACLs for backup_user. User `backup_user` was created by abc admins to substitute the root login in the scripts above. From the above investigation we determined that `backup_user` needs to be able to login to nodes `un1.bigdata.abc.gr` and `wrkmncdsw1.bigdata.abc.gr`\n```bash\n  # un1.bigdata.abc.gr\n  setfacl -R -m d:u:backup_user:rwx /data/1/cm_backup/cmdeploys/\n  setfacl -R -m u:backup_user:rwx /data/1/cm_backup/cmdeploys/\n  setfacl -R -m d:u:backup_user:rwx /data/1/cm_backup/db-backups/db-vip\n  setfacl -R -m u:backup_user:rwx /data/1/cm_backup/db-backups/db-vip\n  setfacl -R -m u:backup_user:rwx /data/1/cm_backup/dfs_backup/\n  setfacl -R -m d:u:backup_user:rwx /data/1/cm_backup/dfs_backup/\n  # Add backup_user to AllowedGroups in /etc/ssh/sshd_config\n```\n```bash\n  # wrkmncdsw1.bigdata.abc.gr\n  setfacl -R -m u:backup_user:rwx /backup\n  setfacl -R -m d:u:backup_user:rwx /backup\n  # Add backup_user to AllowedGroups in /etc/ssh/sshd_config\n```\n6. Setup SSH Access and Keys. Created `backup user` saltstack state that installs a new private key for passwordless ssh under `/root/backup_user_id/id_backup_user_rsa` on all nodes\n7. Update Cron and Backup Scripts. Change the following scripts to use `backup_user` instead of `root`:\n- /usr/local/bin/CM_Config_Backup.sh\n- /home/intra/scripts/MySQL_Dump_All_DBs.sh\n- /home/intra/MySQL_Dump_All_DBs.sh\n- /home/intra/dfs-backup.sh\n- /usr/lib/icom/scripts/cdsw_rsync_backup.sh\n```conf\n# This script uses rsync. For rsync to use another user than the logged in one create /root/.ssh/config with the following contents:\nHost wrkcdsw1.bigdata.abc.gr\n    User backup_user\n    IdentityFile /root/backup_user_id/id_backup_user_rsa\nHost wrkcdsw1.bigdata.abc.gr\n    User root \n    IdentityFile ~/.ssh/id_rsa\n```\n- /home/intra/MySQL_Dump_spec_DB.sh\n8. Enforce SSH Policy with SaltStack. Change `PermitRootLogin` on all hosts. The actions mentioned below are executed as `root` from `admin.bigdata.abc.gr`\n- Get `/etc/ssh/sshd_config to a uniform state\nContents of `/etc/salt/salt/prepare_sshd_config.sh`:\n```bash\ngrep -e \"^PermitRootLogin\" /etc/ssh/sshd_config &> /dev/null\ni=$?\nif [ $i -eq 1 ]; then\n  echo \"The config file is ok\"\nelse \n  sed -i -e 's/^#PermitRootLogin/PermitRootLogin/' /etc/ssh/sshd_config # From step 2 we know that on some files the entry was commented\nfi\n```\n- Get `/etc/ssh/sshd_config to a uniform state\nContents of `/etc/salt/salt/disable_root_login.sh`:\n```bash\ngrep -e \"^PermitRootLogin no\" /etc/ssh/sshd_config &> /dev/null\nif [ $i -eq 0 ]; then \n  echo \"The config file is ok\"\nelse \n  sed -i -e 's/^PermitRootLogin.*yes/PermitRootLogin no/' /etc/ssh/sshd_config\nfi\n```\n- Apply the two scripts on all nodes and reload sshd:\n```bash\nsalt '*' cmd.script salt://prepare_sshd_config.sh\nsalt '*' cmd.script salt://disable_root_login.sh\nsalt '*' cmd.run 'service sshd reload'\n```\n## Affected Systems\nabc Bigstreamer OS\n## Action Points\nWhile investigating the impact of disallowing the root ssh login, we found the following port forwards:\n```bash\nssh -g -f gbenet@unekl2 -L 18636:10.255.240.20:3306 -N\nssh -g -f gbenet@admin -L 8889:admin:5900 -N\nssh -g -f u15@admin -L 8888:172.25.37.237:3000 -N\nssh -g -f gbenet@unc1 -L 8743:172.25.37.241:8743 -N\nssh -g -f gbenet@unc1 -L 9743:172.25.37.241:8743 -N\nssh -g -f root@omnm -L 8888:omnm:5901 -N\nssh -g -f root@hedge1 -L 8998:10.255.240.142:8998 -N\nssh -g -f intra@un2 -L 2525:172.18.20.205:25 -N\nssh -g -f intra@un2 -L 22255:un1:22222 -N\nssh -g -f intra@un2 -L 22255:un1:22222 -N\nssh -g -f u15@un2 -L 6536:172.25.150.68:5432 -N\nssh -g -f gbenet@un2 -L 227:undt1:8522 -N\nssh -g -f root@mncdsw1 -L 5555:172.19.53.146:5555 -N\nssh -g -f gbenet@undt2 -L 21050:10.255.241.239:3306 -N\nssh -g -f u3@undt1 -L 9191:10.95.129.200:9191 -N\nssh -g -f gbenet@undt1 -L 9191:10.95.129.200:9191 -N\nssh -g -f gbenet@undt1 -L 9621:10.53.166.37:1521 -N\nssh -g -f gbenet@undt1 -L 4040:10.255.241.220:3306 -N\nssh -g -f gbenet@undt1 -L 9521:10.53.192.187:1521 -N\nssh -g -f gbenet@undt1 -L 21050:10.53.192.187:1521 -N\nssh -g -f gbenet@undt1 -L 3579:172.26.131.15:3579 -N\nssh -g -f gbenet@undt1 -L 8521:10.53.192.192:1521 -N\nssh -g -f gbenet@undt1 -L 8522:10.53.192.191:1521 -N\nssh -g -f gbenet@undt1 -L 8523:10.53.192.190:1521 -N\nssh -g -f gbenet@un1 -L 3579:172.26.131.15:3579 -N\nssh -g -f intra@un1 -L 6654:10.255.240.20:3306 -N\nssh -g -f intra@un1 -L 6634:10.101.1.230:1521 -N\nssh -g -f intra@un1 -L 6433:172.16.109.237:1433 -N\nssh -g -f intra@un1 -L 7536:10.255.241.239:3306 -N\nssh -g -f intra@un1 -L 6721:172.21.4.68:1521 -N\nssh -g -f intra@un1 -L 3389:10.101.6.41:389 -N\nssh -g -f intra@un1 -L 6644:10.101.16.169:1521 -N\nssh -g -f intra@un1 -L 6633:10.255.240.13:6533 -N\nssh -g -f intra@un1 -L 7535:172.24.104.100:5432 -N\nssh -g -f intra@un1 -L 6646:10.95.129.43:1521 -N\nssh -g -f intra@un1 -L 5525:172.18.20.205:25 -N\nssh -g -f intra@un1 -L 6645:10.95.129.41:1521 -N\nssh -g -f intra@un1 -L 21060:sn38:21050 -N\nssh -g -f intra@un1 -L 7183:10.255.243.215:80 -N\nssh -g -f intra@un1 -L 25020:sn88:25020 -N\nssh -g -f ipvpn@un1 -L 7180:undt2:22 -N\nssh -g -f ipvpn@un1 -L 6531:172.25.119.82:1521 -N\nssh -g -f gbenet@un1 -L 8888:undt2:22 -N\n```\nBoth `intra` and `root` are no longer allowed to ssh and in a future restart most of the above will not be able to be implemented in the same way.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201012-GI7.md"
        }
    },
    "396": {
        "page_content": "---\ntitle: Reduction in sai.voice_quality_hist Table Entries Due to Fewer Source Files\ndescription: Investigation and root cause analysis of a drop in row counts for the `sai.voice_quality_hist` table from 2021-11-23 onward. Covers log analysis, file volume comparison between dates, validation of successful processing via `.LOADED` suffixes, and confirmation that the issue originated from the source system sending fewer files.\ntags:\n  - bigstreamer\n  - voice_quality_hist\n  - voice_quality\n  - sai\n  - row count drop\n  - traffica\n  - abc\n  - data ingestion\n  - file count\n  - fgh\n  - dataparser\n  - missing rows\n  - raw data volume\n  - daily load\n  - trn file\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD1780064\n  sub_issue_id: IM1726312\n  system: abc BigStreamer Traffica voice quality ingestion\n  detection_target: sai.voice_quality_hist\n  start_drop_date: 2021-11-23\n  root_cause: Fewer files received from source system (fgh)\n  data_path: /data/1/trafficaftp/Traffica/\n  trn_file: /shared/abc/traffica/DataParser_voice_quality/scripts/transferlist/01_traffica_voice_quality.trn\n  file_pattern: TrafRTTE_Voice_Quality_abc*\n  file_suffix: .dat\n  success_suffix: .LOADED\n  ingestion_frequency: daily (rotates 7-day data)\n---\n# abc - BigStreamer/backend - SD1780064 - IM1726312 - Reduction in sai.voice_quality_hist Table Entries Due to Fewer Source Files\n## Description\nThis case investigates a noticeable drop in rows in sai.voice_quality_hist from 2021-11-23 due to fewer raw .dat files sent by the external system fgh.\n## Actions Taken\n### Flow Information (TRN File)\nGet info about flow from \"trn\" file:\nmore unc2:/shared/abc/traffica/DataParser_voice_quality/scripts/transferlist/01_traffica_voice_quality.trn\n- files incomming directly from fgh\n- under  spool_area=\"/data/1/trafficaftp/Traffica\"\n- file_type=\"TrafRTTE_Voice_Quality_abc*\"\n- suffix=\".dat\"\n- load_suffix=\"LOADED\"\nnfgh: all files with the \".LOADED\" extention have been successfully processed.\ncheck only for \".dat\" files without the \".LOADED\" extention , to see if something was wrong.\n### Table Verification\nVerify for previous dates (table only has data for 7 days):\n```sql\nselect par_dt, count(*) cnt from sai.voice_quality_hist group by 1 order by 1;\npar_dt   | cnt     \n---------+---------\n20211122 | 24175236   <-- normal rows\n20211123 | 21502579\n20211124 | 17456051\n20211125 | 18409272\n20211126 | 18729343\n20211127 | 14049150\n20211128 | 10861604   <-- fewer rows\n20211129 | 17729519\n20211130 | 19021196\n20211201 |  9955414   <-- not completed yet (today)\n```\n### File Volume Comparison\nThen compare files from day with problem\neg:\n```bash\n[root@unc2 ~]# ll /data/1/trafficaftp/Traffica/TrafRTTE_Voice_Quality_abc_TRAVQTNES_20211128* | wc -l \n4658\n```\nand comrare with a good one :\n```bash\n[root@unc2 ~]# ll /data/1/trafficaftp/Traffica/TrafRTTE_Voice_Quality_abc_TRAVQTNES_20211121* | wc -l\n5764\n```\nalso compare rows contained in above files:\n```bash\n[intra@unc2 Traffica]$ cat /data/1/trafficaftp/Traffica/TrafRTTE_Voice_Quality_abc_TRAVQTNES_20211128* | wc -l\n11155058\n[intra@unc2 Traffica]$ cat /data/1/trafficaftp/Traffica/TrafRTTE_Voice_Quality_abc_TRAVQTNES_20211121* | wc -l \n16411173\n```\n### Root Cause Summary\nThe ingestion scripts ran without error. The reduced row count is directly correlated to a reduced number of input files from fgh. There is no local issue with processing or parsing. Source system must be consulted for missing data.\n## Our Ticket Response\nUpon checks, it was found that less raw data (files) have been sent\ncompared to previous days.\nIndicatively we received\n- Sunday 2021/11/21 : 5764 files, 16411173 rows\n- Sunday 2021/11/28 : 4658 files, 11155058 rows\nAlso no problem was found regarding\nloading the available (less) data into the given table\nPlease check the production of the raw data.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20211220-IM1726312.md"
        }
    },
    "397": {
        "page_content": "---\ntitle: RAN.AI Airflow Scheduler Failure Due to Longhorn PVC Issue\ndescription: Investigation and resolution of a pod failure in the RAN.AI Kubernetes namespace caused by stuck Longhorn PVC replicas, leading to `airflow-scheduler` entering CrashLoopBackOff state and failing to mount volumes. The issue was resolved by manually deleting the problematic PVC and confirming pod recovery.\ntags:\n  - airflow\n  - ran.ai\n  - pvc error\n  - kubernetes\n  - pod failure\n  - crashloopbackoff\n  - airflow-scheduler\n  - longhorn\n  - volume mount failure\n  - configmap\n  - longhorn ui\n  - kube events\n  - persistent volume\n  - instance-manager-r\n  - mke2fs\n  - pod init failure\n  - kubectl describe\n  - failedmount\n  - kubelet\n  - k8s storage bug\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2231148\n  system: RAN.AI Kubernetes cluster\n  root_cause: Airflow scheduler PVC replicas were stuck in a deleting state in Longhorn, preventing volume mount\n  component_affected: airflow-scheduler pod in namespace `ranai-geo`\n  resolution: Deleted the faulty PVC manually to allow pod recovery\n  tools_used:\n    - kubectl describe\n    - kubectl logs\n    - Longhorn UI\n    - Cloudera Manager\n    - PVC/Pod event inspection\n  recommendation: Monitor Longhorn for recurring PVC deletion issues; potentially upgrade or patch Longhorn\n---\n## Subject: pod failure at RAN.AI\nTicket Number: IM2231148\nPriority: High\nDate: 10-11-2023\n## Description\nThe `airflow-scheduler` pod in the RAN.AI Kubernetes namespace (`ranai-geo`) entered a `CrashLoopBackOff` state due to persistent volume (PVC) mount failures. Log events showed repeated errors related to mounting the `logs` volume, with specific references to device formatting failures (e.g., `mke2fs exit status 1`) and timeouts in the Kubernetes event log.\nDespite restarting the pod, it remained in an `Init:0/1` state. Investigations using `kubectl`, Longhorn UI, and `kubemaster` logs revealed that three PVC replicas were stuck in a deleting state, which blocked the volume from attaching correctly. This indicated a storage issue potentially linked to a bug in the Longhorn engine.\nThe airflow scheduler is down due to access rights in the logs folder.\npod airflow-scheduler-0 had entered a restart loop as shown below.\n```bash\nroot@kubemaster1:~# kubectl get pods -n ranai-geo\nNAME READY STATUS RESTARTS AGE\nairflow-postgresql-0 1/1 Running 0 326d\nairflow-scheduler-0 0/2 CrashLoopBackOff 970 (68s ago) 187d\nairflow-statsd-85d5d8768b-hgzzc 1/1 Running 0 326d\nairflow-webserver-6c8448476d-hs4nb 1/1 Running 0 187d\n```\nWe restarted the pod, but it entered Init:0/1 state again.\nLogs and events after pod restart are attached.\n```bash\nroot@kubemaster1:~# kubectl get pods -n ranai-geo\nNAME READY STATUS RESTARTS AGE\nairflow-postgresql-0 1/1 Running 0 326d\nairflow-scheduler-0 0/2 Init:0/1 0 67s\nairflow-statsd-85d5d8768b-hgzzc 1/1 Running 0 326d\nairflow-webserver-6c8448476d-hs4nb 1/1 Running 0 187d\nabc-prod-ranai-geo-be-f7f8fc5c4-rv66z 1/1 Running 0 25h\nabc-prod-ranai-geo-clustering-68975fb5b5-qvztf 1/1 Running 0 25h\nabc-prod-ranai-geo-fe-9c5c7bc7c-72fpn 1/1 Running 0 141d\nabc-prod-ranai-geo-postgres-0 1/1 Running 0 165d\nroot@kubemaster1:~# kubectl get events -n ranai-geo\n```\nLAST SEEN TYPE REASON OBJECT MESSAGE\n25m Warning BackOff pod/airflow-scheduler-0 Back-off restarting failed container\n35m Warning Unhealthy pod/airflow-scheduler-0 Liveness probe failed: Unable to load the config, contains a configuration error....\n21m Normal Scheduled pod/airflow-scheduler-0 Successfully assigned ranai-geo/airflow-scheduler-0 to kubeworker1.bigdata.abc.gr\n13m Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[kerberos-keytab connectors-config jssecacerts kube-api-access-2df5h config logs]: timed out waiting for the condition\n19m Warning FailedMount pod/airflow-scheduler-0 MountVolume.MountDevice failed for volume \"pvc-c826d577-e764-470e-9904-3986042810aa\" : rpc error: code = DeadlineExceeded desc = context deadline exceeded\n8m59s Warning FailedMount pod/airflow-scheduler-0 MountVolume.MountDevice failed for volume \"pvc-c826d577-e764-470e-9904-3986042810aa\" : rpc error: code = Internal desc = format of disk \"/dev/longhorn/pvc-c826d577-e764-470e-9904-3986042810aa\" failed: type:(\"ext4\") target:(\"/var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-c826d577-e764-470e-9904-3986042810aa/globalmount\") options:(\"defaults\") errcode:(exit status 1) output:(mke2fs 1.45.5 (07-Jan-2020)...\n9m26s Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[kube-api-access-2df5h config logs kerberos-keytab connectors-config jssecacerts]: timed out waiting for the condition\n15m Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[connectors-config jssecacerts kube-api-access-2df5h config logs kerberos-keytab]: timed out waiting for the condition\n9m24s Normal Scheduled pod/airflow-scheduler-0 Successfully assigned ranai-geo/airflow-scheduler-0 to kubeworker1.bigdata.abc.gr\n32s Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[connectors-config jssecacerts kube-api-access-nrb25 config logs kerberos-keytab]: timed out waiting for the condition\n2m58s Warning FailedMount pod/airflow-scheduler-0 MountVolume.MountDevice failed for volume \"pvc-c826d577-e764-470e-9904-3986042810aa\" : rpc error: code = Internal desc = format of disk \"/dev/longhorn/pvc-c826d577-e764-470e-9904-3986042810aa\" failed: type:(\"ext4\") target:(\"/var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-c826d577-e764-470e-9904-3986042810aa/globalmount\") options:(\"defaults\") errcode:(exit status 1) output:(mke2fs 1.45.5 (07-Jan-2020)...\n5m4s Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[jssecacerts kube-api-access-nrb25 config logs kerberos-keytab connectors-config]: timed out waiting for the condition\n2m47s Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[kerberos-keytab connectors-config jssecacerts kube-api-access-nrb25 config logs]: timed out waiting for the condition\n9m24s Normal SuccessfulCreate statefulset/airflow-scheduler create Pod airflow-scheduler-0 in StatefulSet airflow-scheduler successful\n## Investigation\nPod Initialization: The initial issue was identified as the airflow-scheduler pod getting stuck during initialization.\ncmd: `kubectl describe <airflow-scheduler-pod> -n ranai-geo`\n![error_mount](.media/mounterror.JPG)\nConfigmap Mounting Issues: Further examination of the pod's logs and configuration revealed that some required configmaps were failing to mount correctly, causing a disruption in the pod initialization process.\nIdentify the airflow pv:\n![pv](.media/getpv.JPG)\n`kubectl logs instance-manager-r-28535c55 -n instance-manager-r`\nPVC logs:\n![pvclogs1](.media/pvc1logs.JPG)\n![pvclogs2](.media/pvc2logs.JPG)\n![pvclogs3](.media/pv3logs.JPG)\n## Root Cause Analysis\nTo investigate possible underlying storage issues, we accessed [Longhorn](https://kubemaster-vip.bigdata.abc.gr/longhorn/) and discovered that the PVC (Persistent Volume Claim) associated with the airflow-scheduler had three replicas of airflow-scheduler pvc stuck in a deleting state.\n![longhornUI](.media/longhornbug.JPG)\n## Resolution\nTo resolve the issue and restore the airflow-scheduler pod's functionality, the following steps were taken:\n### Deletion of Problematic PVC\nThe three problematic replicas of airflow-scheduler PVC in Longhorn, which were stuck in a deleting state, were the problem, as a result we deleted the airflow's PVC.\nNfgh: It is important to mention that this issue might be related to a bug within the Longhorn storage system, as it caused replicated PVCs to get stuck in a deleting state. Further investigation and monitoring of Longhorn may be necessary to prevent such issues from recurring. Also see this [thread](https://github.com/longhorn/longhorn/issues/4278)\n## Actions Taken\nDeleted problematic PVCs in Longhorn under instance-manager-r namespace.\n`kubectl delete pvc pvc-c826d577-e764-470e-9904-3986042810aa -n ranai-geo`\n## Recommendations\nMonitor Longhorn for any recurring issues with PVC management to prevent similar incidents. ",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20231011-IM2231148.md"
        }
    },
    "398": {
        "page_content": "---\ntitle: CDSW Job Failures with \"Engine exited with status 34\" Due to Pod Sandbox Issues\ndescription: Multiple jobs in Cloudera Data Science Workbench (CDSW) failed with \"Engine exited with status 34\" due to Kubernetes pod sandboxing issues caused by networking plugin failures on wrkcdsw1. The issue was mitigated by restarting the Docker daemon via supervisor.\ntags:\n  - bigstreamer\n  - cdsw\n  - engine status 34\n  - job failure\n  - sandbox error\n  - pod sandbox changed\n  - kubernetes\n  - kubectl events\n  - networking\n  - weave\n  - docker\n  - supervisorctl\n  - wrkcdsw1\n  - mncdsw1\n  - engine restart\n  - cni plugin\n  - job diagnostics\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IMxxxxxx\n  system: abc BigStreamer CDSW\n  root_cause: Kubernetes pod sandbox creation failed due to CNI plugin (Weave) failing to assign IP, resulting in engine crash (status 34)\n  affected_nodes:\n    - wrkcdsw1.bigdata.abc.gr\n  user_visible_error: \"Engine exited with status 34\"\n  log_trace: \"Failed to create pod sandbox: networkPlugin cni failed to set up pod network: connect: connection refused\"\n  action_taken:\n    - Inspected job logs via CDSW admin UI\n    - Retrieved event logs using `kubectl get events`\n    - Identified weave network errors from pod sandbox events\n    - Restarted CDSW Docker service via `supervisorctl` on wrkcdsw1\n    - Validated recovery via `cdsw status` and re-run jobs\n  outcome: Jobs executed successfully post-restart; no further failures observed\n---\n# abc - IM2363704 - CDSW issue -  Engine exited with status 34\n## Description\nNoticed since 4/8 many job executions in CDSW fail with \"Engine exited with status 34\" message.\nIn the logs --> There are no logs for this engine node at this time.\nWith re-run the jobs are executed normally.\nIt is observed that many jobs fail during the day.\nfor example:\nhttps://mncdsw1.bigdata.abc.gr/ccharisis/hardware_failures/engines/0nohdhssxz6uebit\nhttps://mncdsw1.bigdata.abc.gr/ccharisis/forecasting/engines/kx29vx7l91i7x567\n## Actions Taken\n1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'\n2. Select the `Site Administration` tab (must have admin privileges)\n3. Select the `Usage` tab which displays the jobs\n4. Inspect the jobs in question.\nThe jobs are in `FAILED` status. The logs for the failed applications are missing.\n5. Troubleshoot from the command line:\nFrom `mncdsw1` as root (use personal account and then sudo):\n```bash\nkubectl get pods -w -A # Wait a pod to fail (namespace should be like default-user-XXX)\n# After a while, a pod has failed, describe it\nkubectl describe pod -n default-user-XXX XXXXXXXX\n```\nIn some cases the pod has failed but it cannot be seen by the `kubectl describe pod` command. In those cases, we use the `kubectl get events` command on the same namespace and search for the appropriate pod name.\n```bash\nkubectl get events -n default-user-XXX\n```\nIn our case, we used the `kubectl get events` command:\n```logs\n60m         Normal    Scheduled                pod/t804rlnpej08xzcg                  Successfully assigned default-user-49/t804rlnpej08xzcg to wrkcdsw1.bigdata.abc.gr\n60m         Warning   FailedCreatePodSandBox   pod/t804rlnpej08xzcg                  Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e\" network for pod \"t804rlnpej08xzcg\": networkPlugin cni failed to set up pod \"t804rlnpej08xzcg_default-user-49\" network: unable to allocate IP address: Post \"\nhttp://127.0.0.1:6784/ip/166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e\":\ndial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container \"166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e\" network for pod \"t804rlnpej08xzcg\": networkPlugin cni failed to teardown pod \"t804rlnpej08xzcg_default-user-49\" network: Delete \"\nhttp://127.0.0.1:6784/ip/166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e\":\ndial tcp 127.0.0.1:6784: connect: connection refused]\n33s         Normal    SandboxChanged           pod/t804rlnpej08xzcg                  Pod sandbox changed, it will be killed and re-created.\n```\nThe jobs were running for about an hour before failing with the message `Pod sandbox changed, it will be killed and re-created.`\n6. Restart the docker daemon to restart all containers on `wrkcdsw1`\n_At the time of the issue, CDSW had stale configuration that required full restart (outage) which was not desirable_\nTo avoid applying the settings, restart the service with the same configuration by triggering a restart by `supervisord` deployed as part of the Cloudera agent\n<details> ![Danger ahead](https://media3.giphy.com/media/vvzMdSygQejBIejeRO/200w.gif?cid=6c09b952aacsm9yssw6k6q0z5v8ejuy82rjpvw6qdhglcwpu&rid=200w.gif&ct=g) </details>\nFrom wrkcdsw4 as root (use personal account and then sudo):\n\n```bash\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf status | grep DOCKER\n# Sample\n# 145071-cdsw-CDSW_DOCKER          RUNNING   pid 39353, uptime 29 days, 0:40:20\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart 145071-cdsw-CDSW_DOCKER\n```\n8. Check that the node is operational after the restart\nFrom `mncdsw1` as root (use personal account and then sudo):\n```bash\ncdsw status # You might have to wait a few minutes\n```\n9. Inform the customer about the problem\n```text\nWe noticed that some jobs on the wrkcdsw1 node were running for about an hour before they failed with the error \"Pod sandbox changed, it will be killed and re-created\". To solve this particular error, we restarted the cdsw service on the wrkcdsw1 node and noticed that the failures with a duration of 1 hour stopped and the corresponding jobs were executed normally.\n```\n## Affected Systems\nabc Bigstreamer CDSW",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20240819-IM2363704.md"
        }
    },
    "399": {
        "page_content": "---\ntitle: Hue Impala Query Failure Due to Expired Kerberos Ticket\ndescription: Impala queries from Hue failed with GSSAPI ticket expiration errors due to outdated Kerberos credential cache path in crontab after a Hue service restart. The issue was resolved by updating the `kinit` cron job to point to the current keytab path.\ntags:\n  - bigstreamer\n  - hue\n  - impala\n  - kerberos\n  - gssapi\n  - sasl error\n  - ticket expired\n  - crontab\n  - hue.keytab\n  - kerberos ticket\n  - kerberos cache\n  - kinit\n  - cloudera\n  - hue workaround\n  - authentication error\n  - hadoop\n  - un2\n  - thrifttransport\n  - impala query\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2278275\n  system: abc BigStreamer Hue & Impala (un2)\n  root_cause: Kerberos ticket expired due to outdated keytab path after Hue service restart\n  affected_component: Hue + Impala authentication via GSSAPI\n  resolution: Updated cron job to point to the latest valid Hue keytab directory and re-initialized Kerberos ticket\n  verification_steps:\n    - Ran `kinit` manually\n    - Confirmed query execution in Hue UI\n  reference: https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage_idm_replication.md#a-brief-history-of-preauthentication\n---\n# abc - BigStreamer - IM2278275 Hue error\n## Description\nImpala & Hue queries from Hue fail with:\nCould not start SASL: Error in sasl_client_start (-1) SASL(-1): generic failure: GSSAPI Error: Unspecified GSS failure. Minor code may provide more information (Ticket expired) (code THRIFTTRANSPORT): TTransportException('Could not start SASL: Error in sasl_client_start (-1) SASL(-1): generic failure: GSSAPI Error: Unspecified GSS failure. Minor code may provide more information (Ticket expired)',)\nApplies to https://172.25.37.236:8888/hue ( un2) & Virtual https://172.25.37.237 :8888/hue .\nNot for https://172.25.37.235:8888/hue (un1)\nThe cause was an outdated Kerberos ticket due to a Hue service restart that changed the keytab process path used by the scheduled `kinit` command.\n## Actions Taken\nAs read from [here](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage_idm_replication.md#a-brief-history-of-preauthentication) we proceeded to the following steps:\n1. List crontab entries and find entry for hue:\n```bash\n[root@un2 ~]# crontab -l | grep hue\n```\nOuput:\n```bash\n### Hue Workaround ###\n5 */2 * * * sudo -u hue /usr/bin/kinit -k -t /var/run/cloudera-scm-agent/process/159288-hue-KT_RENEWER/hue.keytab -c /var/run/hue/hue_krb5_ccache hue/un2.bigdata.abc.gr@BIGDATA.abc.GR\n```\n2. Check if this file exists:\n```bash\n[root@un2 ~]# ll /var/run/cloudera-scm-agent/process/159835-hue-KT_RENEWER/\nls: cannot access /var/run/cloudera-scm-agent/process/159835-hue-KT_RENEWER/: No such file or directory\n```\nThis occurs due to the fact that hue service had been restarted.\n3. Find latest process for Kerberos ticket cache of Hue with ` ll -ltra /var/run/cloudera-scm-agent/process/` and edit crontab with `crontab -e` and replace with the correct directory\n4. Verify that problem is resolved by running below kinit command:\n```bash\n[root@un2 ~]#  sudo -u hue /usr/bin/kinit -k -t /var/run/cloudera-scm-agent/process/159836-hue-KT_RENEWER/hue.keytab -c /var/run/hue/hue_krb5_ccache hue/un2.bigdata.abc.gr@BIGDATA.abc.GR\n```\n5. Go to hue `https://172.25.37.236:8888/hue` and run a sample command like `show databases;` to verify that you can perform querries",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20240212-IM2278275.md"
        }
    },
    "400": {
        "page_content": "---\ntitle: Spark Job Failure - GeoViavi-LTE in abc BigStreamer Geolocation\ndescription: Spark job GeoViavi-LTE failed during Phase #4b execution as reported via geolocation mail alert; manual cleanup and retention policy steps followed from developer documentation.\ntags:\n  - bigstreamer\n  - abc\n  - spark\n  - geolocation\n  - spark-job-failure\n  - viavi\n  - geoviavi\n  - geolocation-alert\n  - return_code_1\n  - hdfs\n  - failure-handling\n  - phase_4b\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  cluster: abc\n  subsystem: Geolocation\n  tool: Spark\n  application:\n    name: GeoViavi-LTE\n    id: application_1599948124043_370289\n  job_metadata:\n    load_id: 1605552391\n    phase: 4b\n    start_time: 1605552487\n    return_code: 1\n  logs:\n    - S550.Geo_Tech_Load_Data.sh\n  cleanup_paths:\n    - /ez/warehouse/geolocation.db/geo_<technology>_fail/\n  retention_days:\n    - eponymous: 2\n    - anonymous: 1\n  reference_docs:\n    - http://10.124.161.38/trac/hadoop/wiki/dev/project/abc/geolocation_viavi\n---\n# abc - BigStreamer - GI9 - abc Bigstreamer Geolocation mail for Spark job failure\n## Description\n### Spark Job Failure Alert Message\nAn automated Spark job failure alert was received via email:\nGeolocation ALERT:[WARN] - Phase #4b, Spark job GeoViavi-LTE failed (1605552391)\nS550.Geo_Tech_Load_Data.sh\n2020-11-16 20:54:05 --> Phase #4\nSpark job GeoViavi-LTE failed with return_code 1.\nApplicationID:application_1599948124043_370289.\nLoadID:1605552391.\nStart time of job :1605552487.\nThis is an automated e-mail.\nPlease do not reply.\n## Actions Taken\nThe [full documentation](http://10.124.161.38/trac/hadoop/wiki/dev/project/abc/geolocation_viavi) provided by the developers' team\nFollowed the `Failure Handling Manual Mechanism` section of the guide above\nIf there are other dates in the `/ez/warehouse/geolocation.db/geo_<technology>_fail/` HDFS folder that are older than two days, they can be deleted. Retention for the geolocation tables is 2 days for the eponymous and 1 day for the anonymous table.\n## Affected Systems\nabc Bigstreamer Geolocation\n## Action Points\nNone, failures of that kind are rare and not worth the extra effort.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201118-GI9.md"
        }
    },
    "401": {
        "page_content": "---\ntitle: HDFS Bad Health Due to Zookeeper Timeout on Failover Controllers\ndescription: HDFS health degraded due to Failover Controller roles shutting down on mn1 and mn2 after timeout connecting to Zookeeper. No hardware or network issues were found. High RPC latency likely caused the timeout. Automatic restart was enabled for failover controllers to prevent recurrence.\ntags:\n  - bigstreamer\n  - hdfs\n  - failover controller\n  - zkfc\n  - zookeeper\n  - mn1\n  - mn2\n  - hdfs bad health\n  - cloudera manager\n  - automatic restart\n  - tick time\n  - rpc latency\n  - failover timeout\n  - log investigation\n  - service availability\n  - high availability\n  - root cause analysis\n  - failover resilience\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2158478\n  system: abc BigStreamer HDFS\n  nodes_affected:\n    - mn1.bigdata.abc.gr\n    - mn2.bigdata.abc.gr\n  roles_down: HDFS Failover Controller (ZKFC)\n  root_cause: Temporary RPC latency spike led to Zookeeper timeout (tickTime 2000ms)\n  fix: Enabled automatic restart for Failover Controller via Cloudera Manager\n  log_paths:\n    - /var/log/hadoop-hdfs/\n    - /var/log/zookeeper/\n  references:\n    - https://actorsfit.com/a?ID=01750-52f7ffb1-84f3-4d85-a855-e06d619799ce\n    - https://community.cloudera.com/t5/Support-Questions/Failover-Controllers-Health-Bad-leads-to-complete-HDFS/m-p/51717\n---\n# abc - BigStreamer - IM2158478 - HDFS Failover Controller Timeout on mn1 & mn2\n## Description\nThe HDFS health alert was triggered due to failover controller role failures on mn1 and mn2. Below is the original request:\nHDFS status is bad as HDFS Failover Controller role is down on nodes mn1, mn2\n## Actions Taken\n1. Since the failover controller roles were down, we investigated their logs under `/var/log/hadoop-hdfs/` on each host and found that they received a timeout in their connection to zookeeper leading them to shutdown until they were manually restarted.\n2. Checking the zookeper server logs under `/var/log/zookeeper/` we observed that they report that the connection had been closed client side. Additionally at the same time frame we checked to see if there were any issues with other services hosted on these nodes, mainly if any Namenodes had any issues, and found none.\n3. Similarly from Cloudera Manager we checked the host's event log for any red flags and found none. Lastly we didn't find any network errors on both hosts.\n4. Through Cloudera Manager health checking we see certain RPC latency alerts popping up with values above 1500 ms. Additionally we checked Zookeeper Server's tick time under `ZooKeeper->Configuration->Tick Time`.\n4. Thus we concluded that, given the zookeeper's tick time of 2000 ms and no other issues found, a spike in latency led to the above timeout and after communicating with the customer we enabled the failover controller's automatic restart to avoid having the failover controllers down for prolonged periods of time. This was done without needing any services or redeploy any configuration by checking the box under `HDFS->Failover Controller->Automatically Restart Processes`.\n## References\n- [Failover Controller Connection Loss](https://actorsfit.com/a?ID=01750-52f7ffb1-84f3-4d85-a855-e06d619799ce#:~:text=Modify%20the%20zookeeper%20configuration%20file.%20In%20zoo.cfg%2C%20modify,ticktime%20to%204000ms%2C%20and%20the%20default%20is%202000ms.)\n- [ZKFC Failure](https://community.cloudera.com/t5/Support-Questions/Failover-Controllers-Health-Bad-leads-to-complete-HDFS/m-p/51717)",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230531-IM2158478.md"
        }
    },
    "402": {
        "page_content": "---\ntitle: Corrupted or Missing Table Statistics in Impala\ndescription: Procedure for detecting, validating, and resolving warnings related to missing or corrupted Impala table statistics across multiple databases (sai, brond, ookla, temip). Includes HDFS checks, Impala `compute stats` and `refresh` commands, and performance considerations for streaming partitions.\ntags:\n  - impala\n  - compute stats\n  - refresh\n  - table stats\n  - corrupted statistics\n  - sai\n  - brond\n  - ookla\n  - temip\n  - sms_raw\n  - voice_raw\n  - voice_quality_hist\n  - brond_retrains_hist\n  - td_dslam_week\n  - ookla_android\n  - ookla_ios\n  - ookla_stnet\n  - temip_impala_terminated_alarms\n  - temip_kudu_terminated_alarms\n  - hdfs\n  - performance\n  - streaming partitions\n  - metadata issues\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1630642\n  system: abc BigStreamer Impala Stats\n  root_cause: statistics outdated or missing due to streaming inserts or no scheduled computation\n  detection: warning from Impala query logs regarding corrupt/missing stats\n  resolution_steps:\n    - Check table size in HDFS\n    - Check stats in Impala\n    - Run compute stats or refresh\n    - Avoid computing stats on actively ingested partitions\n  special_considerations: Some tables ingest data continuously and should not have stats computed during the day\n---\n# Missing or corrupted statistics\n## Description\nThis document addresses how to detect and fix missing or corrupted table statistics in Impala. The issue affects both historical and streaming tables, with warnings appearing due to out-of-date or missing metadata. It includes steps for HDFS size verification, reviewing Impala table stats, running `compute stats`, and `refresh`, as well as caveats for high-frequency streaming tables like `sai.sms_raw`.\nThe following tables have potentially corrupt table statistics. Drop and re-compute statistics to resolve this problem. ->\nsai.voice_quality_hist,\nsai.sms_raw, \nsai.voice_raw,\nbrond.brond_retrains_hist,\nbrond.td_dslam_week,\nookla.ookla_android,\nookla.ookla_ios,\nookla.ookla_stnet,\ntemip.temip_impala_terminated_alarms,\ntemip.temip_kudu_terminated_alarms\n## Actions Taken\nTables like sai.sms_raw ingest data every 5 minutes, which makes real-time statistics invalid or misleading.\n### HDFS Table Size Check\nRun the following commands on un2 as user `intra` to inspect HDFS directory sizes of the affected tables:\n```bash\nhdfs dfs -du -h -s /ez/warehouse/sai.db/voice_quality_hist\nhdfs dfs -du -h -s /ez/warehouse/brond.db/brond_retrains_hist\nhdfs dfs -du -h -s /ez/warehouse/brond.db/td_dslam_week\nhdfs dfs -du -h -s /ez/warehouse/ookla.db/ookla_android\nhdfs dfs -du -h -s /ez/warehouse/ookla.db/ookla_ios\nhdfs dfs -du -h -s /ez/warehouse/ookla.db/ookla_stnet\n``` \n### Check Impala Table Stats\nReview current statistics for the affected tables via the Impala shell:\n```bash\nshow table stats  sai.voice_quality_hist               ;\nshow table stats  sai.sms_raw                          ;\nshow table stats  sai.voice_raw                        ;\nshow table stats  brond.brond_retrains_hist            ;\nshow table stats  brond.td_dslam_week                  ;\nshow table stats  ookla.ookla_android                  ;\nshow table stats  ookla.ookla_ios                      ;\nshow table stats  ookla.ookla_stnet                    ;\nshow table stats  temip.temip_impala_terminated_alarms ;\nshow table stats  temip.temip_kudu_terminated_alarms   ;\n```\n### Recompute Statistics\nExecute `compute stats` for the impacted tables. Note that for streaming tables, this may not be feasible during working hours:\n```bash\ncompute stats   sai.voice_quality_hist               ;\ncompute stats   sai.sms_raw                          ;\ncompute stats   sai.voice_raw                        ;\ncompute stats   brond.brond_retrains_hist            ;\ncompute stats   brond.td_dslam_week                  ;\ncompute stats   ookla.ookla_android                  ;\ncompute stats   ookla.ookla_ios                      ;\ncompute stats   ookla.ookla_stnet                    ;\ncompute stats   temip.temip_impala_terminated_alarms ;\ncompute stats   temip.temip_kudu_terminated_alarms   ;\n```\n\u039dote: If there is a failure in any of the tables, run 'refresh' on this table.\n### Refresh Failing Tables\nUse `refresh` for any tables where `compute stats` fails due to corrupt metadata or loading conflicts:\n```bash\nrefresh   sai.voice_quality_hist               ;\nrefresh   sai.sms_raw, sai.voice_raw           ;\nrefresh   brond.brond_retrains_hist            ;\nrefresh   brond.td_dslam_week                  ;\nrefresh   ookla.ookla_android                  ;\nrefresh   ookla.ookla_ios                      ;\nrefresh   ookla.ookla_stnet                    ;\nrefresh   temip.temip_impala_terminated_alarms ;\nrefresh   temip.temip_kudu_terminated_alarms   ;\n```\n### Notes from Data Engineering (Dionysia)\nThe Data Engineering team clarified why stats appear to be missing or outdated for certain streaming tables:\nAfter investigating the tables (sai.voice_quality_hist, sai.sms_raw, sai.voice_raw) we saw that we calculate statistics every morning for the previous day's data. These tables seem to fill up throughout the day, so the stats don't appear to be up to date.\nThe statistics refer to data in partitions of previous days.\n- In terms of performance, it is not possible to run compute statistics on partitions that continuously receive data during the day.\nThere are streams that load data every 5 minutes.\nDoing so would significantly delay data processing.\n- Obviously, the queries that are executed include the current partition and that is why the Warning for missing statistics is displayed.\nAttached you will find an example for sai.sms_raw for different par_dt, in which it appears that for historical data no warning is displayed.\nIf statistics are needed for specific tables, we should consider it as a project and not under Support. We should consider how the performance of the cluster is affected and make the right design, so that we can agree on the frequency that the statistics should be calculated.\nFurther actions have also been made",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210901-IM1630642.md"
        }
    },
    "403": {
        "page_content": "---\ntitle: Refdata.rd_cells_load Partition Recovery for 10/11 and 11/11\ndescription: Data correction steps for low row count in partition 20201110 of refdata.rd_cells_load table in abc BigStreamer, by copying and renaming Parquet files in HDFS.\ntags:\n  - bigstreamer\n  - abc\n  - impala\n  - hdfs\n  - refdata\n  - rd_cells\n  - data-recovery\n  - partition\n  - data-load\n  - partition-correction\n  - hdfs-copy\n  - csv\n  - row-mismatch\n  - impala-refresh\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1353607\n  cluster: abc\n  table: refdata.rd_cells_load\n  corrected_partitions:\n    - 20201110\n  source_partition: 20201111\n  impala_queries:\n    - show partitions refdata.rd_cells_load\n    - show files in refdata.rd_cells_load partition (par_dt>='20201110')\n    - select par_dt, count(*) from refdata.rd_cells_load where par_dt>='20201109' group by par_dt\n  hdfs_operations:\n    - hdfs dfs -cp ...\n    - hdfs dfs -mv ...\n    - impala refresh refdata.rd_cells_load\n---\n# abc - BigStreamer - IM1353607  - abc BigStreamer (refdata.rd_cells)\n## Description\nPlease load the data for 11/11 and for 10/11\n## Actions Taken\n1. Check the size of current partition from Impala-Shell\n``` bash\nshow partitions refdata.rd_cells_load;\n```\nexample output\n``` bash\npar_dt   | #Rows     | #Files | Size    | Bytes Cached | Cache Replication | Format | Incremental stats | Location                                                                 \n---------+-----------+--------+---------+--------------+-------------------+--------+-------------------+--------------------------------------------------------------------------\n20201109 |    105576 |      1 | 44.82MB | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201109\n20201110 |         6 |      1 | 191B    | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110\n20201111 |    105325 |      1 | 45.63MB | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111\n```\nWe notisted significant less Rows for par_dt \"20201110\" .\n2. We check \"20201110\" & \"20201111\" partition files from HDFS.\n``` bash\nimpala> refresh refdata.rd_cells_load;\nimpala> show files in refdata.rd_cells_load partition (par_dt>='20201110');\n```\nexample output:\n```\nPath                                                                                         | Size    | Partition      \n---------------------------------------------------------------------------------------------+---------+----------------\nhdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv | 191B    | par_dt=20201110\nhdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv | 45.63MB | par_dt=20201111\n```\n3. We copy partition \"20201111\" file to \"20201110\".\n``` bash\nhdfs dfs -cp /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/\n```\n> The 20201110 partition was missing valid data, so we copied the 20201111 file as a substitute.\n4. We rename to hide the file for \"20201110\".\n``` bash\nhdfs dfs -mv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/.cells_20201110.csv\n```\n> This preserves the original but hides it from Impala visibility by prefixing with a dot.\n5. Repeat Step 2.\n6. We execute the query bellow to check if the partitions \"20201111\" & \"20201110\" have the same number of Rows.\n``` bash\nimpala> select par_dt, count(*) cnt from refdata.rd_cells_load where par_dt>='20201109' group by par_dt order by 1;\n```\nexample output\n```\npar_dt   | cnt   \n---------+-------\n20201109 | 105576\n20201110 | 105325\n20201111 | 105325\n```\n> The row count for 20201110 is slightly lower than 20201109 due to missing historical data before correction. Partition 20201111 was reused as a fallback.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201202-IM1353607.md"
        }
    },
    "404": {
        "page_content": "---\ntitle: Radius Data Recovery and Ingestion after Cluster Downtime\ndescription: End-to-end recovery of missing hourly RADIUS and RADARCHIVE files due to system outage, including SFTP retrieval, ingestion via local `.trn` config override, and verification in hist tables.\ntags:\n  - bigstreamer\n  - abc\n  - radius\n  - radacct\n  - radarchive\n  - ingestion\n  - data-recovery\n  - sftp\n  - hive\n  - hist-tables\n  - cluster-outage\n  - file-gap\n  - data-gap\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1391612\n  cluster: abc\n  ingestion_flows:\n    - radius\n    - radacct\n    - radarchive\n  systems:\n    - intra\n    - sftp: 79.128.178.35\n    - impala\n    - HDFS\n  gaps_identified:\n    - radacct: 20201220 (between 03:00 - 16:30)\n    - radarchive: 20201219\n  ingestion_scripts:\n    - radius.pl\n    - 000_radius_ops.sh\n  verification_steps:\n    - radius_date.dat.local\n    - manual override in radius.trn\n    - hist table file count (radacct_hist, radarchive_hist)\n  reference:\n    - https://edn2.bigdata.intranet.gr/abc/BigStreamer/cluster_monitoring/blob/master/future_steps/flows_applications.md\n---\n# abc - IM1391612 - Data loading\n## Description\nCluster instability caused partial ingestion failures in several flows. This document covers the radius and radarchive flow gaps and the manual restoration procedure used to load missing files into Hive hist tables.\nPlease immediately load all the data for all the hours and for all the flows (sai,vantage,radius etc) for the period of time the system was not working and run all the aggregated tables. We should also be sent a summary table with the relevant information so that we know if and in which streams there are data deficiencies.\n## Actions Taken\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `intra` with sudo.\n2. Check files that have not been ingested. As you can see, there is a gap for radacct 20201220 and for radarchive between 3:00 - 16:30\n```bash\n[intra@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radacct_orig_files\n...\n-rwxrwx--x+  3 hive hive  838634257 2020-12-20 02:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_01-30.csv.20201220_021002.utc\n-rwxrwx--x+  3 hive hive  837624575 2020-12-20 03:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_03-00.csv.20201220_031003.utc\n-rwxrwx--x+  3 hive hive  840322537 2020-12-20 17:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_16-30.csv.20201220_171002.utc\n-rwxrwx--x+  3 hive hive  839948348 2020-12-20 18:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_18-00.csv.20201220_181002.utc\n-rwxrwx--x+  3 hive hive  840668651 2020-12-20 20:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_19-30.csv.20201220_201002.utc\n-rwxrwx--x+  3 hive hive  840847248 2020-12-20 21:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_21-00.csv.20201220_211002.utc\n...\n[intra@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radarchive_orig_files\nFound 30 items\n...\n-rwxrwx--x+  3 hive hive 1694918420 2020-12-17 05:14 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-16.csv.20201217_051003.utc\n-rwxrwx--x+  3 hive hive 1635182557 2020-12-18 05:14 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-17.csv.20201218_051002.utc\n-rwxrwx--x+  3 hive hive 1618497473 2020-12-19 05:14 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-18.csv.20201219_051002.utc\n-rwxrwx--x+  3 hive hive 1522580860 2020-12-21 05:13 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-20.csv.20201221_051002.utc\n```\nA gap was observed between `radacct_2020-12-20_03-00.csv` and `radacct_2020-12-20_16-30.csv`. \nAlso, radarchive data for `2020-12-19` was missing.\n3. Change directory and transfer missing files from sftp. \n``` bash\n[intra@un2 radius]$ cd /shared/radius_repo/cdrs\n[intra@un2 cdrs]$ sftp prdts@79.128.178.35\nConnecting to 79.128.178.35...\nsftp> get radacct_2020-12-20_04-30.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_04-30.csv.bz2 to radacct_2020-12-20_04-30.csv.bz2\n/home/prdts/radacct_2020-12-20_04-30.csv.bz2                                                                                                                              100%  201MB  28.8MB/s   00:07    \nsftp> get radarchive_2020-12-19.csv.bz2\nFetching /home/prdts/radarchive_2020-12-19.csv.bz2 to radarchive_2020-12-19.csv.bz2\n/home/prdts/radarchive_2020-12-19.csv.bz2                                                                                                                                 100%  207MB  25.8MB/s   00:08    \nsftp> get radacct_2020-12-20_06-00.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_06-00.csv.bz2 to radacct_2020-12-20_06-00.csv.bz2\n/home/prdts/radacct_2020-12-20_06-00.csv.bz2                                                                                                                              100%  201MB  25.2MB/s   00:08    \nsftp> get radacct_2020-12-20_07-30.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_07-30.csv.bz2 to radacct_2020-12-20_07-30.csv.bz2\n/home/prdts/radacct_2020-12-20_07-30.csv.bz2                                                                                                                              100%  201MB  33.6MB/s   00:06    \nsftp> get radacct_2020-12-20_09-00.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_09-00.csv.bz2 to radacct_2020-12-20_09-00.csv.bz2\n/home/prdts/radacct_2020-12-20_09-00.csv.bz2                                                                                                                              100%  201MB  25.1MB/s   00:08    \nsftp> get radacct_2020-12-20_10-30.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_10-30.csv.bz2 to radacct_2020-12-20_10-30.csv.bz2\n/home/prdts/radacct_2020-12-20_10-30.csv.bz2                                                                                                                              100%  201MB  28.8MB/s   00:07    \nsftp> get radacct_2020-12-20_12-00.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_12-00.csv.bz2 to radacct_2020-12-20_12-00.csv.bz2\n/home/prdts/radacct_2020-12-20_12-00.csv.bz2                                                                                                                              100%  202MB  25.2MB/s   00:08    \nsftp> get radacct_2020-12-20_13-30.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_13-30.csv.bz2 to radacct_2020-12-20_13-30.csv.bz2\n/home/prdts/radacct_2020-12-20_13-30.csv.bz2                                                                                                                              100%  202MB  28.8MB/s   00:07    \nsftp> get radacct_2020-12-20_15-00.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_15-00.csv.bz2 to radacct_2020-12-20_15-00.csv.bz2\n/home/prdts/radacct_2020-12-20_15-00.csv.bz2 \nsftp> exit\n```\n4. Check that /shared/radius_repo/radius_date.dat.local points to an older file:\n``` bash\n[intra@un2 cdrs]$ cat /shared/radius_repo/radius_date.dat.local\n[File]\nlatest_file=\"/shared/radius_repo/cdrs/radarchive_2019-08-14.csv.bz2\"\n[intra@un2 cdrs]$ ll\ntotal 2357692\n-rw-r--r-- 1 intra intra 211140333 Dec 21 16:27 radacct_2020-12-20_04-30.csv.bz2\n-rw-r--r-- 1 intra intra 211020434 Dec 21 16:28 radacct_2020-12-20_06-00.csv.bz2\n-rw-r--r-- 1 intra intra 211125062 Dec 21 16:28 radacct_2020-12-20_07-30.csv.bz2\n-rw-r--r-- 1 intra intra 210696825 Dec 21 16:28 radacct_2020-12-20_09-00.csv.bz2\n-rw-r--r-- 1 intra intra 211175805 Dec 21 16:29 radacct_2020-12-20_10-30.csv.bz2\n-rw-r--r-- 1 intra intra 211440564 Dec 21 16:29 radacct_2020-12-20_12-00.csv.bz2\n-rw-r--r-- 1 intra intra 211670525 Dec 21 16:29 radacct_2020-12-20_13-30.csv.bz2\n-rw-r--r-- 1 intra intra 211765933 Dec 21 16:29 radacct_2020-12-20_15-00.csv.bz2\n-rw-r--r-- 1 intra intra 172240773 Jun 19  2019 radarchive_2019-06-12.csv.bz2\n-rw-r--r-- 1 intra intra 162087027 Jul 30  2019 radarchive_2019-07-27.csv.bz2\n-rw-r--r-- 1 intra intra 168362647 Aug 16  2019 radarchive_2019-08-14.csv.bz2\n-rw-r--r-- 1 intra intra 216716584 Dec 21 16:27 radarchive_2020-12-19.csv.bz2\n```\nSince we want to load files newer than `/shared/radius_repo/cdrs/radarchive_2019-08-14.csv.bz2`, No need to modify radius_date.dat.local; its current pointer is valid for our recovery scope\n5. Change configuration file of ingestion script.\n```bash\n[intra@un2 cdrs]$ vim /shared/abc/radius/DataParser/scripts/transferlist/radius.trn\nDefault Status:\n...\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n#-- local executions --\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n...\nWhen local file is used:\n...\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n#-- local executions --\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n...\n```\n6. Execute ingestion scripts when making sure it is not executed at the moment. Ensure to restore original .trn settings after ingestion completes.\n```bash\n[intra@un2 cdrs]$ tail /shared/abc/radius/DataParser/scripts/log/radius_20201221.log\n...\n--------------END------------\n[intra@un2 cdrs]$ /shared/abc/radius/DataParser/scripts/radius.pl -l -d -D -o >> /shared/abc/radius/DataParser/scripts/log/radius_cron_manual_20201221.log 2>&1\n[intra@un2 cdrs]$ /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.manual_20201221.log 2>&1\n[intra@un2 cdrs]$ vim /shared/abc/radius/DataParser/scripts/transferlist/radius.trn\n```\n> Once ingestion was completed, the `.trn` file was reverted to its default configuration pointing to `radius_date.dat`.\n7. Check that files have been loaded to hist tables.\n```bash\n[u15@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radacct_hist/par_dt=20201220\n...\n-rwxrwx--x+  3 hive hive   65350341 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000013_2054504955_data.0.\n-rwxrwx--x+  3 hive hive  134217741 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000014_1021750110_data.0.\n-rwxrwx--x+  3 hive hive  134217750 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000028_924374267_data.0.\n-rwxrwx--x+  3 hive hive  134217617 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000008_1102168495_data.0.\n-rwxrwx--x+  3 hive hive  134217769 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000001b_924374267_data.0.\n-rwxrwx--x+  3 hive hive  134217906 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000000b_1400128216_data.0.\n-rwxrwx--x+  3 hive hive  134217799 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000005_183542376_data.0.\n-rwxrwx--x+  3 hive hive  134217146 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000001d_1808120301_data.0.\n-rwxrwx--x+  3 hive hive  134217812 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000000d_1400128216_data.0.\n-rwxrwx--x+  3 hive hive  165905440 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000020_2125534478_data.0.\n-rwxrwx--x+  3 hive hive  166171908 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000026_694498725_data.0.\n-rwxrwx--x+  3 hive hive  166671557 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000009_1829852461_data.0.\n-rwxrwx--x+  3 hive hive  134217919 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000027_1432252135_data.0.\n-rwxrwx--x+  3 hive hive  134217610 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000006_975299616_data.0.\n-rwxrwx--x+  3 hive hive  134217617 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000000c_361805242_data.0.\n-rwxrwx--x+  3 hive hive  134217918 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000023_361805242_data.0.\n-rwxrwx--x+  3 hive hive  134217774 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000002_361805242_data.0.\n-rwxrwx--x+  3 hive hive  134217914 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000004_2125534478_data.0.\n-rwxrwx--x+  3 hive hive  134217443 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000011_1541315014_data.0.\n-rwxrwx--x+  3 hive hive  168449504 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000018_924374267_data.0.\n-rwxrwx--x+  3 hive hive  134217940 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000007_975299616_data.0.\n-rwxrwx--x+  3 hive hive  134217515 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000010_2101034182_data.0.\n-rwxrwx--x+  3 hive hive  134217798 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000001_1541315014_data.0.\n-rwxrwx--x+  3 hive hive  134217909 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000024_738850578_data.0.\n-rwxrwx--x+  3 hive hive  134216978 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000017_361805242_data.0.\n-rwxrwx--x+  3 hive hive  134217450 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000016_559180009_data.0.\n[u15@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219\n...\n-rwxrwx--x+  3 hive hive  134217959 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000001_564661313_data.0.\n-rwxrwx--x+  3 hive hive  134217962 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000002_564661313_data.0.\n-rwxrwx--x+  3 hive hive   78601928 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000000_564661313_data.0.\n-rwxrwx--x+  3 hive hive  134217573 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000006_1446698864_data.0.\n-rwxrwx--x+  3 hive hive  134217391 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000004_929468556_data.0.\n-rwxrwx--x+  3 hive hive  134217788 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000007_1446698864_data.0.\n-rwxrwx--x+  3 hive hive  134218011 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000005_1259058186_data.0.\n-rwxrwx--x+  3 hive hive  134217788 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000008_1446698864_data.0.\n-rwxrwx--x+  3 hive hive  134217753 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de0000000a_1960091991_data.0.\n-rwxrwx--x+  3 hive hive  134217789 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000003_2144981155_data.0.\n-rwxrwx--x+  3 hive hive  134217904 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000009_1259058186_data.0.\n-rwxrwx--x+  3 hive hive  134217360 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de0000000b_1446698864_data.0.\n```\n## Affected Systems\nabc Bigstreamer\n## Action Points\nSection `Radius` in https://edn2.bigdata.intranet.gr/abc/BigStreamer/cluster_monitoring/blob/master/future_steps/flows_applications.md",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201220-IM1391612.md"
        }
    },
    "405": {
        "page_content": "---\ntitle: CSI_MOBILE Export Failure due to Delayed SAI Aggregation\ndescription: Full diagnostic and recovery procedure for CSI_MOBILE export issues caused by late SAI aggregations in the `sai.sub_aggr_csi_it` table. Covers detection via logs, temporary table validation, manual reloading, and re-exporting the missing CSI_MOBILE file.\ntags:\n  - bigstreamer\n  - csi_mobile\n  - csi_fixed\n  - export_csi_mob_daily\n  - sai.sub_aggr_csi_it\n  - sai aggregation\n  - cron misalignment\n  - delayed aggregation\n  - weekly_load\n  - temp.sub_aggr_csi\n  - export failure\n  - csi_weekly_load\n  - vantage\n  - monitoring\n  - mtuser\n  - missing file\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1622139\n  system: abc BigStreamer CSI / MOB flows\n  root_cause: sai aggregation finishes after CSI crontab\n  detection_target: missing CSI_MOBILE daily file\n  export_script: /shared/abc/export_sai_csi/export_csi_mob_daily.sh\n  cron_file: /shared/abc/csi/bin/csi_weekly_load.sh\n  agg_log_path: /shared/abc/traffica/log/SAI_AGGREGATION_LOOP.YYYYMMDD_YYYYMMDD.log\n  data_view: sai.sub_aggr_csi_it (view of vantage.sub_aggr_csi)\n  temp_tables:\n    - temp.sub_aggr_csi\n    - temp.sub_aggr_csi_lastmeasure\n---\n# abc - BigStreamer - IM1622139 - CSI_FIXED CSI_MOBILE Collection problem\n## Description\nWe have not received any CSI_MOBILE files today.\nThe latest we have received are the following:\n172.25.37.240 CSI_MOBILE CSI_mob_08162021_08232021.txt 208229499 8/23/2021 11:00:01 AM 20690 8/23/2021 11:00:22 AM\n172.25.37.240 CSI_MOBILE CSI_mob_08162021_08222021.txt 197819583 8/22/2021 11:00:02 AM 23619 8/22/2021 11:00:25 AM\n172.25.37.240 CSI_MOBILE CSI_mob_08162021_08212021.txt 190247108 8/21/2021 11:00:01 AM 21643 8/21/2021 11:00:23 AM\n- Missing CSI_MOBILE daily exports\n- Delayed sai.sub_aggr_csi_it data for par_dt\n- CSI_FIX/MOBILE export_csi_mob_daily.sh fails\n- SAI_AGGREGATION_LOOP finishes after CSI crontab\n- export_csi_mob_daily.sh does not run or runs empty\n## Actions Taken\n1. ssh un2 with your personal account\n2. sudo -iu mtuser\n3. less /shared/abc/export_sai_csi/log/sai_csi.cron.20210913.log\n4. Check the logs for the issue date. If any problem exist for par_dt on table `sai.sub_aggr_csi_it` or received any mail like this ```Mediation/IT Daily Flows: Unavailable Data Table sai.sub_aggr_csi_it has less lines than 3500000. Actual number is 0 for date 23/08/2021``` then follow the below steps:\n5. ssh unc2;sudo -iu intra;crontab -l | grep -i cron_aggregation_parallel.sh; less /shared/abc/traffica/log/SAI_AGGREGATION_LOOP.20210823_20210823.log `IMPROTANT: If the hour that the aggr finished were after 07:30 then we have issue bro`\n6. ssh un2\n7. su - intra\n8. crontab -l | grep -i 'csi_weekly_load'\n```\n##### EZ Population of CSI table (par_dt is always a monday and contains data between (now -1) day and its respective monday)  #####\n30 07 * * * /shared/abc/csi/bin/csi_weekly_load.sh >> /shared/abc/csi/log/csi_weekly_load.`date '+\\%Y\\%m\\%d'`.log 2>&1\n```\n10. secimp\n11. select count(*),par_dt from sai.sub_aggr_csi_it where par_dt >= '20210xxx'group by par_dt; `IMPROTANT` `1. sai.sub_aggr_csi_it is a view of vantage.sub_aggr_csi 2.Table has weekly par_dt but updated every day with new data`\nSteps to restore the data:\n1. First try on temp tables:\n- ssh un2;sudo -iu intra;cd /shared/abc/csi/bin/;\n- ./csi_weekly_load.sh.manual 2 `If 2 days passed then run the script with parameter 3 instead of 2. If no day has passed run the script without any parameter `\n```\nNfgh:tables temp.sub_aggr_csi & temp.sub_aggr_csi_lastmeasure \n```\n2. select count(*),par_dt from temp.sub_aggr_csi where par_dt >= '20210xxx'group by par_dt; `(Check if par_dt created)`\n3. If data imported on test tables then go to production tables:\n- ssh un2;sudo -iu intra;cd /shared/abc/csi/bin/;\n- ./csi_weekly_load 2 `If 2 days passed then run the script with parameter 3 instead of 2. If no day has passed run the script without any parameter `\n4. Run the CSI fix&mob after the above steps completed:\n- - ssh un2;sudo -iu mtuser;\n- /shared/abc/export_sai_csi/export_csi_mob_daily.sh `<missing_date>`\n5. After the successfully execution of step `4` then run again `./csi_weekly_load`\n## Affected Systems\nabc Bigstreamer CSI&MOB Flows\n## Action Points\nChange the hour of the crontab execution for `SAI_AGGREGATIONS` to avoid the below issues.\n```bash\nssh unc2;sudo -iu intra;crontab -l | grep -i cron_aggregation_parallel.sh;\n#### SAI/TRAFFICA raw Stats, aggregations, aggr Stats (initial schedule 07:01:00)\n1 6 * * * /shared/abc/traffica/bin/cron_aggregation_parallel.sh >> /shared/abc/traffica/log/cron_aggregation_parallel.`date '+\\%Y\\%m\\%d' -d \"yesterday\"`.log 2>&1\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210824-IM1622139.md"
        }
    },
    "406": {
        "page_content": "---\ntitle: Fraport SLA Metrics Missing Due to Invalid Probe Naming\ndescription: Investigation into missing SLA metrics for new Fraport locations due to probe names containing invalid device type \"64\", which leads to their exclusion from the SLA CSV exports. Includes Impala verification, CSV inspection, business rule validation, and customer communication.\ntags:\n  - bigstreamer\n  - cdsw\n  - fraport\n  - sla\n  - ipvpn\n  - probe name\n  - device type\n  - sla metrics\n  - nnm\n  - custompoller\n  - impala\n  - csv export\n  - qa_probe_name\n  - ip sla\n  - registration issue\n  - naming convention\n  - excluded metrics\n  - invalid probes\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2076207\n  system: abc BigStreamer CDSW\n  root_cause: Probes for new Fraport locations used \"64\" as device type which is not accepted by the business logic for CSV generation\n  missing_points:\n    - Frap-PVK1\n    - Frap-KVA1\n    - Frap-SKG1\n    - Frap-JMK1\n    - Frap-RHO1\n    - Frap-PVK2\n    - Frap-KVA2\n    - Frap-SKG2\n    - Frap-JMK2\n    - Frap-RHO2\n  business_rule: Only device types ['ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes'] are exported\n  action_taken: Verified data in Impala, custompoller, CSVs; identified invalid device type; informed customer\n---\n# abc - IM2076207 - Fraport SLA Metrics Missing\n## Description\nWe are not getting metrics at the following new points in the Fraport client (there don't seem to be any records at all)\n```\nFrap-PVK1\nFrap-KVA1\nFrap-SKG1\nFrap-JMK1\nFrap-RHO1\nFrap-PVK2\nFrap-KVA2\nFrap-SKG2\nFrap-JMK2\nFrap-RHO2\n```\nFrom a check made through the saa-csr5 probe, it appears that the router takes measurements from the specific points. Indicative:\n```\nsaa-csr5#sh ip sla configuration 4891 | i addre\nTarget address/Source address: 80.106.132.34/212.205.74.72\nsaa-csr5#sh ip sla statistics 4891\nIPSLAs Latest Operation Statistics\nIPSLA operation id: 4891\n        Latest RTT: 8 milliseconds\nLatest operation start time: 12:56:10 EET Thu Feb 2 2023\nLatest operation return code: OK\nNumber of successes: 302\nNumber of failures: 0\nOperation time to live: Forever\n```\nAlso NNM takes the measurements but they are not sent to Bigstreamer.\n## Actions Taken\n### 1. Check presence of probe names in metrics table\n1. As seen from [here](../supportDocuments/applicationFlows/ip_vpn.md) branch metrics are exported by querying `bigcust.nnmcp_ipvpn_slametrics_hist`. We need to investigate if this table contains metrics for the branches mentioned in the description.\nConnect to the Impala shell using the ipvpn user to run queries on SLA metrics.\nFrom `un2.bigdata.abc.gr` with root (cyberark login):\n``` bash\nsu - ipvpn\nimpala-shell -i un-vip.bigdata.abc.gr -k --ssl\n```\nQuery distinct probe names for Fraport entries on 2023-02-02 to confirm metric presence.\n``` sql\nselect distinct qa_probe_name from bigcust.nnmcp_ipvpn_slametrics_hist where par_dt='20230202' and customer = 'fraport';\n```\n    |                     qa_probe_name                      |\n    | :----------------------------------------------------: |\n    |                     _Other probes_                     |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-RHO1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-RHO2_64_ce   |\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-fgh1_64_ce |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-HQs1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JSI2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KGS2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CHQ1_64_ce   |\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-fgh2_64_ce |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-HQs2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-EFL2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CFU1_64_ce   |\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-LH1_64_ce  |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-EFL1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KVA1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JMK2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CFU2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JSI1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JTR1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JTR2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JMK1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-MJT1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-PVK2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CHQ2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SMI1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-PVK1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KGS1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-MJT2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SKG1_64_ce   |\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-LH2_64_ce  |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KVA2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SKG2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-ZTH1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SMI2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-ZTH2_64_ce   |\n    |                     _Other probes_                     |\n### 2. Confirm presence in Custompoller CSVs\n2. These probes were checked in the CSVs created by `custompoller`\nSSH into the custompoller node and check if the CSVs contain metrics for the suspected probe name.\nFrom `un2.bigdata.abc.gr` with root (cyberark login):\n``` bash\nsu - ipvpn\nssh custompoller@nnmdis01\ngrep fraport_Frap-ZTH2_64_ce ipvpn/out/*.LOADED | head -10\n# Here we see that the probe name is the same as the one we see in bigcust.nnmcp_ipvpn_slametrics_hist\n```\n### 3. Validate probe device types against business rules\n3. After reviewing the [business documentation](https://metis.xyztel.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/tree/master/docs) we found that valid device types for export are `'ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes'` and that the `device_type` field is derived from the probe name.\nWe checked the probe names with valid device type.\nFrom `un2.bigdata.abc.gr` with root (cyberark login):\n``` bash\nsu - ipvpn\nimpala-shell -i un-vip.bigdata.abc.gr -k --ssl\n```\nQuery valid probe names from Impala for device types accepted by the CSV export logic.\n``` sql\nselect distinct qa_probe_name from bigcust.nnmcp_ipvpn_slametrics_hist where par_dt='20230202' and customer = 'fraport' and  device_type  IN ('ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes');\n```\n    |                             qa_probe_name                             |\n    | :-------------------------------------------------------------------: |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-HDQ-01_cpe_fraport      |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-KGS-01_cpe_fraport      |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-KGS-02_cpe_fraport      |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-RHO-02_cpe_fraport      |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-SMI-01_cpe_fraport      |\n    |        avail_saa-csr1_ip-sla-probe_fraport_Frap12_cpe_fraport         |\n    |        avail_saa-csr1_ip-sla-probe_fraport_Frap13_cpe_fraport         |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-CFU-01_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-CFU-02_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-JMK-01_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-JSI-01_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-SKG-01_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-SKG-02_cpe_fraport      |\n    |        avail_saa-csr2_ip-sla-probe_fraport_Frap15_cpe_fraport         |\n    |      avail_saa-csr3_ip-sla-probe_fraport_FG4G-CHQ-02_cpe_fraport      |\n    |      avail_saa-csr3_ip-sla-probe_fraport_FG4G-EFL-01_cpe_fraport      |\n    |      avail_saa-csr3_ip-sla-probe_fraport_FG4G-RHO-01_cpe_fraport      |\n    |      avail_saa-csr4_ip-sla-probe_fraport_FG4G-CHQ-01_cpe_fraport      |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap01_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap01_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap02_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap02_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap03_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap03_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap04_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap04_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap05_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap05_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap06_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap06_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap07_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap07_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap09_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap09_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap10_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap10_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap11_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap11_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap12_ce_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap13_ce_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap14_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap14_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap15_ce_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap16_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap16_cpe_fraport         |\n    |    avail_saa-csr5_ip-sla-probe_fraport_frap-21p2000380_ce_fraport     |\n    | avail_saa-csr5_ip-sla-probe_fraport_fraport-DIA-21N1003241_ce_fraport |\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap11-backup_ce_fraport     |\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap12-backup_ce_fraport     |\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap13-backup_ce_fraport     |\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap14-backup_ce_fraport     |\n    |    rttd-pl_saa-csr2_ip-sla-probe_fraport_Frap15-backup_ce_fraport     |\n    |    rttd-pl_saa-csr3_ip-sla-probe_fraport_Frap06-backup_ce_fraport     |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap01-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap01_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap02-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap02_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap03-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap03_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap04-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap04_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap05-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap05_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap06_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap07-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap07_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap09-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap09_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap10-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap10_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap11_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap12_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap13_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap14_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap15_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap16-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap16_ce_fraport        |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap01_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap02_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap03_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap04_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap05_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap06_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap07_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap09_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap10_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap11_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap12_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap13_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap14_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap15_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap16_ce_fraport       |\n### 4. Compare and isolate invalid probe format\n4. By comparing the probe from the two queries, we see that the device type for the new devices is `64` and therefore it is omitted from the CSVs. The probe name is configured by the customer on their end\n### 5. Inform the customer\n5. Inform the customer about the problem:\nFor the new points you mention there are SLA metrics in the respective tables. These points are excluded when generating the CSV as the probe name does not match what is expected. Specifically:\nThe delimeter in the probe name is the \"_\" character, the device type is extracted from the probe name, and the measurements included in the CSVs are for the device types 'ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes'.\nThis probe is valid:\navail_saa-csr1_ip-sla-probe_fraport_Frap12_cpe_fraport\nWhile the following belonging to one of the points you mentioned is not:\navail_saa-csr5_ip-sla-probe_fraport_Frap-PVK1_64_ce\nAs you can see in the device type position it has the value 64 and is therefore excluded in the generated CSVs.\nPlease correct the probe names. If there is nothing else please let us know so we can close the ticket.\n## Affected Systems\nabc Bigstreamer CDSW",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230205-IM2076207.md"
        }
    },
    "407": {
        "page_content": "---\ntitle: Neighbor Tool API Failure Due to Expired JDBC Password\ndescription: The Neighbor Tool interface with Solvatio failed due to the expiration of the `neighb_user` Impala JDBC password, resulting in authentication errors on API calls. Resolved via JMX UI by updating the password in the Impala JDBC URL and restarting Wildfly.\ntags:\n  - bigstreamer\n  - neighbor tool\n  - solvatio\n  - jdbc\n  - impala\n  - password expired\n  - authentication failure\n  - api failure\n  - wildfly\n  - jmx\n  - impala jdbc driver\n  - cloudera\n  - unekl1\n  - unekl2\n  - haproxy\n  - nts\n  - serviceweaver\n  - bigstreamer-api\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2225720\n  system: abc BigStreamer NTS API\n  root_cause: Expired password of `neighb_user` used in JDBC Impala URL caused authentication failure in API\n  affected_component: wildfly-hosted Neighbor Tool API\n  fix_method: Updated JDBC password via JMX console at 172.25.37.247 and restarted Wildfly\n  symptom: API call returned Impala JDBC authentication failure error\n  test_url: https://cne.fgh.gr/bigstreamer-api/rest/neighbourAlgorithm/2109700572\n  api_servers: [unekl1, unekl2]\n---\n# abc - BigStreamer - IM2225720 - Neighbor Tool - Solvatio Interface Failure\n## Description\nThe Solvatio interface with the Neighbor tool does not work.\nThe password of the user neighbor_user expired, we reset it but when we hit the url https://cne.fgh.gr/bigstreamer-api/rest/neighbourAlgorithm/2109700572 (the fixed one you see is mine and you can use it for testing reasons) returns:\n```\n{\"error\": \"org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: [Cloudera][ImpalaJDBCDriver](500164) Error initialized or created transport for authentication: [Cloudera][ImpalaJDBCDriver](500592) Authentication failed..\", \"message\": \"Could not get JDBC Connection; nested exception is java.sql.SQLException: [Cloudera][ImpalaJDBCDriver](500164) Error initialized or created transport for authentication: [Cloudera][ImpalaJDBCDriver](500592) Authentication failed..\"}\n```\n## Root Cause Analysis\nIn order to check the logs of this we did the following:\n```bash\nssh unekl1/unekl2\nsu - wildfly\nbash\n```\nckeck logs:\n```bash\nekltaillog\n```\n## Resolution\nThe issue was caused by the expiration of the JDBC user password (`neighb_user`). After obtaining a new password from ABC Admin or the project's PM, we logged in to the JMX UI to apply the change.\n## Actions Taken\n```\nhttps://172.25.37.247:8543/serviceweaver/jmx/\n```\n[login_info](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/abc-syspasswd.kdbx?ref_type=heads)\nAnd under `servers -> com.xyz.abc_nts.bigstreamer_api.util -> mbeans -> name=BigStreamerApiJmxConfig -> attributes -> ImpalaURL` we change the password.\nUpdate the `PWD` value with the new password received from abc. Be sure not to change the rest of the connection string.\n```\njdbc:impala://172.25.37.237:21090;SSL=1;AuthMech=3;UID=neighb_user;PWD=HERE_ENTER_PASSWORD;\n```\nRetrieve the password from the abc Admin or xyz's PM\nThen start the wildfly service or restart:\nstart wildfly:\n`ekl-start`\nFor checks of the api from Haproxy : \n```\nhttps://cne.fgh.gr/bigstreamer-api/rest/neighbourAlgorithm/phone_number_provided_from_abc\n```\nFor checks of the API directly from the servers:\n```\nhttps://unekl1/bigstreamer-api/rest/neighbourAlgorithm/phone_number_provided_from_abc\nhttps://unekl2/bigstreamer-api/rest/neighbourAlgorithm/phone_number_provided_from_abc\n```\n## Recommendations\nCan be found on the following [issue](https://metis.xyztel.com/obss/bigdata/abc/nts/nts-devops-bigstreamer/-/issues/4)",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230928-IM2225720.md"
        }
    },
    "408": {
        "page_content": "---\ntitle: Huawei TV Load Failure for rel_play Tables on 20210831\ndescription: Investigation and partial recovery steps for missing partition in `huawei_tv.rel_play_*_hist` tables for 31/08, caused by empty source CSVs fetched from the remote SFTP server. Includes diagnosis via Impala, log inspection, SFTP verification, and manual reloading guidance.\ntags:\n  - bigstreamer\n  - huawei_tv\n  - rel_play_tv_hist\n  - rel_play_tvod_hist\n  - rel_play_vod_hist\n  - rel_vod_info_hist\n  - impala\n  - missing partitions\n  - hdfs\n  - intra\n  - sftp\n  - manual load\n  - huawei_tv_load.sh\n  - csv empty\n  - partition verification\n  - abc\n  - un2\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1631218\n  system: abc BigStreamer Huawei TV\n  root_cause: Empty CSVs pulled from SFTP for 20210901 which loads data into 20210831 partitions\n  affected_tables:\n    - huawei_tv.rel_play_tv_hist\n    - huawei_tv.rel_play_tvod_hist\n    - huawei_tv.rel_play_vod_hist\n    - huawei_tv.rel_vod_info_hist\n  missing_partition: 20210831\n  healthy_partition: 20210901\n  data_source_path: /export/20210901/\n  recovery_script: /shared/abc/huawei_tv/bin/huawei_tv_load.sh\n---\n# abc - IM1631218 - huawei tv  \n## Description\nLoad the tables huawei_tv.rel_play_tv_hist, huawei_tv.rel_play_tvod_hist, huawei_tv.rel_play_vod_hist for 31/8-1/9 and huawei_tv.rel_vod_info_hist for 1/9 with data.\nRequest to backfill missing partitions for 20210831 and 20210901 across multiple `huawei_tv` history tables due to previously ingested empty CSVs.\n## Actions Taken\n### Partition Verification in Impala\n1. Login to Hue and go to `Editor` > `Impala`\n2. Check for missing partitions as stated in description\n```sql\nselect count(*), par_dt \nfrom huawei_tv.rel_vod_info_hist  \nwhere par_dt between '20210825' and '20210905' \ngroup by par_dt \norder by par_dt;\nResult:\ncount(*)\tpar_dt\t\n...\n15026\t20210830\t\n14728\t20210901\t\n14748\t20210902\t\n...\n```\n3. Partition for `20210831` is actually missing but `20210901` is here and seems fine.\n### Check Ingestion Logs and Confirm Data Mappings\n4. Check logs of flow but first login to `un2`. And yes, huawei tv loads data for yesterday's partition from today's table #crazyright\n``` bash\n$ su - intra\n$ cd /shared/abc/huawei_tv/\n$ less log/huawei_tv_load.20210901.log\n...\nSFTP get files from : ./export/20210901\nConnected to 172.28.128.150.\nsftp> get export/20210901/*.csv /data/1/huawei_tv_LZ/\nFetching /export/20210901/EPG_SCHEDULE.csv to /data/1/huawei_tv_LZ/EPG_SCHEDULE.csv\nFetching /export/20210901/REL_PLAY_TV.csv to /data/1/huawei_tv_LZ/REL_PLAY_TV.csv\nFetching /export/20210901/REL_PLAY_TVOD.csv to /data/1/huawei_tv_LZ/REL_PLAY_TVOD.csv\nFetching /export/20210901/REL_PLAY_VOD.csv to /data/1/huawei_tv_LZ/REL_PLAY_VOD.csv\nFetching /export/20210901/REL_VOD_INFO.csv to /data/1/huawei_tv_LZ/REL_VOD_INFO.csv\n```\n### Inspect Source Files on SFTP Server\n5. Let's inspect those files\n```bash\n$ sftp bigdata@172.28.128.150:/export\nsftp> cd 20210901\nsftp> ls -l\n-rw-r--r--    1 0        0          476541 Sep  1 05:13 EPG_SCHEDULE.csv\n-rw-r--r--    1 0        0               0 Sep  1 05:11 REL_PLAY_TV.csv\n-rw-r--r--    1 0        0               0 Sep  1 05:10 REL_PLAY_TVOD.csv\n-rw-r--r--    1 0        0               0 Sep  1 05:11 REL_PLAY_VOD.csv\n-rw-r--r--    1 0        0         3470903 Sep  1 05:11 REL_VOD_INFO.csv\n-rw-r--r--    1 0        0        11414886 Sep  1 05:13 SubscriberID_STBMACAddress_Relationship.csv\n```\n### Root Cause and Recovery\nAs it is obvious, there are empty files, so abc needs to reload data into the remfgh server and we need to run the script for the missing partition:\n```bash\n$ /shared/abc/huawei_tv/bin/huawei_tv_load.sh 20210831\n```\n## Affected Systems\nabc Bigstreamer\n## Action Points\nWe have informed abc about each case and wait for their response.\nPartition `20210901` is fine but `20210831` needs manual action as nfghd in step 5.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210902-IM1631218.md"
        }
    },
    "409": {
        "page_content": "---\ntitle: Missing Partitions in aums.archive_data Table (March 30\u201331, 2022)\ndescription: Manual diagnosis and re-upload procedure for missing partitions in the `aums.archive_data` table for 2022-03-30 and 2022-03-31. Details SFTP file verification, local backup and re-upload, Streamsets automated reprocessing, and verification via HDFS and Impala.\ntags:\n  - bigstreamer\n  - aums\n  - archive_data\n  - streamsets\n  - missing data\n  - sftp\n  - partition load\n  - hdfs\n  - impala\n  - data pipeline\n  - data ingestion\n  - intra\n  - zip files\n  - reload\n  - streamsets logs\n  - manual reingestion\n  - aems_data\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1829518\n  system: abc BigStreamer AUMS ingestion\n  affected_table: aums.archive_data\n  missing_dates:\n    - 2022-03-30\n    - 2022-03-31\n  data_source: SFTP server at 172.16.166.30\n  file_pattern: aems_data_YYYYMMDDHHMMSS.zip\n  ingestion_tool: Streamsets\n  root_cause: Files existed on SFTP but ingestion was skipped\n  recovery_steps:\n    - Pull, delete, and re-upload missed zip files\n    - Trigger Streamsets to re-ingest\n    - Validate new partitions in Impala\n    - Monitor /shared/sdc/log\n---\n# abc - BigStreamer - IM1829518 -  missing data  aums.archive_data\n## Description\naums schema archive_data table has not loaded data for 03/30/2022\n## Actions Taken\n### 1. Check Partition in HDFS\n1.  Login to un2 and change to intra user with `sudo su - intra ` command\n2.  Give the following command in order to check the wanted partition\n```bash\n[intra@un2 ~]$ sudo -u hdfs hdfs dfs -ls /ez/warehouse/aums.db/archive_data/par_dt=20220330`\n```\nYou must be able to see the following ouput \n```bash\nls: /ez/warehouse/aums.db/archive_data/par_dt=20220330': No such file or directory\n```\n### 2. Refresh and Verify Table in Impala\n3.  Connect to impala with `intra` user in order to refresh the table\n` > refresh aums.archive_data;`\n4.  Check if you can see the missing data with the following command from impala using `intra` user:\n```bash\n> show files in aums.archive_data partition (par_dt>='20220329');\n```\nIf not then let's check the sftp server. You will notice that files for 31/03/2022 also missing.\n### 3. Validate Files on SFTP Server\n5.  From un2: `ssh bigd@172.16.166.30/;`\n```bash\nsftp> ls aums\n```\nYou must be able to see the zip files : aems_data_20220329233417.zip and aems_data_20220330233347.zip for 30/03/2022 and 31/03/2022.\n### 4. Re-download and Re-upload Missing Files\n6. Lets try to put those file to a local directory, remove them and upload them with the following commands: \n```bash\n[intra@un2 data_aums]$ sftp bigd@172.16.166.30\n```\nConnected to 172.16.166.30.\nLocally transfer the file for 30/03/2022:\n```bash\nsftp> get aems_data_20220330233347.zip\n```\nRemove file:\n```bash\nsftp> rm  aems_data_20220330233347.zip\n```\nLocally transfer the file for 31/03/2022:\n```bash\nsftp> get aems_data_20220331233417.zip\n```\nRemove file:\n```bash\nsftp> rm  aems_data_20220331233417.zip\n```\nNow, let's upload those files again:\n```bash\nsftp> put aems_data_20220330233347.zip`\nsftp> put aems_data_20220331233417.zip\n```\n### 5. Verify Partition Reload via Impala\n7. Streamsets won't upload those files simultaneously. You will be able to see first the partition for 30/03/2022 and secondly partition for 31/03/2022.\nFrom impala shell with `intra` user run the following command and make sure you will be able to see the missing partitions\n```bash\n> show files in aums.archive_data partition (par_dt>='20220330');\n```\n### 6. Monitor Streamsets Logs\n8. Check logs at un2:/shared/sdc/log",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20220331-IM1829518.md"
        }
    },
    "410": {
        "page_content": "---\ntitle: Duplicate CDSW Scheduled Job Executions in Energy Bills Project\ndescription: Investigation of duplicate executions of CDSW jobs (notably `Set_Point_Automation`) caused by multiple scheduler entries. Steps include CDSW usage confirmation, pod and cron job inspection, PostgreSQL analysis on the `sense` database, and reproduction of root cause via UI misuse.\ntags:\n  - bigstreamer\n  - cdsw\n  - scheduler\n  - job duplication\n  - kubernetes\n  - cron\n  - set_point_automation\n  - energy bills\n  - postgres\n  - cloudera\n  - cdsw cron\n  - duplicate jobs\n  - cron pod\n  - cron job\n  - sample project\n  - job history\n  - cdsw ui bug\n  - kubectl\n  - mncdsw1\n  - sense db\n  - postgres queries\n  - root cause analysis\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1900072\n  system: abc BigStreamer CDSW scheduler\n  root_cause: Duplicate cron entries in PostgreSQL `crons` table caused by multiple clicks on Update button\n  affected_jobs:\n    - Set_Point_Automation\n    - Energy Bills project jobs\n    - Additional duplicates in 25+ jobs across multiple projects\n  tools:\n    - Cloudera CDSW\n    - kubectl\n    - PostgreSQL (database: `sense`)\n  recovery_advice: Ensure users only click once when editing job schedules; use a supported browser\n---\n# abc - BigStreamer - IM1900072 - Execution schedule job\n## Description\nThere is a problem with the jobs in cloudera data science, specifically the Energy Bills project and the Set_Point_Automation job at the time and date that it is scheduled to run, it was observed that it starts and runs 2 times at the same time.\nThis is easy to understand from the job history and happens in particular cases.\n## Actions Taken\n### 1. Validate Duplicate Job Execution in CDSW GUI\n1. Login to CDSW on https://mncdsw1.bigdata.abc.gr with personal account.\n2. From the Admin tab select usage.\n3. Confirm duplicate submitted jobs.\n### 2. Access the CDSW Kubernetes Environment\n4. SSH to `mncdsw1.bigdata.abc.gr` with personal account and change to `root` with sudo\n### 3. Identify and Inspect the Cron Scheduler Pod\n5. Find the scheduler pod\n``` bash\nkubectl get pods\nNAME                                          READY   STATUS             RESTARTS   AGE\narchiver-7c6656cf45-nklb2                     1/1     Running            0          159d\ncdsw-compute-pod-evaluator-849b98f9fd-rvg58   1/1     Running            0          159d\ncron-7d5f5656c7-ws77j                         1/1     Running            0          159d                 <---- This looks good\ndb-84f767b84c-tnr2j                           1/1     Running            0          159d\ndb-migrate-f260849-x6f9z                      0/1     Completed          0          159d\nds-cdh-client-6bd7476b5b-r268f                1/1     Running            0          159d\nds-operator-576c7459d6-wr4hc                  1/1     Running            1          159d\nds-reconciler-5cd476896d-8lnn7                1/1     Running            1          159d\nds-vfs-7f6578594b-dgnrx                       1/1     Running            0          159d\nfeature-flags-54f7f97948-zd4gw                1/1     Running            0          159d\ngrafana-cml-dashboards-f260849-4v7k4          0/1     Completed          0          159d\ngrafana-core-cd44d8dff-b2hhg                  1/1     Running            0          159d\nimage-puller-5cxg2                            1/1     Running            1          159d\nimage-puller-5khss                            1/1     Running            31         159d\nimage-puller-cgbls                            1/1     Running            35         159d\nimage-puller-f8876                            1/1     Running            34         159d\nimage-puller-vjkcp                            1/1     Running            40         159d\nimage-puller-vs6b7                            1/1     Running            38         159d\nimage-puller-w8wc2                            1/1     Running            1          159d\ningress-controller-78fc7d87b8-jntf8           1/1     Running            0          159d\nkube-state-metrics-656687dd48-zh66p           1/1     Running            0          159d\nlivelog-85fb8d8974-fnwkr                      1/1     Running            0          159d\nlivelog-cleaner-1656288000-gjfjw              0/1     Completed          0          2d11h\nlivelog-cleaner-1656374400-5pqd7              0/1     Completed          0          35h\nlivelog-cleaner-1656460800-vnjfw              0/1     Completed          0          11h\nlivelog-publisher-5rhbv                       1/1     Running            39         159d\nlivelog-publisher-f68qs                       1/1     Running            37         159d\nlivelog-publisher-j9p22                       1/1     Running            1          159d\nlivelog-publisher-rp4pp                       1/1     Running            39         159d\nlivelog-publisher-rv6h5                       1/1     Running            3          159d\nlivelog-publisher-wb6cn                       1/1     Running            43         159d\nlivelog-publisher-xc2wm                       1/1     Running            40         159d\nmodel-proxy-69867f6ff6-ljcdv                  1/1     Running            1          159d\nprometheus-core-686874bbbc-nzn9p              0/1     CrashLoopBackOff   35926      159d                 <---- This looks not good\nprometheus-node-exporter-d6n5v                1/1     Running            0          159d\nprometheus-node-exporter-flhq7                1/1     Running            23         159d\nprometheus-node-exporter-gxh2h                1/1     Running            0          159d\nprometheus-node-exporter-kvvjv                1/1     Running            24         159d\nprometheus-node-exporter-n47w5                1/1     Running            23         159d\nprometheus-node-exporter-sxtxp                1/1     Running            23         159d\nprometheus-node-exporter-wb4lf                1/1     Running            23         159d\nruntime-repo-puller-74f488b875-dj8f8          1/1     Running            0          159d\ns2i-builder-775cc65845-28k88                  1/1     Running            0          159d\ns2i-builder-775cc65845-qww92                  1/1     Running            0          159d\ns2i-builder-775cc65845-t8rp6                  1/1     Running            0          159d\ns2i-client-7979d87646-skh8m                   1/1     Running            0          159d\ns2i-git-server-5b6c4c4df9-8jczc               1/1     Running            0          159d\ns2i-queue-65cc5dd86b-6sckk                    1/1     Running            0          159d\ns2i-registry-75565bc6d4-zls79                 1/1     Running            0          159d\ns2i-registry-auth-58c4b8ddb-lgbf5             1/1     Running            0          159d\ns2i-server-6549bc9f86-zbxl9                   1/1     Running            1          159d\nsecret-generator-76994558c6-fl8sn             1/1     Running            0          159d\nspark-port-forwarder-29gfq                    1/1     Running            0          159d\nspark-port-forwarder-5w9hr                    1/1     Running            0          159d\nspark-port-forwarder-jss7r                    1/1     Running            23         159d\nspark-port-forwarder-kpkrh                    1/1     Running            23         159d\nspark-port-forwarder-r2lrj                    1/1     Running            23         159d\nspark-port-forwarder-tm757                    1/1     Running            23         159d\nspark-port-forwarder-zkb2h                    1/1     Running            24         159d\ntcp-ingress-controller-647b484f4c-fl6tr       1/1     Running            1          159d\nusage-reporter-d46bcdb59-cswll                1/1     Running            0          159d\nweb-6c75f94ff4-k2z7m                          1/1     Running            8          159d\nweb-6c75f94ff4-vfb2h                          1/1     Running            9          159d\nweb-6c75f94ff4-vl4p4                          1/1     Running            8          159d\n```\n### 4. Verify Duplicate Job Triggers from Scheduler Logs\n6. Confirm that jobs were submitted by the CDSW scheduler\n``` bash\nkubectl logs cron-7d5f5656c7-ws77j | grep job=624 \n# Job ID can be obtained from the URL when inspecting the job from the Web GUI\n2022-06-29 06:00:00.001\t1\tINFO   \tCron                          \tStart  submitting cron job\tdata = {\"jobId\":624,\"spec\":\"0 0 9 * * *\",\"timezone\":\"Europe/Athens\",\"url\":\"http://web.default.svc.cluster.local/api/v1/tasks/start-job?job=624\"}\n2022-06-29 06:00:00.001\t1\tINFO   \tCron                          \tStart  submitting cron job\tdata = {\"jobId\":624,\"spec\":\"0 0 9 * * *\",\"timezone\":\"Europe/Athens\",\"url\":\"http://web.default.svc.cluster.local/api/v1/tasks/start-job?job=624\"}\n```\n### 5. Investigate Cron Pod and Image Configuration\n7. Find out how scheduling works\n``` bash\nkubectl describe pod cron-7d5f5656c7-ws77j \nName:         cron-7d5f5656c7-ws77j\nNamespace:    default\nPriority:     0\nNode:         mncdsw1.bigdata.abc.gr/10.255.241.130\nStart Time:   Fri, 21 Jan 2022 02:24:28 +0200\nLabels:       app=cron\n              hash=f260849\n              pod-template-hash=7d5f5656c7\n              role=cron\n              version=f260849\nAnnotations:  <none>\nStatus:       Running\nIP:           100.66.0.9\nIPs:\n  IP:           100.66.0.9\nControlled By:  ReplicaSet/cron-7d5f5656c7\nContainers:\n  cron:\n    Container ID:   docker://c7f4ef220646d428b24a6f3fbc2460605997509ee3db4e2abea472e165b85178\n    Image:          docker-registry.infra.cloudera.com/cdsw/cron:f260849\n    Image ID:       docker://sha256:c18d89235586e85434bd1fd3878317926d337c27c0e59ab360bed04f33c9c904\n    Port:           <none>\n    Host Port:      <none>\n    State:          Running\n      Started:      Fri, 21 Jan 2022 02:28:34 +0200\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      memory:  100Mi\n    Requests:\n      cpu:     50m\n      memory:  100Mi\n    Environment:\n      POSTGRESQL_USER:         <set to the key 'postgresql.user' in secret 'internal-secrets'>  Optional: false\n      POSTGRESQL_PASS:         <set to the key 'postgresql.pass' in secret 'internal-secrets'>  Optional: false\n      POSTGRESQL_DB:           <set to the key 'postgresql.db' in secret 'internal-secrets'>    Optional: false\n      ZONEINFO:                /zoneinfo.zip\n      WEB_IP:                  web.default.svc.cluster.local\n      DB_IP:                   db.default.svc.cluster.local                                                                    <---- This looks good\n      SERVICE_ACCOUNT_SECRET:  <set to the key 'service.account.secret' in secret 'internal-secrets'>  Optional: false\n      DOMAIN:                  mncdsw1.bigdata.abc.gr\n      LOG_LEVEL:               INFO\n    Mounts:                    <none>\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:            <none>\nQoS Class:          Burstable\nNode-Selectors:     <none>\nTolerations:        node.kubernetes.io/not-ready:NoExecute for 300s\n                    node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:             <none>\n```\n### 6. Locate and Connect to the PostgreSQL DB Pod\n8. Find the database pod\n```bash\nkubectl get svc -o wide\nNAME                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                         AGE    SELECTOR\narchiver                     ClusterIP   100.77.53.223    <none>        4444/TCP                                        159d   role=archiver\ncdsw-compute-pod-evaluator   ClusterIP   100.77.186.84    <none>        443/TCP                                         159d   app.kubernetes.io/instance=cdsw-compute,app.kubernetes.io/name=pod-evaluator\ndb                           ClusterIP   100.77.236.38    <none>        5432/TCP                                        159d   role=db       <---- This looks good         \nds-cdh-client                ClusterIP   100.77.254.36    <none>        80/TCP                                          159d   role=ds-cdh-client\nds-operator                  ClusterIP   100.77.155.3     <none>        80/TCP                                          159d   role=ds-operator\nds-vfs                       ClusterIP   100.77.33.62     <none>        80/TCP                                          159d   role=ds-vfs\nfeature-flags                ClusterIP   100.77.113.165   <none>        80/TCP                                          159d   role=feature-flags\ngrafana                      ClusterIP   100.77.6.156     <none>        3000/TCP                                        159d   app=grafana,component=core\nkube-state-metrics           ClusterIP   100.77.234.114   <none>        8080/TCP                                        159d   app=kube-state-metrics\nkubernetes                   ClusterIP   100.77.0.1       <none>        443/TCP                                         159d   <none>\nlivelog                      ClusterIP   100.77.177.53    <none>        80/TCP                                          159d   app=livelog\nmodel-proxy                  ClusterIP   100.77.170.230   <none>        80/TCP                                          159d   role=model-proxy\nprometheus                   ClusterIP   100.77.188.189   <none>        9090/TCP                                        159d   app=prometheus,component=core\nprometheus-node-exporter     ClusterIP   None             <none>        9100/TCP                                        159d   app=prometheus,component=node-exporter\nruntime-repo-puller          ClusterIP   100.77.192.194   <none>        3000/TCP                                        159d   role=runtime-repo-puller\ns2i-builder                  ClusterIP   100.77.191.97    <none>        5051/TCP                                        159d   role=s2i-builder\ns2i-client                   ClusterIP   100.77.185.37    <none>        5051/TCP                                        159d   role=s2i-client\ns2i-git-server               ClusterIP   100.77.212.112   <none>        80/TCP                                          159d   role=s2i-git-server\ns2i-queue                    ClusterIP   100.77.175.149   <none>        5672/TCP                                        159d   role=s2i-queue\ns2i-registry                 ClusterIP   100.77.0.134     <none>        5000/TCP                                        159d   role=s2i-registry\ns2i-registry-auth            ClusterIP   100.77.0.139     <none>        5001/TCP                                        159d   role=s2i-registry-auth\ns2i-server                   ClusterIP   100.77.232.217   <none>        5051/TCP                                        159d   role=s2i-server\ntcp-ingress-controller       ClusterIP   100.77.133.250   <none>        80/TCP                                          159d   role=tcp-ingress-controller\nusage-reporter               ClusterIP   100.77.187.214   <none>        3000/TCP                                        159d   role=usage-reporter\nweb                          ClusterIP   100.77.204.152   <none>        80/TCP,9229/TCP,35729/TCP,20050/TCP,20051/TCP   159d   role=web\n\nkubectl get pods -l role=db\nNAME                  READY   STATUS    RESTARTS   AGE\ndb-84f767b84c-tnr2j   1/1     Running   0          159d\n```\n8. Connect to the database\n```\n kubectl exec -it db-84f767b84c-tnr2j bash\nbash-4.4$ psql\npsql (12.1)\nType \"help\" for help.\npostgres=# \\l\n```\n| Name      | Owner    | Encoding | Collate    | Ctype      | Access privileges      |\n| --------- | -------- | -------- | ---------- | ---------- | ---------------------- |\n| postgres  | postgres | UTF8     | en_US.utf8 | en_US.utf8 |                        |\n| sense     | postgres | UTF8     | en_US.utf8 | en_US.utf8 |                        |\n| template0 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          + |\n|           |          |          |            |            | postgres=CTc/postgres  |\n| template1 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          + |\n|           |          |          |            |            | postgres=CTc/postgres  |\n### 7. Explore the CDSW Job Metadata in PostgreSQL (`sense`)\n9.  Database sense is the only viable candidate\n```sql\npostgres=# \\c sense\nYou are now connected to database \"sense\" as user \"postgres\".\nsense=# select * from pg_tables;\n```\n| schemaname    | tablename               | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity |\n| ------------- | ----------------------- | ---------- | ---------- | ---------- | -------- | ----------- | ----------- |\n| feature_flags | client_applications     | sense      |            | t          | f        | f           | f           |\n| feature_flags | migrations              | sense      |            | t          | f        | f           | f           |\n| feature_flags | client_instances        | sense      |            | f          | f        | f           | f           |\n| feature_flags | client_metrics          | sense      |            | t          | f        | f           | f           |\n| feature_flags | events                  | sense      |            | t          | f        | f           | f           |\n| feature_flags | features                | sense      |            | t          | f        | f           | f           |\n| pg_catalog    | pg_statistic            | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_type                 | postgres   |            | t          | f        | f           | f           |\n| feature_flags | strategies              | sense      |            | t          | f        | f           | f           |\n| public        | batch_runs              | sense      |            | t          | f        | t           | f           |\n| public        | authorized_keys         | sense      |            | t          | f        | t           | f           |\n| public        | build_details           | sense      |            | t          | f        | t           | f           |\n| public        | applications            | sense      |            | t          | f        | t           | f           |\n| public        | access_keys             | sense      |            | t          | f        | t           | f           |\n| public        | dashboard_pods          | sense      |            | t          | f        | t           | f           |\n| public        | crons                   | sense      |            | t          | f        | f           | f           |\n| pg_catalog    | pg_foreign_server       | postgres   |            | t          | f        | f           | f           |\n| public        | clusters                | sense      |            | t          | f        | t           | f           |\n| public        | custom_quota            | sense      |            | t          | f        | f           | f           |\n| pg_catalog    | pg_authid               | postgres   | pg_global  | t          | f        | f           | f           |\n| public        | engine_images           | sense      |            | t          | f        | t           | f           |\n| public        | dashboards_usage        | sense      |            | t          | f        | f           | f           |\n| public        | default_quota           | sense      |            | t          | f        | f           | f           |\n| public        | engine_images_editors   | sense      |            | t          | f        | t           | f           |\n| pg_catalog    | pg_statistic_ext_data   | postgres   |            | t          | f        | f           | f           |\n| public        | engine_sizes            | sense      |            | t          | f        | f           | f           |\n| public        | followers               | sense      |            | t          | f        | t           | f           |\n| public        | flow_uploads            | sense      |            | t          | f        | t           | f           |\n| public        | licenses                | sense      |            | t          | f        | t           | f           |\n| public        | invitations             | sense      |            | t          | f        | t           | f           |\n| public        | kerberos                | sense      |            | t          | f        | t           | f           |\n| public        | jobs                    | sense      |            | t          | f        | t           | f           |\n| public        | migrations              | sense      |            | t          | f        | f           | f           |\n| public        | flow_upload_chunks      | sense      |            | t          | f        | t           | f           |\n| public        | engine_statuses         | sense      |            | t          | f        | t           | f           |\n| public        | job_notifications       | sense      |            | t          | f        | t           | f           |\n| public        | model_builds            | sense      |            | t          | f        | t           | f           |\n| pg_catalog    | pg_user_mapping         | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_subscription         | postgres   | pg_global  | t          | f        | f           | f           |\n| public        | models                  | sense      |            | t          | f        | t           | f           |\n| pg_catalog    | pg_attribute            | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_proc                 | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_class                | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_attrdef              | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_constraint           | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_inherits             | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_index                | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_operator             | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_opfamily             | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_opclass              | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_am                   | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_amop                 | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_amproc               | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_language             | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_largeobject_metadata | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_aggregate            | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_largeobject          | postgres   |            | t          | f        | f           | f           |\n| public        | model_deployments       | sense      |            | t          | f        | t           | f           |\n| pg_catalog    | pg_statistic_ext        | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_rewrite              | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_trigger              | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_event_trigger        | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_description          | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_cast                 | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_enum                 | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_namespace            | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_conversion           | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_depend               | postgres   |            | t          | f        | f           | f           |\n### 8. Match Project and Job Metadata\n10. Insepect the public.projects table \n```sql\nsense=# select * from public.projects limit 1;\n```\n| id  | user_id | parent_id | name     | slug     | description | repository | created_at                 | updated_at                 | environment | organization_permission | project_visibility | creator_id | shared_memory_limit | size | crn                                   | default_project_engine_type | creation_status | creation_error_message |\n| --- | ------- | --------- | -------- | -------- | ----------- | ---------- | -------------------------- | -------------------------- | ----------- | ----------------------- | ------------------ | ---------- | ------------------- | ---- | ------------------------------------- | --------------------------- | --------------- | ---------------------- |\n| 311 | 31      |           | testRpac | testrpac |             |            | 2021-09-17 07:37:01.476738 | 2021-09-20 14:37:37.470235 |             |                         | private            | 31         |                     |      | /5c173548-19ed-4a05-8546-e99fdf4fc35f | legacy_engine               |                 |                        |\n11. Insepect the public.jobs table \n```sql\nsense=# select * from public.jobs limit 1;\n```\n| id  | project_id | creator_id | name                    | description | script                                                                                               | schedule | parent_id | timeout | timeout_kill | paused | created_at                 | updated_at                 | type   | original_parent_id | original_id | environment | timezone        | success_recipients | failure_recipients | timeout_recipients | include_logs | report_attachments | send_from_creator | share_token | cluster_id | director_instance_type | director_job_type | director_worker_count | reply_to | stopped_recipients | memory | cpu | engine_image_id | kernel  | nvidia_gpu | shared_view_visibility | arguments | runtime_id |\n| --- | ---------- | ---------- | ----------------------- | ----------- | ---------------------------------------------------------------------------------------------------- | -------- | --------- | ------- | ------------ | ------ | -------------------------- | -------------------------- | ------ | ------------------ | ----------- | ----------- | --------------- | ------------------ | ------------------ | ------------------ | ------------ | ------------------ | ----------------- | ----------- | ---------- | ---------------------- | ----------------- | --------------------- | -------- | ------------------ | ------ | --- | --------------- | ------- | ---------- | ---------------------- | --------- | ---------- |\n| 28  | 91         | 19         | training_until_20180117 |             | Aris_tmp_dev/20180209_Training in all measurements from Dec to 17 Jan, Scoring on 18 Jan/Training.py |          |           |         | f            | f      | 2018-02-09 17:28:05.412371 | 2018-02-09 17:28:05.412371 | manual |                    |             | {}          | Europe/Helsinki |                    |                    |                    | t            |                    | f                 |             |            |                        |                   |                       |          |                    | 8      | 4   | 2               | python2 | 0          | private                |           |\n### 9. Detect Duplicate Cron Records\n12. Inspect the public.cron\n```sql\nsense=# select * from public.crons limit 1;\n--- The deleted column seems to be a boolean\n```\n\n| id  | schedule      | url                     | description | timezone        | deleted | processed | job_id |\n| --- | ------------- | ----------------------- | ----------- | --------------- | ------- | --------- | ------ |\n| 85  | 0 0 20 10 * * | /tasks/start-job?job=35 |             | Europe/Helsinki | t       | f         | 35     |\n\n13. Check for duplicate active jobs\n```sql\nsense=# select job_id from public.crons where deleted='f' group by job_id having count(job_id) > 1;\n```\n\n| job_id |\n| ------ |\n| 617    |\n| 514    |\n| 450    |\n| 451    |\n| 504    |\n| 529    |\n| 575    |\n| 543    |\n| 456    |\n| 574    |\n| 67     |\n| 571    |\n| 443    |\n| 598    |\n| 523    |\n| 459    |\n| 463    |\n| 544    |\n| 586    |\n| 572    |\n| 606    |\n| 618    |\n| 442    |\n| 614    |\n| 429    |\n| 619    |\n| 628    |\n### 10. Map Duplicate Cron Jobs to Project Names\n14. Find which jobs correspond to the IDs using the tables above\n```sql\nsense=# select b.name as project,\n    a.name as job\nfrom public.jobs a\n    join public.projects b on a.project_id = b.id\nwhere a.id in (\n        select job_id\n        from public.crons\n        where deleted = 'f'\n        group by job_id\n        having count(job_id) > 1\n    );\n```\n| project                           | job                                       |\n| --------------------------------- | ----------------------------------------- |\n| Machine_Learning                  | NetworkInventory_update                   |\n| Machine_Learning                  | Predictive_Dslam_Actor_ Siebel_Check      |\n| Machine_Learning                  | Predictive_BNG_Cisco_Alcatel              |\n| Machine_Learning                  | Predictive_Maintenance_Ericsson_VSWR      |\n| Machine_Learning                  | Critical_Huawei_Dslam                     |\n| Energy Bills                      | Pollaploi                                 |\n| Hardware_Failures                 | Burned FXs                                |\n| Energy Bills                      | Truncate & Refresh BTS Live Measurements  |\n| Mini_Events_&_NPA                 | NPA                                       |\n| Energy Efficiency                 | Element_buildings_actions_second_phase    |\n| Energy Efficiency                 | Set_Point_Automation                      |\n| CNEA                              | NeighbourAlgorithm_Start                  |\n| Cloudera_Training                 | Monthy dataset                            |\n| Power_Outage_Detector_Load_To_QPM | Power Detector Load to QPM 1              |\n| Power_Outage_Detector_Load_To_QPM | Power Detector Load to QPM 2              |\n| Power_Outage_Detector_Load_To_QPM | Power Detector Load to QPM 4              |\n| GeoFencing                        | GeoFencing                                |\n| Machine_Learning                  | Report_Penitas                            |\n| Aumms                             | air_condition_Setpoints_to_HUE            |\n| Mini_Events_&_NPA                 | Mini_Events                               |\n| Energy Bills                      | features_export                           |\n| Energy Bills                      | refresh_11                                |\n| Monitoring Flows                  | Flows_update_all_counters_15:00_no_par_dt |\n| Monitoring Flows                  | Flows_update_all_counters_07:00           |\n| Monitoring Flows                  | Flows_update_all_counters_12:00_no_par_dt |\n| Monitoring Flows                  | Flows_update_all_counters_07:00_no_par_dt |\n| Sample Project                    | test                                      |\n### 11. Reproduce the Problem in UI and Confirm Root Cause\n15. Reproduce the problem\nThe `test` job on `Sample Project` was created for testing how duplicate jobs are submitted. It turns out that if you change the job scheduling and you can manage to click the `Update` job button more than one times, it creates more than one job schedules. See the answer below:\n```text\nWe were able to reproduce the problem. It seems that when updating the job's execution time, the update button has been pressed twice, resulting in duplicate entries being created in the database that holds the schedules.\nTo fix the problem, please update the schedules by pressing the update job only once, even if it seems that no action is being performed. Also make sure you are using one of the app's supported browsers https://docs.cloudera.com/cdsw/1.9.2/requirements/topics/cdsw-supported-browsers.html.\n```\n## Affected Systems\nabc Bigstreamer CDSW",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20220629-IM1900072.md"
        }
    },
    "411": {
        "page_content": "---\ntitle: radacct_hist Incomplete Inserts Due to Hive Metastore Lock Timeout\ndescription: Investigation and resolution of missing hourly data in radius.radacct_hist for 2021-02-08 to 2021-02-10, traced to Hive Metastore lock timeout causing skipped inserts despite successful ingestion.\ntags:\n  - bigstreamer\n  - abc\n  - radius\n  - radacct\n  - radacct_hist\n  - ingestion\n  - impala\n  - hive\n  - hive-metastore\n  - lock-timeout\n  - data-missing\n  - load-table\n  - partition-fix\n  - cloudera\n  - 000_radius_ops\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1443515\n  cluster: abc\n  affected_hours:\n    - 2021-02-08 06:00 UTC\n    - 2021-02-09 06:00 UTC\n    - 2021-02-10 06:00 UTC\n  tables_investigated:\n    - radius.radacct_hist\n    - radius.radacct_load\n    - radius.radacct_orig_files\n  ingestion_files_restored:\n    - RAD___radacct_2021-02-08_07-30.csv\n    - RAD___radacct_2021-02-09_07-30.csv\n    - RAD___radacct_2021-02-10_07-30.csv\n  tools_used:\n    - impala-shell\n    - Hue\n    - 000_radius_ops.sh\n  root_cause:\n    - Hive Metastore innodb lock wait timeout\n  resolution:\n    - File recovery and manual re-run of ingestion procedure\n  related_cloudera_case: 752877\n---\n# abc - IM1443515 - radius.radacct_hist\n## Description\nBetween February 8\u201310, 2021, at 06:00 and 07:00 UTC, the radius.radacct_hist table failed to register expected counts due to skipped inserts. Although ingestion to radacct_load succeeded, no rows were inserted into the radacct_hist table. This was later attributed to a Hive Metastore lock timeout issue.\nthree-day reduction in registrations at 06:00 and 07:00 (02/08 - 02/10)\n## Actions Taken\n- Login to `un2.bigdata.abc.gr` with personal account and change to `intra` with sudo.\n- Compare the count of the inserted data between the radius.radacct_hist and the original files radius.radacct_orig_files\n```bash\n[intra@un2 ~]$ secimp\n[un-vip.bigdata.abc.gr:22222] > select par_dt,substr(acctupdatetime,1,13),count(*) from radius.radacct_hist where par_dt>'20210209' group by 1,2 order by 1,2;\n| 20210209 | 2021-02-09 06 | 597 |\n| 20210209 | 2021-02-09 07 | 697082 |\n\n[un-vip.bigdata.abc.gr:22222] > select substr(acctupdatetime,1,13),count(*) from radius.radacct_orig_files where acctupdatetime>'2021-02-09' group by 1 order by 1;\n| 2021-02-09 06                 | 1430757  |\n| 2021-02-09 07                 | 1393639  |\n```\nThe ingestion pipeline ran and files were stored in radacct_load, but the final insert into radacct_hist failed silently, confirming a backend issue in the post-processing phase.\n- Compare the total ingested lines with the total inserted lines for the provided dates/hours (the provided hours are in UTC time - Impala)\n```bash\n[intra@un2 ~]$ for i in {08..09};do grep -E \"2021/02/09 ${i}.*Total lines\" /shared/abc/radius/DataParser/scripts/log/radius_cron.log;done\n[2021/02/09 08:12:01] - info - Total lines :  <2130925>\n[2021/02/09 09:12:37] - info - Total lines :  <2136145>\n[intra@un2 ~]$ for i in {08..09};do grep -B 5 Modified /shared/abc/radius/log/000_radius_ops.20210209.log | grep -A 6 \"insert into radius.radacct_hist\" | grep -C 3 \"Query submitted at: 2021-02-09 ${i}\" | grep Modified;done\nModified 0 row(s) in 0.58s\nModified 2136145 row(s) in 12.15s\n```\nAs you can see, the data had been correctly inserted into radius.radacct_load (2130925) but the insert into the radius.radacct_hist had insert 0 rows @  09/02/2021 08:18:15.\nRepeat the same verification and file recovery steps for 08/02/2021 and 10/02/2021 and report the outcomes.\n- The ingested files have been backed up in the radius.radacct_orig_files. Find the correct hourly files corresponding to the missing records in `radacct_hist` for the given dates/hours\n```bash\n[intra@un2 ~]$ for i in {08..10};do hdfs dfs -ls /ez/warehouse/radius.db/radacct_orig_files/ | grep 202102${i}_08;done\n-rwxrwx--x+  3 hive hive  839787710 2021-02-08 08:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-08_07-30.csv.20210208_081002.utc\n-rwxrwx--x+  3 hive hive  844035825 2021-02-09 08:12 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-09_07-30.csv.20210209_081002.utc\n-rwxrwx--x+  3 hive hive  844035825 2021-02-09 08:12 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-10_07-30.csv.20210210_081001.utc\n```\n- Copy the files to the load table\n```bash\n[intra@un2 ~]$ hdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-08_07-30.csv.20210208_081002.utc /ez/warehouse/radius.db/radacct_load/\n```\nDo the same for the other two files.\n- Manually trigger the ingestion pipeline to finalize inserts into `radacct_hist`\n```bash\n[intra@un2 ~] /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.20210211.log.manual 2>&1\n```\n## Affected Systems\nabc Bigstreamer\n## Action Points\nResolution of Cloudera Issue 752877 - Hive Metastore innodb lock await time out which is the root cause of this issue.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210211-IM1443515.md"
        }
    },
    "412": {
        "page_content": "---\ntitle: Nagios Notifications Disabled on admin.bigdata.abc.gr\ndescription: Daily email notifications from nagios@bigdata.abc.gr were disabled upon customer request by setting `enable_notifications=0` in nagios.cfg and restarting the Nagios service. Includes GUI and log verification steps.\ntags:\n  - bigstreamer\n  - nagios\n  - notifications\n  - disable alerts\n  - admin.bigdata.abc.gr\n  - email sender\n  - monitoring\n  - nagios.cfg\n  - systemctl restart\n  - root login\n  - nagios GUI\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IMxxxxxxx\n  system: abc BigStreamer monitoring\n  root_cause: Customer no longer wishes to receive alert emails from Nagios; sender was nagios@bigdata.abc.gr\n  action_taken:\n    - Confirmed customer request to stop notifications\n    - Checked nagios.cfg and changed `enable_notifications=1` to `enable_notifications=0`\n    - Restarted Nagios service on admin.bigdata.abc.gr\n    - Verified change from Nagios GUI and log file\n  outcome: All Nagios notifications have been successfully disabled\n---\n# abc - IM2386183 - Disable Nagios Notifications from nagios@bigdata.abc.gr\n## Description\nDaily notification emails were being sent from `nagios@bigdata.abc.gr`, despite the monitoring alerts being non-critical and undesired by the customer. The customer confirmed they no longer wish to receive these Nagios notifications.\nnagios@bigdata.abc.gr\n20241003063537.8E45B2031700@admin.bigdata.abc.gr\n## Actions Taken\nWe initially contacted the customer and concluded that they wish not to receive any kind of notification anymore.\nWe therefore proceeded as follows:\n1. Login as root on admin.bigdata.abc.gr\n2. Check logs inside `/var/log/nagios/nagios.log`\nThen we checked the basic configuration of nagios service:\n```bash\ncd /etc/nagios\nvi nagios.cfg, where enable_notifications=1\n```\n3. We suggested to disable the notifications -> **enable_notifications=0** and to restart the nagios service again. After the change:\n```bash\nsystemctl restart nagios\nsystemctl status nagio\n```\n4.  Log in the Nagios GUI (https://admin.bigdata.abc.gr/nagios) with our groupnet credentials and check the **Notifications** Tab. After the restart, incoming notifications stopped.\nUseful info: https://support.nagios.com/forum/viewtopic.php?t=40328, https://bobcares.com/blog/nagios-turn-off-all-notifications/",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "XSD2453618-IM2386183.md"
        }
    },
    "413": {
        "page_content": "---\ntitle: AUMS Archive Metadata Table Not Loaded Due to Streamsets Pipeline Stalling\ndescription: Investigation and resolution of missing partitions in the `aums.archive_metadata` table after August 10. Streamsets pipeline was running without processing files; issue was resolved by manual restart and metadata refresh.\ntags:\n  - bigstreamer\n  - aums\n  - streamsets\n  - archive_metadata\n  - pipeline failure\n  - partition missing\n  - impala\n  - hive\n  - refresh metadata\n  - sftp files\n  - ingestion issue\n  - pipeline restart\n  - data loss\n  - manual load\n  - streamsets troubleshooting\n  - metadata pipeline\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2201796\n  system: abc BigStreamer\n  table_affected: aums.archive_metadata\n  root_cause: Streamsets pipeline was running but stalled; files remained unprocessed on SFTP\n  resolution: Manual restart of Streamsets pipeline and metadata refresh in Impala\n  pipeline_name: AUMS Metadata File Feed\n  recovery_date: 2023-08-16\n  verification: Manual queries on Impala confirmed data presence after restart\n---\n# abc - BigStreamer - IM2201796 - Table  aums.archive_metadata not loaded\n## Description\nThe aums.archive_metadata table is not loaded since 10/8.\n## Actions Taken\nWe followed the troubleshooting steps described in [this](../supportDocuments/applicationFlows/streamsets.md) support document.\n1. Logged in to Streamsets (https://172.25.37.236:18636/) with `sdc` user and credentials found [here](../../abc-devpasswd.kdbx)\n2. Checked `AUMS Metadata File Feed` pipeline status. It was running.\n3. Checked pipeline's logs. There were no logs and there were no records found.\n4. Login to un2 and then switch to user `intra`\n5. Check files on sftp server\n```bash\nsftp bigd@172.16.166.30\ncd aums\nls\n```\nThere were multiple zip files that had not been processed by the pipeline.\n6. Open an impala connection and check if there are any partitions created for the days the client had reported\n```sql\nsecimp   \nselect count(*) from aums.archive_metadata where par_dt = '20230811';\nselect count(*) from aums.archive_metadata where par_dt = '20230812';\nselect count(*) from aums.archive_metadata where par_dt = '20230813';\n```\nThe queries returned zero rows.\n7. Then we proceeded with the manual data insertion described [here](../supportDocuments/applicationFlows/streamsets.md#manually-inserting-missing-data-in-hive-and-impala). More specifically:\nAfter the **put command at step 3** of the mentioned guide, continue with the following steps below(8-10).\n8. Refreshed table on impala with `refresh aums.archive_metadata` and checked if new partitions were created (impaala queries at step 6). No new partitions had been created.\n9. From Streamsets UI, stop and then start  `AUMS Metadata File Feed` pipeline. If stopping takes too long you can **Force Stop** the pipeline.\n10. Then refresh impala again and ran queries about the days that the data was missing. The queries verified that data had loaded.\n11. Since the pipeline successfully resumed and processed the missing metadata files:\n- remove the <filename>_tmp.csv/zip file from the remfgh sftp directory with the sftp command rm <filename>_tmp.csv/zip. !IMPORTANT\n- clear the local directory from the unnecessary fetched data.\n## Our Ticket Response\n```\n16/08/23 15:20:30 Europe/Eastern (dsimantirakis):\nThank you. The ticket can be closed.\n16/08/23 15:09:29 Europe/Eastern (linker):\nRelated Incident IM2201796 has been updated.\nSuccessfully loaded the aums.archive_metadata table.\n```\n```\n16/08/23 15:09:29 Europe/Eastern (linker):\nRelated Incident IM2201796 has been updated.\nIncident Status Change to Pending Customer from Work In Progress\n14/08/23 16:53:50 Europe/Eastern (linker):\nRelated Incident IM2201796 has been updated.\nWe will get back to you when the investigation is complete.\n```\n## References\n- https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/streamsets.md\n- https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/issues/20220331-IM1829518.md\n",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230814-IM2201796.md"
        }
    },
    "414": {
        "page_content": "---\ntitle: Cyberark SSH Connection Fails Intermittently on Virtualop Node\ndescription: Investigation of intermittent SSH connection failures to virtualop node (172.26.169.11) via Cyberark. Issue caused by restricted source IPs not included in `/etc/hosts.allow`. Resolved by whitelisting all Cyberark subnets.\ntags:\n  - bigstreamer\n  - virtualop\n  - ssh\n  - cyberark\n  - access denied\n  - sshd_config\n  - hosts.allow\n  - ssh failure\n  - connection refused\n  - /var/log/secure\n  - whitelisting\n  - login issue\n  - intermittent failure\n  - subnet restriction\n  - security\n  - remote access\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2211937\n  system: abc BigStreamer\n  node_affected: virtualop (172.26.169.11)\n  root_cause: Source IPs used by Cyberark not allowed by /etc/hosts.allow\n  resolution: Added all Cyberark subnets to hosts.allow; SSH access stabilized\n  logs_reference: /var/log/secure\n  security_files: /etc/hosts.allow, /etc/ssh/sshd_config\n---\n# abc - BigStreamet - IM2211937 - virtualop ssh error\n## Description\nOn the virtualop node - 172.26.169.11, sometimes we can connect via Cyberark with ssh and other times it shows us the attached error message.\nInitial inspection of the SSH configuration (`/etc/ssh/sshd_config`) revealed no misconfigurations. We also restarted the sshd service, but the behavior remains.\n![IM2211937](.media/IM2211937.jpg)\n## Actions Taken\n1. After confirming the same problem on our end, ie connecting to the host through Cyberark, we login to the `admin` server first and then ssh into `virtualop`. We checked the sshd config under `/etc/ssh/sshd_config` for any glaring issues, but as was communicated by the customer, none were found.\n2. The next step is to check `/var/log/secure` in order to see if the authentication issue is due to the host or if it doesn't even reach it. In there, while performing a test connection, we identified the following log entry `refused connect from 10.53.134.71 (10.53.134.71)` which signifies that the host itself refused the connection.\n3. We check for the existence of `/etc/host.allow` and we added the above address, before testing again. The test failed again, this time with a diffrent IP being refused inside `/var/log/secure`, which lead us to believe that Cyberark is using multiple IPs and subnets to facilitate connections.\n4. We requested all subnets used by Cyberark from the customer in order to add them to `/etc/hosts.allow` and after doing so no more connection issues appeared. One thing to nfgh here is that for any new subnets that Cyberark will use they must also be added to `/etc/hosts.allow` or the same connection issues will reappear.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230904-IM2211937.md"
        }
    },
    "415": {
        "page_content": "---\ntitle: RStudio Connect License Expired - Activation Recovery Procedure\ndescription: Step-by-step procedure for resolving expired license issues in RStudio Connect, including system time sync, proxy setup, license activation/deactivation with `license-manager`, and service restarts. Covers common errors and recovery instructions for failed activation due to internet or certificate issues.\ntags:\n  - bigstreamer\n  - rstudio\n  - rstudio-connect\n  - license expired\n  - license-manager\n  - activation error\n  - proxy\n  - time sync\n  - reboot\n  - unrstudio1\n  - rstudio-connect\n  - ssl\n  - product key\n  - verify license\n  - timezone\n  - systemctl\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: login failure 04/10\n  system: abc BigStreamer RStudio Connect\n  error_message: License Expired\n  root_cause: license-manager could not reach license server due to proxy/SSL issues\n  server: unrstudio1\n  recovery_commands:\n    - license-manager deactivate\n    - license-manager activate\n    - license-manager verify\n    - systemctl restart rstudio-connect\n  special_case: activation error (13) caused by incorrect system time or timezone\n---\n# abc - RStudio - login failure 04/10\n## Description\nRStudio Connect returned a license expiration message on 04/10. Attempts to connect failed due to inability to verify or reactivate the license. This was likely caused by system time issues or inability to reach the license server over HTTPS.\nThe message was the following:\n```\nAn error has occurred\nLicense Expired\nYour RStudio Connect license has expired. Please contact your Customer Success representative or email sales@rstudio.com to obtain a current license.\n```\n## Actions Taken\n### Initial Troubleshooting and Environment Preparation\n1. ssh unrstudio1\n2. Make sure the time zone is correct for the machine. (sudo timedatectl)\n3. Resync the date and time of the machine. (sudo hwclock -w)\n### Deactivation and Activation Attempt\n4. /opt/rstudio-connect/bin/license-manager deactivate\n```\nError deactivating product key: (19): Connection to the server failed. Ensure that you have a working internet connection, you've configured any required proxies, and your system's root CA certificate store is up to date; see https://rstudio.org/links/licensing_ssl for more information.\n```\n5. /opt/rstudio-connect/bin/license-manager activate <product-key>\n```\nError verify: (19): The product is activated however the license manager is currently unable to connect to the license server to verify the activation.\nPlease ensure that you can make a connection to the activation server and then re-activate the product.\n```\n### Proxy Configuration for License Server Access\n6. To pass the error from the step 4&5 then export `export http_proxy=<ip:port>` & 'export https_proxy=<ip:port>' 'export http_proxy=<ip:port>'\n7. Try again to deactivate like step 4\n### License Status and Verification\n8. Try again to activate like step 5 and then run the below commands:\n```bash\nsudo /opt/rstudio-connect/bin/license-manager status\nsudo /opt/rstudio-connect/bin/license-manager verify\n/opt/rstudio-connect/bin/license-manager verify #run without sudo\n```\n### Service Restart\n9. systemctl restart rstudio-connect # ONLY IF the Activaton Status on step 8 was `Activated`\n10. systemctl status rstudio-connect\n## Affected Systems\nabc Bigstreamer Rstudio-Connect\n### Troubleshooting Activation Error Code (13)\n**In case you  receive the following error while executing step 5:**\n```\nError activating product key: (13): The activation has expired or the system time has been tampered with. Ensure your time, timezone, and date settings are correct. If you're sure the license is not expired, try performing the following steps, in order: \n1. Fix the timezone on your system.\n2. Fix the date on your system.\n3. Fix the time on your system.\n4. Perform a system restart (important!)\n```\nYou must **reboot** your node and then repeat 1-10 steps",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20211005-IM1663315.md"
        }
    },
    "416": {
        "page_content": "---\ntitle: CSI_fix_01212021_w03.txt Exported Empty Due to Missing DSL Stats Partition\ndescription: Root cause analysis of missing data in CSI_fix_01212021_w03.txt caused by gaps in the dsl_stats_week_xdsl_hist table, and steps to identify, validate, and manually reinsert missing data from ADSL/VDSL source tables.\ntags:\n  - bigstreamer\n  - abc\n  - csi_fix\n  - brond\n  - dsl_stats\n  - xdsl_hist\n  - partition-missing\n  - hue\n  - impala\n  - coordinator\n  - manual-insert\n  - export-validation\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1421557\n  cluster: abc\n  export_file: CSI_fix_01212021_w03.txt\n  tables_checked:\n    - brond.dsl_stats_week_xdsl_hist\n    - brond.brond_vdsl_stats_week\n    - brond.brond_adsl_stats_week\n  root_cause:\n    - coordinator failure for par_dt 20210119\n  actions_taken:\n    - data validated in source tables\n    - missing insert executed manually via impala-shell\n  followup_docs:\n    - knowledge-base/abc/BigStreamer/20201125-IM1363226.md\n    - systems-info/abc/BigStreamer/Brond/cube_indicators_pipeline.md\n  scripts_used:\n    - /user/intra/brond_dsl_stats/impala-shell/populate*.sql\n---\n# abc - IM1421557 - CSI_fix_01212021_w03.txt file with no data \n## Description\nThis issue occurred due to missing records in a dependent partition of the brond.dsl_stats_week_xdsl_hist table used in the CSI fix pipeline. The result was an empty export for CSI_fix_01212021_w03.txt.\nYesterday's file was zero.\n-1 172.25.37.240 CSI_FIXED CSI_fix_01212021_w03.txt 0 1/21/2021 11:00:10 AM 35 1/21/2021 11:00:10 AM CSI_FIXED:CSI_fix_01212021_w03.txt:20210121110010035918\n## Actions Taken\n1. Execute steps 1-5 from [this doc](knowledge-base/abc/BigStreamer/20201125-IM1363226.md)\n2. After the check we saw that table `brond.dsl_stats_week_xdsl_hist` had no data for 20210119 and 20210120, even though upstream tables were populated.\n```bash\nselect count(*), par_dt\nfrom brond.dsl_stats_week_xdsl_hist\nwhere par_dt >= '20210115'\ngroup by 2\norder by 2;\nResult:\ncount(*)\tpar_dt\t\n...\n2491814\t20210117\t\n2491872\t20210118\t\n2494261\t20210121\t\n...\n```\n3. Check coordinator `coord_brond_load_dsl_daily_stats` that populates this table as explained in [here](systems-info/abc/BigStreamer/Brond/cube_indicators_pipeline.md). \n4. Check that source tables `brond.brond_vdsl_stats_week` and `brond.brond_adsl_stats_week` have data for these partitions. Same query for `brond.brond_adsl_stats_week`\n```sql\nselect count(*), par_dt\nfrom brond.brond_vdsl_stats_week\nwhere par_dt >= '20210115'\ngroup by 2\norder by 2;\nResult:\ncount(*)\tpar_dt\t\n...\n1806006\t20210116\t\n1806256\t20210117\t\n1806306\t20210118\t\n1808049\t20210119\t\n1808918\t20210120\t\n1810234\t20210121\t\n1811401\t20210122\t\n```\nAs source tables have data, we have to execute only the missing inserts for the affected partitions.\n> Since both ADSL and VDSL source tables contained data for `20210119` and `20210120`, the issue is isolated to the execution of the DSL stats coordinator.\n5. Impala insert queries are under `/user/intra/brond_dsl_stats/impala-shell/populate*.sql`. Change conditions `..par_dt > '20210127..` to get the dates that are missing. In this case `20210119`. Execute the queries using Hue or impala-shell.\n6. Repeat step 2 to validate that data are loaded correctly for missing dates.\n7. Complete steps 6-10 from [this doc](knowledge-base/abc/BigStreamer/20201125-IM1363226.md).\n## Affected Systems\nabc Bigstreamer",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210122-IM1421557.md"
        }
    },
    "417": {
        "page_content": "---\ntitle: Location Mobility File Export Failure Due to SSH Permission Denial\ndescription: Location Mobility file exports stopped on 2023-09-15 at 11:00 due to the `mtuser` not having SSH access to `un-vip.bigdata.abc.gr`, preventing the execution of the export script. Restored by reauthorizing the user.\ntags:\n  - bigstreamer\n  - location mobility\n  - file export\n  - mtuser\n  - oozie\n  - hue\n  - ssh failure\n  - un-vip\n  - permission error\n  - automation failure\n  - reconciliation logs\n  - lte\n  - smsIn\n  - smsOut\n  - voiceIn\n  - voiceOut\n  - export stopped\n  - script failure\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM-untracked\n  system: abc BigStreamer\n  root_cause: `mtuser` lacked SSH permissions to run export script on `un-vip.bigdata.abc.gr`\n  date_detected: 2023-09-15\n  first_failed_file: LM_02_lte_20230915_00004.txt\n  resolution: SSH access restored, file export resumed\n  validation_method: Checked via Oozie Editor for mtuser workflows in Hue\n  export_script: ssh -i id_rsa mtuser@un-vip.bigdata.abc.gr \"script\"\n---\n# abc - BigStreamer - IM2217968 - Location Mobility Files Not Exported on 15/9\n## Description\nFrom 15/9 11:00 Location Mobility Files are not exported.\n```bash\nun2 /shared/abc/location_mobility/logging\nrw-rw-r-- 1 mtuser mtuser  867968 Sep 15 11:03 LM_02_lte_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1254780 Sep 15 11:01 LM_03_smsIn_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1254098 Sep 15 11:01 LM_04_smsOut_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1333387 Sep 15 11:03 LM_05_voiceInOut_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1285358 Sep 15 11:04 LM_06_voiceIn_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1302383 Sep 15 11:04 LM_07_voiceOut_reconciliation.log\n```\nExport history shows that the last successful LTE file was:\n2023-09-15 05:02:15 LM_02_lte_20230915_00001.txt 2023091502 7742068\n2023-09-15 07:02:27 LM_02_lte_20230915_00002.txt 2023091504 5880766\n2023-09-15 09:02:37 LM_02_lte_20230915_00003.txt 2023091506 8227530\n2023-09-15 11:03:21 LM_02_lte_20230915_00004.txt 2023091508 19753878\nThe location mobility files are updated each day by running the following script:\n```bash\nssh -o \"StrictHostKeyChecking no\" -i ./id_rsa mtuser@un-vip.bigdata.abc.gr \"script\"\n```\nThis command initiates a remote execution of the export script by user `mtuser` on the primary data node `un-vip.bigdata.abc.gr`.\n## Actions Taken\nAfter investigating we found that the **mtuser** user did not have the necessary permissions and was not authorized to connect to the main server, so the script never ran.\nAfter updating the permissions for the user, the script started running again.\nYou can verify that the script is executing correctly by logging into the Hue Server and opening the Oozie Editor. Filter workflows by user `mtuser` to see the status of scheduled jobs.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230915-IM2217968.md"
        }
    },
    "418": {
        "page_content": "---\ntitle: NNMi Integration Testing with BigStreamer and nnmprd01\ndescription: First-stage integration testing of the upgraded NNMi 2020.11 (nnmprd01) with BigStreamer Custompoller for IPVPN/Syzefxis and validation of Oozie workflows (Postgres to Impala ingestion).\ntags:\n  - bigstreamer\n  - abc\n  - nnmprd01\n  - nnmdis01\n  - nnmi\n  - custompoller\n  - snmp\n  - ipvpn\n  - syzefxis\n  - postgres\n  - oozie\n  - integration\n  - ssh\n  - sqoop\n  - cron\n  - impala\n  - workflow\n  - coordinator\n  - snmpwrapper\n  - directory-creation\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: GI7\n  system: IPVPN, Syzefxis, and Oozie pipelines\n  environment: abc BigStreamer\n  source_host: nnmprd01\n  java_wrapper: SNMPWrapperRunner\n  workflow_name: NNM_Workflow\n  connection_type: SSH, Postgres, Sqoop, SNMP polling\n  primary_blockers:\n    - missing directories at /opt/OV for custompoller user\n    - PostgreSQL access restrictions (pg_hba.conf)\n  data_validation: manual count comparison with nnmdis01\n---\n# abc - BigStreamer - GI7 - Integration of xyz services with nnmprd01\n## Description\nInitial integration of the upgraded NNMi 2020.11 (hosted on nnmprd01) with BigStreamer\u2019s Custompoller SNMP tools and Oozie-based data ingestion. Focus areas include:\n1. Execution and validation of SLA Custompoller for IPVPN and Syzefxis from nnmprd01\n2. Validation of Oozie workflows that extract reference data from NNMi PostgreSQL database\n## Actions Taken\nfor SLA Custompoller:\n1. Login at `un2` with your personal account and switch to `ipvpn` user:\n```bash\n[u15@un2 ~]$ sudo su - ipvpn\n[ipvpn@un2 ~]$ \n```\n2. Check ssh to `nnmprd01`. Communication must be passwordless:\n```bash\n[ipvpn@un2 ~]$ ssh custompoller@nnmprd01\npassword:\n```\nWe were asked for the password. We had to execute the following and try again:\n```bash\n[ipvpn@un2 ~]$ ssh-copy-id custompoller@nnmprd01\n[ipvpn@un2 ~]$ ssh custompoller@nnmprd01\nActivate the web console with: systemctl enable --now cockpit.socket\nThis system is not registered to Red Hat Insights. See https://cloud.redhat.com/\nTo register this system, run: insights-client --register\nLast login: Wed Apr 21 14:23:46 2021 from 172.25.37.236\n[custompoller@nnmprd01 ~]$ \n```\nSsh connectivity good to go!\n3. Execute Custompoller for IP-VPN and inspect output and logs:\n```bash\n[custompoller@nnmprd01 ~]$ crontab -l\n#*/5 * * * * /home/custompoller/run/run_syzeyksis_standby.sh &>> /home/custompoller/log/syzeyksis-`date +\"\\%Y-\\%m-\\%d\"`.log\n#17 03 * * * /home/custompoller/run/zip_folders_syzeyksis.sh &>> /home/custompoller/log/syzeyksis-`date +\"\\%Y-\\%m-\\%d\"`.log\n######## IP VPN #################################\n#*/5 * * * * /home/custompoller/ipvpn/run/run_ipvpn.sh &>> /home/custompoller/ipvpn/log/ipvpn-`date +\"\\%Y-\\%m-\\%d\"`.log\n#03 */6 * * * /home/custompoller/ipvpn/run/zip_folders_ipvpn.sh &>> /home/custompoller/ipvpn/log/ipvpn-`date +\"\\%Y-\\%m-\\%d\"`.log\n[custompoller@nnmprd01 ~]$ cat /home/custompoller/ipvpn/run/run_ipvpn.sh\n#!/bin/bash\nexport JAVA_HOME=/home/custompoller/jdk1.8.0_144\nexport PATH=$JAVA_HOME/bin:$PATH\ncd /home/custompoller/ipvpn/run \n/home/custompoller/ipvpn/run/java -Xms1024m -Xmx4096m -DlogFilename=/home/custompoller/ipvpn/log/ipvpn -Dlog4j.configurationFile=/home/custompoller/ipvpn/conf/log4j2.xml -cp ./bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner -config /home/custompoller/ipvpn/conf/vpn.config -directorytomove /home/custompoller/ipvpn/out/ -version 1 -timeout 1500 -retries 2\nsleep 10\necho \"[`date '+%Y/%m/%d %T'`] - INFO: Executing ssh command..\"\nssh ipvpn@172.25.37.236 'nohup /shared/abc/nnm_custompoller_ipvpn/DataParser/scripts_nnmprod/nnm_custompoller_ipvpn.pl -r -m -po >> /shared/abc/nnm_custompoller_ipvpn/log/nnmcustompoller_ipvpn_cron.`date \"+%Y%m%d\"`.log 2>&1 &'\n[custompoller@nnmprd01 ~]$ export JAVA_HOME=/home/custompoller/jdk1.8.0_144\n[custompoller@nnmprd01 ~]$ export PATH=$JAVA_HOME/bin:$PATH\n[custompoller@nnmprd01 ~]$ cd /home/custompoller/ipvpn/run \n[custompoller@nnmprd01 run]$ \n[custompoller@nnmprd01 run]$ /home/custompoller/ipvpn/run/java -Xms1024m -Xmx4096m -DlogFilename=/home/custompoller/ipvpn/log/ipvpn -Dlog4j.configurationFile=/home/custompoller/ipvpn/conf/log4j2.xml -cp ./bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner -config /home/custompoller/ipvpn/conf/vpn.config -directorytomove /home/custompoller/ipvpn/out/ -version 1 -timeout 1500 -retries 2\n[custompoller@nnmprd01 run]$ \n[custompoller@nnmprd01 run]$ less /home/custompoller/ipvpn/log/ipvpn-2021-04-21.log \n...\n  14:26:50.446 ERROR [Thread-25] [athe-saa-new] SNMPWalkTool: snmpWalkByOidsException: \n  java.io.IOException: No such file or directory\n        at java.io.UnixFileSystem.createFileExclusively(Native Method) ~[?:1.8.0_144]\n        at java.io.File.createNewFile(File.java:1012) ~[?:1.8.0_144]\n        at com.xyz.bigstreamer.snmp.tools.snmp4jwalk.SNMPWalkTool.createLockFile(SNMPWalkTool.java:183) ~[bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar:1.0.1-SNAPSHOT]\n        at com.xyz.bigstreamer.snmp.tools.snmp4jwalk.SNMPWalkTool.snmpWalkByOids(SNMPWalkTool.java:68) [bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar:1.0.1-SNAPSHOT]\n        at com.xyz.bigstreamer.snmp.tools.wrapper.runnables.NodeRunner.run(NodeRunner.java:33) [bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar:1.0.1-SNAPSHOT]\n```\nOups! Are all directories ok?\n```\n[custompoller@nnmprd01 run]$ ll /home/custompoller/ipvpn/\ntotal 12\nlrwxrwxrwx 1 custompoller custompoller   28 Feb 25 11:07 backup -> /opt/OV/_CUSTOM_POLLER/ipvpn       ### it's red and blinking\ndrwxrwxr-x 2 custompoller custompoller   42 Apr 21 12:23 conf\ndrwxrwxr-x 2 custompoller custompoller   90 Apr 21 12:12 log\nlrwxrwxrwx 1 custompoller custompoller   33 Feb 25 11:07 out -> /opt/OV/_CUSTOM_POLLER_PROD/ipvpn     ### it's red and blinking\ndrwxrwxr-x 2 custompoller custompoller 4096 Apr 21 12:24 out2\ndrwxrwxr-x 2 custompoller custompoller 4096 Jul 30  2020 run\n[custompoller@nnmprd01 ipvpn]$ ll /opt/OV/_CUSTOM_POLLER/ipvpn\nls: cannot access '/opt/OV/_CUSTOM_POLLER/ipvpn': No such file or directory\n[custompoller@nnmprd01 ipvpn]$ ll /opt/OV/_CUSTOM_POLLER/\nls: cannot access '/opt/OV/_CUSTOM_POLLER/': No such file or directory\n[custompoller@nnmprd01 ipvpn]$ mkdir /opt/OV/_CUSTOM_POLLER/\nmkdir: cannot create directory \u2018/opt/OV/_CUSTOM_POLLER/\u2019: Permission denied\n### custompoller is not a sudoer. I tried :(\n```\nOk, so first nfgh: System administrator of this server must give us some directories to store our data. Why?\n```bash\n[custompoller@nnmprd01 run]$ df -h .\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/mapper/vg0-home   10G  800M  9.3G   8% /home\n```\nWell, 10GB will be done in the first day or 2.\nOk, but just to check the files, lets create a second directory for output files and execute the java thingy again.\n```bash\n[custompoller@nnmprd01 ipvpn]$ pwd\n/home/custompoller/ipvpn\n[custompoller@nnmprd01 ipvpn]$ mkdir out2\ncustompoller@nnmprd01 ~]$ export JAVA_HOME=/home/custompoller/jdk1.8.0_144\n[custompoller@nnmprd01 ~]$ export PATH=$JAVA_HOME/bin:$PATH\n[custompoller@nnmprd01 ~]$ cd /home/custompoller/ipvpn/run \n[custompoller@nnmprd01 ipvpn]$ /home/custompoller/ipvpn/run/java -Xms1024m -Xmx4096m -DlogFilename=/home/custompoller/ipvpn/log/ipvpn -Dlog4j.configurationFile=/home/custompoller/ipvpn/conf/log4j2.xml -cp ./bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner -config /home/custompoller/ipvpn/conf/vpn.config -directorytomove /home/custompoller/ipvpn/out2/ -version 1 -timeout 1500 -retries 2\n[custompoller@nnmprd01 run]$ less /home/custompoller/ipvpn/log/ipvpn-2021-04-21.log \n...\n    12:24:16.796 INFO [Thread-12] [athe-saa13] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:16.796 INFO [Thread-12] [athe-saa13] NodeRunner: Ended Thread for output file = [athe-saa13]\n    12:24:16.943 INFO [Thread-14] [athe-saa15] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:16.943 INFO [Thread-14] [athe-saa15] NodeRunner: Ended Thread for output file = [athe-saa15]\n    12:24:17.020 INFO [Thread-17] [athe-saa18] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:17.020 INFO [Thread-17] [athe-saa18] NodeRunner: Ended Thread for output file = [athe-saa18]\n    12:24:17.156 INFO [Thread-16] [athe-saa17] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:17.156 INFO [Thread-16] [athe-saa17] NodeRunner: Ended Thread for output file = [athe-saa17]\n    12:24:19.262 ERROR [Thread-5] [athe-saa2] SNMPWalkTool: For ip=62.103.1.232, Table OID=1.3.6.1.4.1.9.9.42.1.3.5.1.12, error=Request timed out.\n    12:24:22.656 INFO [Thread-20] [athe-saa21] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:22.656 INFO [Thread-20] [athe-saa21] NodeRunner: Ended Thread for output file = [athe-saa21]\n    12:24:24.462 ERROR [Thread-5] [athe-saa2] SNMPWalkTool: For ip=62.103.1.232, Table OID=1.3.6.1.4.1.9.9.42.1.3.5.1.31, error=Request timed out.\n    12:24:25.162 INFO [Thread-5] [athe-saa2] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:25.162 INFO [Thread-5] [athe-saa2] NodeRunner: Ended Thread for output file = [athe-saa2]\n    12:24:25.162 INFO [main] [] SNMPWrapperRunner: END SNMPWrapperRunner\n[custompoller@nnmprd01 ipvpn]$ ll out2/\ntotal 179892\n-rw-rw-r-- 1 custompoller custompoller    70882 Apr 21 12:17 nnmcp.athe384g.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller    70924 Apr 21 12:24 nnmcp.athe384g.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  1028483 Apr 21 12:17 nnmcp.athe384o.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  1028930 Apr 21 12:24 nnmcp.athe384o.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller   621224 Apr 21 12:17 nnmcp.athe384q.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller   621550 Apr 21 12:24 nnmcp.athe384q.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4183130 Apr 21 12:17 nnmcp.athe-saa10.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4183386 Apr 21 12:24 nnmcp.athe-saa10.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4477609 Apr 21 12:17 nnmcp.athe-saa11.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4478033 Apr 21 12:24 nnmcp.athe-saa11.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  3422809 Apr 21 12:17 nnmcp.athe-saa1.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  3422557 Apr 21 12:24 nnmcp.athe-saa1.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  3940362 Apr 21 12:17 nnmcp.athe-saa12.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  3940483 Apr 21 12:24 nnmcp.athe-saa12.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4800030 Apr 21 12:17 nnmcp.athe-saa13.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4798949 Apr 21 12:24 nnmcp.athe-saa13.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4635658 Apr 21 12:17 nnmcp.athe-saa14.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4636384 Apr 21 12:24 nnmcp.athe-saa14.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4941176 Apr 21 12:17 nnmcp.athe-saa15.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4940436 Apr 21 12:24 nnmcp.athe-saa15.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4546605 Apr 21 12:17 nnmcp.athe-saa16.202104211215.txt\n...\n```\nOk, seems better! \n4. Compare the number and lines of the files with `nnmdis01`:\n```\n[custompoller@nnmprd01 ipvpn]$ wc -l out2/*202104211220*\n     496 out2/nnmcp.athe384g.202104211220.txt\n    7060 out2/nnmcp.athe384o.202104211220.txt\n    4268 out2/nnmcp.athe384q.202104211220.txt\n   28170 out2/nnmcp.athe-saa10.202104211220.txt\n   30532 out2/nnmcp.athe-saa11.202104211220.txt\n   23134 out2/nnmcp.athe-saa1.202104211220.txt\n   26692 out2/nnmcp.athe-saa12.202104211220.txt\n   32426 out2/nnmcp.athe-saa13.202104211220.txt\n   31288 out2/nnmcp.athe-saa14.202104211220.txt\n   33530 out2/nnmcp.athe-saa15.202104211220.txt\n   30788 out2/nnmcp.athe-saa16.202104211220.txt\n   31782 out2/nnmcp.athe-saa17.202104211220.txt\n   33070 out2/nnmcp.athe-saa18.202104211220.txt\n   25454 out2/nnmcp.athe-saa19.202104211220.txt\n   25920 out2/nnmcp.athe-saa20.202104211220.txt\n   73738 out2/nnmcp.athe-saa21.202104211220.txt\n   22634 out2/nnmcp.athe-saa2.202104211220.txt\n   24054 out2/nnmcp.athe-saa22.202104211220.txt\n   19006 out2/nnmcp.athe-saa23.202104211220.txt\n   28160 out2/nnmcp.athe-saa24.202104211220.txt\n    5516 out2/nnmcp.athe-saa25.202104211220.txt\n   30868 out2/nnmcp.athe-saa7.202104211220.txt\n   26232 out2/nnmcp.athe-saa8.202104211220.txt\n   28730 out2/nnmcp.athe-saa9.202104211220.txt\n    1304 out2/nnmcp.athe-saa-new.202104211220.txt\n      96 out2/nnmcp.n3400-ekal9kb.202104211220.txt\n      96 out2/nnmcp.n3400-maro9ka.202104211220.txt\n      48 out2/nnmcp.n3400-thes9ka.202104211220.txt\n      48 out2/nnmcp.n3400-toub9ka.202104211220.txt\n  625140 total\n[custompoller@nnmprd01 ipvpn]$ logout\nConnection to nnmprd01 closed.\n[ipvpn@un2 ~]$ ssh custompoller@nnmdis01\nLast login: Wed Apr 21 13:37:06 2021 from un2e.bigdata.abc.gr\n[custompoller@nnmdis01 ~]$ wc -l ipvpn/out/*202104211220*\n     496 ipvpn/out/nnmcp.athe384g.202104211220.txt.LOADED\n    7060 ipvpn/out/nnmcp.athe384o.202104211220.txt.LOADED\n    4268 ipvpn/out/nnmcp.athe384q.202104211220.txt.LOADED\n   28170 ipvpn/out/nnmcp.athe-saa10.202104211220.txt.LOADED\n   30532 ipvpn/out/nnmcp.athe-saa11.202104211220.txt.LOADED\n   23134 ipvpn/out/nnmcp.athe-saa1.202104211220.txt.LOADED\n   26692 ipvpn/out/nnmcp.athe-saa12.202104211220.txt.LOADED\n   32426 ipvpn/out/nnmcp.athe-saa13.202104211220.txt.LOADED\n   31288 ipvpn/out/nnmcp.athe-saa14.202104211220.txt.LOADED\n   33530 ipvpn/out/nnmcp.athe-saa15.202104211220.txt.LOADED\n   30788 ipvpn/out/nnmcp.athe-saa16.202104211220.txt.LOADED\n   31782 ipvpn/out/nnmcp.athe-saa17.202104211220.txt.LOADED\n   33070 ipvpn/out/nnmcp.athe-saa18.202104211220.txt.LOADED\n   25454 ipvpn/out/nnmcp.athe-saa19.202104211220.txt.LOADED\n   25920 ipvpn/out/nnmcp.athe-saa20.202104211220.txt.LOADED\n   73736 ipvpn/out/nnmcp.athe-saa21.202104211220.txt.LOADED\n   23526 ipvpn/out/nnmcp.athe-saa2.202104211220.txt.LOADED\n   24054 ipvpn/out/nnmcp.athe-saa22.202104211220.txt.LOADED\n   19006 ipvpn/out/nnmcp.athe-saa23.202104211220.txt.LOADED\n   28160 ipvpn/out/nnmcp.athe-saa24.202104211220.txt.LOADED\n    5516 ipvpn/out/nnmcp.athe-saa25.202104211220.txt.LOADED\n   30868 ipvpn/out/nnmcp.athe-saa7.202104211220.txt.LOADED\n   26232 ipvpn/out/nnmcp.athe-saa8.202104211220.txt.LOADED\n   28730 ipvpn/out/nnmcp.athe-saa9.202104211220.txt.LOADED\n    1304 ipvpn/out/nnmcp.athe-saa-new.202104211220.txt.LOADED\n      96 ipvpn/out/nnmcp.n3400-ekal9kb.202104211220.txt.LOADED\n      96 ipvpn/out/nnmcp.n3400-maro9ka.202104211220.txt.LOADED\n      48 ipvpn/out/nnmcp.n3400-thes9ka.202104211220.txt.LOADED\n      48 ipvpn/out/nnmcp.n3400-toub9ka.202104211220.txt.LOADED\n  626030 total\n```\nSeems fine for a first check.\n5. Repeat the same for Syzefxis. Changes are for the java process and the test directory.\n```\n[custompoller@nnmprd01 ~]$ mkdir out2\n[custompoller@nnmprd01 ~]$ java -Xms1024m -Xmx4096m -DlogFilename=/home/custompoller/log/syzeyksis -Dlog4j.configurationFile=/home/custompoller/conf/log4j2.xml -cp ./bigstreamer-snmp-tools-1.1.0.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner -config /home/custompoller/conf/syzeyksis.config -directorytomove /home/custompoller/out2/ -version 1 -timeout 1500 -retries 2\n```\nApart from the directories in step 3 nothing came up at this first check.\nfor Oozie Workflows:\n1. Login at Hue with your personal account and switch to Hue 3. Then go to `https://un1.bigdata.abc.gr:8889/oozie/list_oozie_coordinators/#`\n2. Search for `c_nms` coordinators.\n3. Click on coordinator `C_NMS Config, mplsdb.nms_node` and then its latest execution. At the top there is the name of the workflow, `NNM_Workflow`.\n4. At the left bar, under `Variables` we will obtain anything we need for a manual execution. Specifically:\n    * SPLIT_COLUMN\n    * partition\n    * tablename (Impala)\n    * password\n    * sourceTable\n    * schema\n5. We will first check for source table `nms_node`. Go to Impala and issue the following query to create a look-alive test table:\n```bash\nCREATE table nnmnps.nms_node_test\nlike nnmnps.nms_node;\n```\n6. Switch back to Hue 4. On the top search bar, write the workflow name `NNM_Workflow` and click on the result.\n7. Before `Submit` change the iptables@un2 to bind the nnmprd01 \n```\nFrom nnmdis01:\n-A PREROUTING -i bond0.300 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.25.150.68:5432\n-A OUTPUT -d 10.255.240.13/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.25.150.68:5432\n-A OUTPUT -d 10.255.240.14/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.25.150.68:5432\n-A OUTPUT -d 10.255.240.15/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.25.150.68:5432\nTo nnmprd01:\n-A PREROUTING -i bond0.300 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\n-A OUTPUT -d 10.255.240.13/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\n-A OUTPUT -d 10.255.240.14/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\n-A OUTPUT -d 10.255.240.15/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\nservice iptables reload\n```\n8. At the workflow page, click on `Submit`. Fill variables with the information taken at step 4. Change `tablename` to `nms_node_test` so that you don't overwrite the data. Click `Submit`.\n9. The first time the sqoop command failed. Inspect logs by clicking the `View logs` of the sqoop 1 stage. At the default logs page we will see the following error: `org.postgresql.util.PSQLException: FATAL: no pg_hba.conf entry for host \"172.25.37.236\", user \"postgres\", database \"nnm\"`\nOups vol.2! Second nfgh: abc must enable us to login to the database.\n## Affected Systems\nabc Bigstreamer IPVPN Syzefxis\n## Action Points\nAfter abc fixes the above we have a series of checks to perform. Check out the GI for details.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210421-GI12.md"
        }
    },
    "419": {
        "page_content": "---\ntitle: Agama Schema Missing Data for 02/04/2021 - Manual Script Execution\ndescription: Investigation and resolution of missing Agama schema data for 02/04/2021 in abc BigStreamer, including verification of file presence on SFTP, manual script parameter override, and Impala data validation.\ntags:\n  - bigstreamer\n  - abc\n  - agama\n  - data-ingestion\n  - sftp\n  - missing-data\n  - impala\n  - manual-run\n  - bash-scripts\n  - cron\n  - intra\n  - par_dt\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1500475\n  system: agama\n  ingestion_type: daily\n  ingestion_method: cron + bash script\n  ingestion_host: un2.bigdata.abc.gr\n  tools_used:\n    - cronta\n    - bash\n    - sftp\n    - impala-shell\n  missing_date: 2021-04-02\n  source_files:\n    location: sftp server\n    availability: confirmed\n  root_cause: automated ingestion did not trigger for 02/04/2021\n  resolution: manual override of script date parameters and re-run\n---\n# abc - IM1500475 - agama schema missing data 02/04\n## Description\nOn 2021-04-02, no data was ingested into the Agama schema. This issue required verifying whether source files were available and, if so, manually running the ingestion script with hardcoded date parameters.\n## Actions Taken\n1. SSH into `un2.bigdata.abc.gr` from `admin` using personal LDAP credentials.\n2. Switch to `intra` user:\n```bash\nsudo -iu intra\n```\n3. Verify ingestion cronjobs for Agama:\n```bash\ncronta -l | grep agama\n```\n4. Locate and inspect logs for the Agama table where data is missing.\n5. Also check if the files exist at `sftp_server:directory`\n6. If files exist modify script for the <table> un2:/shared/abc/agama/bin/`table`.sh\n7. comment dynamic date lines`yest_sftp`,`yest`,`dt_sftp`,`dt`. Uncomment the static  date values `dt_sftp`,`dt` (e.g `##dt_sftp=2021/07/07 ##dt=20210707`)\n8. Run the script and when is finished connnect to impala-sheel `select count(*),par_dt from agama.table where par_dt >= '2021xxxx' group by 2;`\n9. Edit again the script on un2 with the default values to run tomorrow\n## Affected Systems\nabc Bigstreamer",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210409-IM1500475.md"
        }
    },
    "420": {
        "page_content": "---\ntitle: brond.an_rollout_data_hist - Data Not Loaded After 20210924\ndescription: Step-by-step analysis and resolution of missing data in `brond.an_rollout_data_hist` after 20210924, including error diagnosis in logs, root cause identification due to ambiguous field reference post-upgrade, script patching, and manual reload of missed partitions using `000_brond_rollout_post.sh`.\ntags:\n  - bigstreamer\n  - brond\n  - rollout\n  - an_rollout_data_hist\n  - missing partitions\n  - manual reload\n  - field ambiguity\n  - script fix\n  - data pipeline\n  - cronjob\n  - data load failure\n  - hql\n  - impala\n  - yarn\n  - upgrade regression\n  - abc\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD1716699\n  system: abc BigStreamer Brond rollout ingestion\n  detection_target: partition gap in `brond.an_rollout_data_hist`\n  failed_partitions: [\"20210925\", \"20210926\", \"20211003\", \"20211004\", \"20211005\", \"20211006\"]\n  script_path: /shared/abc/brond/bin/000_brond_rollout_post.sh\n  log_path: /shared/abc/brond/log/brond_rollout_cron.YYYYMMDD.log\n  root_cause: field alias ambiguity in post-upgrade Impala SQL\n  fix: rename colid to colid1 in subquery alias in join\n  reload_command: /shared/abc/brond/bin/000_brond_rollout_post.sh <YYYYMMDD>\n---\n# abc - SD1716699 ( brond.an_rollout_data_hist ) \n## Description\nThis document describes the resolution of missing data in the `brond.an_rollout_data_hist` table after 24/09, because it stopped loading data due to a post-upgrade SQL ambiguity error in a shell script. The issue was corrected by modifying the join alias for colid, followed by re-execution of the backfill script for affected dates.\n### Flow Overview\n0. Flow info:\n```runs every day via crontab at 02:00: \nun2:/shared/abc/brond/DataParser/scriptsRollout/brond_rollout.pl\nas intra \nConnects to sftp 172.16.166.30\ntakes parameters from :\n/shared/abc/brond/DataParser/scriptsRollout/transferlist/brond_rollout.trn\n/shared/abc/brond/bin/000_brond_rollout_post.sh 20211006\nruns through:  brond_rollout.pl\nLOGs : /shared/abc/brond/log/brond_rollout_cron.*\n```\n## Actions Taken\n### Step 1 \u2013 Verify Recent Partition Loads\n1. Following query shows last 10 loads (it is Normal fow weekends to have no data):\n```sql\nselect par_dt,count (*) from brond.an_rollout_data_hist group by 1 order by 1 desc limit 10; \nQuery: select par_dt,count (*) from brond.an_rollout_data_hist group by 1 order by 1 desc limit 10\nQuery submitted at: 2021-10-07 12:05:44 (Coordinator: http://sn65.bigdata.abc.gr:25000)\nQuery progress can be monitored at: http://sn65.bigdata.abc.gr:25000/query_plan?query_id=70404f65e4fa418c:fc1d536d00000000\n+----------+----------+\n| par_dt   | count(*) |\n+----------+----------+\n| 20211001 | 27673    |\n| 20210930 | 27673    |\n| 20210929 | 27673    |\n| 20210928 | 27673    |\n| 20210927 | 27671    |\n| 20210924 | 27671    |\n| 20210923 | 27671    |\n| 20210922 | 27671    |\n| 20210921 | 27671    |\n| 20210920 | 27671    |\n+----------+----------+\n```\n### Step 2 \u2013 Check Cron Logs and Identify Root Cause\n2. Checked log /shared/abc/brond/log/brond_rollout_cron.xxx  at un2, \nit shows : \n```bash\n...\nWARNING: Use \"yarn jar\" to launch YARN applications.\n...\nERROR: AnalysisException: Column/field reference is ambiguous\n...\nWARNINGS: No partitions selected for incremental stats update\n...\n```\n### Step 3 \u2013 Fix the Failing SQL in Shell Script\n3. Due to upgrade, the following change was required at the \"/shared/abc/brond/bin/000_brond_rollout_post.sh\" script:\nchanging the query from:\n```sql\n( select eett,dslam, *colid*,colvalue from brond.brond_rollout_data_hist where par_dt='20210927' ) d on c.colid=*d.colid*\n```\nto :\n```sql\n( select eett,dslam, **colid colid1**,colvalue from brond.brond_rollout_data_hist where par_dt='20210927' ) d on c.colid=**d.colid1**\n```\n### Step 4 \u2013 Reload Missing Data Using Manual Script Execution\n4. to reload missing data eg for dates 20211003-7 , run :\n```bash\n/shared/abc/brond/bin/000_brond_rollout_post.sh 20211007\n/shared/abc/brond/bin/000_brond_rollout_post.sh 20211006\n/shared/abc/brond/bin/000_brond_rollout_post.sh 20211005\n/shared/abc/brond/bin/000_brond_rollout_post.sh 20211004\n/shared/abc/brond/bin/000_brond_rollout_post.sh 20211003\n```\n### Step 5 \u2013 Validate Partition Load Completion\n5. Check again with following query shows last 10 loads (it is Normal fow weekends to have no data):\n```sql\nselect par_dt,count (*) from brond.an_rollout_data_hist group by 1 order by 1 desc limit 10; \nQuery: select par_dt,count (*) from brond.an_rollout_data_hist group by 1 order by 1 desc limit 10\nQuery submitted at: 2021-10-07 12:05:44 (Coordinator: http://sn65.bigdata.abc.gr:25000)\nQuery progress can be monitored at: http://sn65.bigdata.abc.gr:25000/query_plan?query_id=70404f65e4fa418c:fc1d536d00000000\n+----------+----------+\n| par_dt   | count(*) |\n+----------+----------+\n| 20211007 | 27673    |\n| 20211006 | 27673    |\n| 20211005 | 27673    |\n| 20211004 | 27673    |\n| 20211001 | 27673    |\n| 20210930 | 27673    |\n| 20210929 | 27673    |\n| 20210928 | 27673    |\n| 20210927 | 27671    |\n| 20210924 | 27671    |\n+----------+----------+\n```\n## Affected Systems\nabc Bigstreamer",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20211006-IM1665032.md"
        }
    },
    "421": {
        "page_content": "---\ntitle: mn3 Node Lost Contact with Cloudera Manager Due to CPU Machine Check Error\ndescription: The mn3 node lost connectivity with Cloudera Manager due to CPU hardware errors (\"Machine check error detected\") which required a manual cold reboot via iDRAC. Post-reboot system logs confirmed a restart and recovery. No additional cause was identified.\ntags:\n  - bigstreamer\n  - mn3\n  - cloudera manager\n  - host monitor\n  - cpu error\n  - machine check\n  - idrac\n  - cold reboot\n  - health check\n  - cloudera-scm-agent\n  - bad health\n  - node unreachable\n  - remote reboot\n  - dell hardware\n  - logs\n  - /var/log/messages\n  - root cause analysis\n  - ticket response\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2104114\n  system: abc BigStreamer Master / Management\n  root_cause: CPU machine check errors on mn3 requiring a cold reboot via iDRAC\n  component: mn3 host / hardware layer\n  resolution: Manual reboot via iDRAC, Cloudera agent verified post-recovery\n  cloudera_host_status: Out of contact due to CPU halt\n  logs_checked:\n    - /var/log/messages\n    - journalctl\n    - dmesg\n    - cloudera-scm-agent.log\n---\n# abc - BigStreamer - IM2104114 -  mn3 bad health\n## Description\nThe mn3 node is in bad health with the message:\nThis host has been out of contact with the Cloudera Manager Server for too long. This host is not in contact with the Host Monitor.\n## Actions Taken\n1. From `admin` tried to reach mn3 but with no response:\nAttempt to verify connectivity from admin to mn3 node.\n```bash\nping mn3\n```\n2. Since there is no response from `mn3` time to reboot server.\nLogin to `admin` from ad hoc connection and then type from command line `firefox`.\nSelect `default profile` and go to idrac for mn3 `https://10.255.242.85/` with the credentials you can find [here](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/prodsyspasswd.kdbx).\n3. From left Column click on `Server` and then go to `Logs`.\nAt that moment we see the following error for CPU:\n```bash\nMon Mar 13 2023 08:44:27 CPU 1 machine check error detected.\nMon Mar 13 2023 08:44:27 CPU 2 machine check error detected.\nMon Mar 13 2023 08:44:27 CPU 1 machine check error detected.\n```\n4. Time to reboot server:\nFrom `https://10.255.242.85/` go to Server and then click on `Power Cycle System (cold boot)`\n5. In order to check that server is up and running open java console, otherwise check logs from left Column click on `Server` and then go to `Logs`\n6. Login to Cloudera Manager with your personal account and then to `Hosts` -> `All hosts` and search for mn3.\nVerify that it is back again to cluster.\n7. Login to `admin` and then ssh to `mn3` in order to verify that cloudera agent is up and running.\n```bash\nsystemctl status cloudera-scm-agent status\n```\n## Investigation\n1. Investigated `/var/log/messages` and we saw bellow info:\n```bash\nMar 13 09:10:47 mn3.bigdata.abc.gr systemd-logind[1628]: New session 6590707 of user nagios.\nMar 13 09:10:47 mn3.bigdata.abc.gr systemd[1]: Started Session 6590707 of user nagios.\nMar 13 09:10:47 mn3.bigdata.abc.gr systemd-logind[1628]: Removed session 6590707.\nMar 13 09:10:47 mn3.bigdata.abc.gr systemd[1]: Removed slice User Slice of nagios.\nMar 13 10:52:20 mn3.bigdata.abc.gr rsyslogd:  [origin software=\"rsyslogd\" swVersion=\"8.24.0-57.el7_9\" x-pid=\"1848\" x-info=\"http://www.rsyslog.com\"] start\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] microcode: microcode updated early to revision 0x44, date = 2020-05-27\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Initializing cgroup subsys cpuset\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Initializing cgroup subsys cpu\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Initializing cgroup subsys cpuacct\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Linux version 3.10.0-1160.15.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) ) #1 SMP Wed Feb 3 15:06:38 UTC 2021\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] Command line: BOOT_IMAGE=/vmlinuz-3.10.0-1160.15.2.el7.x86_64 root=/dev/mapper/vg00-root ro crashkernel=auto rd.lvm.lv=vg00/root rd.lvm.lv=vg00/swap rhgb quiet transparent_hugepage=never ipv6.disable=1 LANG=en_US.UTF-8\nMar 13 10:52:20 mn3.bigdata.abc.gr kernel: [    0.000000] e820: BIOS-provided physical RAM map:\n```\nAs we see at `09:10:47` mn3 lost contact from Cloudera Manager.\n2. Investigated below logs between time that problem occurred but we didn't find any useful information\n```bash\njournalctl -S \"2023-03-13 08:00:00\" -U \"2023-03-13 10:50:00\"\nless /var/log/dmesg\nless /var/log/cloudera-scm-agent/cloudera-scm-agent.log\n```\n## Affected Systems\nabc Bigstreamer Master/Management Services\n## Our Ticket Response\nAfter investigation we noticed that there was a problem with the CPU due to some process and the node had to be rebooted in order to recover.\nWe will monitor the node and in case the problem appears again we will proceed with a ticket to Dell.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230313-IM2104114.md"
        }
    },
    "422": {
        "page_content": "---\ntitle: Procedure for Handling Missing Data in Radius Flow (radacct)\ndescription: End-to-end manual procedure for detecting, verifying, and recovering missing or unprocessed data in `radius.radacct_hist`, either from original HDFS files or SFTP source, and executing the ingestion and post-processing flow safely.\ntags:\n  - bigstreamer\n  - radius\n  - radacct\n  - radacct_hist\n  - radacct_orig_files\n  - missing data\n  - ingestion recovery\n  - hdfs\n  - sftp\n  - cron\n  - impala\n  - radius_ops\n  - dataparser\n  - manual reload\n  - log analysis\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: GI3\n  system: abc BigStreamer Radius ingestion\n  detection_target: missing hourly data in `radacct_hist`\n  log_path: /shared/abc/radius/log/000_radius_ops.YYYYMMDD.log\n  load_dir: /ez/warehouse/radius.db/radacct_load/\n  orig_dir: /ez/warehouse/radius.db/radacct_orig_files/\n  sftp_dir: /shared/radius_repo/cdrs/\n  metadata_file: /shared/radius_repo/radius_date.dat.local\n---\n# abc - BigStreamer - GI3 -  Radius : Procedure for Missing Data\n## Description\nStep-by-step diagnostic and recovery procedure for missing hourly files in the Radius ingestion pipeline. Covers both scenarios: data existing in radacct_orig_files (but not propagated to radacct_hist) and data missing entirely (needs SFTP transfer and ingestion replay). It includes log-based validation, manual recovery actions, and SQL verification queries.\n## Actions Taken\n**Question** :  Does  **radius.radacct_orig_files** table contains missing data ?\n**Compare files from SFTP Repository with files in Table**\ne.g\n**Sftp Repo Contents :**\n```\n-rw-r--r-- 1 intra intra 219749225 May  6 11:40 radacct_2021-05-04_00-00.csv.bz2\n-rw-r--r-- 1 intra intra 219497773 May  6 11:40 radacct_2021-05-04_01-30.csv.bz2\n-rw-r--r-- 1 intra intra 219166609 May  6 11:40 radacct_2021-05-04_03-00.csv.bz2\n-rw-r--r-- 1 intra intra 219090980 May  6 11:40 radacct_2021-05-04_04-30.csv.bz2\n-rw-r--r-- 1 intra intra 218865632 May  6 11:40 radacct_2021-05-04_06-00.csv.bz2\n-rw-r--r-- 1 intra intra 219100909 May  6 11:41 radacct_2021-05-04_07-30.csv.bz2\n-rw-r--r-- 1 intra intra 219262945 May  6 11:41 radacct_2021-05-04_09-00.csv.bz2\n-rw-r--r-- 1 intra intra 219734952 May  6 11:41 radacct_2021-05-04_10-30.csv.bz2\n-rw-r--r-- 1 intra intra 219753745 May  6 11:41 radacct_2021-05-04_12-00.csv.bz2\n-rw-r--r-- 1 intra intra 219985878 May  6 11:41 radacct_2021-05-04_13-30.csv.bz2\n-rw-r--r-- 1 intra intra 220428037 May  6 11:41 radacct_2021-05-04_15-00.csv.bz2\n-rw-r--r-- 1 intra intra 220573605 May  6 11:42 radacct_2021-05-04_16-30.csv.bz2\n-rw-r--r-- 1 intra intra 220440718 May  6 11:42 radacct_2021-05-04_18-00.csv.bz2\n-rw-r--r-- 1 intra intra 220170325 May  6 11:42 radacct_2021-05-04_19-30.csv.bz2\n-rw-r--r-- 1 intra intra 220153678 May  6 11:42 radacct_2021-05-04_21-00.csv.bz2\n-rw-r--r-- 1 intra intra 220329041 May  6 11:42 radacct_2021-05-04_22-30.csv.bz2\n```\n**radius.radacct_orig_files**\nCommand : show files in <table>\n```\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_00-00.csv.20210504_001006.utc | 823.99MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_01-30.csv.20210504_021002.utc | 822.76MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_03-00.csv.20210504_031002.utc | 821.74MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_04-30.csv.20210504_051001.utc | 821.36MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_06-00.csv.20210504_061002.utc | 821.05MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_07-30.csv.20210504_081003.utc | 821.63MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_09-00.csv.20210504_091002.utc | 822.62MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_19-30.csv.20210504_221010.utc | 825.45MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_21-00.csv.20210504_221010.utc | 825.52MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_22-30.csv.20210504_231007.utc | 825.74MB |           |\n```\n---\n**Answer (Yes ) :**\nIf files exist at **radius.radacct_orig_files** \na. How to Detect Missing Files \n1. Checking Hourly Data at radacct_hist table . \n| hour| rows |\n| ------ | ------ |\n| ..| .. |\n| 16 | 1489477 |\n| 17             | 1441560 |\t\t\n| **18**             | **732**     |\t\t\n| **19**             | **739837**  |\t\n| 20             | 1450106 |\t\n| ..| .. |\nIn this case missing file is the one with timestamp after the last hour with less data : ( 1930 )\n2. Checking the OPS cron log file \nFile : **/shared/abc/radius/log/000_radius_ops.20210506.log**  \n-   Normal Entry (1) ( file exists , impala table populated ) :\n```\nINFO: 2021-05-06 05:14:36 --> insert into radius.RADACCT_HIST completed. 2174145 rows\nINFO: 2021-05-06 05:14:36 --> HDFS:Clean-up RAD___radacct_*.utc files\nhdfs dfs -rm -skipTrash /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_04-30.csv.20210506_051001.utc 2>/dev/null\nDeleted /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_04-30.csv.20210506_051001.utc\n```\n-   Normal Entry (2) (No file exists , impala table not populated ) :\n```\nINFO: 2021-05-06 07:11:01 --> insert into radius.RADACCT_HIST completed. 0 rows\nINFO: 2021-05-06 07:11:01 --> HDFS:Clean-up RAD___radacct_*.utc files\n```\n- Abnormal Entry : ( File exists , Impala table NOT Populated ) \n```\nINFO: 2021-05-06 08:13:02 --> insert into radius.RADACCT_HIST\nINFO: 2021-05-06 08:13:26 --> insert into radius.RADACCT_HIST completed. 0 rows\nINFO: 2021-05-06 08:13:26 --> HDFS:Clean-up RAD___radacct_*.utc files\nhdfs dfs -rm -skipTrash /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_07-30.csv.20210506_081001.utc 2>/dev/null\nDeleted /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_07-30.csv.20210506_081001.utc\n```\n- **completed. 0 rows**\n- File **RAD___radacct_2021-05-06_07-30.csv.20210506_081001.utc** should be copied \nb. Copy missing hdfs files from radacct_orig_files to radacct_load.\nThis command manually requeues a missing file for re-ingestion by copying it back to the load directory from backup.\n```\nhdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_20xx-yy-zz_rr-mm.csv.yyyymmdd_hhmmss.utc /ez/warehouse/radius.db/radacct_load/\n....\n```\nc. Execute post script :\n```bash\n/shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.`date '+\\%Y\\%m\\%d'`.manually.log\n```\nThis script performs the final processing and insert into radacct_hist, based on loaded files. Run it after manually copying missing data.\n---\n**Answer (No ) :**  \nIf files do not exist at **radius.radacct_orig_files** , execute following steps:\n1. sftp Requested  Files from radius sftp server\n2. mv files to\n  [Remfgh]\n`local_spool_area=\"/shared/radius_repo/cdrs\"`\n3. Modify file  **radius.trn** :\nDefault Status:\n```\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n#-- local executions --\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n```\nWhen local file is used :\n```\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n#-- local executions --\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n```\n4.Check if entry @ **/shared/radius_repo/radius_date.dat.local** , exist\ne.g\n```\n[File]\nlatest_file=\"/shared/radius_repo/cdrs/radarchive_2019-06-12.csv.bz2\"\n```\nand it is older than the new files arrived .\nIf not , create a dummy file on /shared/radius_repo/cdrs/ with older date from new files , and update /shared/radius_repo/radius_date.dat.local accordingly\n5.Then execute the respective commands:\n```bash\n-  /shared/abc/radius/DataParser/scripts/radius.pl  -l -d -D -o >> /shared/abc/radius/DataParser/scripts/log/radius_cron_manual.log  2>&1\n- /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.manual.log 2>&1\n```\n6. Rollback trn file to initial mode :\n```bash\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n#-- local executions --\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n```\n---\n**Important :**\n**All above actions should not be performed during scheduled crontab job for Radius ( Usually xx:10 ~ xx:15 ) \nCheck respective cron logs before manual executions described above .**\n---\n**Useful SQL statements :**\nUse the SQL below to compare hourly row counts between radacct_orig_files (raw SFTP ingested files) and radacct_hist (final table). These help pinpoint missing or partially loaded intervals.\n```sql\nselect hour(acctupdatetime) acctupdatetime, count(*)\n cnt\nfrom radius.radacct_orig_files where acctupdatetime like '2021-05-04%'\ngroup by 1 order by 1;\n```\n```sql\nselect hour(acctupdatetime) acctupdatetime, count(*)\n cnt\nfrom radius.radacct_hist where par_dt='20210504'\ngroup by 1 order by 1;\n```\n## Affected Systems\nabc Bigstreamer Radius\n## Action Points\nVerufy  Procedure Execution  by using mentioned Sql Statements",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210506-GI13.md"
        }
    },
    "423": {
        "page_content": "---\ntitle: osix.sip Ingestion Stopped on 25/11/2020 - Topology and Listener Investigation\ndescription: Resolution steps for halted data ingestion in osix.sip table starting 25/11/2020 07:00, including OSIX-SIP-NORM topology checks, listener health validation, and manual topology resubmission.\ntags:\n  - bigstreamer\n  - abc\n  - osix\n  - osix.sip\n  - data-ingestion\n  - listener\n  - topology\n  - resubmit\n  - kudu\n  - impala\n  - yarn\n  - coord_OsixStreaming_SIP_MonitorResubmit\n  - monitoring\n  - streaming\n  - sip_norm\n  - log-analysis\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1367129\n  cluster: abc\n  component: osix.sip\n  affected_node: unosix1\n  ingestion_stopped_at: 2020-11-25 07:00\n  scripts_used:\n    - submit_sip_norm.sh\n  monitoring_tools:\n    - coord_OsixStreaming_SIP_MonitorResubmit\n    - http://172.25.37.251/dashboard/#osix_listeners\n  logs_checked:\n    - /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM/date_monitor_sip_norm.log\n  commands_executed:\n    - yarn application -list\n    - impala-shell SELECT par_dt\n  reference_docs:\n    - 18316_abc_Generic_MOP_CDH_5_16_2_Upgrade\n---\n# abc - BigStreamer - IM1367129 - osix.sip 25/11/2020\n## Description\nNo data is being loaded from 25/11/2020 at 07:00 on osix.sip.\n## Actions Taken\n1. ssh unosix1 with your personal account\n2. sudo -iu osix\n3. kinit -kt osix.keytab osix\n4. yarn application -list | grep OSIX-SIP-NORM\n5. In our case the topology was down and the kudu script didn't resubmit it.\n> Root cause: The OSIX-SIP-NORM topology was not running and the automated monitor did not restart it.\n6. Check if `coord_OsixStreaming_SIP_MonitorResubmit` is running.\n7. listener is healthy and receiving data `http://172.25.37.251/dashboard/#osix_listeners`\n8. The rate for `listen_sip_core` should be between 12K and 22K messages.If there is an extreme problem e.g. the rate is 0, consider restarting the problematic listener.\n> If monitor script failed to detect the downed topology, inspect failure reasons in its log.\n9. Check the logs of monitor script `hdfs dfs -ls /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM` and `hdfs dfs -cat /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM/date_monitor_sip_norm.log`\n10. Start again the topology `unosix1:/home/users/osix/topologies/binary-input-impala-output/sip_norm/` and execute `./submit_sip_norm.sh` until the state appeared `RUNNING`\n11. yarn application -list | grep OSIX-SIP-NORM\n12. Connect to impala-shell or Hue and execute `SELECT count(*), par_dt FROM osix.sip WHERE par_dt>'20201124' group by par_dt;` to check if the data inserted on the table.\n> Ensure new partitions are created by verifying that `par_dt='20201125'` is present.\n## Affected Systems\nabc Bigstreamer\n## Nfgh\nRecommended Mop for help `18316_abc_Generic_MOP_CDH_5_16_2_Upgrade`",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201126-IM1367129.md"
        }
    },
    "424": {
        "page_content": "---\ntitle: PowerEdge C6320 BIOS and iDRAC Update After Hardware Failure on sn87\ndescription: Step-by-step hardware remediation for sn87 node removal from BigStreamer cluster due to CPU issue, including opening Dell case, collecting lifecycle logs, updating iDRAC and BIOS firmware via OS CLI on PowerEdge C6320 servers.\ntags:\n  - bigstreamer\n  - abc\n  - sn87\n  - hardware-failure\n  - dell\n  - poweredge-c6320\n  - idrac\n  - bios\n  - firmware-upgrade\n  - cli-update\n  - lifecycle-controller\n  - ipmitool\n  - tsr\n  - support-assist\n  - dell-case\n  - server-out-of-cluster\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  cluster: abc\n  node: sn87\n  server_model: PowerEdge C6320\n  issue_id: IM1363402\n  vendor: Dell\n  dell_case_id: 2108129800\n  troubleshooting_interface: iDRAC\n  update_tools:\n    - ipmitool\n    - Support Assist\n    - OS shell CLI\n  firmware_components_updated:\n    - BIOS: 2.13.0\n    - iDRAC: 2.81.81.81\n  update_files:\n    - BIOS_CCTDP_LN64_2.13.0.BIN\n    - iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\n  reference_links:\n    - https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Updating%20BIOS%20on%20Dell%2013G%20PowerEdge%20Servers.pdf\n    - https://dl.dell.com/topicspdf/idrac_2-75_rn_en-us.pdf\n---\n# abc - BigStreamer - IM1363402 - abc BigStreamer - HW\n## Description\nWe see that sn87 has a problem with the CPU (attached). It has gone out of cluster.\n## Root Cause\nsn87 was removed from the cluster due to a CPU-related hardware fault, verified through iDRAC logs. BIOS and iDRAC versions were outdated.\n## Actions Taken\n1. Check Idrac logs for the description error `Overview-->Server-->Logs`\n2. Export the lifecycle logs `Overview-->Server-->Troubleshooting-->Support Assist-->Export Collection` and save the TSR*.zip\n3. Open a case on DELL SUPPORT(2108129800). Dell need the service tag from `Overview` of Idrac\n4. Send them the TSR*.zip\n5. In this case necessary was the update of BIOS & Lifecycle Controller of Idrac\n6. Dell send us the right update files based on our servers `PowerEdge C6320`\n7. Updated the BIOS base on the link `https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Updating%20BIOS%20on%20Dell%2013G%20PowerEdge%20Servers.pdf`\n(to update the BIOS via OS-CLI, see APPENDIX below )\n8. Updated the Lifecycle Controller base on the link `https://dl.dell.com/topicspdf/idrac_2-75_rn_en-us.pdf`\n(to update the Lifecycle Controller via OS-CLI, see APPENDIX below )\n9. After the update of both versions the host was up with the roles stopped for 1 day.\n10. After 1 day send the lifecycle logs like `Step 2` and forward the zip file to Dell.\n11. If any error exist start the roles.\n## Affected Systems\nabc Bigstreamer HW\n## References\nThe following appendix describes the full CLI-based firmware upgrade process for Dell PowerEdge C6320 servers.\n### Appendix: BIOS and iDRAC Firmware Upgrade via OS Shell on PowerEdge C6320\n-------------------------------------------------------------------------------\n- Download new iDRAC FW from link below (Nfgh: download the \".bin\" format, not the \".exe\" format): https://www.dell.com/support/home/en-us/drivers/driversdetails?driverid=5hn4r&oscode=naa&productcode=poweredge-c6320\neg: iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \n- Download new BIOS from (Nfgh: download the \".bin\" format, not the \".efi\" format): https://www.dell.com/support/home/en-us/drivers/driversdetails?driverid=cctdp&oscode=naa&productcode=poweredge-c6320\neg : BIOS_CCTDP_LN64_2.13.0.BIN\nProcedure :\n---------------\nLogin to C6320 eg sn75 as root\nStore the downloaded files under /tmp/\nProcedure executed via OS shell\nGet current BIOS version \n---------------------------\n[root@sn75 /]# ipmitool  mc getsysinfo system_fw_version 2.3.4\nGet current iDRAC version\n---------------------------\n[root@sn75 /]# ipmitool   mc info | grep Firmware\nFirmware Revision         : 2.40\nUPDATE iDRAC PROCEDURE (mc cold restart is preformed automatically) :\n------------------------------------------------------------------------\n```\n[root@sn75 /]# \n[root@sn75 tmp]# ll iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \n\n  -rw-r--r-- 1 root root 111350247 Dec  6 14:17 iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\n[root@sn75 tmp]# chmod +x iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \n[root@sn75 tmp]# ./iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \n  Update Package 21.04.200 (BLD_1123)\n  Copyright (C) 2003-2021 Dell Inc. All Rights Reserved.\n  Release Title:\n  iDRAC 2.81.81.81, A00\n  Release Date:\n  July 02, 2021\n  Default Log File Name:\n  5HN4R_A00\n  Reboot Required:\n  No\n  Running validation...\n  iDRAC\n  The version of this Update Package is newer than the currently installed version.\n  Software application name: iDRAC\n  Package version: 2.81.81.81\n  Installed version: 2.40.40.40\n  Continue? Y/N:Y\n  Executing update...\n  WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER PRODUCTS WHILE UPDATE IS IN PROGRESS.\n  THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE!\n  ...............................................................   USB Device is not found\n  ..............................................................   USB Device is not found\n  ...............................................................   USB Device is not found\n  Device: iDRAC\n    Application: iDRAC\n    Failed to reach virtual device. This could be caused by BitLocker or other security software being enabled. For more information, see the\n    Update Package User\u00e2\u20ac\u2122s Guide.\n  The update completed successfully.\n```\n## Nfgh\n------\nIF THE ABOVE ERROR IS SHOWN, THEN REBOOT THE iDRAC (\"#ipmitool  -U root -P c0sm0t31 mc reset cold\")   and REPEAT to get the below correct output, without the \"Failed to reach virtual device.\" message:!!!\n```  \nDevice: iDRAC\nApplication: iDRAC\nUpdate Successful.\nThe update completed successfully.\n```\nUpdate BIOS PROCEDURE (REBOOT REQUIRED !!!)\n----------------------------------------------\n```\n[root@sn75 /]# cd /tmp\n[root@sn75 /]#  ll BIOS_CCTDP_LN64_2.13.0.BIN \n[root@sn75 /]#  chmod +x BIOS_CCTDP_LN64_2.13.0.BIN \n[root@sn75 /]#  ./BIOS_CCTDP_LN64_2.13.0.BIN \n  Running validation...\n  \n  PowerEdge C6320 BIOS\n  \n  The version of this Update Package is newer than the currently installed version.\n  Software application name: BIOS\n  Package version: 2.13.0\n  Installed version: 2.3.4\n  Continue? Y/N:Y\n  Executing update...\n  WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER PRODUCTS WHILE UPDATE IS IN PROGRESS.\n  THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE!\n  ................................................................................................................\n  Device: PowerEdge C6320 BIOS\n    Application: BIOS\n    The BIOS image file is successfully loaded. Do not shut down, cold reboot, power cycle, or turn off the system, till the BIOS update is complete otherwise the\n    system will be corrupted or damaged. Bios update takes several minutes and it may be unresponsive during that time. Nfgh: If OMSA is installed on the system,\n    the OMSA data manager service stops if it is already running.\n  \n  Would you like to reboot your system now?\n  \n  Continue? Y/N:Y \n``` \nGet NEW BIOS version \n------------------------\n[root@sn75 /]# ipmitool  mc getsysinfo system_fw_version 2.3.4\nGet NEW iDRAC version\n------------------------\n[root@sn75 /]# ipmitool   mc info | grep Firmware\nFirmware Revision         : 2.40",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201125-IM1363402.md"
        }
    },
    "425": {
        "page_content": "---\ntitle: RA_Dsession and RA_Dtraffic Export Failure - Troubleshooting and Manual File Generation Guide\ndescription: Investigation and resolution steps for missing RA_Dsession and RA_Dtraffic exports in abc BigStreamer due to absence of source table data partitions, with manual export instructions.\ntags:\n  - bigstreamer\n  - abc\n  - data-export\n  - hive\n  - oozie\n  - impala\n  - partition-check\n  - par_dt\n  - mtuser\n  - RA_Dsession\n  - RA_Dtraffic\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  cluster: abc\n  nodes:\n    - un2.bigdata.abc.gr\n  user: mtuser\n  components:\n    - device_session\n    - device_traffic\n  issues:\n    - IM1333238\n  related_logs:\n    - ra_export_bs_01.oozie.YYYYMMDD.log\n    - ra_export_bs_02.oozie.YYYYMMDD.log\n  validation_tools:\n    - impala-shell\n    - oozie\n    - export_ra_bs_01.sh\n    - export_ra_bs_02.sh\n---\n# abc - BigStreamer - IM1333238 - abc BigStreamer IT files (RA Dsession / RA Dtraffic)\n## Description\nRA_Dsession/RA_Dtraffic are not exported\n## Actions Taken\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `mtuser` with sudo\n2. Inspect logs of *RA* flow\n```bash\ncd /shared/abc/location_mobility/log/\nless ra_export_bs_01.oozie.20201026.log\nless ra_export_bs_02.oozie.20201026.log\n```\n3. Check export logs for missing par_dt partitions. Check if max partition of source tables is greater or equal than the export date.\nIn `ra_export_bs_01.oozie.20201026.log`:\n``` bash\nQuery: SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201024'\n...\n[2020/10/26 09:00:22] - INFO: max_date=NULL and export_date=20201024\n```\nIn `ra_export_bs_02.oozie.20201026.log`:\n``` bash\nQuery: SELECT MAX(par_dt) FROM device_traffic WHERE par_dt >= '20201024'\n...\n[2020/10/26 09:00:22] - INFO: max_date=NULL and export_date=20201024\n```\nThe above messages show that data from the source tables have been exported to files already.\n4. Validate source table partition data using Impala. Validate the results of the query:\nFor `RA_Dsession`:\n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201023';\";\n...\n+-------------+\n| max(par_dt) |\n+-------------+\n| 20201023    |\n+-------------+\nFetched 1 row(s) in 1.38s\n```\nFor `RA_Dtraffic`:\n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"SELECT MAX(par_dt) FROM device_traffic WHERE par_dt >= '20201023';\";\n...\n+-------------+\n| max(par_dt) |\n+-------------+\n| 20201023    |\n+-------------+\nFetched 1 row(s) in 1.38s\n```\n5. Run manual export scripts if needed. Files will be exported at the next execution if the source tables contain new entries. Due to the size of the exported files runs only for the previous day `par_dt`.\nIf the customer requests to generate the files for the missing days:\n``` bash\ncd /shared/abc/location_mobility/run\n./export_ra_bs_01.sh -t 20201115 # Run for specific date\n./export_ra_bs_02.sh -t 20201115 # Run for specific date\n```\n## Affected Systems\nabc Bigstreamer Backend\n## Action Points\nN/A",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201026-IM1333238.md"
        }
    },
    "426": {
        "page_content": "---\ntitle: CDSW SparkPortForwarder Failures Causing Engine Exit Status 33\ndescription: CDSW jobs failing with engine exit status 33 due to SparkPortForwarder connection errors on wrkcdsw1. Includes log traces, restart of Docker Daemon Worker, and validation through CDSW UI and Kubernetes logs.\ntags:\n  - bigstreamer\n  - cdsw\n  - spark\n  - engine status 33\n  - sparkportforwarder\n  - wrkcdsw1\n  - job failure\n  - pod termination\n  - kubernetes\n  - docker\n  - kubelet\n  - port forwarding\n  - cloudera manager\n  - cdsw ui\n  - kubectl logs\n  - spark forwarder restart\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2379531\n  system: abc BigStreamer CDSW\n  root_cause: SparkPortForwarder pod on wrkcdsw1 stuck in terminating state, causing port-forwarder.sock connection refused errors and engine failures\n  affected_nodes:\n    - wrkcdsw1.bigdata.abc.gr\n    - mncdsw1.bigdata.abc.gr\n  user_visible_error: Engine exited with status 33\n  error_trace: \"dial unix /run/cloudera/data-science-workbench/port-forwarder/port-forwarder.sock: connect: connection refused\"\n  resolution:\n    - Restarted Docker Daemon Worker on wrkcdsw1 via Cloudera Manager\n    - Restarted CDSW Application role on mncdsw1\n    - Verified healthy SparkPortForwarder logs and job success via CDSW UI\n  outcome: Spark jobs resumed execution without failure; port mapping verified\n---\n# abc - IM2379531 - CDSW failed jobs\n## Description\nFailed CDSW jobs with a common error have been observed since yesterday (and today).\nFailed setting up spark (node: wrkcdsw1.bigdata.abc.gr) (error: dial unix /run/cloudera/data-science-workbench/port-forwarder/port-forwarder.sock: connect: connection refused)\nxEngine exited with status 33.\n________________________________________\nCDSW status\n|             spark-port-forwarder-w9zjv            |    1/1    |    Running    |      1       |   2024-09-19 09:06:51+00:00   |   10.255.241.133   |   10.255.241.133   |       spark-port-forwarder       |\n|             spark-port-forwarder-z7cdt            |    1/1    |    Running    |      1       |   2024-09-19 09:07:00+00:00   |   10.255.241.132   |   10.255.241.132   |       spark-port-forwarder       |\n|      tcp-ingress-controller-5b46dd4877-qm77x      |    1/1    |    Running    |      0       |   2024-09-19 09:21:22+00:00   |    100.66.0.22     |   10.255.241.130   |      tcp-ingress-controller      |\n|          usage-reporter-55b457bccd-nbt7q          |    1/1    |    Running    |      0       |   2024-09-19 09:06:41+00:00   |    100.66.0.37     |   10.255.241.130   |          usage-reporter          |\n|                web-7db65dccd9-g49qt               |    1/1    |    Running    |      0       |   2024-09-19 09:19:18+00:00   |    100.66.0.10     |   10.255.241.130   |               web                |\n|                web-7db65dccd9-ksff4               |    1/1    |    Running    |      0       |   2024-09-19 09:20:15+00:00   |    100.66.0.21     |   10.255.241.130   |               web                |\n|                web-7db65dccd9-xcxs2               |    1/1    |    Running    |      0       |   2024-09-19 09:20:15+00:00   |    100.66.0.11     |   10.255.241.130   |               web                |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nAll required pods are ready in cluster default.\nAll required Application services are configured.\nAll required secrets are available.\nPersistent volumes are ready.\nPersistent volume claims are ready.\nIngresses are ready.\nChecking web at url: https://mncdsw1.bigdata.abc.gr\nOK: HTTP port check\nCloudera Data Science Workbench is ready!\n## Actions Taken\n1. After checking logs of `wrkcdsw1.bigdata.abc.gr` `SparkPortForwarder` we saw that the latest request that handled was:\n```bash\ncdsw status | grep wrkcdsw1.bigdata.abc.gr\nkubectl logs spark-port-forwarder-thrr9 -n <namespace>\n```\nThe output of the logs:\n```bash\n2024-09-21 22:35:16.863 11 INFO SparkPortForwarder Failed to dial onward connection data = {\"err\":\"dial tcp 100.66.1.227:30742: connect: connection refused\",\"name\":\"spark-driver\",\"podId\":\"2liofp42ubkcj7yc\",\"port\":30742,\"target\":\"100.66.1.227:30742\"}\n2024-09-22 02:25:23.457 11 INFO SparkPortForwarder Returning port mappping data = {\"mapping\":{\"spark-blockmanager\":26577,\"spark-driver\":22768}}\n2024-09-22 02:26:29.689 11 INFO SparkPortForwarder Garbage collecting forwarders for pod data = {\"podId\":\"z48obsz9bocvu2wz\"}\n```\n2. We tried to delete the pod of `wrkcdsw1.bigdata.abc.gr` `SparkPortForwarder` but it stucked on `Terminating` status.\n```bash\nkubectl delete pod <pod_name> -n <namespace>\n```\n3. Thus, CDSW Application(mncdsw1) from Cloudera UI was down.\n4. From [Cloudera Manager UI](https://172.25.37.232:7183/cmf/home) we have restarted the `Docker Deamon Worker` of `wrkcdsw1.bigdata.abc.gr` and `Application` role of `mncdsw1`. After that we have checked the logs and `wrkcdsw1.bigdata.abc.gr` `SparkPortForwarder` handled succefully all the requests.\nActions:\n```bash\nCloudera Manager -> CDSW -> `Docker Deamon Worker` Role of `wrkcdsw1 -> Restart\nCloudera Manager -> CDSW -> `Application` role of `mncdsw1` -> Restart\n```\nLogs:\n1. Checks:\n```bash\ncdsw status | grep wrkcdsw1.bigdata.abc.gr\nkubectl logs <pod-name-spark-forwarder> -n <namespace>\n```\n2. Output:\n```\n2024-09-23 09:20:40.579 11 INFO SparkPortForwarder Start mapping ports data = {\"podId\":\"1t47ok1jnxqb1pi9\"}\n2024-09-23 09:20:40.579 11 INFO SparkPortForwarder Start trying to forward port data = {\"name\":\"spark-driver\",\"podId\":\"1t47ok1jnxqb1pi9\",\"port\":26404}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Finish trying to forward port, success data = {\"name\":\"spark-driver\",\"podId\":\"1t47ok1jnxqb1pi9\",\"port\":26404}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Start trying to forward port data = {\"name\":\"spark-blockmanager\",\"podId\":\"1t47ok1jnxqb1pi9\",\"port\":30123}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Finish trying to forward port, success data = {\"name\":\"spark-blockmanager\",\"podId\":\"1t47ok1jnxqb1pi9\",\"port\":30123}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Finish mapping ports data = {\"podId\":\"1t47ok1jnxqb1pi9\"}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Returning port mappping data = {\"mapping\":{\"spark-blockmanager\":30123,\"spark-driver\":26404}}\n2024-09-23 09:21:29.302 11 INFO SparkPortForwarder Garbage collecting forwarders for pod data = {\"podId\":\"1t47ok1jnxqb1pi9\"}\n```\n5. Additional checks made from the [CDSW UI](https://mncdsw1.bigdata.abc.gr). We reviewed the status of running jobs and examined the logs of them.\n```bash\nSite Administration -> Usage -> Select job Name -> Logs Tab\n```\nIn the logs of an example job we searched for `SparkPortForwarder` entries for `wrkcdsw1` in order to evaluate that no errors appeared.\n## Affected Systems\nCDSW",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20240923-IM2379531.md"
        }
    },
    "427": {
        "page_content": "---\ntitle: Energy Efficiency - pollaploi Table Update Monitoring and Validation\ndescription: Troubleshooting steps and validation procedure for verifying updates in the energy_efficiency.pollaploi table in abc BigStreamer, including workflow status, file comparison, and row count consistency.\ntags:\n  - bigstreamer\n  - abc\n  - energy_efficiency\n  - pollaploi\n  - workflow\n  - hue\n  - sftp\n  - impala\n  - data-validation\n  - table-update\n  - data-ingestion\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1382364\n  schema: energy_efficiency\n  table: pollaploi\n  source_server: 172.16.166.30\n  source_directory: energypm\n  workflow: energy_efficiency_load_pollaploi\n  nodes:\n    - un2.bigdata.abc.gr\n  user: intra\n  log_files:\n    - /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log\n    - /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.next_date.log\n  source_data_path: /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/\n  validation_steps:\n    - file row count vs table count match\n    - workflow success\n    - Impala query execution check\n---\n# abc - IM1382364 - Energy efficiency info update\n## Description\nThis task involves monitoring the pollaploi table to confirm it is updated whenever a new file arrives and diagnosing why recent data may not have been ingested.\nPlease let us know whenever the pollaploi table in schema energy efficiency is updated. \nAlso to investigate why an update has not been made based on the latest file.\n## Actions Taken\n1. ssh un2 with your personal account\n2. sudo -iu intra\n3. sftp `bigd@172.16.166.30`\n4. cd energypm\n5. ls -ltr\n6. Open HUE dashboard and search for `energy_efficiency_load_pollaploi` Workflow\n7. Check if workflow failed.\n8. ssh `un2` with your personal account.\n9. sudo -i\n10. less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log` and less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.next_date.log`. The next date should return no changes.\n> Confirm that the log mentions \"no new data\" or similar, indicating no update was needed.\n11. At un2 `wc -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/2020_10_pollaploi.txt`\n12. Connect toImpala using impala-shell and execute `select count(*) from energy_efficiency.pollaploi`\n> The number of rows in the 2020_10_pollaploi.txt file (from `wc -l`) should match the row count in the pollaploi table.\n13. The row counts from step 11 and step 12 must match\n14. Check on Impala Queries UI if the queries ran without exception `STATEMENT RLIKE '.*energy_efficiency_load_pollaploi.*'`\n## Affected Systems\nabc Bigstreamer",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201211-IM1382364.md"
        }
    },
    "428": {
        "page_content": "---\ntitle: CDSW Jobs Fail to Schedule Due to Resource Exhaustion\ndescription: CDSW jobs in abc BigStreamer failed to schedule due to CPU, memory, and GPU exhaustion; resolved by force-deleting pending and stuck pods after identifying a burst job submission.\ntags:\n  - bigstreamer\n  - abc\n  - cdsw\n  - kubernetes\n  - pod-pending\n  - unschedulable\n  - cpu\n  - memory\n  - gpu\n  - resource-exhaustion\n  - cluster-utilization\n  - kubectl\n  - stuck-jobs\n  - pod-cleanup\n  - container-scheduling\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1476278\n  system: CDSW\n  nodes:\n    - mncdsw1.bigdata.abc.gr\n  tools_used:\n    - kubectl\n    - CDSW Admin Console\n  failure_symptom: CDSW jobs stuck in 'Scheduling' with \"Unschedulable: No host has enough CPU, memory and GPU\"\n  root_cause:\n    - Excessive pod creation (800+ jobs on 2025-03-14 07:00)\n    - Resource starvation\n  resolution:\n    - Force-deletion of Pending and Init:0/1 pods\n  preventative_actions:\n    - Resource monitoring from pod scheduler\n    - Alerting when system hits high pending pod count\n---\n# abc - IM1476278 - CDSW Not enough CPU/GPU/Memory \n## Description\nFor two consecutive days, no CDSW job could be scheduled due to node resource exhaustion. CDSW displayed the message:\n\"Unschedulable: No host in the cluster currently has enough CPU, memory and GPU to run the engine.\"\nThis was traced to a large-scale job submission event that saturated available resources.\n## Actions Taken\n1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'\n2. Navigate to the Admin console.\n3. Select `Activity` tab.\n4. Check `CPU` and `Memory` graphs.\n5. In our case all scheduled jobs were stuck at `Scheduling` due to a job that ran `800 times at 14/03 07:00`\n- Delete all PENDING pods to free resources. This releases blocked resources and allows new jobs to be scheduled.\n```bash\n[root@mncdsw1 ~]# kubectl get pods\n[root@mncdsw1 ~]# kubectl get pods | grep Pending | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n[root@mncdsw1 ~]# kubectl get pods | grep \"Init:0/1\" | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n[root@mncdsw1 ~]# kubectl get pods\n```\n## Affected Systems\nabc Bigstreamer CDSW\n## Action Points\nIf the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a place can be found. However, it will not be killed for excessive CPU usage.\nAs an action point we could `monitoring compute & memory resource usage` from the Pod status via our monitoring tool.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210317-IM1476278.md"
        }
    },
    "429": {
        "page_content": "---\ntitle: BigStreamer Namenode Failover and Service Instability Recovery\ndescription: Incident response to cluster-wide failures in BigStreamer due to both Namenodes entering standby mode, affecting HDFS, HBase, Oozie, and geolocation streams; includes manual failover, service validation, and Cloudera case escalation.\ntags:\n  - bigstreamer\n  - abc\n  - hdfs\n  - namenode\n  - failover\n  - hbase\n  - oozie\n  - impala\n  - yarn\n  - cloudera\n  - hue\n  - cluster-health\n  - service-recovery\n  - geolocation\n  - locmob\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1391585\n  cluster: abc\n  interfaces:\n    - Cloudera Manager: https://172.25.37.232:7183\n    - HUE: https://172.25.37.236:8888/oozie/list_oozie_workflows/\n    - Kibana: http://10.20.9.82:5601/app/kibana\n  symptoms:\n    - Both Namenodes in standby\n    - HDFS/HBase/Oozie/Impala service alerts\n    - Geolocation & Location Mobility stream failures\n  resolution:\n    - Restarted nn1 \u2192 nn2 became active\n    - Manually stabilized HDFS services\n    - Opened Cloudera support case with diagnostics\n---\n# abc - IM1391585 - issue BigStreamer\n## Description\nPlease check immediately if BigStreamer is working properly.\nWe have received many alerts over the weekend and today regarding HDFS, for various nodes as well as for services (eg oozie). We also noticed problems with geolocation streams and loc mob files.\n## Actions Taken\n1. Connect with personal creds `https://172.25.37.232:7183` Cloudera Manager\n2. Both Namenodes entered standby mode simultaneously which caused bad health on HDFS,HBASE,OOZIE,IMPALA. After nn1 restarted nn2 became the Active and nn1 the Standby namenode. All the other services was stable after this manual action except HBASE which restarted.\n> The dual-standby state of both Namenodes caused a loss of HDFS coordination, which cascaded failures to other dependent services like HBase, Oozie, and Impala. After restarting nn1, high availability was restored with nn2 becoming active.\n3. Since all services were stable check HUE `https://172.25.37.236:8888/oozie/list_oozie_workflows/` to ensure that all workflows running.\n4. The specific timeline which namenodes crashed the load,cpu,network,hdfs_read/write,nodes health,,namenodes health,impala queries if something heavy executed,yarn applications if something heavy executed `http://10.20.9.82:5601/app/kibana`\n5. Opened a case on Cloudera with namenodes diagnostics.\n## Affected Systems\nabc Bigstreamer\n## Action Points\nMonitor the status/health of services and inform with mail/alert when a service/role is down.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201220-IM1391585.md"
        }
    },
    "430": {
        "page_content": "---\ntitle: refdata.rd_cells Not Updated Due to Script Synchronization Race Condition\ndescription: The `refdata.rd_cells` table was not updated on 2023-09-12 due to a race condition between `210_refData_Load.sh` and `220_refData_Daily_Snapshot.sh` caused by asynchronous execution of separate Cloudera coordinators. Resolved by adding `set SYNC_DDL=1` to ensure synchronization.\ntags:\n  - bigstreamer\n  - refdata\n  - rd_cells\n  - sync ddl\n  - ddl sync\n  - cloudera\n  - coordinator\n  - daily snapshot\n  - impala\n  - race condition\n  - data lag\n  - script conflict\n  - refdata.rd_cells_load\n  - shell script\n  - data refresh\n  - etl bug\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM-untracked\n  system: abc BigStreamer\n  root_cause: `220_refData_Daily_Snapshot.sh` ran before `210_refData_Load.sh` finished, causing `refdata.rd_cells` to reflect outdated data\n  resolution: Added `set SYNC_DDL=1` to force metadata consistency\n  affected_tables:\n    - refdata.rd_cells\n    - refdata.rd_cells_load\n  related_scripts:\n    - /shared/abc/refdata/bin/210_refData_Load.sh\n    - /shared/abc/refdata/bin/220_refData_Daily_Snapshot.sh\n  data_lag_date: 2023-09-12\n---\n# abc - BigStreamet - IM2215792 - refdata.rd_cells Not Updated Due to Script Synchronization Race Condition\n## Description\nrefdata.rd_cells was not loaded by refdata.\n```sql\nrd_cells_load\nselect max(par_dt) from refdata.rd_cells_load  --> 20230911\nselect max(refdate) from refdata.rd_cells -->  20230910\n```\nThis resulted in refdata.rd_cells reflecting data from 20230910 instead of 20230911, even though the load job had successfully ingested the latest partition into refdata.rd_cells_load.\n## Actions Taken\nAt path `/shared/abc/refdata/bin` there is the script `210_refData_Load.sh` that at the beginning of each day loads the reference data at the `refdata.rd_cells_load` table and then updates the table so that this data appears as the latest data for other tables. At the same path there is the script `220_refData_Daily_Snapshot.sh` that loads the reference data from the `refdata.rd_cells_load` table to the `refdata.rd_cells` table, so that the two tables have the latest data recorded. These actions are handled by different server coordinators of cloudera manager. \nWe checked the log files for each month at the `/shared/abc/refdata/log` path to see which coordinator was responsible for these processes at September 12. After that we logged in Cloudera Manager, and checked the log files at the specific time interval. We found that due to synchronization issues , the coordinator that updated `refdata.rd_cells` updated it before the procedure that refreshed the data at  `refdata.rd_cells_load` so it read the data from the previous date.\nThe solution was to add the parameter **set SYNC_DDL=1** at the necessary scripts so there are no synchronization issues.\nThe logs at cloudera manager are deleted after a certain period of time, so they need to be checked soon after the ticket.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230912-IM2215792.md"
        }
    },
    "431": {
        "page_content": "---\ntitle: Delayed or Missing CPU_LOAD and MEMORY_USAGE Files Due to Export Lag\ndescription: Analysis and resolution steps for delayed or missing CPU_LOAD and MEMORY_USAGE metrics in abc BigStreamer due to Impala query delays and ingestion timing, with recommendations for query optimization and ingestion coordination.\ntags:\n  - bigstreamer\n  - abc\n  - impala\n  - data-ingestion\n  - cpu_load\n  - memory_usage\n  - metrics-export\n  - timestamp-delay\n  - nnm\n  - ip_vpn\n  - flume\n  - sftp\n  - log-analysis\n  - par_dt\n  - min_5\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  issue_id: IM1336999\n  cluster: abc\n  nodes:\n    - un2.bigdata.abc.gr\n  user: ipvpn\n  files:\n    - CPU_LOAD_YYYY-MM-DD_HH.MM.00.csv\n    - MEM_USAGE_YYYY-MM-DD_HH.MM.00.csv\n  root_causes:\n    - Impala query delay\n    - export timing mismatch\n    - late ingestion alignment\n  log_sources:\n    - initiate_export_components.cron.YYYYMMDD.log\n    - compute_cpu_kpis.YYYYMMDD.log\n    - compute_memory_kpis.YYYYMMDD.log\n    - nnm_component_metrics.cron.YYYYMMDD.log\n  affected_tables:\n    - bigcust.nnm_ipvpn_componentmetrics_hist\n  tools:\n    - impala-shell\n    - flume\n    - cloudera-manager\n    - hue\n---\n# abc - BigStreamer - IM1336999 - abc BigStreamer SM-MISSING DATA\n## Description\nSince 27/10/2020 12:40 pm 3 files have not been registered to EEM due to delays. Normal offset is 8 minutes e.g. metrics for 13:05 have to be transferred to the exchange directory before 13:13.\n## Actions Taken\n1. In the screenshot sent via email there is one file is missing for 13:05 and three files have been delayed from 13:55 to 14:05\n2. Login to `un2.bigdata.abc.gr` with personal account and change to `ipvpn` with sudo\n3. Inspect logs of *export component files* flow\n```bash\ncd /shared/abc/ip_vpn/log/\nless initiate_export_components.cron.20201027.log\n```\n3. Check messages for missing file\n``` bash\n[2020/10/27 13:13:00] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_13.05.00.csv is empty.\n[2020/10/27 13:13:00] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_13.05.00.csv is empty.\n```\n4. Check Impala queries execution for that file\nFor `CPU_LOAD`:\n```bash\nless compute_cpu_kpis.20201027.log\n...\nQuery: SELECT straight_join a.min_5 AS time,\n...\n    a.component_type='CPU' AND\n   a.min_5='2020-10-27 13:05:00' AND\n    a.par_dt='20201027'\n...\nQuery submitted at: 2020-10-27 13:12:51\n...\nFetched 0 row(s) in 8.68s\nINFO: CPU file exported.\nTue Oct 27 13:13:00 EET 2020\n```\nFor `MEM_USAGE`:\n``` bash\nless compute_memory_kpis.20201025.log\n...\nQuery: SELECT straight_join a.min_5 AS time,\n...\na.component_type='MEMORY' AND\na.min_5='2020-10-27 13:05:00' AND\na.par_dt='20201027'\n...\nQuery submitted at: 2020-10-27 13:12:50\n...\nFetched 0 row(s) in 9.00s\nINFO: Memory file exported.\nTue Oct 27 13:13:00 EET 2020\n```\n5. Check input metrics table\nExecute the Impala query either from Hue or impala-shell\n```sql\nSELECT count(*)\nFROM bigcust.nnm_ipvpn_componentmetrics_hist a\nWHERE        \n    a.min_5='2020-10-27 13:05:00' AND\n    a.par_dt='20201027';\nResult = 4286\n```\n6. Inspect logs of input metrics ingestion\n```bash\nless /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20201027.log\n...\n[2020/10/27 13:11:03] - INFO - /bin/mv /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201027130601110.20201027_111102UTC.csv.tmp /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201027130601110.20201027_111102UTC.csv\n...\n```\nInput metrics file has been loaded before the execution of the export query. So queries in step 4 should have returned about 1260 rows. This needs further investigation on why even after the table has been refreshed the query returns the wrong result. The discrepancy between available data and query results needs further investigation. Nfghd as Action Point 1.\n3. Check messages for delayed files\n``` bash\n[2020/10/27 14:03:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_13.55.00.csv at /shared/abc/ip_vpn/out/saismpm\n[2020/10/27 14:03:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_13.55.00.csv at /shared/abc/ip_vpn/out/saismpm\n...\n[2020/10/27 14:08:09] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_14.00.00.csv at /shared/abc/ip_vpn/out/saismpm\n[2020/10/27 14:08:09] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_14.00.00.csv at /shared/abc/ip_vpn/out/saismpm\n...\n[2020/10/27 14:14:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_14.05.00.csv at /shared/abc/ip_vpn/out/saismpm\n[2020/10/27 14:14:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_14.05.00.csv at /shared/abc/ip_vpn/out/saismpm\n```\nSince SFTP GET occurs on 3rd, 8th, 13th, 18th etc minute of every hour, the files above should have been transferred in `/shared/abc/ip_vpn/out/saismpm` before 14:03, 14:08, 14:13 respectively.\n4. Check Impala queries execution for those files\nFor `CPU_LOAD`:\n``` bash\nless compute_cpu_kpis.20201027.log\n...\nTue Oct 27 14:02:02 EET 2020\nStarting Impala Shell using LDAP-based authentication\n...\nINFO: CPU file exported.\nTue Oct 27 14:03:00 EET 2020\n...\n...\nTue Oct 27 14:07:07 EET 2020\nStarting Impala Shell using LDAP-based authentication\n...\nINFO: CPU file exported.\nTue Oct 27 14:08:09 EET 2020\n...\n...\nTue Oct 27 14:12:06 EET 2020\nStarting Impala Shell using LDAP-based authentication\n...\nINFO: CPU file exported.\nTue Oct 27 14:14:03 EET 2020\n```\nSame for `MEM_USAGE`.\nThe cause of the delays is the duration time of the Impala queries. To reduce the times we need to investigate if the schema of bigcust.nnm_ipvpn_componentmetrics_hist can be improved, if we can delete the `REFRESH nnmnps.nms_node ...` queries etc. This should be analyzed further (see Action Point 2) to determine if schema optimization or query logic simplification can reduce query execution time. Nfghs as Action Point 2.\n## Affected Systems\nabc Bigstreamer Backend\n## Action Points\n1. Investigate empty response of Impala query for 13:05 even after refreshing the table\n2. Investigate slightly increased response times of Impala queries and ways to reduce them.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201029-IM1336999.md"
        }
    },
    "432": {
        "page_content": "---\ntitle: CDSW Job Failing Due to Excessive Resource Allocation in Energy Bills Project\ndescription: Repeated failures of the \"pollaploi\" Spark job in the CDSW Energy Bills project due to excessive executor allocation (100 instances with 400GB RAM and 200 vcores). Issue affects cluster stability and is not caused by the flow itself.\ntags:\n  - bigstreamer\n  - cdsw\n  - energy bills\n  - spark\n  - pollaploi\n  - job failure\n  - spark configuration\n  - spark executor\n  - out of memory\n  - cluster resources\n  - spark optimization\n  - cdh\n  - resource exhaustion\n  - spark dynamic allocation\n  - job history\n  - memory overhead\n  - cdsw session\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2172470\n  system: abc BigStreamer CDSW\n  root_cause: Pollaploi Spark job in CDSW uses 100 executors with 400GB RAM and 200 vcores, exhausting cluster resources\n  project: Energy Bills\n  script_path: Energy_Bills_Automation/Energy_Bills_Automation.py\n  platform: Cloudera Data Science Workbench\n  spark_details:\n    - executor.instances: 100\n    - executor.memory: 4g\n    - executor.cores: 2\n    - driver.memory: 4g\n  recommendation: Reduce executor count and apply tuning guidance from past communications\n---\n# abc - BigStreamer - IM2172470 - abc (492) BigStreamer - CDH - energy_efficiency.pollaploi\n## Description\nWe have a problem with the pollaploi job located in the Energy Bills project in impala\nthe frequency of it failing has increased quite a bit.\n## Actions Taken\nAfter communication with customer we understand that the issue occurs for job at workbench and not for flow. So:\n1. Login to Cloudera Data Science Workbench with your personal account (https://mncdsw1.bigdata.abc.gr/)\n2. Click on the left **Sessions** tab and then on **Scope** select **All Projects** and click on **Energy Bills** Project and find **Pollaploi** job.\n3. Go on **History** tab and you will see that there are a lot of pollaploi jobs with status Failure\n4. Click on one job with status Failure and then go to **See job details** and then click on **Script: Energy_Bills_Automation/Energy_Bills_Automation.py**\n5. When investigated the script we saw below snippet of spark configuration:\n```bash\nspark = SparkSession.builder\\\n.master(\"yarn\")\\\n.config(\"spark.submit.deployMode\", \"client\")\\\n.config(\"spark.eventLog.enabled\", \"true\")\\\n.config(\"spark.executor.instances\", \"100\")\\\n.config(\"spark.executor.cores\", \"2\")\\\n.config(\"spark.executor.memory\", \"4g\")\\\n.config(\"spark.rpc.message.maxSize\", \"1024\")\\\n.config(\"spark.executor.memoryOverhead\", \"800\")\\\n.config(\"spark.driver.memory\", \"4g\")\\\n.config(\"spark.driver.memoryOverhead\", \"800\")\\\n.config(\"spark.spark.driver.maxResultSize\", \"4g\")\\\n.config(\"spark.executor.dynamicAllocation.initialExecutors\", \"4\")\\\n.config(\"spark.executor.dynamicAllocation.minExecutors\", \"4\")\\\n.config(\"spark.executor.dynamicAllocation.maxExecutors\", \"4\")\\\n.config(\"spark.sql.broadcastTimeout\", \"1000\")\\\n.config(\"spark.kryoserializer.buffer.max\", \"1024m\")\\\n.getOrCreate()\n```\nSo, there are 100 instances * 2 cores = 200 vcores and 100 instances * 4G ram = 400GB ram.\nThe cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.\n## Our Ticket Response\nUpon investigation we noticed that the job you mentioned fails with an out-of-memory error.\nAdditionally, according to the spark configuration snippet below in your job:\n```\n.master(\"yarn\")\\\n.config(\"spark.submit.deployMode\", \"client\")\\\n.config(\"spark.eventLog.enabled\", \"true\")\\\n.config(\"spark.executor.instances\", \"100\")\\\n.config(\"spark.executor.cores\", \"2\")\\\n.config(\"spark.executor.memory\", \"4g\")\\\n```\nWe see that you have given 100 instances * 2 cores = 200 vcores and 100 instances * 4Gram = 400GB ram.\nThe cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.\nTherefore, the problem concerns the specific job. For this reason, jobs should be optimized according to the guidance given in an earlier communication for a similar issue, taking into account the configuration that has already been done in the cluster.\n## Affected Systems\nabc Bigstreamer CDSW",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20230621-IM2172470.md"
        }
    },
    "433": {
        "page_content": "---\ntitle: Missing Data in radius.radarchive_hist for Specific Date\ndescription: Full step-by-step troubleshooting and recovery process for missing data in the `radius.radarchive_hist` table, including SFTP inspection, dummy file creation, custom ingestion with radius.pl and 000_radius_ops.sh, and TRN reconfiguration.\ntags:\n  - bigstreamer\n  - radius\n  - radarchive\n  - radarchive_hist\n  - ingestion\n  - missing data\n  - manual reload\n  - hdfs\n  - sftp\n  - radius_ops\n  - dataparser\n  - intra\n  - impala\n  - cronjob\n  - zero size file\n  - transferlist\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1629405\n  system: abc BigStreamer Radius ingestion\n  detection_target: missing daily partition in `radarchive_hist`\n  target_partition: 20210831\n  export_source: radarchive_2021-08-31.csv.bz2\n  root_cause: missing or zero-sized SFTP file\n  ingestion_scripts:\n    - /shared/abc/radius/DataParser/scripts/radius.pl\n    - /shared/abc/radius/bin/000_radius_ops.sh\n  logs:\n    - /shared/abc/radius/DataParser/scripts/log/radius_cron.log\n    - /shared/abc/radius/log/000_radius_ops.YYYYMMDD.log\n    - /shared/abc/radius/log/002_wfm_radius_traffic_day.YYYYMMDD.log\n  trn_file: /shared/abc/radius/DataParser/scripts/transferlist/radius.trn\n  metadata_file: /shared/radius_repo/radius_date.dat.local\n  sftp_host: 79.128.184.153\n---\nabc\nBigStreamer\nSyslog(un2)\nIssue Number: IM1629405 \nTitle:  radius.radarchive_hist  missing data\n## Description\nPlease check why no data has been loaded into the table for 08/31\nUrgency \t2 - High\n- radarchive file not loaded\n- missing partition in radius.radarchive_hist\n- zero-byte SFTP file ingestion\n- how to rerun radius.pl for historical radarchive data\n- loading radarchive from .bz2 after failed ingestion\n- how to use dummy reference files for custom TRN loads\n## Actions Taken\n0. refresh table via impala-shell:\n```bash\n[un-vip.bigdata.abc.gr:21000] > refresh  radius.radarchive_hist;\n[un-vip.bigdata.abc.gr:21000] > select count(*),par_dt from radius.radarchive_hist  where par_dt = '20210831' group by par_dt;\n```\nIf 'refresh' does not resolve the issue, then proceed ass below:\n1. intra crontab in un2:\n```bash\n10 * * * *  /shared/abc/radius/DataParser/scripts/radius.pl -d -D -o >> /shared/abc/radius/DataParser/scripts/log/radius_cron.log  2>&1;/shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.`date '+\\%Y\\%m\\%d'`.log 2>&1;/shared/abc/radius/bin/002_wfm_radius_traffic_day.sh >> /shared/abc/radius/log/002_wfm_radius_traffic_day.`date '+\\%Y\\%m\\%d'`.log 2>&1\n```\n2. Script #1: /shared/abc/radius/DataParser/scripts/radius.pl\n3. Log #1: /shared/abc/radius/DataParser/scripts/log/radius_cron.log\n4. Script #2: /shared/abc/radius/bin/000_radius_ops.sh \n5. Log #2: /shared/abc/radius/log/002_wfm_radius_traffic_day.20210831.log\n6. check Sftp server : as 'intra'\n```bash\n[intra@un2 ~]$ sftp intra@79.128.184.153\n```\n9. check if file exists and its size \neg:\n```bash\nsftp> ls -l  radarchve_2021-08-31.csv.bz2\n-rw-r--r--    0 0        0              14 Sep  1 04:32 radarchive_2021-08-31.csv.bz2\n```\ntransfer it locally :\n```bash\nsftp> get  radarchive_2021-08-31.csv.bz2\nexit\n```\nextract it :\n```bash\n[intra@un2 ~]$ bzip2 -d  radarchive_2021-08-31.csv.bz2\n```\ncheck it :\n```bash\n[intra@un2 ~]$ ll  radarchive_2021-08-31.csv\n-rw-r--r-- 1 intra intra 0 Sep  1 12:21 radarchive_2021-08-31.csv\n```\n### What to Do If the File Was Zero-Sized or Recreated (upon re-creating the data by abc)\n--------------------------------------------------------------------\nThe following procedure describes loading radius data.\nThe process is common for both file types: radararchive_hist, radacct_hist\nDepending on the type of missing data, we also load the corresponding files.\nFor radararchive_hist one archive is produced per day. \nWhile for radacct_hist more.\n```bash\necho \"ls -ltr rada*.csv.bz2\" | sftp prdts@79.128.184.153 | tail -n10\n```\n183791490 Jul 27 10:30 radacct_2019-07-27_10-30.csv.bz2\n184011607 Jul 27 12:00 radacct_2019-07-27_12-00.csv.bz2\n184124420 Jul 27 13:30 radacct_2019-07-27_13-30.csv.bz2\n184099607 Jul 27 15:00 radacct_2019-07-27_15-00.csv.bz2\n184046108 Jul 27 16:30 radacct_2019-07-27_16-30.csv.bz2\n184108503 Jul 27 18:00 radacct_2019-07-27_18-00.csv.bz2\n184117618 Jul 27 19:30 radacct_2019-07-27_19-30.csv.bz2\n183999927 Jul 27 21:00 radacct_2019-07-27_21-00.csv.bz2\n184176100 Jul 27 22:30 radacct_2019-07-27_22-30.csv.bz2\n162087027 Jul 28 04:32 radarchive_2019-07-27.csv.bz2\n### Manual Reload Procedure for `radarchive_hist`:\nThe Case\nMissing data: radarchive\nMissing date: 2019-07-27\nThe following actions will be performed by UN2 as an intra user\n1. Check TRN file: \n/shared/abc/radius/DataParser/scripts/transferlist/radius.trn\n[Remfgh]\n;SFTP Server Settings\naddress=\"79.128.184.153\"\nprotocol=\"SFTP\"\nport=\"22\"\nusername=\"prdts\"\nspool_area=\"/home/prdts\"\nmove_area=\"\"\nfile_type=\"rada*\"\nsuffix=\".csv.bz2\"\nload_suffix=\"LOADED\"\nsuffix_tmp=\".tmp\"\n2. Check if the missing files still exist on the remfgh server\n```bash\n$ sftp prdts@79.128.184.153\nsftp> cd /home/prdts\nsftp> ls -ltr radarchive*2019-07-27.csv.bz2\n...\nJul 27 04:33 radarchive_2019-07-26.csv.bz2\nJul 28 04:32 radarchive_2019-07-27.csv.bz2\n...\n```\n3. get the missing files\n```bash\n$ sftp prdts@79.128.184.153\nsftp> lcd /shared/radius_repo/cdrs (@un2)\nsftp> cd /home/prdts\nsftp> get radarchive_2019-07-27.csv.bz2\n```\n4. Change TRN file\n```bash\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat\" --Production Load\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\" --Customade Load\n```\nAttention:\nthe file /shared/radius_repo/radius_date.dat.local should contain the filename of a file with an older date than the one to be loaded.\nfor example:\n```bash\n$ cat /shared/radius_repo/radius_date.dat.local\n[File]\nlatest_file=\"/shared/radius_repo/cdrs/radarchive_2019-06-12.csv.bz2\"\n```\nIf we want to load the data of the file radarchive_2019-07-27.csv.bz2 then THERE SHOULD BE A SIMILAR FILE WITH AN OLDER DATE IN THE SAME DIRECTORY\nFile_a) Jun 19 12:38 radarchive_2019-06-12.csv.bz2 <--older file or dummy file\nFile_b) Jul 30 10:38 radarchive_2019-07-27.csv.bz2 <--file to load\nFile_a will simply be used as a reference point to load all files later than it.\nIf we don't have such a file, we can alternatively create a dummy file.\nThe following command creates a dummy file based on the timestamp of the regular file by subtracting 1 hour\n```bash\n$ touch -r radarchive_2019-07-27.csv.bz2 -d '-1 hour' radarchive_2019-07-26.csv.bz2\n$ ls -ltr\n-rw-rw-r-- 1 intra intra         0 Jul 29 09:38 radarchive_2019-07-26.csv.bz2\n-rw-r--r-- 1 intra intra 162087027 Jul 30 10:38 radarchive_2019-07-27.csv.bz2\n```\n5. Loading of missing data\nwe serially execute the following.\nWe change the LOAD date in the log files\n```bash\n/shared/abc/radius/DataParser/scripts/radius.pl  -l -d -D -o > /shared/abc/radius/DataParser/scripts/log/MAN.radius_cron.20190727.log  2>&1\n/shared/abc/radius/bin/000_radius_ops.sh > /shared/abc/radius/log/MAN.000_radius_ops.20190727.log 2>&1\n```\n6. check partitions\n```bash\n$ bee -e \"show partitions radius.radarchive_hist;\"\n$ bee -e \"show partitions radius.radacct_hist;\"\n    +------------------+\n\n    |    partition     |\n\n    +------------------+\n\n    | par_dt=20190726  |\n\n    | par_dt=20190727  | <-- Loaded\n\n    | par_dt=20190728  |\n\n    | par_dt=20190729  |\n\n    +------------------+\n```\n7. Rollback Changes in TRN file\nrevert the changes of step 5)\n```bash\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat\" --Production Load\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\" --Custom Load\n```\nIn the above case, the file was of zero size and reported to abc.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20210831-IM1629405.md"
        }
    },
    "434": {
        "page_content": "---\ntitle: Hive2Script Job Failure - Corrupt Parquet File in osix.sip Partition\ndescription: Resolution of failed Hive2Script Oozie job in abc BigStreamer due to corrupt Parquet file in osix.sip partition (par_dt=20201123, par_hr=08); includes file inspection, fsck, and table refresh commands.\ntags:\n  - bigstreamer\n  - abc\n  - hive2script\n  - hive\n  - impala\n  - spark\n  - oozie\n  - parquet\n  - corrupted-parquet\n  - fsck\n  - metadata\n  - partition-refresh\n  - stale-metadata\n  - osix\n  - osix.sip\n  - hdfs\n  - application_failure\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  cluster: abc\n  issue_id: IM1364500\n  table: osix.sip\n  affected_partition:\n    par_dt: \"20201123\"\n    par_hr: \"08\"\n  error:\n    message: File has an invalid version number\n    cause: Corrupt Parquet file\n  nodes:\n    - unosix1.bigdata.abc.gr\n    - sn87.bigdata.abc.gr\n    - un2.bigdata.abc.gr\n  tools:\n    - parquet-tools\n    - hdfs fsck\n    - impala-shell\n    - yarn logs\n    - hive\n    - oozie\n    - spark\n    - kinit\n  corrupt_files:\n    - part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000\n  commands_executed:\n    - REFRESH osix.sip PARTITION\n    - parquet-tools meta\n    - hdfs dfs -mv\n---\n# abc - BigStreamer - IM1364500 - abc BigStreamer oozie job hive2script failed / stale metadata\n## Description\nImpala queries finish with error for table osix.sip and partition 20201123\nMessage:\nERROR processing query/statement. Error Code: 0, SQL state: File 'hdfs://nameservice1/ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/par_method=REGISTER/part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000' has an invalid version number: .??6\nThis could be due to stale metadata. Try running \"refresh osix.sip\".\n## Actions Taken\n1. Checked that same query results in error using Hive.\n2. Checked that the problem occurs only with par_hr=08 partition. \n```bash\nselect distinct sip.callinguser \nas callinguser \nfrom OSIX.sip where par_dt='20201123' \nAND par_hr != '08' \nAND sip.callingUser IS NOT NULL;\n...\nFetched X rows in X seconds.\n```\n2. Inspected logs of Osix SIP application for that time. Login in `unosix1.bigdata.abc.gr`, switch user to `osix` and kinit first.\n```bash\n$ sudo su - osix\n$ cd\n$ kinit -kt osix.keytab osix\n$ yarn logs -applicationId application_1599948124043_405502\n```\n3. As `sn87.bigdata.abc.gr` was running a Spark executor of this application the time it was forced to shutdown, inspected if there are any corrupt files in the table. Login to any datanode first.\n``` bash\n$ cd /var/run/cloudera-scm-agent/process/ \n$ ls -lahtr | grep -i hdfs\n$ cd <last directory>\n$ kinit -kt hdfs.keytab hdfs/`hostname -f`\n$ hdfs fsck /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08 -includeSnapshots\n...\nStatus healthy\n```\n4. Inspected format of written files. After communication with the dev team the batch id was retrieved so only a few files were checked. Login to un2.  \n```\n$ hdfs dfs -copyToLocal /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/part-*-17ead666-d5cb-437e-a849-c08ef825bec4.* .\n$ parquet-tools meta part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000\nfile:/home/users/u15/part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000 is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [46, -19, -49, 54]\n```\nThis confirms the file is corrupted and unreadable by Hive or Impala.\n4. Some of the files didn't have a correct parquet format so we removed them from the table.\n```bash\n$ hdfs dfs -mv hdfs dfs -mv  /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/par_method=OTHER/part-00005-17ead666-d5cb-437e-a849-c08ef825bec4.c000 /ez/landingzone/tmp/osix_sip/other\n...\n```\n### Resolution: Remove corrupt file and refresh Hive metadata\n5. Refresh the table and check that problem is fixed.\n```\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='REGISTER');\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='OTHER');\nselect count(*) from OSIX.sip where par_dt='20201123' ;\n``` \n## Affected Systems\nabc Bigstreamer Backend",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201124-IM1364500.md"
        }
    },
    "435": {
        "page_content": "---\ntitle: CSI_fix Export Empty Due to Missing Partitions in brond.cube_indicators\ndescription: Investigation and resolution of missing CSI_fix export files caused by absent data in brond.cube_indicators for 20201120\u201320201122; includes dependent table validation, coordinator rerun, and manual export execution.\ntags:\n  - bigstreamer\n  - abc\n  - hive\n  - impala\n  - hue\n  - workflow\n  - coordinator\n  - csi_fix\n  - brond.cube_indicators\n  - par_dt\n  - partition-missing\n  - export-failure\n  - cube_indicators\n  - cube_indicators_it\n  - sai.fix\n  - spark\n  - impala-shell\n  - mtuser\n  - sequential-export\n  - delayed-export\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  environment: BigStreamer\n  cluster: abc\n  issue_id: IM1363226\n  user: mtuser\n  export_script: /shared/abc/export_sai_csi/export_csi_fix.sh\n  logs:\n    - sai_csi.cron.YYYYMMDD.log\n    - CSI_fix_reconciliation.log\n  views_and_tables:\n    - sai.cube_indicators_it (view)\n    - brond.cube_indicators (base)\n    - brond.brond_retrains_hist\n    - brond.fixed_radio_matches_unq_inp\n    - brond.fixed_brond_customers_daily_unq\n    - radius.radacct_hist\n    - brond.dsl_stats_week_xdsl_hist\n  missing_partitions:\n    - 20201120\n    - 20201121\n    - 20201122\n  tools:\n    - impala-shell\n    - Hue\n    - Cloudera Manager\n    - bash\n    - cron\n---\n# abc - BigStreamer - IM1363226 - abc BigStreamer csi_fixed coollection issue\n## Description\n CSI_fix_11222020_w47.txt was exported empty\n## Actions Taken\n1. Login to `un2.bigdata.abc.gr` with personal account and change to `mtuser` with sudo\n2. Inspect logs of *CSI fix* flow. Nfgh that filename format is `CSI_fix_<mmddyyyy>_w<week>.txt` and that there is a 2 day delay between the export time and the exported data. In this case there was a problem with the file containing data for 2020-11-22 which was exported at 2020-11-24. So we checked the logs for 2020-11-24. \n```bash\ncd /shared/abc/export_sai_csi/log\nless sai_csi.cron.20201124.log\n```\n3. Check if the source table contained data for the export date.\nIn `sai_csi.cron.20201124.log`:\n``` bash\nQuery: use `sai`\nQuery: select nvl ... from sai.cube_indicators_it where par_dt='20201122'\n...\nFetched 0 row(s) in 0.06s\nProblem with 20201122.\n```\n4. Check if the source table contains data for this date. Please nfgh that the source table is just a view of table `brond.cube_indicators`.\n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d brond -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"select count(*), par_date from brond.cube_indicators where par_date >= '20201118' group by 2 order by 2;\";\n...\ncount(*),par_date\n2454925,20201118\n2453089,20201119\n2458393,20201123\n```\n5. Identify Missing Data in brond.cube_indicators. Since 3 dates are missing 2020/11/20-22, we need to run the workflow that populates `brond.cube_indicators`. But first we have to make sure all its table dependecies are loaded. Execute the following queries in an impala-shell or an Impala editor in Hue and make sure not only that partitions exist for those dates but also that there are an identical amount of lines.\n```sql\nselect count(*), par_dt\nfrom brond.brond_retrains_hist\nwhere par_dt >= '20201118'\ngroup by 2\norder by 2;\n...\ncount(*),par_dt\n2499833,20201118\n2497948,20201119\n*2496522,20201120*\n*2497810,20201121*\n*2497480,20201122*\n2496932,20201123\n2497130,20201124\n2505791,20201125\n``` \nExecute the same query for tables:\n- brond.fixed_radio_matches_unq_inp\n- brond.fixed_brond_customers_daily_unq\n- radius.radacct_hist\n- brond.brond_retrains_hist\n- brond.dsl_stats_week_xdsl_hist\nThis confirms the root cause: the brond.cube_indicators table lacked data for these dates, resulting in empty CSI_fix exports.\n6. Rerun Coord_Cube_Spark_Indicators for Missing Partitions. Run Cube_Indicators workflow. Login to Hue as intra and navigate to Workflows > Dashboards > Coordinators. Search for `Coord_Cube_Spark_Indicators` and click on the coordinator to view its executions. Select **one** at a time of the executions that need to be repeated. Nfgh that `brond.cube_indicators` is populated with a 2 day delay so if we want to load data for 2020-11-20 we have to execute the workflow of 2020-11-22. Select the execution from the check box and click `Rerun`. \n7. After each workflow execution repeat the query of step 4 to verify that table has been loaded. \n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d brond -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"select count(*), par_date from brond.cube_indicators where par_date >= '20201118' group by 2 order by 2;\";\n...\ncount(*),par_date\n2454925,20201118\n2453089,20201119\n2454321,20201120\n2458393,20201123\n```\n8. Gather dates that need to be exported. As we saw in step 5 the source table was empty for 3 partitions: 20201120, 20201121, 20201122. The files produced for them was empty. Verify by checking `/shared/abc/export_sai_csi/logging/CSI_fix_reconciliation.log`:\n```bash\n2020-11-21 09:01:39  CSI_fix_11212020_w47.txt  20201119  2453089\n2020-11-22 09:00:43  CSI_fix_11222020_w47.txt  20201120  0\n2020-11-23 09:28:14  CSI_fix_11232020_w47.txt  20201121  0\n2020-11-24 09:01:01  CSI_fix_11242020_w47.txt  20201122  0\n2020-11-25 09:03:13  CSI_fix_11252020_w48.txt  20201123  2458393\n```\n*CSI fix* provides a mechanism to avoid manual re-export of empty files. The most recent date of an empty file is stored in a table and automatically exported with the next execution. To view the date stored issue the query:\n```bash\nselect * from refdata.mediation_csi_load_info;\n...\nload_time,flow_name\nNULL,sai.mob\n20201122,sai.fix\n```\nSo we don't need to export manually date 20201122, only dates 20201120 & 20201121.\n9. Manually Export CSI_fix Files Using export_csi_fix.sh. Finally, to export the files we have to repeat step 1 and execute the export script **sequentially** for the desired dates + 2 days:\n```bash\n/shared/abc/export_sai_csi/export_csi_fix.sh 20201122 >> /shared/abc/export_sai_csi/log/sai_csi.cron.$(date '+%Y%m%d').log 2>&1 &\n/shared/abc/export_sai_csi/export_csi_fix.sh 20201123 >> /shared/abc/export_sai_csi/log/sai_csi.cron.$(date '+%Y%m%d').log 2>&1 &\n```\nThis confirms the missing CSI_fix files were regenerated successfully with the expected number of lines.\n10. Afterwards check the reconciliation log file that files have been exported:\n```bash\nless /shared/abc/export_sai_csi/logging/CSI_fix_reconciliation.log\n...\n2020-11-25 10:46:40  CSI_fix_11222020_w47.txt  20201120  4915294\n2020-11-25 10:51:17  CSI_fix_11232020_w47.txt  20201121  2457858\n```\nThe first one contains more lines as it includes dates 20201120 & 20201122.\n## Affected Systems\nabc Bigstreamer Backend",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20201125-IM1363226.md"
        }
    },
    "436": {
        "page_content": "---\ntitle: Cloudera Agent Memory Leak on idm2.bigdata.abc.gr Due to Parcel Download Loop\ndescription: The Cloudera Manager Agent (`cmf-agent`) on idm2.bigdata.abc.gr entered a high memory usage state due to a known Cloudera bug causing a memory leak while attempting to download stale parcels. The issue was resolved by removing the host from the cluster and restarting the agent.\ntags:\n  - bigstreamer\n  - cloudera\n  - cloudera manager\n  - idm2\n  - idm1\n  - memory leak\n  - cmf-agent\n  - parcel download\n  - host removal\n  - cluster health\n  - host not healthy\n  - host memory\n  - parcel distribution\n  - opsaps-59764\n  - known issue\n  - cloudera-scm-agent\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2352302\n  system: abc BigStreamer\n  affected_host: idm2.bigdata.abc.gr\n  secondary_host: idm1.bigdata.abc.gr\n  root_cause: Cloudera Manager Agent memory leak triggered by repeated parcel downloads for stale parcels\n  bug_reference: OPSAPS-59764\n  action_taken:\n    - Identified excessive memory usage by cmf-agent\n    - Verified parcel download loop in Cloudera Manager UI\n    - Removed idm1 and idm2 from the cluster\n    - Restarted cloudera-scm-agent service\n  outcome: Memory usage stabilized, hosts returned to healthy state\n---\n# abc - IM2352302 - idm2.bigdata.abc.gr Change to Faulty State\n## Description\nidm2.bigdata.abc.gr --> Not Healthy state.\n## Actions Taken\n1. After checking the error on Cloudera Manager idm2.bigdata.abc.gr briefly entered a 'Not Healthy' state due to memory swapping. Host monitoring graphs showed memory usage exceeding 60 GB out of the available 64 GB.\n2. Logged in with SSH and checked memory per process\n``` bash\nps aux --sort -rss\n```\n3. The top process consuming memory was `cmf-agent`, which is the Cloudera Manager Agent process. After a quick search we found the following bug [Cloudera bug: OPSAPS-59764: Memory leak in the Cloudera Manager agent while downloading the parcels](https://docs.cloudera.com/cdp-private-cloud-base/7.1.8/manager-release-nfghs/topics/cm-known-issues-773.html)\n4. To verify that this bug is triggered we checked the parcel page of Cloudera Manager and we found that two hosts (`idm1.bigdata.abc.gr`/`idm2.bigdata.abc.gr`) where constantly trying to download some of the parcels that are distributed, but not available anymore.\n5. We proposed to the client to remove the two hosts from the logical cluster, since they did not have any roles.\n```\nFrom Cloudera Manager\nHosts > All Hosts > Check `idm1.bigdata.abc.gr` and `idm2.bigdata.abc.gr` > Actions > Remove from Cluster\n```\n```bash\nsystemctl restart cloudera-scm-agent\n```\n6. After removing the two hosts from the cluster and restarting the Cloudera Manager Agent memory consumption for the two hosts has remained stable.\n## Affected Systems\nabc Bigstreamer",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20240722-IM2352302.md"
        }
    },
    "437": {
        "page_content": "---\ntitle: brond.brond_retrains_hist Missing Data Due to Invalid SFTP Filenames\ndescription: Root cause analysis and recovery actions for missing partitions in the `brond.brond_retrains_hist` table for 2023-01-01 to 2023-01-03 due to incorrect SFTP file naming. Includes query verification, log review, ingestion logic from `brond_retrains.pl`, and communication with upstream data providers.\ntags:\n  - bigstreamer\n  - brond\n  - brond_retrains\n  - brond_retrains_hist\n  - ingestion\n  - missing data\n  - partition missing\n  - sftp\n  - filename format\n  - cronjob\n  - DataParser\n  - abc\n  - logs\n  - retrains\n  - 000_brond_retrains_ops.sh\n  - brond_retrains.pl\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD1811952\n  system: abc BigStreamer Brond Retrains ingestion\n  target_table: brond.brond_retrains_hist\n  missing_partitions:\n    - 20230101\n    - 20230102\n    - 20230103\n  ingestion_scripts:\n    - /shared/abc/brond/DataParser/scripts/brond_retrains.pl\n    - /shared/abc/brond/bin/000_brond_retrains_ops.sh\n  trn_file: /shared/abc/brond/DataParser/scripts/transferlist/brond_retrains.trn\n  sftp_host: 172.16.166.30\n  file_format_required: Counter_Collection_24H.<number>_<yyyy>_<mm>_<dd>.csv\n  root_cause: invalid filename format on SFTP prevented par_dt creation\n  resolution_status: pending corrected file reupload from abc\n---\n# abc - SD1811952 (brond.brond_retrains_hist missing data)\n## Description\nLoad brond.brond_retrains_hist table for 01/01, 02/01 and 03/01\n### Flow Information\n0. Flow info:\n```\nruns every day via crontab at 08:00: \nun2:/shared/abc/brond/DataParser/scripts/brond_retrains.pl\nas intra \nConnects to sftp 172.16.166.30\ntakes parameters from :\n/shared/abc/brond/DataParser/scripts/transferlist/brond_retrains.trn\n/shared/abc/brond/bin/000_brond_retrains_ops.sh runs after /shared/abc/brond/DataParser/scripts/brond_retrains.pl\nLOGs: \n- /shared/abc/brond/DataParser/scripts/log/brond_rollout_cron.* for /shared/abc/brond/DataParser/scripts/brond_retrains.pl\n- /shared/abc/brond/log/000_brond_retrains_ops.* for /shared/abc/brond/bin/000_brond_retrains_ops.sh\n```\n## Actions Taken\n### 1. Check for Missing Partitions in Impala\n1. Verify that there are no data for the corresponding par_dts (Impala shell) by choosing a convenient date:\n```sql\nselect count(*), par_dt from brond.brond_retrains_hist where par_dt >= '20221231' group by 2 order by 2;\n```\nNo data was found for 1/1, 2/1, and 3/1.\n### 2. Review Ingestion Logs and Filename Format Issue\n2. By checking the **/shared/abc/brond/DataParser/scripts/log/brond_rollout_cron** log file, it was found out that the corresponding par_dts could not be created in Hive because their names, e.g. 20220101, could not be deduced. The brond_retrains.pl script deduces the name of the par_dt it will create by examining the name of the uploaded files on the SFTP server. The names of the uploaded files must have the following format:\nCounter_Collection_24H.<number>_<yyyy>_<mm>_<dd>.csv\nHowever, that was not the case for the files corresponding to those days.\n### 3. Communication with abc for Correct File Reupload\n3. We asked abc to re-upload the files with correct names.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20220103-IM1757150.md"
        }
    },
    "438": {
        "page_content": "---\ntitle: CDSW Job Failures due to CNI Plugin Network Error (Status 34)\ndescription: Investigation into failed CDSW jobs (e.g., Set_Point_Automation) with engine exit status 34. Root cause traced to CNI plugin issues on node `wrkcdsw4`, resolved by restarting Docker via Cloudera's supervisord. Covers logs, Kubernetes inspection, node-specific recovery, and customer communication.\ntags:\n  - bigstreamer\n  - cdsw\n  - status 34\n  - engine exited\n  - job failure\n  - cni plugin\n  - weave\n  - kubernetes\n  - docker restart\n  - wrkcdsw4\n  - supervisord\n  - energy bills\n  - set_point_automation\n  - cabins_live_measurements\n  - monitoring_flows\n  - job logs missing\n  - root cause analysis\n  - cdsw recovery\n  - mncdsw1\n  - cluster node issue\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1923742\n  system: abc BigStreamer CDSW\n  root_cause: Weave network plugin unresponsive on node wrkcdsw4\n  node: wrkcdsw4.bigdata.abc.gr\n  affected_jobs:\n    - Set_Point_Automation (Set Point Automation)\n    - Cabins Live Measurements (Energy Bills)\n    - Flows_update_all_counters_12:00_no_par_dt (Monitoring Flows)\n  error_code: Engine exited with status 34\n  resolution: Docker restarted via supervisorctl to restore container runtime health\n---\n# abc - IM1923742 - Job's problem\n## Description\nit has been observed that jobs show the problem Engine exited with status 34.\nsome of them are:\n\u2022 Set_Point_Automation job in the Set Point Automation project (error today 22/7)\n\u2022 Cabins Live Measurements job in the Energy Bills project (error yesterday 21/7)\n\u2022 Flows_update_all_counters_12:00_no_par_dt job in the Monitoring Flows project (error yesterday 7/15)\n## Actions Taken\n### 1. Identify Affected Jobs and Confirm Failure via CDSW UI\n1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'\n2. Go to last tab(admin).\n3. Select `Activity` tab.\n4. Inspect the Jobs in question.\nThe jobs are in `FAILED` status. The logs for the failed applications are missing.\n### 2. Investigate Job Pod Failures in Kubernetes\n5. Troubleshoot from the command line:\nFrom `mncdsw1` as root (use personal account and then sudo):\n```bash\nkubectl get pods -w -A # Wait a pod to fail (namespace should be like default-user-XXX)\n# After a while, a pod has failed, describe it\nkubectl describe pod -n default-user-XXX XXXXXXXX\n```\n### 3. Identify Root Cause from CNI/Weave Logs\n```logs\nEvents\nWarning  FailedCreatePodSandBox  10s                    kubelet, wrkcdsw4.bigdata.abc.gr  Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"...\" network for pod \"XXXXXXXX\": networkPlugin cni failed to set up pod \"XXXXXXXX_default\" network: unable to allocate IP address: Post http://127.0.0.1:6784/ip/....: dial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container \"....\" network for pod \"XXXXXXXX\": networkPlugin cni failed to teardown pod \"XXXXXXXX_default\" network: Delete http://127.0.0.1:6784/ip/....: dial tcp 127.0.0.1:6784: connect: connection refused]\n```\nThis error points us to the CNI plugin\nCheck the logs for the weave pods:\n``` bash\nkubectl logs -n kube-system weave-net-XXXXX\n# Weave pod in wrkcdsw4 has stopped logging events\n```\nThe pod was not responding and could not be deleted.\n### 4. Restore Weave Functionality by Restarting Docker\n7. Restart the docker daemon to restart all containers on `wrkcdsw4`\n_At the time of the issue, CDSW had stale configuration that required full restart (outage) which was not desirable_\nTo avoid applying the settings, restart the service with the same configuration by triggering a restart by `supervisord` deployed as part of the Cloudera agent\n#### Details\n ![Danger ahead](https://media3.giphy.com/media/vvzMdSygQejBIejeRO/200w.gif?cid=6c09b952aacsm9yssw6k6q0z5v8ejuy82rjpvw6qdhglcwpu&rid=200w.gif&ct=g)\nFrom wrkcdsw4 as root (use personal account and then sudo):\n```bash\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf status | grep DOCKER\n# Sample\n# 145071-cdsw-CDSW_DOCKER          RUNNING   pid 39353, uptime 29 days, 0:40:20\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart 145071-cdsw-CDSW_DOCKER\n```\n### 5. Confirm Recovery and Notify Customer\n8. Check that the node is operational after the restart\nFrom `mncdsw1` as root (use personal account and then sudo):\n```bash\ncdsw status # You might have to wait a few minutes\n```\n9. Inform the customer about the problem\n``` text\nA component of CDSW on worker node 4 encountered a problem resulting in jobs running on that node not being able to start. The function of the component has been restored and the jobs are now running normally.\n## Affected Systems\nabc Bigstreamer CDSW",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20220722-IM1923742.md"
        }
    },
    "439": {
        "page_content": "---\ntitle: SM Metrics Missing in PE_BRNCH_QoS for Piraeus \u2013 numofrtt Zero\ndescription: Analysis of missing metrics for Piraeus entries in the `PE_BRNCH_QoS` output file on 2022-02-15 15:20. Explains that blank fields occur when the `numofrtt` field is zero in the SLA computation script, which prevents metrics like `rttd` from being calculated and populated in the file.\ntags:\n  - bigstreamer\n  - sm\n  - qos\n  - pe_brnch_qos\n  - piraeus\n  - perf_interfacemetrics\n  - sla\n  - custompoller\n  - ip_vpn\n  - vpn metrics\n  - missing data\n  - probe metrics\n  - numofrtt\n  - rttd\n  - sla computation\n  - compute_qos_kpis.sh\n  - csv output\n  - blank metric fields\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1793457\n  system: abc BigStreamer VPN SLA metrics (SM)\n  affected_table: bigcust.nnmcp_ipvpn_slametrics_hist\n  affected_file: PE_BRNCH_QoS_2022-02-15_15.20.00.csv\n  metric_missing: rttd, pl12, jitter\n  root_cause: numofrtt = 0 in probe output leads to blank values\n  metric_script: /shared/abc/ip_vpn/run/compute_qos_kpis.sh\n  reference_guide: 19025_abc VPN SLA BigStreamer Guide v2.3\n---\n# abc - BigStreamer - IM1793457 - SM - Missing Data 14022022\n## Description\nAlarm in the SM system. There seems to be missing metrics for Piraeus in the PE_BRNCH_QoS file.\n### Customer Update\nAs an example, here are 2 cases below.\n```\nPE_BRANCH_QoS received by SM for 5min 15:20 seems to be missing metrics in each QoS\nPB-2084-ce.piraeusbank.customers.fghnet.gr\nSM-MISSING DATA 14022022_1.png\nPB-2600-ce.piraeusbank.customers.fghnet.gr\nSM-MISSING DATA 14022022_2.png\n```\nAccordingly in the file received by SM for 5 minutes 00:20 there were measurements: \nSM-MISSING DATA 14022022_3.png\n## Actions Taken\n### 1. Validate Output File Contents\n1. Check the metrics in the output file:\n```bash\n[root@un2]# less /shared/abc/ip_vpn/out/custompoller/PE_BRNCH_QoS_2022-02-15_15.20.00.csv | grep \"piraeusbank.customers.fghnet.gr||\"\n[root@un2]# less /shared/abc/ip_vpn/out/custompoller/PE_BRNCH_QoS_2022-02-15_15.20.00.csv | grep \"PB-2600-ce.piraeusbank.customers.fghnet.gr\"\n```\n### 2. Check Source Data in Hive Tables\n2. Input files are loaded into Hive table: bigcust.perf_interfacemetrics_ipvpn_hist\\\nCheck the metrics for several files and compare them:\n```bash\n#From Hue\nselect * from bigcust.nnmcp_ipvpn_slametrics_hist where n5_minute='2022-02-15 00:20:00' and site_code='PB-2600';\nselect * from bigcust.nnmcp_ipvpn_slametrics_hist where n5_minute='2022-02-15 15:20:00' and site_code='PB-2600';\n```\n### 3. Understand CSV Metric Format\n3. The format of the PE_BRNCH_QoS_<yyyy-mm-dd_HH.MM.SS>.csv is the following:\n```\nn5_minute|network_element_name|rttd|pl12|jitter||pl3||300\n```\n### 4. Analyze rttd Computation Logic\n4. Ckeck the implementation for each metric:\neg rttd:\nCheck the formula from file: https://metis.xyztel.com/obss/bigdata/abc/ipvpn/ipvpnsla-customers-devops/-/blob/master/docs/19025_abc%20VPN%20Customers%20SLA%20@%20BigStreamer%20User%20and%20Administrator%20Guide%20v2.3.docx\n```\nrttd = sumofrtt / numofrtt\n```\n### 5. Review Implementation Logic in compute_qos_kpis.sh\nIf <b>numofrtt</b> is 0 or NULL check the query implementaion in the file: `/shared/abc/ip_vpn/run/compute_qos_kpis.sh`\nIn our case numofrtt is 0, so the implementation puts the character:\"\" in the output file.\n```sql\n...\ncase when rank=1 then if(NONNULLVALUE(r.numofrtt) && NONNULLVALUE(r.sumofrtt) && r.Numofrtt!=0 , cast(cast(r.SumOfRTT/r.NumOfRTT AS DECIMAL(20,1)) AS STRING),'')\n                        else ''\n                        end as rttd,\n                        '' as pl3\n...\n```\n## Our Ticket Response\nafter investigation we saw that for the specific measurements the implementation throws a blank in the cases where the denominator of the formula is zero.\nChecked the readings we got from the probe and saw that the ones used as the denominator are zero resulting in a gap in the readings you are seeing.",
        "metadata": {
            "category": "issues",
            "client": "Client_abc",
            "name": "X20220215-IM1793457.md"
        }
    },
    "440": {
        "page_content": "---\ntitle: YARN NodeManager Failure Due to Full Root Partition from krb5kdc Log Growth\ndescription: Critical YARN NodeManager alert on pr1node01 caused by krb5kdc logs filling the root partition; issue resolved by log cleanup and adjusting logrotate retention policy across nodes.\ntags:\n  - mno\n  - bigstreamer\n  - yarn\n  - nodemanager\n  - pr1node01\n  - krb5kdc\n  - logrotate\n  - root partition full\n  - process status alert\n  - disk usage\n  - cloudera manager\n  - logs cleanup\n  - log retention\n  - kerberos\n  - IM2095156\n  - hadoop-yarn\n  - weekly rotation\n  - ibank\n  - streaming crash\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2095156\n  system: mno BigStreamer PR YARN Cluster\n  root_cause: krb5kdc logs grew uncontrollably, filling up the root partition on pr1node01 and causing NodeManager process failure\n  resolution_summary: Deleted oversized krb5kdc logs from 2022 and implemented new weekly rotation policy across PR and DR nodes\n  affected_nodes:\n    - pr1node01\n    - pr1node02\n    - dr1node01\n    - dr1node02\n  secondary_impact:\n    - Streaming and batch jobs failed (see SD2157107 and SD2157111)\n  remediation_action:\n    - Restarted NodeManager on pr1node01\n    - Logrotate policy modified to prevent recurrence\n---\n# mno - BigStreamer - IM2095156 - Alarm on PRDBA  Cloudera Manager\n## Description\nA **YARN NodeManager Process Status critical alert** was triggered on `pr1node01` in the PR Cloudera Manager cluster. This caused streaming and batch job failures in the iBank environment.\n```\nYARM -- Node Manager (pr1node03)\nProcess Status\n```\n## Actions Taken\n1. Login to Cloudera UI for the PR Site\n2. Cloudera > Yarn\n3. Upon inspection we noticed that the alert was about pr1node01 (Node Manager) and not pr1node03 (JobHistory Server)\n4. Ssh pr1node01 and inspect logs at /var/loh/hadoop-yarn. We could not find the root cause from logs\n5. Restart the Node Manager role for the specific node. After the restart the alert disappeared.\n6. During further investigation, from Cloudera UI we saw that prior to ```Process Status``` alert there was a ```NODE_MANAGER_LOG_FREE_SPACE``` alert\n7. From pr1node01 as root `df -h /`. The usage of `/` was at 98% at that time\n8. Upon inspection we noticed that the krb5kdc logs had increased over the last months peaking the monthly log file to ~80G.\n9. We proceeded to the removal of krb5kdc log files for 2022.\n10. As a permanent solution, we implemented changes to retention policy for krb5kdc logs. Specifically, we changed the rotation to weekly from monthly and the storage to 7 old logs from 12 logs files that it was prior the change.  This change was implemented at pr1node02, dr1node01 and dr1node02 as well.\n![logrotate_krb5kdc](.media/IM2095156/IM2095156_logrotate_krb5kdc.PNG)\n## Our Ticket Response\n```\n09/03/23 15:47:41 Europe/Eastern (MASTROKOSTA MARIA):\nFollowing the investigation, we have changed the retention for krb5kdc logs. Specifically, we have set the rotation to be weekly instead of monthly and to keep 7 log files. Note that the monthly krb5kdc log file had reached 80G.\nPlease let us know if we can proceed with closing the ticket.\n```\n```\n01/03/23 07:16:41 Europe/Eastern (MASTROKOSTA MARIA):\nThere was a malfunction in the yarn node manager since 3.42, resulting in the ibank and online streaming topologies falling as recorded in ticket SD2157107.\nWe proceeded to restart at 4:53 to get it back up. During the restart, the online merge batch crashed, which was resubmitted (related ticket SD2157111).\nAt this time, yarn and the flows are running normally.\nFrom the investigation it appears that the root partition on pr1node01 had filled up, which was caused by the local kdc logs. We have proceeded to clean the corresponding log files and are investigating changes to the retention of the logs to avoid future problems.\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230301-IM2095156.md"
        }
    },
    "441": {
        "page_content": "---\ntitle: DWH_IBank EXPORT Job Failure Due to Compute Stats Resource Contention\ndescription: EXPORT job for component CARD failed with Code 6 due to Impala resource saturation caused by COMPUTE STATS on large table `prod_trlog_ibank.service_audit`, which blocked metadata operations and caused extract script timeouts.\ntags:\n  - mno\n  - bigstreamer\n  - dwh_ibank\n  - card export\n  - compute stats\n  - impala\n  - query timeout\n  - code 6\n  - resource pool\n  - disaster recovery\n  - service_audit\n  - export failure\n  - grafana alert\n  - cloudera manager\n  - metadata refresh\n  - monitoring dashboard\n  - impala insert\n  - sqoop export\n  - job recovery\n  - im2024442\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2024442\n  system: mno BigStreamer DWH Disaster Recovery Site\n  root_cause: EXPORT job timed out because Impala was saturated by a COMPUTE STATS operation on the service_audit table, which blocked subsequent metadata and extract queries\n  resolution_summary: The compute stats query completed and resources were released; workaround was to rerun job and disable stats generation for the problematic table\n  blocking_query: COMPUTE STATS prod_trlog_ibank.service_audit\n  affected_job: DWH_IBank EXPORT CARD\n  job_status_code: 6\n---\n# mno - BigStreamer - IM2024442 - Failed job at Grafana\n## Description\nOn 26/10/2022, the DWH_IBank EXPORT job for component `CARD` failed with status code 6, indicating a timeout in the control script while waiting for the `EXTRACT` phase to finish.\n```bash\nApplication: DWH_IBank\nJob Name: EXPORT\nComponent: CARD\nDate: 26/10/2022\nStatus: FAILED\nDescription: Code:6\n```\n## Actions Taken\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account and confirm that Datawarehouse Flows have failed from `Monitoring/Monitoring PR/DR` dashboard.\n2. All flows have failed with `Code: 6` which means that the control script has timed-out while monitoring the `EXTRACT` script. The `EXTRACT` step has 2 sub-steps: `Impala Insert` and `Sqoop Export`.\n3. Check logs:\nFrom `dr1edge01.mno.gr` with personal account:\n``` bash\nless /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\n```\nIn this file for all flows that failed we see that the last log entry is the submission of the `Impala Insert` part of the `EXTRACT`, which was still running. This means that another query is hogging all resources for Impala and our flows are waiting to be executed.\n4. Login to Disaster Site Cloudera Manager `https://dr1edge01.mno.mno.gr:7183` and check for resource intensive Impala queries `Clusters > Impala > Queries`. The key resource here is memory as this is the only metric that can be defined in Resource Pools.\nThe query that created the problem was `COMPUTE STATS prod_trlog_ibank.service_audit`. Pictures are not included because Impala does not report statistics for the `COMPUTE STATS` queries, but given the size of the table and the time of execution it matched. This hypothesis was later confirmed when the same problem appeared on a later `COMPUTE STATS` execution.\n5. We informed the customer to re-run the failed jobs and proposed to stop computing statistics for that table as they did not impact our application.\n``` text\n27/10/22 13:16:01 Europe/Eastern (POULAS GIORGOS):\nPlease rerun the flow steps that encountered a problem.\nWe are continuing to investigate the root cause of the problem.\n**Workaround**\n27/10/22 13:32:44 Europe/Eastern (POULAS GIORGOS):\nFollowing the previous answer, the compute statistics on the prod_trlog_ibank.service_audit table committed many resources on the Calatog Server and the query Coordinator (dr1node02), resulting in REFRESH/INVALIDATE METADATA operations experiencing long execution times and causing jobs to time out. After the compute statistics were completed, Impala released the resources and resumed.\nAs we have communicated in the past, from our perspective the statistics of the table are not necessary. Please let us know if we can disable the production of statistics for this particular table.\n**Resolved**\n```\n6. The changes for the statistics were implemented as part of [this ticket](obss/oss/sysadmin-group/mno/cloudera-cluster#180).\nDisaster Recovery Site Datawarehouse",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20221027-IM2006951.md"
        }
    },
    "442": {
        "page_content": "---\ntitle: iBank_Migration Historical Batch Job Failed Due to Impala OOM\ndescription: The iBank_Migration Historical batch job failed on the Primary site due to Impala out-of-memory (OOM) errors triggered by high-memory queries running during the same window. The issue was mitigated by backing up the data, re-running the script manually, and analyzing Impala query memory usage via Cloudera Manager.\ntags:\n  - mno\n  - bigstreamer\n  - ibank\n  - impala_insert\n  - ibank_migration\n  - batch job\n  - historical\n  - oom\n  - out of memory\n  - grafana alert\n  - cloudera manager\n  - pr1edge01\n  - sqoop import\n  - ibank_histMigrate_aggr\n  - merge batch\n  - impala\n  - memory usage\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1868465\n  system: mno BigStreamer - Primary Site\n  root_cause: Impala out-of-memory (OOM) due to high-memory queries overlapping with batch job execution\n  user_visible_error: Historical batch job and Impala_Insert component failed on Grafana\n  detection_method:\n    - Grafana alert\n    - Log analysis from ibank_histMigrate_aggr_MergeBatchWithLock_v2.log\n    - Impala query chart in Cloudera Manager\n  action_taken:\n    - Verified Grafana failure for Impala_Insert\n    - Identified Impala OOM from log and CM charts\n    - Backed up HDFS files for raw data\n    - Manually re-executed batch job script\n    - Investigated Impala query load during the failure window\n  outcome: Job re-executed successfully; customer notified to avoid heavy queries during processing hours\n---\n# mno - IBANK - IM1868465 - Batch Job failed on Grafana\n## Description\nOn 23/05/2022, the iBank_Migration Historical batch job failed on the Primary Site, as shown in Grafana. The Impala_Insert step failed due to an out-of-memory (OOM) error in Impala, which occurred during a high-load period where multiple large queries were running. This caused the final validation step to compare fewer Impala records than expected from the Sqoop import.\n```\napplication: iBank_Migration\njob_name: Historical\ncomponent: Impala_Insert\ndate: 23/05/2022\nstatus: Failed\ndescription:\nhost: pr1edge01.mno.gr\n\napplication: iBank_Migration\njob_name: Historical\ncomponent: JOB\ndate: 23/05/2022\nstatus: Failed\ndescription: Impala rcords are less than retrieved sqoop records\nhost: pr1edge01.mno.gr\n```\n## Actions Taken\nWorkaround Steps\n1. Check Grafana\nGrafana -> LOCAL MONITOR/ Batch Jobs PR\n```bash\nHistorical | Sqoop_Import : SUCCESS \nHistorical | Impala_Insert : FAILED\n```\n2. Check Main script logs\n```bash\nsudo su - PRODREST\ncrontab -l #find log file\nless /var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log\n```\nWe saw the following:\n```bash\nrestval_sqoop_2=1\nsqoop import failed again\n```\nAccording to the log file, the first step of the Main Script failed with exit code 1.\nMore Info for the Steps here:[<https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#mssql-sqoop-import>]\n3. Check \"MSSQL Sqoop Import\" step logs:\n```bash\nless /opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log\n```\nWe found out the the INSERT Impala query failed with error: Memory limit exceeded.\n4. Backup prod_trlog_ibank.historical_service_audit_raw_v2 files\n```bash\nhdfs dfs -mkdir /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/20220524_import_historical_service_audit_raw_v2_dvoul\nhdfs dfs -mv /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2/* /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/20220524_import_historical_service_audit_raw_v2_dvoul/\n```\n5. Run Main script with PRDREST user\n```bash\nscreen\n/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh >> /var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log\n```\nA copy of the main batch script was used to re-run the process safely. The historical data files were backed up before reprocessing, ensuring no data loss.\n## Root Cause Analysis\nNow that we are certain that we have an Impala OOM issue, we have to check Impala resources specifically.\n1. From Cloudera Manager select Impala Service and check the Out of Memory Impala Queries Chart\nIn our case we had a large number of OOM quries between 23:00 and 02:00.\n2. Check for large queries in the corresponding time slot\nCloudera Manager -> Impala -> Queries\nSelect those that have a lot of Aggregate Peak Memory Usage from the filters on the left.\n## Our Ticket Response\n```\nThe topology has been re-executed and is in running status. The analysis revealed that there was an Out of Memory problem in Impala due to some queries that ran between 23:00 and 02:00 and requested many MB of memory.\nThese specific queries are hindering the production process. Attached you will find the relevant screenshots.\nWe ask for your immediate actions as well as for the control of your own flows during the above period.\n```\n![PR_OOM_Impala_Queries.png](.media/PR_OOM_Impala_Queries.png)\n![PR_Queries1.png](.media/PR_Queries1.png)\n![PR_Queries2.png](.media/PR_Queries2.png)\n## Affected Systems\nPrimary Site IBank Batch",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20220524-IM1868465.md"
        }
    },
    "443": {
        "page_content": "---\ntitle: Disaster Site IBANK - Incorrect Historical Data in service_audit_stream and PR to DR Partition Sync\ndescription: Historical data inconsistencies found in DR table prod_trlog_ibank.service_audit_stream were resolved by transferring correct partitions from PR to DR using temporary tables and Cloudera Replication. The issue was initially caused by a past Kudu malfunction at DR site.\ntags:\n  - bigstreamer\n  - mno\n  - ibank\n  - dr site\n  - pr site\n  - kudu\n  - replication\n  - impala\n  - cloudera\n  - service_audit\n  - partition sync\n  - replication schedule\n  - prod_trlog_ibank\n  - audit stream\n  - data correction\n  - drop partition\n  - insert overwrite\n  - data reconciliation\n  - mobaXterm\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1389913\n  system: MNO BigStreamer - IBANK DR Site\n  root_cause: Kudu malfunction caused old data to be inserted in `service_audit_stream` at DR. PR had correct values.\n  action_taken:\n    - Verified partition counts across PR and DR using impala-shell\n    - Created and populated temporary table in PR with correct partitions\n    - Synchronized data to DR using Cloudera Replication Manager\n    - Dropped incorrect partitions from DR\n    - Re-inserted correct data from temp table into DR `service_audit`\n    - Validated data parity post-restore\n  outcome: All target partitions in DR now match PR site; corrupted historical data was successfully replaced\n---\n# mno - BigStreamer - IM1389913 - Disaster Site IBANK - Incorrect Historical Data in service_audit_stream and PR to DR Partition Sync\n## Description\nDiscrepancies were identified in the `prod_trlog_ibank.service_audit_stream` table at the Disaster Recovery site, where older historical records had incorrectly reappeared. The issue did not affect the Primary site, confirming the corruption was isolated to DR, most likely due to Kudu service instability in the past. A full validation and restoration from PR to DR was carried out via Impala and Replication Manager.\n```\ntime count(*)\n20201030 22.927.271\n20201110 5.947.186\n20201114 3.294.430\n20201116 3.090.276\n20201118 5.325.057\n20201124 3.465.507\n20201125 3.527.222\n20201201 2.405.322\n```\n## Actions Taken\n1. Initial investigation by lmn team indicated the root cause was DR-local Kudu issues. PR data was intact.\n```\nThe problem you are reporting only appears on the Disaster Site. It is related to the problems that the Kudu service was experiencing at that time.\nFrom the comparison of data with the Primary Site we see that the data has been passed correctly to service_audit.\nPlease confirm that you have the same picture of the problem and if we can proceed to delete the data from the service_audit_stream.\n```\nCustomer then wanted to transfer data from PR to DR.\n5. Connected  on MobaXterm via ssh to `dr1edge01` and changed user to `PRODREST` using sudo.\n7. Open `impala-shell` for DR site and check row count of `prod_trlog_ibank.service_audit`  for these specific dates.\n```bash\nimpala-shell -i dr1edge.mno,gr -k --ssl\nselect par_dt, count(*)  from prod_trlog_ibank.service_audit where par_dt in (20201105, 20201111, 20201120, 20201127, 20201202) group by 1 order by 1;\npar_dt count\n20201105 20189258\n20201111 18855105\n20201120 20212408\n20201127 36624142\n20201202 25042327\n```\nExecute the same query for PR site by opening the `impala-shell` for PR, to compare row counts.\n```\nimpala-shell -i pr1edge.mno,gr -k --ssl\n```\n8. Create temporary table at PR and insert the partitions we want to transfer to DR\n```\nimpala-shell -i pr1edge.mno,gr -k --ssl\n...\nCREATE TABLE prod_trlog_ibank.service_audit_temp LIKE prod_trlog_ibank.service_audit;\nset MAX_ROW_SIZE=100mb;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201105;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201111;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201120;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201127;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201202;\n```\n8. Create an empty temporary table at DR.\n```\nimpala-shell -i pr1edge.mno,gr -k --ssl\n...\nCREATE TABLE prod_trlog_ibank.service_audit_temp LIKE prod_trlog_ibank.service_audit;\n```\n9. Open Cloudera Manager `https://dr1edge01.mno.gr:7183` at DR and login with your `EXXXXX` account.\n10. Go to `Backup`>`Replication Schedules` from the top bar.\n11. Edit configuration of `one_off_replication` and change `Databases` field to point to the new temporary table at DR `Databases | prod_trlog_ibank | service_audit_temp `\n10. From `Actions` of `one_off_replication` execute `Dry Run`. Check logs of the execution. Sometimes there is an `Connection timed out` error. If that's the case, click on `Abort` and execute `Dry Run` once again.\n11. After `Dry Run` has finished, under `one_off_replication` there will be information on the number of databases, tables and partitions that are going to be transferred. In this case we had 1 database, 1 table and 400 partitions (each day has 40 subpartitions and we had 5 dates).\n11. From actions of `one_off_replication` click on `Run Now`.\n11. After it finishes, check that all rows have been transferred correctly. Open `impala-shell` for DR site and check row count of `prod_trlog_ibank.service_audit_temp`  for these specific dates.\n```\nimpala-shell -i dr1edge.mno,gr -k --ssl\nINVALIDATE METADATA prod_trlog_ibank.service_audit_temp;\nselect par_dt, count(*)  from prod_trlog_ibank.service_audit_temp where par_dt in (20201105, 20201111, 20201120, 20201127, 20201202) group by 1 order by 1;\npar_dt count\n20201105 20189258\n20201111 18855105\n20201120 20212408\n20201127 36624142\n20201202 25042327\n```\n12. Delete problematic partitions of `prod_trlog_ibank.service_audit` at DR.\n```\nimpala-shell -i dr1edge.mno,gr -k --ssl\n...\nALTER TABLE prod_trlog_ibank.service_audit DROP PARTITION (PAR_DT IN (20201105, 20201111, 20201120, 20201127, 20201202));\n```\n14. Insert data from temporary table.\n```\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201105;\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201111;\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201120;\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201127;\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201202;\n```\n7. Open `impala-shell` for DR site and validate that row count of `prod_trlog_ibank.service_audit`  is the same as seen in PR.\n```\nimpala-shell -i dr1edge.mno,gr -k --ssl\nselect par_dt, count(*)  from prod_trlog_ibank.service_audit where par_dt in (20201105, 20201111, 20201120, 20201127, 20201202) group by 1 order by 1;\npar_dt count\n20201105 ...\n20201111 ...\n20201120 ...\n20201127 ...\n20201202 ...\n```\n## Affected Systems\nDisaster Site IBANK\n",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20201218-IM1389913.md"
        }
    },
    "444": {
        "page_content": "---\ntitle: Host Bad Health Alert on dr1edge01 Due to Full /var Partition\ndescription: Cloudera Manager reported agent health issues for dr1edge01 due to a full /var partition; root cause was oversized Graphite metrics directories under /var/lib/carbon; resolved by deleting obsolete DEV directories.\ntags:\n  - mno\n  - bigstreamer\n  - cloudera\n  - agent status\n  - disk full\n  - graphite\n  - carbon\n  - /var/lib/carbon\n  - dr1edge01\n  - host health\n  - bad health\n  - sd2070794\n  - dr site\n  - metrics storage\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD2070794\n  system: mno BigStreamer DR Site\n  root_cause: Full disk on /var partition caused by accumulated Graphite metrics data under /var/lib/carbon\n  resolution_summary: Deleted obsolete DEV Graphite folders from /var/lib/carbon to free space; host returned to healthy status in Cloudera Manager\n  affected_node: dr1edge01\n  impacted_service: Cloudera Agent Health\n---\n# mno - BigStreamer - SD2070794 - Alert at Cloudera Manager on DR\n## Description\nService affected: Hosts\nBad Health issue for dr1edge01.mno.gr (Agent Status)\n## Actions Taken\n1. Logged in to Cloudera Manager `https://dr1edge01.mno.gr:7183` with personal account in order to check the status of the host \"dr1edge01.mno.gr\".\n2. Checked `https://dr1edge01.mno.gr:7183/cmf/hardware/hosts` the host.\n3. Continued in Cloudera Manager on host \"dr1edge01.mno.gr\" and checked the disks.\n4. Logged in to \"dr1edge01.mno.gr\" node with personal account.\n5. Executed the following command and disk usage check performed on the nodes.\n```bash\ndf -h\n```\n6. We saw that the usage in \"/var\" partition was 100%.\n7. We proceed to find wich directory has big enough size.\n```bash\nsudo du -sh /var/*\n```\nAs we noticed \"/var/lib/carbon\" directory was enormous, up to 199 G. This directory concerns **Graphite** application. <br/>\n8. We navigated to /var/lib/carbon/whisper/translog-api/spark\n```bash\ncd /var/lib/carbon/whisper/translog-api/spark\n```\nIn this directory we can delete without any confirmation all the \"DEV\" directories.\n```bash\n-bash-4.2$ sudo rm -rf IBank_IngestStream_DEV_mno IBank_MergeBatch_DEV_mno IBank_MergeBatch_DEV_mno_Hourly Online_IngestStream_DEV_mno Online_MergeBatch_DEV_mno Online_MergeBatch_DEV_mno_Hourly\n```\n9. Disk usage has been checked again as step [5](#step-5).\n10. Cloudera Manager has been checked to validate that the bad status host alert has been eliminated as step [1](#step-1).\n## Root Cause\nThe `/var` partition on dr1edge01 was full due to accumulated Graphite metrics data from DEV topologies under `/var/lib/carbon/whisper/translog-api/spark`, causing agent heartbeat failures.",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "IM2010302-SD2070794.md"
        }
    },
    "445": {
        "page_content": "---\ntitle: MAN_DATE Extract and Export Job Failed Due to Duplicate Key in SQL Server Table\ndescription: On 28/03/2023 the MAN_DATE extract job failed with error \"duplicate key\" during insertion into SQL Server; resolved by rerunning both extract and export with force (-f) option to truncate and reload the table.\ntags:\n  - mno\n  - bigstreamer\n  - grafana\n  - batch job\n  - man_date\n  - dwh_ibank\n  - duplicate key\n  - sql server\n  - extract\n  - export\n  - truncate\n  - mandateDetails\n  - sched_extract_details.sh\n  - sched_export_details.sh\n  - im2117067\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2117067\n  system: mno BigStreamer DWH\n  root_cause: Duplicate key already present in srcib.MandateDetails table, causing extract to fail\n  resolution_summary: Extract and export scripts rerun with `-f` option to truncate target table before insertion\n  scripts:\n    - /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n    - /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n  knowledge_links:\n    - https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-extract\n    - https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-export\n---\n# mno - BigStreamer - IM2117067 - Grafana Batch Job failed\n## Description\nToday 29/03 in Grafana application a failed Batch Job appeared.\n```\nApplication : DWH_IBank\nJob_Name : Extract\nComponent: MAN_DATE\nDate: 28/03/23\nStatus: Failed\nDescription Code 1\n```\nInformation regarding the extract, logs etc. available [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-extract)\nInformation regarding the export, logs etc. available [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-export) \n## Actions Taken\nWe tried to invoke the Extract MAN_DATE script but it failed multiple times. After investigating the logs of the executor using internal firefox and the logs of the DWH_Ibank_MAN_DATE script we saw the following:\n`Cannot insert duplicate key in object srcib.MandateDetails. The duplicate key value is (e5435435-4354254235-121nfdgd33)`\nThat means that the table already has records in it, so we have to drop this records or invoke the script with the `-f` option that gives the ability to truncate the table and then insert the records. \n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n```\nAfter executing the extract script, we use the same `-f` to run the export script. The reason that we are running export is because in the above mentioned step we run the extract and trancated the table so the export is neccessary. \n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n```\nThe issue has been resolved by executing the above two commands. The key in this case was to find in the logs that we have dublicate keys. ",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230329-IM2117067.md"
        }
    },
    "446": {
        "page_content": "---\ntitle: HBase Master and RegionServer Alerts on DR1 Due to Corrupt Table Metadata\ndescription: Multiple HBase alerts occurred in DR1 due to orphaned metadata from a deleted table (PROD_BANK:TAX_FREE_20220404), causing regions in transition and connection failures; resolved by removing HBCK lock and cleaning metadata via HBCK2.\ntags:\n  - mno\n  - bigstreamer\n  - hbase\n  - hbck\n  - hbck2\n  - regions in transition\n  - unexpected exits\n  - hbase metadata\n  - regionserver\n  - datanode connection failure\n  - hdfs cleanup\n  - dr1node06\n  - dr1node05\n  - dr1edge01\n  - PROD_BANK\n  - TAX_FREE_20220404\n  - hbase repair\n  - alerts\n  - im1910783\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1910783\n  system: mno BigStreamer DR1 HBase\n  root_cause: HBase table metadata (TAX_FREE_20220404) was not removed after table deletion, leading to stale region info in meta and block connection failures\n  resolution_summary: Orphaned regions cleaned with HBCK2, HDFS files removed, and cluster health restored\n  affected_nodes:\n    - dr1node06\n    - dr1node05\n    - dr1edge01\n---\n# mno - BigStreamer - IM1910783 - Multiple Alerts on DR1 HBASE\n## Description\nMultiple alerts were raised in Cloudera Manager for the DR1 HBase cluster. The master node `dr1node06` reported **\"HBase Regions In Transition Over Threshold\"**, and the regionserver on `dr1node05` experienced **unexpected exits**. Investigation revealed that a previously deleted HBase table (`PROD_BANK:TAX_FREE_20220404`) still had **orphaned metadata** in `hbase:meta`, causing **regions to remain stuck in transition** and **HDFS connection failures**. \nThe corrupted metadata was cleaned using **HBCK2**, and the corresponding HDFS directory was manually removed. After cleanup, HBase master was restarted and the cluster returned to a healthy state.\n## Actions Taken\n1. Login to `dr1edge01.mno.gr` with personal account and then from command line type `firefox`\n2. In firefox type: `https://dr1node06.mno.gr:16010/master-status`\nWe see that 7 RegionServers are in Transition.\n3. Login to `dr1edge01.mno.gr` with personal account and then ssh to a node\n4. Move to the process folder:\n```bash\ncd /var/run/cloudera-scm-agent/process/\n```\n3. Find the latest hbase process and go to that folder. In our case is `12269-hbase-MASTER-togglebalancer`. So move to that folder:\n```bash\ncd 12269-hbase-MASTER-togglebalanncer\n```\n4. Use the keytab you just found in that folder:\n```bash\nkinit -kt hbase.keytab hbase/`hostname`\n```\n5. Check the health of HBASE:\n```bash\nhbase hbck (On this command the status was not HEALTHY)\n```\n6. Remove the file that we found from above test\n```bash\nhdfs dfs rm hdfs://DRBDA-ns/hbase/.tmp/hbase-hbck.lock\n```\n7. After investigating logs at `/var/log/hbase/hbase-cmf-hbase-REGIONSERVER-dr1node01.mno.gr.log.out` for the time that the ticker occured we show the following error:\n```bash\n2022-07-07 19:30:07,874 WARN org.apache.hadoop.hdfs.DFSClient:Connectionfailure:Failedtoconnecttodrinode05.mno.gr/I\n999.999.999.999:50010 for file /hbase/data/PROD BANK/TAX _FREE_20220404/2c093lelcf6178a3548f7162a5a5965a/D/e2ea28cd22ad402394bd8\n6acb8002f58 for block BP-1157034308-999.999.999.999-1530642985707:blk_1164232343_90497946:0rg.apache.hadoop.hdfs.security.toke\n```\nAt that moment we realised that table `TAX _FREE_20220404` was removed but metadata weren't. So, let's remove them too!\n> Ndef: We have hbase 2.1 version and basic `hbase hbck` tool doesn't support basic options and flags like repair/fix. So we had to download HBCK2 from [here](https://jar-download.com/artifact-search/hbase-hbck2)\n8. Fix extra regions in hbase:meta region/table\n```bash\nhbase hbck -j hbase-hbck2-1.2.0.jar extraRegionsInMeta PROD_BANK:TAX_FREE_20220404 --fix\n```\n9. Get states of regions of PROD_BANK:TAX_FREE_20220404 table in hbase\n```bash\nhbase shell\n> scan 'hbase:meta',{FILTER=>\"PrefixFilter('PROD_BANK:TAX_FREE_20220404')\"}\n```\nOutput: No rows\n10. Remove table from hdfs:\n```bash\nhdfs dfs -rm -r hdfs://DRBDA-ns/hbase/data/PROD_BANK/TAX _FREE_20220404\nhdfs dfs -ls hdfs://DRBDA-ns/hbase/data/PROD_BANK/\n```\n11. Restart HBASE master and check health\n```bash\nhbase hbck\n```\n## Affected Systems\nmno Bigstreamer DR1 HBASE",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20220707-IM1910783.md"
        }
    },
    "447": {
        "page_content": "---\ntitle: TimeDeposit Extraction Job Blocked Due to Incomplete Monitoring Status Entry\ndescription: The `sched_extract_details.sh -t timeDeposit` script failed due to a previously incomplete job status entry in `dwh_monitoring` caused by Hive metastore downtime during an upsert; resolved by manually correcting the status in Impala.\ntags:\n  - mno\n  - bigstreamer\n  - time_deposit\n  - prod_submit\n  - extract job\n  - scheduler\n  - dwh_monitoring\n  - hive metastore\n  - impala upsert\n  - metadata sync\n  - job failure\n  - script crash\n  - sd2048346\n  - sd2046350\n  - impala shell\n  - manual correction\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD2048346\n  related_issue: SD2046350\n  system: mno BigStreamer DWH\n  root_cause: Hive metastore unavailability caused the upsert to fail, leaving the monitoring status in RUNNING, blocking re-execution\n  resolution_summary: Manually updated job status in `prod_trlog_ibank_analytical.dwh_monitoring` to SUCCESS, allowing the script to rerun\n  affected_table: prod_trlog_ibank_analytical.dwh_monitoring\n  impacted_component: timeDeposit extract scheduler job\n  manual_query_used: true\n---\n# mno - BigStreamer - SD2048346 - error in PROD_SUBMIT SH TIMEDEPOSIT sh script\n## Description\nThe `sched_extract_details.sh -t timeDeposit` job failed during execution. The script crashed with error: ERROR DESCRIPTION=ANOTHER_PROCESS_RUNNING\nThis was due to a previously incomplete status entry for the same job in the `prod_trlog_ibank_analytical.dwh_monitoring` table. During the previous execution, an `UPSERT` that should have marked the job status as `SUCCESS` failed because the Hive Metastore was unavailable (see SD2046350). As a result, the job status remained `RUNNING`, preventing subsequent executions.\n## Actions Taken\n1. Login to `dr1edge01.mno.gr` with personal account and then `sudo su - PRODUSER`\n2. We tried to re-run the script using the following command:\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t timeDeposit\n```\nBut we got the following error:\n**ERROR DESCRIPTION=ANOTHER_PROCESS_RUNNING**\n3. After investigation at impala queries and alert notification we realized that at `10/04/2022 8:29 AM` which was the time that the `extract` of `time_deposit` was running, the upsert that updates the status (SUCCEDED/RUNINNG/FAILED) of the job, failed due to hive metastore unavailability at that time. \n> Ndef: Hive metastore issue is discribed at **SD2046350**\n## Root Cause Analysis\nWe found that with below steps:\n- From DR CM UI -> Impala -> Queries \n- In searh bar type: `statement rlike 'upsert into prod_trlog_ibank_analytical.*'` and click on `Search` to find the query\nLogin to dr1edge01 with your personal account and then `sudo su - PRODUSER`\nInvestigate impala tables with below commands:\n```bash\nimpala-shell -k -i dr1edge01.mno.gr --ssl\nSHOW CREATE TABLE prod_trlog_bank_analytical.dwh_monitoring;\nSELECT * FROM prod_trlog_bank_analytical.dwh_monitoring WHERE details_type='TIME_DEPOSIT' and\n'procedure'='EXTRACT' and par_dt > 20221002;\n```\nTherefore, at `10/05/2022` when the script executed again, the job crashed due to the fact that the value on the impala table was set to RUNNING. \nWe had to manually change he value of yesterday's job to `SUCCESS` using the following impala query.\nFirst, login to dr1edge01 with your presonal account, change to PRODUSER with `sudo su - PRODUSER` and login to impala-shell using following command:\n```bash\nimpala-shell -k -i dr1edge01.mno.gr --ssl\n```\nIn our case we run the following query:\n```bash\nUPSERT INTO prod_trlog_ibank_analytical.dwh_monitoring\n(details_type, procedure_par_dt,status.start_time,end_time,description) VALUES\n('TIME DEPOSIT','EXTRACT','20221003','SUCCESS','2022-10-04','08:32-42.000','2022-10-04','08-39:21.000',\")\n```\n> Ndef: In case you want to run the above query for a different job modify VALUES according the procedure_par_dt, details_type, etc\n4. Repeat **step 2** in order script to succeed or ask mno to do that.\n## Affected Systems\nmno Bigstreamer",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20221005-SD2048346.md"
        }
    },
    "448": {
        "page_content": "---\ntitle: [PR][IBANK] Query Average Response Time Alert on Grafana\ndescription: Investigation of Grafana alert \"[PR][IBANK] Query Average Response Time\" caused by a few high-latency queries due to user activity. The alert was caused by bias in the mean response time and resolved without intervention.\ntags:\n  - bigstreamer\n  - mno\n  - ibank\n  - grafana\n  - pr site\n  - response time\n  - latency\n  - average query time\n  - mean vs max\n  - cloudera\n  - wildfly\n  - edge nodes\n  - monitoring\n  - grafana alerts\n  - query metrics\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1317401\n  system: MNO BigStreamer IBANK PR Site\n  root_cause: Few user-triggered long queries skewed the mean query response time, triggering the Grafana alert\n  user_visible_alert: \"[PR][IBANK] Query Average Response Time alert\" on Grafana dashboard\n  action_taken:\n    - Edited the Grafana graph to temporarily display max instead of mean query time\n    - Checked access logs on PR edge nodes\n    - Verified Cloudera cluster health on PR site\n    - Coordinated with MNO to confirm user-triggered activity\n  outcome: Alert cleared on its own; no intervention required\n---\n# mno - BigStreamer - IM1317401 - [PR][IBANK] Query Average Response Time alert\n## Description\nAlert message on Grafana:\n[PR][IBANK] Query Average Response Time alert\n## Actions Taken\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account\n2. Inspected `[PR][IBANK] Query Average Response Time alert` graph\n```\nI edited the graph temporarily to display the max response time instead of the mean value. Usually the problem affects 3-4 queries and is due to user actions in the PR site.\nThese problematic queries add bias to the mean time and create the alarm. By checking the max values, I saw that that was the case.\n```\n3. Login to pr1edge01.mno.gr/pr1edge02.mno.gr with personal account and check access logs under `/var/log/wildfly/prodrestib/access.log`\n4. Login to Primary Site Cloudera Manager `https://pr1edge01.mno.mno.gr:7183` and check that the cluster is in healthy status\n5. No action taken. The alarm will clear without the need for manual action.\n6. Since no actions had been initiated from our team on the PR site at the time of the incident, we requested that MNO verify whether internal user activity may have caused the query load.\n## Affected Systems\nPrimary Site IBANK query",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20201014-IM1317401.md"
        }
    },
    "449": {
        "page_content": "---\ntitle: IBANK MergeBatch Failure on DR Site Due to Memory Exhaustion - Manual Recovery\ndescription: The IBANK_Migration MergeBatch job failed on 2021-04-30 due to excessive data volume and memory exhaustion. The Spark job was reconfigured with increased coalesce and shuffle partitions, and rerun from the merge section of the ingestion script to complete the Data Warehouse pipeline.\ntags:\n  - bigstreamer\n  - mno\n  - ibank\n  - mergebatch\n  - yarn\n  - spark\n  - coalesce\n  - shuffle.partitions\n  - memory error\n  - ingestion pipeline\n  - spark tuning\n  - merge failed\n  - dr site\n  - historical migration\n  - prodrest\n  - parquet ingestion\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1317401\n  system: MNO BigStreamer - IBANK DR Site\n  root_cause: Spark MergeBatch job failed due to memory issues caused by large data volume; default configuration insufficient\n  user_visible_error: Job failed in Yarn and did not appear in active applications list\n  action_taken:\n    - Inspected job status in Grafana\n    - Verified failure in `monitor_sched_jobs` and via `yarn`\n    - Increased `coalesce` and `shuffle.partitions` values in Spark submit script\n    - Restarted ingestion script from merge section\n  outcome: MergeBatch job completed successfully after reconfiguration\n---\n# mno - BigStreamer - IM1317401 - [PR][IBANK] Data warehouse flows\n## Description\nThe IBank_Migration job pipeline appeared successful for all historical stages on `20210429`, except for the `MergeBatch` job, which failed. This job is critical for aggregating and inserting the final data into the Data Warehouse. Investigation showed that it crashed due to memory issues when processing large data volumes, and a reconfiguration of the Spark parameters was required to complete execution.\nData Warehouse jobs have not run:\n```sql\nselect * from prod_trlog_ibank.monitor_sched_jobs where par_dt=20210429\n\nIBank_Migration Historical JOB 20210429 SUCCESS 2021-04-30 02:00:01.000 2021-04-30 02:03:50.000 dr1edge01.mno.gr\n2 IBank_Migration Historical Sqoop_Import 20210429 SUCCESS 2021-04-30 02:00:01.000 2021-04-30 02:01:51.000 dr1edge01.mno.gr\n3 IBank_Migration Historical Impala_Insert 20210429 SUCCESS 2021-04-30 02:03:07.000 2021-04-30 02:03:50.000 dr1edge01.mno.gr\n4 IBank_Migration Historical to SA Impala_Insert 20210429 SUCCESS 2021-04-30 02:04:23.000 2021-04-30 02:06:21.000 dr1edge01.mno.gr\n5 IBank_Migration Historical to SA JOB 20210429 SUCCESS 2021-04-30 02:04:23.000 2021-04-30 02:06:21.000 dr1edge01.mno.gr\n6 IBank_Ingestion MergeBatch JOB 20210429 FAILED 2021-04-30 09:37:35.000 2021-04-30 09:37:35.000 dr1edge.mno.gr\n```\nThe merge batch has also crashed.\n## Actions Taken\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account\n2. Inspected `[PR][IBANK] Overview` graph\n3. Merge Batch job has FAILED\n4. MergeBatch job was not running : `yarn application -list | grep -i merge | grep -v Hourly`\n5. Found Spark job failure due to OOM (Out Of Memory) error in Yarn logs.\n6. Failure was reproducible; retry without changes failed again.\n7. `Vi /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh`\n8. Search for \u201ccoalesce\u201d , Change/replace `-coalesce=$NUMBER_OF_EXECUTORS \\ ` , To : `-coalesce=96 \\ `\n9. Search for `--spark.sql.shuffle.partitions=16  \\`  to : `--spark.sql.shuffle.partitions=96  \\`\n10. As user PRODREST, rerun ingestion script from merge step: `/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh`\n## Affected Systems\nDR Site IBANK",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20210430-IM1317401.md"
        }
    },
    "450": {
        "page_content": "---\ntitle: HDFS DataNode Disk Replacement on dr1node07 Due to Data Directory Warning\ndescription: Warning for HDFS Data Directory Status on dr1node07 led to Oracle-coordinated online disk replacement and reconfiguration using bdadiskutility. Services remained uninterrupted.\ntags:\n  - mno\n  - bigstreamer\n  - dr1node07\n  - hdfs\n  - datanode\n  - data directory status\n  - disk replacement\n  - cloudera manager\n  - bdadiskutility\n  - oracle\n  - efis1entry\n  - lsblk\n  - mirrored disk\n  - impala shutdown\n  - yarn\n  - disk failure\n  - data directory warning\n  - SD1951890\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD1951890\n  system: mno BigStreamer DR - HDFS\n  root_cause: Disk failure on dr1node07 triggered HDFS Data Directory Status warning; Oracle scheduled disk replacement\n  resolution_summary: Disk replaced online, reconfigured with bdadiskutility using Oracle\u2019s documented procedure\n  requires_restart: No, services remained active; Impala/YARN roles paused only if active\n  downtime: None\n  external_doc: https://support.oracle.com/epmos/faces/DocumentDisplay?id=2642582.1\n---\n# mno - BigStreamer - SD1951890 - hdfs - Data Directory Status\n## Description\nOn 20/06/2022 at 12:40 PM, Cloudera Manager triggered a Data Directory Status - Warning alert for HDFS DataNode dr1node07 in the DR cluster.\n```\nhdfs - DataNode(dr1node07) - Data Directory Status - Warning\n```\n## Actions Taken\nAfter investigation we saw that the problem occurred due to disk issue on dr1node07.\nWe communicated with Oracle and disk replacement was scheduled.\n> Ndef: that disk replacement performed online so there was no downtime.\nAccording to [this document](https://support.oracle.com/epmos/faces/DocumentDisplay?_afrLoop=134521948780510&parent=EXTERNAL_SEARCH&sourceId=REFERENCE&id=2642582.1&_afrvwxowMode=0&_adf.ctrl-state=150blaep6z_4) we perfomed the following steps:\n1. Before proceed with the following procedure make sure that the partition is not used from any process with the following command:\n```bash\nlsof /u09\n```\nIf you get an active process you should stop it first and start it again after step 7.\nFor example, if there are processes running on YARN Node Manager then proceed with following the steps [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/208) about stopping impala daemon & YARN roles on the corresponding node, in more detail:\n- Connect from the corresponding drnode with `impala-shell -i dr1edge.mno.gr -k --ssl`\n- Give permissions from inside impala-shell to your user with `grant role icom_adm to group EXXXXX`\n- Run `:SHUTDOWN('FQDN')` from inside impala-shell\n2. Back up `efis1entry`:\n```bash\nls -l /opt/oracle/bda/compmon/efis1entry\nmv /opt/oracle/bda/compmon/efis1entry /opt/oracle/bda/compmon/efis1entry.BAK_`date +%d%b%Y%H%M%S`\n```\n3. Download \"45R.zip\" from above doc and and place in a temporary/staging location on the server where the disk will be or was replaced.\n4. Extract \"bdadiskutility\" into /opt/oracle/bda/bin/bdadiskutility and \"efis1entry\" into /opt/oracle/bda/compmon/efis1entry\n```bash\nunzip 45R.zip bdadiskutility -d /opt/oracle/bda/bin/\nunzip 45R.zip efis1entry -d /opt/oracle/bda/compmon/\nls -l /opt/oracle/bda/bin/bdadiskutility #Verify that it is extracted\nls -l /opt/oracle/bda/compmon/efis1entry #Verify that it is extracted\n```\n5. Set the permissions on both scripts and verify\n```bash\nchmod 0755 /opt/oracle/bda/bin/bdadiskutility\nchmod 0755 /opt/oracle/bda/compmon/efis1entry\nls -l /opt/oracle/bda/bin/bdadiskutility \nls -l /opt/oracle/bda/compmon/efis1entry \n```\n6. Confirm the latest bdadiskutility is in place\n```bash\nbdadiskutility -v #Must be 45R\n```\n7. Display a summary of all disk states\n```bash\nbdadiskutility -i\n```\n8. After the disk is replaced, and the slot and mount point of the replaced disk fully identified, configure the replaced disk. In our case is u01\n```bash\nbdadiskutility /u01\n```\nWith force option:\n```bash\nbdadiskutility -f /u01\n```\n9. Configure OS disk\n> Ndef: Perform this step only if the partition is mirrored. Otherwise skip this step. You can check that by running the following command:\nMake sure that you can see `/u09`\n```bash\nlsblk\n```\nAfter successfully configuring an OS disk with bdadiskutility confirm the  mirrored partitions are in an \"active sync\" state before rebooting the server.\nMonitor the state of the  mirrored partitions for the equivalent device paths for \"/boot\" and \"/\" on the system.\nFor example if \"/boot\" and \"/\" are /dev/md126 and /dev/md127 respectively, monitor the status with:\n```bash\ncat /proc/mdstat\nmdadm -Q --detail /dev/md126\nmdadm -Q --detail /dev/md127\n```\n*Congrats!* Disk replacement is complete.\n## Affected Systems\nDisaster Site  dr1node07",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20220620-SD1951890.md"
        }
    },
    "451": {
        "page_content": "---\ntitle: IBank_Migration Job Failure - Enrich SA from SA_old (Memory Limit Exceeded)\ndescription: The 'Enrich SA from SA_old' batch job on 30-03-2023 failed due to Impala memory exhaustion during execution on pr1node05; rerun manually after verifying service_audit partition and updating Grafana monitoring DB.\ntags:\n  - mno\n  - bigstreamer\n  - ibank\n  - service_audit\n  - impala\n  - memory limit exceeded\n  - grafana\n  - enrichment\n  - upsert to hbase\n  - duplicates\n  - kudu\n  - hbase\n  - impala query failure\n  - retry job\n  - monitoring update\n  - prodrest\n  - im2180781\n  - pr1edge01\n  - date_20230330\n  - manual rerun\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD2180781\n  system: mno BigStreamer PR Site\n  root_cause: Impala memory limit exceeded on pr1node05 during join query for service_audit enrichment\n  resolution_summary: Reran enrichment script after verifying missing par_dt=20230330, then manually updated monitoring DB and ran HBase upsert and duplication report scripts\n  related_scripts:\n    - ibank_service_audit_insert_join_distinct.sh\n    - ibank_visible_trn_hbase_daily_upsert_STABLE.sh\n    - report_duplicates_kudu_hbase_impala_STABLE.sh\n  related_tables:\n    - prod_trlog_ibank.service_audit\n    - prod_trlog_ibank.service_audit_stream\n    - prod_trlog_ibank.service_audit_old\n  monitoring_updated: true\n---\n# mno - BigStreamer - SD2180781 - Failed job at Grafana \n## Description\nOn 30-03-2023, the \"Enrich SA from SA_old\" job failed due to an Impala memory allocation error on pr1node05.\nThe following failed job appeared in Grafana:\n```\nApplication: IBank_Migration\nJob_name: Enrich SA from SA_old\nComponment: JOB\nDate: 30-03-2023\nStatus: FAILED\nHost: pr1edge01.mno.gr\n```\n## Actions Taken\n1. We have to check first the logs from the failed job `Enrich SA from SA_old` as described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\nDetailed information from the above link:\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n2. The error was the below from logs:\n```\nERROR: Memory limit exceeded: Failed to allocate row batch\nEXCHANGE_NODE (id=5) could not allocate 1.00 MB without exceeding limit.\nError occurred on backend pr1node05.mno.gr:22000\nMemory left in process limit: 27.82 GB\n```\n3. To verify the below error also checked from Cloudera > Impala > Queries\n![impala_query_error](.media/SD2180781/SD2180781_IMPALA_QUERY_ERROR.PNG)\n4. Now we have to go to `troubleshooting steps` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit) in order to check that no records are present in `prod_trlog_ibank.service_audit`\n``` bash\n# eg. 09-11-2019\nimpala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\"\n```\n> Ndef : The par_dt is -1 from today\n5. No records exists on `par_dt` `20230330`\n6. Now we have to run the command `For the previous day:` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\nIf no records exist and no other process is up, you can ran the script again.\n- For the previous day:\n``` bash\n/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n```\n- For a specified date:\n``` bash\n# e.g. 09-11-2019\n/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n```\n7. When the job finished `succesfully` we have to updated the monitoring postgres database in order for the entry `Enrich SA from SA_old` to appeared green/success in Grafana.\n```bash\nssh Exxxx@pr1edge01.mno.gr\nsudo -i -u postgres\npsql -d monitoring\nselect * from prod.monitoring where par_dt = 20230330;\nINSERT INTO prod.monitoring (application, job_name,component,status,par_dt,start_time,end_time,description,params,host) VALUES ('IBank_Migration','Enrich SA from SA_old','JOB',0,20230330,'2023-03-31 03:18:30.000','2023-03-31 05:00:42.000','','','pr1edge01.mno.gr') ON CONFLICT (application, job_name,component,par_dt) DO UPDATE SET status=0, start_time='2023-03-31 03:18:30.000', end_time='2023-03-31 05:00:42.000',description='';\n```\n8. Check from Grafana that the job `Enrich SA from SA_old` is now `succeded`\n9. After i execute the below steps mannualy from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md)\na. [Upsert to HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\n**Alerts**:\n- IBank_Migration Enrich hbase tables JOB\n- IBank_Migration Enrich hbase tables Impala_insert\n- IBank_Migration Enrich hbase tables Spark\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n> Ndef: If job failed and the following error appears :`ERROR: RetriesExhaustedWithDetailsException: Failed <num> actions: CallTimeoutException: <num> times, servers with issues: [dr/pr]1node02.mno.gr`,  execute script again. The error has to do with HBase merging/spliting on a region server, but a detailed reason is unknown.\nThe script uses upsert and can be safely run many times.\n- For the previous day:\n``` bash\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n```\n- For a specified date:\n``` bash\n# e.g. 09-11-2019\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20191109  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n```\nb. [Duplicates between Impala and Kudu/HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#duplicates-between-impala-and-kuduhbase)\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Check `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n``` bash\n/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_ibank.service_audit_stream prod_trlog_ibank.service_audit_old ibank >> /var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log 2>&1\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230331-SD2180781.md"
        }
    },
    "452": {
        "page_content": "---\ntitle: Hue and Impala Query Failure Due to Missing Group Mapping for User E70529\ndescription: User E70529 was unable to execute queries on Hue or via impala-shell due to a missing group name mapping in the system. The issue was resolved by clearing the SSSD cache and restarting the service across all affected nodes.\ntags:\n  - bigstreamer\n  - hue\n  - impala\n  - sssd\n  - sssd cache\n  - user access\n  - kerberos\n  - hadoop users\n  - group mapping\n  - id command\n  - dcli\n  - impala-shell\n  - hue groups\n  - ldap\n  - mno\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1769421\n  system: MNO BigStreamer\n  user_id: E70529\n  root_cause: Missing group name mapping in SSSD, causing query access failure in Hue and Impala\n  error_signature: \"no group name for gid 871556062\"\n  nodes_affected:\n    - dr1node03\n    - dr1node05\n    - dr1node06\n    - dr1node07\n    - dr1node08\n    - dr1node09\n    - dr1node10\n  action_taken:\n    - Checked group mapping using `id E70529 | grep <gid>`\n    - Cleared SSSD cache using `sss_cache -E` and restarted SSSD\n    - Moved local SSSD DB and restarted service via dcli\n    - Validated group presence after restart\n  outcome: Query access restored across nodes for the user\n---\n# mno - BigStreamer - IM1769421 - User Cannot Execute Queries in Hue or Impala\n## Description\nUser E70529 was unable to execute queries via Hue or impala-shell despite having the correct permissions. The issue started after a group mapping problem occurred\u2014no group was shown for the user's GID in Hue, and attempting to manually create and sync the group in Hue's user management interface resulted in a crash.\n## Actions Taken\n1. Login to dr1node3 as root and check the groups of E70529 user:\n`# id E70529 | grep 871556062`\nYou will notice that the name of the above group is empty.\nLets fix that!\nNdef1: This occurs for dr1node03, dr1node05, dr1node06, dr1node07,dr1node08,dr1node09,dr1node10\nNdef2: Perform the above actions for all nodes\n2. `# sss_cache -E;id E70529 | grep 871556062`\nNow you must be able to see the name of the group. If not, continue with the following steps in order to clear cache and restart sssd with the right name of the group:\n1.  `# dcli -c dr1node03, dr1node05, dr1node06, dr1node07,dr1node08,dr1node09.dr1node10  'mv /var/lib/sss/db/* /tmp;systemctl restart sssd'`\n2.  `# dcli -C 'id E70529 | grep -v \"CMS Way4Manager PROD RDS DevTOOLS\"'`\n3.  `# dcli -C 'id E70529 | grep -v \"CMS Way4Manager PROD RDS DevTOOLS\"' | wc -l`\n## Affected Systems\nmno Bigstreamer",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20211701-IM1769421.md"
        }
    },
    "453": {
        "page_content": "---\ntitle: IBank HBase Batch Job Failure on DR Site Due to Oversized Row Key\ndescription: The IBank_Migration job \"Enrich HBase Visible Tables\" failed due to a row key exceeding HBase\u2019s 32,767-byte limit. A modified query with length filtering was used to rerun the job and successfully complete the HBase upsert.\ntags:\n  - bigstreamer\n  - mno\n  - ibank\n  - hbase\n  - hbase upsert\n  - hbase row key\n  - ibank_visible_trn\n  - length > 32767\n  - batch job\n  - job failure\n  - grafana\n  - dr1edge01\n  - dr1edge02\n  - upsert\n  - daily job\n  - prodrest\n  - hadoop\n  - bash script\n  - spark\n  - ingestion\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1426379\n  system: MNO BigStreamer - IBANK DR Site\n  root_cause: HBase insert failure caused by row key exceeding 32,767-byte limit\n  user_visible_error: \"java.lang.IllegalArgumentException: Row length is > 32767\"\n  action_taken:\n    - Investigated Grafana alert on failed job\n    - Identified HBase key length violation in log output\n    - Isolated faulty row with Impala query using `length(...) > 32767`\n    - Duplicated and modified upsert script to filter out oversized keys\n    - Re-executed job using the new script\n  outcome: Job completed successfully after filtering invalid row; Grafana alert cleared\n---\n# mno - BigStreamer - IM1426379 - 2 Batch Job failed on dr1edge01.mno.gr\n## Description\nThe IBank_Migration batch job failed on the Disaster Recovery site (`dr1edge01.mno.gr`) during the `Enrich HBase Visible Tables` stage due to an HBase row key exceeding the 32,767-byte limit. This triggered an `IllegalArgumentException` and caused the upsert to terminate abnormally. The issue was resolved by filtering out the violating row and rerunning the job with a modified script.\n## Actions Taken\n1. Login to Grafana `https://dr1edge02.mno.gr:3000` with personal account\n2. Inspected `LOCAL MONITORING`/`I-Bank Batch Jobs Overview` \n3. From the diagram stage `Enrich HBase Visible Tables` had failed\n4. According to [Hadoop Trac](http://999.999.999.999/trac/hadoop/wiki/dev/project/mno/support), execution is done with the following command:\n```bash\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n```\n5. Connected  on MobaXterm via ssh to `dr1edge01` and changed user to `PRODREST` using sudo.\n7. Check the logs of failed script.Look for IllegalArgumentException in logs. \n```bash\nless /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log\n...\nWARNING: java.lang.IllegalArgumentException: Row length 34XXX is > 32767\n```\n8. After investigation this error indicates that the key column for the HBase table is greater than the limit, so the insert fails.\n9. Find how many lines are causing this error by executing the `SELECT` part of the query and adding `WHERE length(...)>32767` at the end:\n```sql\nSELECT\n        concat(default.hbase_salt_md5hex(client_username,4,TRUE), chr(0), hex(CAST(CAST(u_timestamp AS DOUBLE) * 1000000 AS bigint)), chr(0), a.id),\n        'true',\n        'true',\n        originate_timestamp,\n        ...\nON a.service_name = b.name\nWHERE b.show_customer=TRUE\n       AND length(concat(default.hbase_salt_md5hex(client_username,4,TRUE), chr(0), hex(CAST(CAST(u_timestamp AS DOUBLE) * 1000000 AS bigint)), chr(0), a.id))>32767;\n```\nThis query returned 1 row. Copy the above query into a file for future reference.\n10. Copy script `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`.\n11. Edit the copy. Append `AND length(concat(default.hbase_salt_md5hex(client_username,4,TRUE), chr(0), hex(CAST(CAST(u_timestamp AS DOUBLE) * 1000000 AS bigint)), chr(0), a.id))<32767` at the end of the query.\n11. Execute the new script. Check the execution is successful.\n```bash\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh.pqr `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n```\n12. In the Grafana charts, check that stage is green now.\n## Affected Systems\nDisaster Site IBANK",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20210127-IM1426379.md"
        }
    },
    "454": {
        "page_content": "---\ntitle: IBank_Ingestion MergeBatch Failure Due to OOM - Recovery via Parallel Execution\ndescription: The IBank_Ingestion MergeBatch Spark job on DR site failed due to Out Of Memory (OOM) error. Job was manually split and re-executed in three time intervals. Postgres monitoring database was updated to reflect job success.\ntags:\n  - mno\n  - bigstreamer\n  - ibank\n  - mergebatch\n  - batch job\n  - spark\n  - oom\n  - yarn\n  - dr1edge01\n  - manual rerun\n  - postgres monitoring\n  - grafana\n  - support script\n  - submitmnoSparkTopology\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1805149\n  system: mno BigStreamer - DR IBank\n  root_cause: Spark job ran out of memory during MergeBatch execution\n  user_visible_error: MergeBatch marked as failed in Grafana monitoring tool\n  resolution_method:\n    - Manually executed job in three parts using known script\n    - Updated Postgres monitoring database to mark job as successful\n    - Used support flow documentation to resume job from the proper pipeline stage\n  action_taken:\n    - Verified failure in YARN UI\n    - Copied and modified batch job master script to skip preprocessed stages\n    - Validated Grafana and logs to confirm success\n  affected_node: dr1edge01.mno.gr\n---\n# mno - BigStreamer - IM1805149 - IBank_Ingetion batch job failed\n## Description\nOn 1/3/2022, the IBank_Ingestion MergeBatch Spark job failed due to an Out Of Memory error. The job was re-executed in three intervals to work around memory constraints. Postgres monitoring tables were manually updated to show job success, and the pipeline was resumed from the appropriate downstream step using a modified master script.\njob_name: MergeBatch\ncomponent: Job\nStatus: Failed\nHost: dr1edge01.mno.gr\n## Actions Taken\n1. Login to `dr1edge01` and open firefox\n2. At the YARN UI search for `PRODREST` and sort by End date. You will find the failed application.\n3. From the UI we saw that Spark exited due to OOM errors.\n4. Using this [document](KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#merge-batch) we executed Merge Batch in 3 steps in parallel:\n```bash\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2022-02-28 00:00:00\" \"2022-02-28 12:00:00\"\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2022-02-28 12:00:00\" \"2022-02-28 18:00:00\"\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2022-02-28 18:00:00\" \"2022-01-03 00:00:00\"\n```\n5. We updated the monitoring postgres database in order for the entry to appear green in Grafana. You can get a success query for Merge batch from the log file of the master script and change the dates. To confirm resolution, verify that Grafana shows no failed MergeBatch jobs for 2022-02-28.\n6. We created a copy of the master script as `PRODREST` at `dr1edge01`.\n7. nside the copied script, we removed the pre-processing steps before the [Distinct join to Service Audit](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit) stage.\n8. We executed the copy script and performed checks as ndefd in the support document.\n## Affected Systems\nDisaster Site IBank Batch\n",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20220103-IM1805149.md"
        }
    },
    "455": {
        "page_content": "---\ntitle: All Hosts Report Critical State in Cloudera Due to NFS Unavailability\ndescription: Cloudera Manager reported all nodes on PR and DR clusters as critical due to NFS unavailability, which prevented the Host Monitor from collecting filesystem metrics; confirmed false alarm as flows ran successfully and issue cleared after freeing up NFS space.\ntags:\n  - mno\n  - bigstreamer\n  - cloudera manager\n  - nfs\n  - host monitor\n  - node metrics\n  - pr1edge01\n  - dr1edge01\n  - grafana\n  - critical state\n  - cloudera-scm-agent\n  - timeout\n  - cluster health\n  - false positive\n  - im2241809\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2241809\n  system: mno BigStreamer PR & DR Sites\n  root_cause: NFS mount point on all nodes became unavailable due to full capacity, preventing Host Monitor from retrieving metrics\n  resolution_summary: NFS space was freed, restoring monitoring functionality; services remained operational throughout the incident\n  affected_nodes:\n    - pr1edge01\n    - dr1edge01\n    - all PR/DR hosts\n  false_alert: true\n---\n# mno - BigStreamer - IM2241809 - Every Host on PR1 and DR1 are in critical state\n## Description\nAll nodes in PR and DR clusters appeared as critical in Cloudera Manager due to NFS becoming unavailable. This prevented the Host Monitor from retrieving node metrics. However, all jobs continued to run correctly and the issue was resolved after the NFS storage was freed.\nAll Hosts in Dr1 and PR1 are in critical state\n## Actions Taken\n### Investigation Steps\n1. Login to `PR` and `DR` cloudera manager in order to check the health of each cluster. The status was unhealthy for all services on both clusters.\n2. Login to `Grafana` in order to check that applications running. All the applications were running without errors.\n3. ssh to `pr1edge01.mno.gr` with personal account\n4. sudo to root\n5. Move to the log folder:\n```bash\ncd /var/log\n```\n6. Check messages file\n```bash\nless messages\n```\nThe output was:\n![image](.media/IM2241809/pr1edge01_messages.png)\n7. From the above output we saw that at `22:13:02 pr1edge01_kernel: nfs: server 999.999.999.999 not responding`.\n8. Now lets check the `agent logs` of an internal node.\n9. ssh to `pr1node03.mno.gr` with personal account\n10. sudo to root\n11. Move to the log folder:\n```bash\ncd /var/log/cloudera-scm-agent\n```\n12. Check `cloudera-scm-agent.log` file\n```bash\nless cloudera-scm-agent.log\n```\nThe output was:\n![image](.media/IM2241809/pr1node03_agent_logs.png)\n13. Due to unavaliability of `nfs storage`(responisibility of the customer to maintain), `Host Monitor` service of Cloudera management services had `timeout` errors because couldn't collect metrics from each filesystem of the nodes.\n14. Customer informed that `nfs storage` caused the issue on both clusters and the unhealthy state of all services was not real because `Host Monitor` was not able to collect metrics in order to be appeared on `CM`. Also all flows ran without errors during the issue.\n15. Customer informed us that the `nfs` storage was full and after their actions it's ok. We checked the `CM` and all the services now is healthy.\n## Root Cause Analysis\nThis problem occurred due to `nfs` unavaliability.\n## Our Ticket Response\n```\nThe issue was caused by the nfs storage used on the nodes of both clusters becoming full. This resulted in the host monitor of the cloudera management services timeouting as it was unable to collect metrics for each filesystem of the nodes.\nRelevant screenshots are attached showing the above causes of the issue.\nThroughout the issue, the flows were up and running as seen in grafana as it was a malfunction of the management services resulting in the incorrect image of all services in Cloudera Manager PR & DR respectively.\nAfter space was freed up on the nfs, both clusters returned to good health.\n```\n## Affected Systems\nDisaster/Primary Site",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20231117-IM2241809.md"
        }
    },
    "456": {
        "page_content": "---\ntitle: DWH_IBANK EXPORT Batch Job Failed Due to Overlapping Execution\ndescription: The DWH_IBANK EXPORT batch job failed because it was manually started before the previous scheduled job completed, causing a conflict in execution on the DR site. The issue was identified via Grafana and YARN logs and resolved by advising proper job sequencing.\ntags:\n  - mno\n  - bigstreamer\n  - ibank\n  - dwh_ibank\n  - batch job\n  - export job\n  - yarn\n  - application_1651064786946_8190\n  - application_1651064786946_8294\n  - grafana\n  - job conflict\n  - job failure\n  - dr site\n  - produser\n  - spark\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1851937\n  system: mno BigStreamer - Disaster Site\n  root_cause: Manual rerun of DWH_IBANK EXPORT job started before the scheduled job completed, causing failure\n  user_visible_error: Batch job failed in Grafana for application DWH_IBANK, component SERVICE_AUDIT\n  detection_method:\n    - Grafana monitoring alert\n    - YARN job history UI\n  action_taken:\n    - Confirmed job failure through YARN UI\n    - Identified overlapping executions between job application_1651064786946_8294 and application_1651064786946_8190\n    - Advised customer to rerun job only after scheduled flow completes\n  outcome: No system-wide impact; job was to be rescheduled manually\n---\n# mno - BigStreamer - IM1851937 - DWH_IBANK batch job failed\n## Description\nOn 5/5/2022, the EXPORT batch job for the DWH_IBANK application failed in the Disaster Recovery (DR) site. The failure was detected through a Grafana alert and was attributed to conflicting executions: a manual run was triggered before the scheduled job had completed.\napplication: DWH_IBANK\njob_name: EXPORT\ncomponent: SERVICE_AUDIT\ndescription: Code 1\n## Actions Taken\n1. Login to grafana to make sure that the alert is about DR SITE\n2. Login to `dr1edge01` and open firefox\n3. At the YARN UI search for `PRODUSER`, sort by End date and search with \"PROD_IBANK_DWH_EXPORT_ServiceAudit. You will find the failed application.\n4. From the UI we noticed that the job with id application_1651064786946_8294 started manually before the completion of the automated job with id application_1651064786946_8190, which led to the failure of the second job.\n5. We informed the client that they should rerun the failed job manually from the scheduler after the completion of the manual step. Also, we pointed out that before proceding with manual actions they should make sure beforehand that all scheduled flows have completed.\nA review of the application_ids on YARN revealed that application_1651064786946_8294 was launched manually before the completion of the automated scheduled job application_1651064786946_8190, resulting in a resource conflict and batch job failure.\nWe advised the client to avoid overlapping manual executions with scheduler runs to prevent such conflicts in the future.\n## Affected Systems\nDisaster Site IBank Batch",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20220504-IM1851937.md"
        }
    },
    "457": {
        "page_content": "---\ntitle: Spark Waiting Batches Alert Due to Resource Contention on MySQL Node (dr1node03)\ndescription: Spark Waiting Batches alerts appeared due to physical user jobs monopolizing resources on dr1node03, where MySQL service is hosted, causing permission denied errors; recommendation was made to disable Impala and YARN roles on this node to prevent contention.\ntags:\n  - mno\n  - bigstreamer\n  - spark\n  - spark waiting batches\n  - impala\n  - yarn\n  - mysql\n  - resource contention\n  - dr1node03\n  - impala daemon\n  - node manager\n  - grafana\n  - alert\n  - performance tuning\n  - cluster optimization\n  - disaster site\n  - im2099957\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2099957\n  system: mno BigStreamer DR Site\n  root_cause: User-initiated jobs on dr1node03 consumed system resources, affecting MySQL and delaying Spark jobs\n  resolution_summary: No action taken directly; recommendation made to disable Impala Daemon and YARN NodeManager on dr1node03 to avoid future contention\n  affected_node: dr1node03\n  recommendation_links:\n    - https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/66\n    - https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/196\n---\n# mno - BigStreamer - IM2099957 - Alert on Grafana\n## Description\nA `[DR][IBANK] Spark Waiting Batches Alert` was triggered in Grafana due to prolonged Spark job delays. The issue stems from physical user workloads (queries, jobs) monopolizing critical system resources\u2014specifically on `dr1node03` where MySQL is hosted. This results in Spark topologies entering a stalled state due to delayed authorization responses from the overloaded MySQL service.\nThe following alert appeared in the grafana system:\n```\n[DR][IBANK] Spark Waiting Batches Alert\n```\n## Actions Taken\n1. The following text has been sent to mno/PM and explains the problem, as well as the recommended actions:\n```text\nSpark Waiting Batches Problem: The first and most important problem we have is the \"Spark Waiting Batches\" which opens a ticket for this monitoring. This is due to physical user actions (queries/jobs) that occupy/bind production resources on the disaster site mainly. This results in there being no resources available for the MySQL process (a central point that the entire cluster has a dependency on), the service that does the authorization is unable to process its data in the database and thus causes a delay in the spark topologies until the \"permission denied\" error is resolved. The spark topologies, while up and running, are unable to process the data, as a result of which they continue to execute after the execution of the jobs that occupied the resources on the server where the MySQL service is located has finished. We do not take any action in this nor can we do anything and mno has asked us to close the ticket directly.\nSuggestion: Disable Impala Daemon and YARN Node Manager on dr1node03.mno.gr, pr1node03.mno.gr where the primary MySQL service is located. This will not affect our cluster workload as, as you will see in the attached screenshots:\n1. impala_mean.png: The average memory occupied by Impala Daemon is much smaller than the limit (150GB) that we have set even in the evening hours when all the flows \"close\" the previous day and the largest load is concentrated on the cluster.\n```\n![impala mean usage](.media/IM2099957/impala_mean.PNG)\n``` text\n2. impala_total.png: The total memory commitment from Impala cumulatively for all nodes is at its peak about 500GB less than the total available, which means that by removing a node there will still be room for resources even with the addition of new flows\n```\n![impala total usage](.media/IM2099957/impala_total.PNG)\n```\n3. yarn.png: throughout the day, the available yarn resources are sufficient even with the removal of more than one node\nThe above applies to both sites and the screenshots are from the Disaster site, where the most resources are reserved compared to the 2 sites. It is important as in this MySQL has a dependency on the entire cluster.\n```\n![yarn usage](.media/IM2099957/yarn.PNG)\n``` text\nBy monitoring the remaining nodes of the 2 clusters, we see how they can manage the workload at CPU levels and with the above proposal we will reduce the CPU levels on critical node03 which anyway uses increased CPU for cluster management processes.\n```\n## Affected Systems\nmno Disaster Site\n## Action Points\n- [Issue 66 \u2013 Resource contention on MySQL node (Disable roles)](https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/66)  \n- [Issue 196 \u2013 Cluster role optimization for Spark performance](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/196)\n",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230307-IM2099957.md"
        }
    },
    "458": {
        "page_content": "---\ntitle: Kerberos Authentication Errors on Way4Streams (RHEL 8) Due to Keytab and Ticket Cache Issues\ndescription: A Kerberos authentication issue occurred on a new Way4Streams installation (RHEL 8) due to invalid ticket cache type and deprecated key encryption (arc4-hmac); resolved by updating krb5.conf and enabling weak crypto support.\ntags:\n\n* mno\n* bigstreamer\n* kerberos\n* keytab\n* kinit\n* klist\n* arc4-hmac\n* openjdk\n* rhel8\n* krb5.conf\n* allow\\_weak\\_crypto\n* ticket cache\n* kcm\n* file-based ticket cache\n* way4streams\n* authentication failure\n* deprecated encryption\n* sssd-kcm\n* java kerberos\n* teams call\n* way4\n  last\\_updated: 2025-05-01\n  author: ilpap\n  context:\n  issue\\_id: way4streams-venia\n  system: Way4Streams QA (non-jkl supported)\n  root\\_cause: Keytab used deprecated arc4-hmac encryption and RHEL 8 defaulted to KCM ticket cache, which is incompatible\n  resolution\\_summary: Forced use of FILE ticket cache, removed sssd-kcm, and enabled weak crypto in krb5.conf to support legacy keytab\n  operating\\_system: RHEL 8\n  java\\_version: OpenJDK 11\n  kerberos\\_principal: [DEVUSER@BANK.CENTRAL.mno.GR](mailto:DEVUSER@BANK.CENTRAL.mno.GR)\n---\n# mno - BigStreamer - way4streams-venia - Kerberos Authentication Errors on new Way4Streams installation\n## Description\nKerberos authentication failed during setup of Way4Streams on a new RHEL 8 server. The main issue was that the provided keytab used deprecated `arc4-hmac` encryption, and the OS defaulted to using an incompatible KCM ticket cache. This blocked application authentication with error messages indicating missing keys. The problem was resolved by forcing a file-based ticket cache, removing the `sssd-kcm` service, and enabling support for weak crypto in `/etc/krb5.conf`.\n```\n/way4/DEVUSER.keytab does not contain any keys for DEVUSER@BANK.CENTRAL.mno.GR\n```\n## Actions Taken\n1. The new server hosting the application is RHEL 8 instead of Solaris. We tried to manually `kinit`\nFrom the server with `way4`:\n``` bash\nkinit DEVUSER@BANK.CENTRAL.mno.GR -kt /way4/DEVUSER.keytab\n```\nOutput:\n```bash\nTicket cache: KCM:1500\nDefault principal: DEVUSER@BANK.CENTRAL.mno.GR\nValid starting       Expires              Service principal\n15/03/2023 12:35:29  16/03/2023 12:35:29  krbtgt/BANK.CENTRAL.mno.GR@BANK.CENTRAL.mno.GR\nrenew until 22/03/2023 12:35:29\n```\n2. **Anything** but `FILE` ticket caches is sure to create a problem.\nFrom the server with `root`:\n``` bash\nvi /etc/krb5.conf\n```\nChange the following under `libdefaults` section:\n``` conf\ndefault_ccache_name = FILE:/tmp/krb5cc_%{uid}\n```\nAlso, remove `sssd-kcm`:\n```bash\nyum remove sssd-kcm\n```\n3. After that the klist output used a `FILE` cache, but the problem persisted.\nSince the OS problems were resolved we focused the keytab.\nFrom the server with `way4`\n``` bash\nklist -kte /way4/DEVUSER.keytab\n```\nOutput:\n```\nKeytab name: FILE:/way4/DEVUSER.keytab\nKVNO Timestamp           Principal\n---- ------------------- ------------------------------------------------------\n  0 01/01/1970 00:00:00 DEVUSER@BANK.CENTRAL.mno.GR (DEPRECATED:arc4-hmac) \n```\nThat DEPRECATED flag is not a good sign. \n4. Searching for `rc4-hmac` and `OpenJDK11` we stumbled upon this link https://bugs.openjdk.org/browse/JDK-8262273\nFrom the server with `root`:\n``` bash\nvi /etc/krb5.conf\n```\nAdd the following under `libdefaults` section:\n``` conf\nallow_weak_crypto = true\n```\nAfter enabling weak crypto and restarting authentication, `kinit` and application login worked correctly. The Kerberos authentication problem was resolved.\n## Affected Systems\nWay4Streams QA (Not supported by jkl)\n## Troubleshooting Keywords\narc4-hmac, deprecated keytab, OpenJDK 11, Kerberos ticket cache, RHEL 8, authentication error, weak crypto, DEVUSER, krb5.conf, KCM, sssd-kcm, kinit fails, way4streams",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230313-way4streams-venia.md"
        }
    },
    "459": {
        "page_content": "---\ntitle: Online MergeBatch Failure Due to Kudu Timeout and Excessive Partition Count\ndescription: The Online_Ingestion MergeBatch job failed due to a Kudu timeout caused by excessive partitions (468 instead of 180) on the prod_trlog_online.service_audit_stream table; resolved after partition correction and re-execution.\ntags:\n  - mno\n  - bigstreamer\n  - grafana\n  - online ingestion\n  - mergebatch\n  - kudu\n  - spark\n  - spark ui\n  - spark partitions\n  - timeout\n  - stage 0\n  - org.apache.kudu.client.NonRecoverableException\n  - prod_trlog_online.service_audit_stream\n  - impala partitioning\n  - pr1edge01\n  - im2193241\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2193241\n  system: mno BigStreamer PR Site\n  root_cause: MergeBatch Spark job failed due to high partition count (468) and Kudu timeout on service_audit_stream\n  resolution_summary: Re-executed flow after reducing table partitions to expected count (180); resolved without job failures\n  affected_table: prod_trlog_online.service_audit_stream\n  affected_node: pr1edge01.mno.gr\n  fix_applied_by: development team\n---\n# mno - BigStreamer - IM2193241 - Failed job in Grafana\n## Description\nThe MergeBatch job in Online_Ingestion failed due to a `Kudu NonRecoverableException`, caused by an unexpectedly high number of partitions (468 instead of 180) in the Spark job. The development team corrected the partitioning in `prod_trlog_online.service_audit_stream`, and the job succeeded on re-execution.\nThe following failed job appeared in Grafana today 26/07:\n```\nApplication: Online_Ingestion\nJob_name: MergeBatch\nComponment: JOB\nDate: 25-07-2023\nHost: pr1edge01.mno.gr\n```\n## Actions Taken\nThe job failed with a timeout while querying Kudu due to an abnormally high number of partitions in Stage 0 of the Spark job.\n1. Re-run the failed step as described [here](../supportDocuments/applicationFlows/online.md#batch)\n2. The flow completed successfully, we proceeded with the investigation\nLogs from the application:\n```\nCaused by: org.apache.kudu.client.NonRecoverableException: cannot complete before timeout: ScanRequest(scannerId=\"22c757bfcf674a05a08f14c316e745e9\", tablet=c42b07f18435403297fee37add478c0b, attempt=1, KuduRpc(method=Scan, tablet=c42b07f18435403297fee37add478c0b, attempt=1, TimeoutTracker(timeout=30000, elapsed=30004), Trace Summary(0 ms): Sent(1), Received(0), Delayed(0), MasterRefresh(0), AuthRefresh(0), Truncated: false \n```\nSpark UI:\n![Spark UI](.media/IM2193241_1.png)\n3. Stage 0 should have 180 partitions not 468\n![Spark UI normal](.media/IM2193241_2.png)\n4. Informed development team to correct the number of partitions for `prod_trlog_online.service_audit_stream`. This deleted unnecessary data from Kudu's disks and next run (see 3) did not have any failed tasks.\n## Affected Systems\nmno Primary Site",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230726-IM2193241.md"
        }
    },
    "460": {
        "page_content": "---\ntitle: IBank MergeBatch Job Failed Due to HBase Quotas and Impala Resource Exhaustion\ndescription: The MergeBatch job for IBank_Ingestion failed due to Impala daemon resource exhaustion triggered by HBase namespace quotas and large data volume; workaround involved disabling quotas and rerunning in distributed mode.\ntags:\n  - mno\n  - bigstreamer\n  - ibank\n  - mergebatch\n  - batch job\n  - impala\n  - hbase\n  - quotas\n  - pr1node04\n  - spark\n  - yarn\n  - resource exhaustion\n  - visible table\n  - service audit\n  - impala daemon\n  - impala concurrency\n  - code 6\n  - IM2095966\n  - impala parallelism\n  - spark-submit\n  - job retry\n  - prodrest\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2095966\n  system: mno BigStreamer PR Site\n  root_cause: HBase write quotas limited Impala to a single daemon, which crashed under end-of-month load, causing Impala connection pool exhaustion and failure of the MergeBatch job\n  resolution_summary: MergeBatch was rerun in 3 time windows after disabling quotas in the PROD_IBANK namespace to allow full Impala daemon usage\n  workaround: quota removal and distributed rerun\n  affected_nodes:\n    - pr1node04\n    - pr1edge01\n  impacted_component: IBank_Ingestion MergeBatch (Spark + Impala + HBase)\n  customer_impact: night flows disrupted, visible table update delayed\n---\n# mno - BigStreamer - IM2095966 - Failed Batch Job on Grafana\n## Description\nThe MergeBatch job for the IBank_Ingestion pipeline failed on 28/02/2023 on the PR Site with a Grafana alert. The failure occurred during the `JOB` component execution and was traced to Impala resource exhaustion caused by HBase write quotas and high data volume at end-of-month.\nThe following failed batch job appeared in the Grafana system:\n```\napplication :  IBank_Ingestion\njob_name : MergeBatch\ncomponent : JOB\ndate : 28-02-2023\nstatus : FAILED\ndescription :\nhost : pr1edge01.mno.gr\n```\n## Actions Taken\n1. We identified the failed step using the alarm name. Steps `MSSQL Sqoop Import (Migration)` and `Insert to Service Audit` had been executed successfully. We rerun the `Merge Batch` according to [this](../supportDocuments/applicationFlows/ibank.md#merge-batch).\n2. The job had not completed at approximately 9.pm on 01/03/2023 we terminated the job after communication with the customer in order for the night flow to run without any problems. We scheduled to rerun the job in the following day after the completion of the daily MergeBatch.\n3. On 02/03/2023 we reran the job in 3 patches \n```bash\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2023-02-28 00:00:00\" \"2023-02-28 12:00:00\"\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2023-02-28 12:00:00\" \"2023-02-28 18:00:00\"\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2023-02-28 18:00:00\" \"2023-03-01 00:00:00\"\n```\n4. The `Upsert to HBase` stage that synchronises the `Visible` table caused an Impala problem during which Impala stopped to process this job as well as other requests.\n5. The problem is described below.\n## Our Ticket Response\n```\n03/03/23 11:17:49 Europe/Eastern (POULAS GIORGOS):\nAfter investigating yesterday's Impala issue, we found the following:\nDue to HBase quotas set in the PROD_IBANK namespace, we have limited the parallelism in the Impala query to run on an Impala daemon.\nThe daemon that ran the query to enrich the Service Audit Visible (pr1node04) encountered a problem as it did not have the resources required to process the large volume of records we had at the end of the month, while at the same time accepting requests from the REST APIs of the live streams.\nAs a result of the above, the queries from the live systems were not completing and accumulating, exhausting the available connections that Impala can accept. The malfunction of the live streams is also the problem you observed last night.\nWe propose as a workaround today after 9pm. disable quotas in the PROD_IBANK namespace and rerun the script without the single node limitation, so that the load is shared across all 9 available Impala daemons. We will then examine the alternatives for modifying the flow and re-enabling quotas.\nThere is no downtime required for the above actions.\nG. Poulas\n03/03/23 00:53:26 Europe/Eastern (MASTROKOSTA MARIA):\nThe service audit has been filled on the PR Site. The job that fills the visible table is pending as it was canceled in the context of ticket SD2159021.\n02/03/23 15:56:38 Europe/Eastern (MASTROKOSTA MARIA):\nThe execution on both sites started after the scheduled execution of the Merge Batch for 01/03/2023, which has been completed without a problem. At this time, the DR has processed until 18:00, while the PR has processed the data until 12:00. The executions on both sites are being monitored so that they can be resubmitted in case of a problem.\n01/03/23 21:11:19 Europe/Eastern (MASTROKOSTA MARIA):\nFollowing our telephone communication, the job has been stopped and will be re-executed tomorrow in order to avoid problems with the evening streams.\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230301-IM2095966.md"
        }
    },
    "461": {
        "page_content": "---\ntitle: HDFS Block Count Alert on dr1node09 Due to service_audit_old Table Fragmentation\ndescription: An HDFS alert was triggered on the Disaster Site due to an excessive number of blocks on dr1node09 caused by unmanaged historical partitions in the prod_trlog_ibank.service_audit_old table. The issue was mitigated by dropping old partitions manually and recommending a retention policy.\ntags:\n  - bigstreamer\n  - hdfs\n  - cloudera manager\n  - block count\n  - dr1node09\n  - datanode alert\n  - impala\n  - service_audit_old\n  - retention\n  - purge partitions\n  - grafana monitoring\n  - hive partitioning\n  - memory pressure\n  - performance issue\n  - mno\n  - dr site\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1681883\n  system: MNO BigStreamer DR Site\n  root_cause: Excessive historical data in Hive table caused high block count and memory issues on dr1node09\n  user_visible_error: Cloudera Manager HDFS Datanode Data Directory Status alert\n  action_taken:\n    - Verified datanode imbalance in Namenode UI\n    - Identified service_audit_old table as root cause\n    - Proposed and executed manual partition purge\n    - Coordinated with monitoring and customer teams\n  outcome: Alert cleared after dropping old partitions; retention mechanism recommended\n---\n# mno - BigStreamer - IM1681883 - hdfs issue on cloudera manager\n## Description\nOn October 20th, 2021, an HDFS \"Datanode Data Directory Status\" alert appeared in Cloudera Manager for dr1node09 on the Disaster Recovery site. Investigation showed that the alert was caused by excessive block count due to the absence of a retention policy for the `prod_trlog_ibank.service_audit_old` table in Hive. Manual purging of old partitions was performed to mitigate the issue.\n## Actions Taken\n1. Login to Cloudera Manager in DR and check the alarm\n2. It was a block count alarm on `dr1node09`\n3. Login to `dr1edge01` with your personal account and execute `firefox` to view the Namenode UI.\n4. Go to `https://dr1node02.mno.gr:50470` and from there in the tab `Datanodes`. Order the datanodes using the block count by desceding order to overview the situation.\n5. Change user to PRODREST and use HDFS command or Impala query to check how many partitions exist in `prod_trlog_ibank.service_audit_old`. This is a big table that has no retention mechanism yet so its data are stored in many blocks. HDFS command is `hdfs dfs -ls /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit_old` and Impala query is `show partitions prod_trlog_ibank.service_audit_old`. In either case observe how many `par_dt` directories exist.\n6. In our case there were at least 6 months in there so we informed mno. Sample customer notification:\n```text\nRegarding ticket SD1734269, the error shown in Cloudera Manager refers to a block count threshold exceeded on dr1node09 and is due to the increased number of blocks stored on the new nodes dr1node07-10. The impact on the datanode lies in the performance as the metadata for the blocks it has in memory does not fit.\nFrom the investigation we did in the HDFS data, we saw that there are many files in the prod_trlog_ibank.service_audit_old table. Specifically, there is history in it from 01/03.\nBy keeping only recent data through a retention mechanism, the total block count in HDFS and on each datanode individually will be reduced.\nAs an immediate solution, we suggest deleting data older than 40 days, as has been done in the past. Consequently, it will not be possible to investigate problems that occurred older than 40 days.\nPlease let us know if you agree with the above and when you would like us to proceed with the immediate solution.\n```\nRecipients are `ZEVGAROPOULOU.GEORGIA@mno.gr,krekoukias.konst@mno.gr,papakostas.athanasios@mno.gr`. Add our team and kbikos in CC.\n5. When mno agreed to the immediate action, we informed the monitoring team to ignore alarms in Cloudera Manager of DR site regarding HDFS, Hive, Impala and YARN. Their email address is `csocmonitoringops@jkl-telecom.com`. Example mail:\n```\nWe will proceed with actions on the National Bank of Greece Disaster Site infrastructure.\nDuring this time, you will probably have alarms in the Cloudera Manager UI related mainly to the Health of HDFS, Hive, Impala, Yarn.\nThe work will start at 16:40 today. We will inform you after the work is completed.\n```\n8. When the time comes, login to Grafana and check charts of topologies running in DR. Login to Cloudera Manager and check throughout the process the health status of Cloudera services. Ndef that `Hive Metastore Canary` alerts will pop up as expected.\n9. Login to dr1edge01 with your personal account and change user to `PRODREST`.\n10. Open an impala shell on DR:`impala-shell -i pr1edge.mno.gr -k --ssl`.\n11. Change database to `prod_trlog_ibank` with `use prod_trlog_ibank;` and `show tables;` to check that `service_audit_old` is here.\n12. Drop partitions. Seperate the amount of days you have to delete in batches of 15 days in order to avoid disruptions of applications. `ALTER TABLE service_audit_old DROP IF EXISTS PARTITION(par_dt<=20200715) PURGE;`\n12. When the query is done, execute the next one and so on: `ALTER TABLE service_audit_old DROP IF EXISTS PARTITION(par_dt<=20200801) PURGE;`\n13. Can't stress this enough. **Throughout steps 12, 13 check Grafana and Cloudera Manager**.\n14. When done, inform mno and the monitoring team that actions have been successfully finished.\n## Affected Systems\nmno Bigstreamer\n## Action Points\nDevelop retention mechanism to automatically drop old partitions.",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20211021-IM1681883.md"
        }
    },
    "462": {
        "page_content": "---\ntitle: HiveServer2 OutOfMemory Crash on dr1node04 Due to Large Queries on pmnt_response_stg_0\ndescription: HiveServer2 on dr1node04 crashed with OutOfMemory errors after executing multiple large queries on dev_trlog_card.pmnt_response_stg_0; identified via JVM heap/GC logs and Cloudera monitoring charts, then resolved by service restart.\ntags:\n  - mno\n  - bigstreamer\n  - hiveserver2\n  - dr1node04\n  - outofmemory\n  - java heap space\n  - garbage collector\n  - pause duration\n  - cloudera manager\n  - jvm heap\n  - gc logs\n  - dev_trlog_card\n  - pmnt_response_stg_0\n  - query failure\n  - yarn\n  - application timeout\n  - hive crash\n  - im2024442\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2024442\n  system: mno BigStreamer DR1 HiveServer2\n  root_cause: Multiple concurrent heavy queries caused Java heap saturation in HiveServer2, leading to long GC pauses and service crash\n  resolution_summary: HiveServer2 on dr1node04 was restarted and returned to normal operation; queries from E30825 and E36254 on pmnt_response_stg_0 table identified as root cause\n  affected_component: HiveServer2\n  crashing_queries:\n    - application_1665578283516_50081\n    - application_1665578283516_50084\n    - application_1665578283516_50085\n    - application_1665578283516_50088\n    - application_1665578283516_50089\n    - application_1665578283516_50090\n    - application_1665578283516_50095\n    - application_1665578283516_50096\n  affected_table: dev_trlog_card.pmnt_response_stg_0\n---\n# mno - BigStreamer - IM2024442 - Critical alarm in Cloudera Application - dr1edge01\n## Description\nA critical alarm appeared in Cloudera Application in dr1edge01.\nHive --> HiveServer2 (dr1node04) // Pause Duration.\n## Actions Taken\n1. Check HiveServer2 JVM Heap Memory Usage and JVM Pause Time Charts from Cloudera Manager.\n```bash\ncluster -> Hive -> HiveServer2 -> Charts\n```\n2. Restart HiveServer2 Instance if needed (workaround).\n``` bash\nIn our case the service had Unexpected Exits due to OutOfMemory. \n```\n3. Search for \"Java Heap Space\" failed Jobs in HiveServer2 Service Logs.\n```bash\ngrep Java /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node 04.mno-gr.log.out\n```\nExample Output:\n![hiveServer2Logs.PNG](.media/hiveServer2Logs.PNG)\n4. Check failed Yarn Applications from Cloudera Manager that match those of the previous step.\n```bash\nCluster -> Yarn -> Applications -> Filter: \"application_type = MAPREDUCE\"\n```\n5. Search for GC Pause Duration in HiveServer2 Service Logs and make sure that the warnings started after the submission of the failed jobs.\n```bash\ngrep GC /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node 04.mno-gr.log.out\n```\n6. Compare the timestamps of all the above to be sure that you have found the queries that caused the problem.\n## Our Ticket Response\n```\nHiveServer2 of dr1node04 is back up. Services and flows have been checked and there is no problem at this time.\n```\n```\nThe following findings emerged from the analysis:\nHiveServer2 of dr1node04 crashed from OutOfMemory, because the Java Heap Space was full.\nThe Pause Duration messages in Cloudera Manager are related to the Garbage Collector delay.\nSpecifically, from the analysis of the logs we saw that between 14:19 and 15:24, HiveServer2 of dr1node04 was called to manage 8 Queries which crashed with a Java Heap Space error. The GC started throwing warnings from 15:08, as it could not clean the memory of the above. The service crashed with an OutOfMemory error, restarted and returned to normal operation.\nBelow are details for the specific queries:\n14:19 application_1665578283516_50081 user:E30825\n14:25 application_1665578283516_50084 user:E30825\n14:29 application_1665578283516_50085 user:E30825\n14:32 application_1665578283516_50088 user:E30825\n14:37 application_1665578283516_50089 user:E30825\n14:41 application_1665578283516_50090 user:E30825\n15:23 application_1665578283516_50095 user:E36254\n15:24 application_1665578283516_50096 user:E36254\nAll queries are for the table: dev_trlog_card.pmnt_response_stg_0.\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20221119-IM2024442.md"
        }
    },
    "463": {
        "page_content": "---\ntitle: [DR][IBANK] Internet Banking DW Weekend Extraction Partition Missing\ndescription: Investigation into missing partitions in the `prod_trlog_ibank_analytical` database on the disaster recovery (DR) site, where weekend extraction jobs showed SUCCESS status but last inserted partition was outdated. Root cause resolved by refreshing the table before executing the Spark job.\ntags:\n  - bigstreamer\n  - mno\n  - ibank\n  - internet banking\n  - dr site\n  - data warehouse\n  - impala\n  - partitions\n  - prod_trlog_ibank_analytical\n  - spark\n  - monitoring\n  - batch jobs\n  - par_dt\n  - flow health\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1332456\n  system: MNO BigStreamer IBANK DR Site\n  root_cause: Partition data was missing because the table had not been refreshed before running the Spark flow, even though the monitoring table reported success\n  user_visible_symptom: Latest partition in `prod_trlog_ibank_analytical` was 20201023 instead of 20201025\n  flow_status: SUCCESS in monitoring dashboard\n  action_taken:\n    - Verified Spark topology health\n    - Checked partitions in Impala\n    - Validated job status in Grafana (LOCAL MONITOR/Batch Jobs DR)\n    - Coordinated with customer to refresh the table before job execution\n  outcome: Table was refreshed and issue resolved\n---\n# mno - BigStreamer - IM1332456 - [DR][IBANK] Internet Banking Data Warehouse - Weekend extraction jobs\n## Description\nA discrepancy was found in the `prod_trlog_ibank_analytical` database on the DR site: the last available partition was `20201023`, while it should have included `20201025`. Despite this, the weekend extraction flows appeared as `SUCCESS` in the Grafana monitoring table, leading to confusion about actual data completeness.\n## Actions Taken\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account\n2. Inspected `LOCAL MONITOR/Batch Jobs DR`\n3. DW JOBS `Check that all is SUCCESS`\n4. Open an SSH session to `dr1edge01.mno.gr` using your personal account.\n5. Connect to Impala:\n```bash\nimpala-shell -i dr1edge.mno.gr -k --ssl\n```\n6. Execute the query `select count(*),par_dt from service_audit where par_dt > 20200919 group by 2 order by 2;`\n7. Check that par_dt has inserted data\n8. After the above checking procedure, customer informed to refresh the above table before execute a spark/flow and that the spark topology was healthy.\n## Affected Systems\nDisaster Site IBANK query",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20201026-IM1332456.md"
        }
    },
    "464": {
        "page_content": "---\ntitle: HDFS Data Directory Status Alert on dr1node02 Due to Failing Disk\ndescription: Cloudera Manager triggered a Data Directory Status alert on dr1node02 caused by a failing disk; resolved through disk replacement, RAID reconfiguration, and validation using bdadiskutility and MegaCli64.\ntags:\n  - mno\n  - bigstreamer\n  - cloudera\n  - hdfs\n  - datanode\n  - disk replacement\n  - data directory status\n  - dr1node02\n  - bdadiskutility\n  - megacli\n  - raid\n  - nfs\n  - oracle support\n  - dr site\n  - disk mirroring\n  - sd2389640\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD2389640\n  system: mno BigStreamer DR Site\n  affected_node: dr1node02\n  root_cause: Data Directory Status alert caused by failing disk in slots s1 and s7 on dr1node02\n  resolution_summary: Disks were replaced online with no downtime using bdadiskutility and MegaCli64 to clear cache, reconfigure RAID, and validate partitions; DataNode role restarted successfully\n  maintenance_window_required: false\n  impact: No downtime\n---\n# mno - BigStreamer - SD2389640 - hdfs - Data Directory Status\n## Description\nAn HDFS Data Directory Status alert on dr1node02 was triggered due to a failing disk in slots s1 and s7. The issue was resolved through coordinated disk replacement, cache clearing, and RAID reinitialization using MegaCli64.\nThe following alert has appeared in Cloudera Manager (DR):\n```\nDataNode (dr1node02)\nData Directory Status\n```\n## Actions Taken\nThere are references from the similar issue [20220620-SD1951890.md](20220620-SD1951890.md).\nAfter investigation we saw that the problem occurred due to disk issue on dr1node02.\nWe communicated with Oracle and disk replacement was scheduled.\n> Ndef: that disk replacement perfomerd online so there was no downtime.\nWe followed the steps as described at [20220620-SD1951890.md](20220620-SD1951890.md) and [sync_mysql.md](sync_mysql.md), which include the following:\n1. Stopping the processes that specifically run at the disk slots `s1` and `s7` of the server `dr1node02`. On our case was the hdfs datanode and some yarn applications. We identified them with:\n2. Stopping the mysql slaves using the command:\n```bash\nmysql -u root -p\nSHOW SLAVE STATUS\\G;\n```\n3. Ensuring that the no processes are running at the partitions with the following commands:\n```bash\nlsof /u02\n```\n```bash\nlsof /u08\n```\n4. Unmounting the two partitions, so the disks can be replaced.\n```bash\numount <mountpoint>\n```\n5. Once the disks have been replaced we ran the following command for both partitions:\n```bash\nbdadiskutility /u02\n```\n6. After running the command, we got the following error:\n```\nVirtual Drive <VIRTUAL_DRIVE_NUMBER> is incorrectly mapped.\n<TIMESTAMP> : Error executing 'MegaCli64 CfgLdAdd r0[<ENCLOSURE>:<SLOT>] a0'\n<TIMESTAMP> : Error code is 84 . Response is <<\nAdapter 0: Configure Adapter Failed\nFW error description:\nThe current operation is not allowed because the controller has data in cache for offline or missing virtual disks.\nExit Code: 0x54>>\nFound a disk with a Firmware State of Unconfigured(good).\nSuccessfully cleared the cache for the logical drive.\nSuccessfully added the disk to its own RAID(0) volume.\n```\n7. After communicating with Oracle Support [SR 3-36895603206 : Wrong disk status after replacement](https://support.oracle.com/epmos/faces/SrDetail?_afrLoop=206254157461870&srNumber=3-36895603206&queryModeName=Technical&needSrDetailRefresh=true&_afrvwxowMode=0&_adf.ctrl-state=iwvcvrye_184), we ran the following commands to solve the issue:\n- `For s1 # The disk slot 1 of the server that corresponds to mount point /u02`\n- `For s7 # The disk slot 7 of the server that corresponds to mount point /u08`\n- Validated if there is a cache pinned for any device, running command:\n```bash\nMegaCli64 -GetPreservedCacheList -a0 \n```\nIf the old disk has pinned the cache, the command will return something like:\n```\nAdapter #0\nVirtual Drive(Target ID 07): Missing.\nExit Code: 0x00\n```\n- In this case, the disk in slot 7 had the pinned cache and had to clear.\nRemove the pinned cache by running command:\n```bash\n#MegaCli64 -DiscardPreservedCache -L7 -force -a0 <<<< where -LX should be replaced by the Target ID number reported in previous step.\n```\nGet the `ENCLOSURE_NUMBER`\n```bash\nMegaCli64 LdPdInfo a0 | more\n```\n- Added the virtual disk back\n```bash\nMegaCli64 CfgLdAdd r0[ENCLOSURE_NUMBER:slot] a0\n```\nOn our case was:\nFor `s1`\n```bash\nMegaCli64 CfgLdAdd r0[252:1] a0\n```\nFor `s7`\n```bash\nMegaCli64 CfgLdAdd r0[252:7] a0\n```\nStarted configuring the disk at `slot1`\n```bash\nbdadiskutility -f /u02\n```\nWait until the mirroring is finished and after that.\nStarted configuring the disk at `slot7`\n```bash\nbdadiskutility -f /u08\n```\n- Checks:\nFor `s1`:\n```bash\nparted /dev/disk/by-hba-slot/s1 -s unit chs print\niscsi # Check that all disks appeared\nlsblk # Check that all disks appeared\n```\nFor `s7`:\n```bash\nparted /dev/disk/by-hba-slot/s7 -s unit chs print\niscsi # Check that all disks appeared\nlsblk # Check that all disks appeared\n```\n8. We proceed with the start of the `datanode` role of `dr1node02`",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20240306-IM2323192.md"
        }
    },
    "465": {
        "page_content": "---\ntitle: Spark History Server on DR Site Crashed Due to OutOfMemoryError\ndescription: The Spark on YARN History Server on dr1node03 (DR site) exited unexpectedly due to a Java heap OutOfMemoryError; resolved by increasing heap size from 512MB to 2GB to match PR Site configuration and restarting the role.\ntags:\n  - mno\n  - bigstreamer\n  - spark\n  - yarn\n  - history server\n  - java heap size\n  - outofmemory\n  - dr1node03\n  - dr site\n  - cloudera\n  - service restart\n  - role config\n  - unexpected exit\n  - IM2098517\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2098517\n  system: mno BigStreamer DR Site\n  root_cause: Spark History Server on DR crashed due to insufficient Java heap size (512MB), resulting in OutOfMemoryError\n  resolution_summary: Increased heap size to 2GB to align with PR configuration and restarted the role successfully\n  affected_host: dr1node03\n  cloudera_service: Spark on YARN\n---\n# mno - BigStreamer - IM2098517 - Health issue on dr1edge01\n## Description\nOn 07/03/2023, Cloudera Manager reported an unexpected exit for the Spark on YARN History Server running on `dr1node03` in the Disaster Recovery (DR) site. The crash was caused by an `OutOfMemoryError`, due to the default Java heap size being set too low (512MB).\nThe following health issue has occurred in the cloudera manager system:\n```\nSpark on yarn - History Server (dr1node03) - Unexpected Exits\n```\n## Actions Taken\n### Investigation\n1. Login to Cloudera for DR Site\n2. We inspected logs for this role for the time that the problem arose: `Cloudera > Diagnostics > Logs` and chose `Service: Spark on Yarn` and `Role: History Server`. We could not identify the root cause by these logs\n3. ssh to dr1node03 as root, went to `/var/run/process` , and inspected logs from the process that ran at the time of the problem and found out that the process with pid 51291 was killed while a `OutOfMemoryError` occured\n![terminal_screenshot](.media/IM2098517/spark_on_yarn.png)\n### Resolution\n4. We checked the  `java heap size` of the History Server through Cloudera UI configuration tab. It was set to 512M.\n5. We checked the respective option for the PR Site and it was set to 2G\n6. We set the `java heap size` of the History Server to 2G at the DR Site\n7. We proceeded to restart of the role after communication with the customer\n## Our Ticket Response\n```\n07/03/23 16:28:55 Europe/Eastern (MASTROKOSTA MARIA):\nWe have restarted the History Server after a phone call. There was no problem during the restart.\nThank you\n07/03/23 16:21:21 Europe/Eastern (MASTROKOSTA MARIA):\nThe exit occurred due to an out of memory error. We have changed the java heap size of the History Server from 512MB to 2GB as in the PR. We will need to restart the role. There will be no outage.\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230305-IM2098517.md"
        }
    },
    "466": {
        "page_content": "---\ntitle: HDFS Data Directory Status Alert on dr1node02 Due to Failing Disk\ndescription: Cloudera Manager triggered a Data Directory Status alert on dr1node02 caused by a failing disk; resolved through disk replacement, RAID reconfiguration, and validation using bdadiskutility and MegaCli64.\ntags:\n  - mno\n  - bigstreamer\n  - cloudera\n  - hdfs\n  - datanode\n  - disk replacement\n  - data directory status\n  - dr1node02\n  - bdadiskutility\n  - megacli\n  - raid\n  - nfs\n  - oracle support\n  - dr site\n  - disk mirroring\n  - sd2389640\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD2389640\n  system: mno BigStreamer DR Site\n  affected_node: dr1node02\n  root_cause: Data Directory Status alert caused by failing disk in slots s1 and s7 on dr1node02\n  resolution_summary: Disks were replaced online with no downtime using bdadiskutility and MegaCli64 to clear cache, reconfigure RAID, and validate partitions; DataNode role restarted successfully\n  maintenance_window_required: false\n  impact: No downtime\n---\n# mno - BigStreamer - IM1908793 - Error on creating kudu table\n## Description\nAn HDFS Data Directory Status alert on dr1node02 was triggered due to a failing disk in slots s1 and s7. The issue was resolved through coordinated disk replacement, cache clearing, and RAID reinitialization using MegaCli64.\nError while creating a temporary kudu table:\n```\nERROR: ImpalaRuntimeException: Error creating Kudu table 'prod_trlog_card_analytical.opticash_dispencing_atm_tmp'\nCAUSED BY: NonRecoverableException: failed to wait for Hive Metastore notification log listener to catch up: failed to retrieve notification log events: failed to get Hive Metastore next notification: Thrift SASL frame is too long: 338.01M/100.00M\n```\n## Actions Taken\n**Steps in order to investigate and make sure that the table is not created**\n1. Login to `dr1edge01.mno.gr` with personal account and then to `dr1node01.mno.gr`\n2. Move to the process folder:\n```bash\ncd /var/run/cloudera-scm-agent/process/\n```\n3. Find the latest process and go to that folder. In our case is 12200-kudu-KUDU_TSERVER. So move to that folder:\n```bash\ncd 12200-kudu-KUDU_TSERVER\n```\n4. Use the keytab you just found in that folder:\n```bash\nkinit -kt kudu.keytab kudu/`hostname`\n```\n5. Check kudu cluster health and specifically for `prod_trlog_card_analytical` database in order to check if the wanted table is created.\n```bash\nkudu cluster ksck dr1node04.mno.gr dr1node05.mno.gr dr1node06.mno.gr | grep -i prod_trlog_card_analytical\n```\nAs you can see the `prod_trlog_card_analytical.opticash_dispencing_atm_tmp` table is not created.\n**Optional**: You can also verify that from impala-shell running the following commands:\n- Login to `dr1edge01.mno.gr` with personal account\n- impala-shell -i dr1edge01 -k --ssl\n- `[dr1edge01.mno.gr:21000] default> use prod_trlog_card_analytical;`\n- `[dr1edge01.mno.gr:21000] default> show tables;`\nAs you can see the `prod_trlog_card_analytical.opticash_dispencing_atm_tmp` table is not created.\n6. Login to CM DR with your pesonal account > Go to impala > Queries\n7. In the search bar type the following in order to find the query:\n`STATEMENT RLIKE '.*prod_trlog_card_analytical.opticash_dispencing_atm_tmp'.*` and click on the query details for investigation.\nWe found that the query they try to run is the following:\n```bash\nCREATE TABLE IF NOT EXISTS prod_trog_card analytical.opticash ispencing_atm_tmp\ncashp id, STRING NOT NULL\n, transaction date STRING NOT NULL\n,denom id STRING\n, cassette STRING\n, crncy id STRING\n, open Bal BIGINT\n, norm del BIGINT\n, norm rtr BIGINT\n, unpl_ del BIGINT\n, unpl_tr BIGINT\n, wthdrwls BIGINT\n, pre_wdrw BIGINT\n, deposits BIGINT\n, clos_bal BIGINT\n, bal_disp BIGINT\n, bal_escr BIGINT\n, bal_unav BIGINT\n, opr_stat STRING\n, excld_fl STRING\n\u201aPRIMARY KEY (cashp_id, transaction date, denom_id, cassette)\n) STORED AS KUDU\n```\nWe try to rerun the above query and we get the following error:\n```bash\nERROR: ImpalaRuntimeException: Error creating Kudu table 'prod_trlog_card_analytical.opticash_dispencing_atm_tmp'\nCAUSED BY: NonRecoverableException: failed to wait for Hive Metastore notification log listener to catch up: failed to retrieve notification log events: failed to get Hive Metastore next notification: Thrift SASL frame is too long: 338.01M/100.00M\n```\n### Resolution\nAs a first step, let's try to fix `Thrift SASL frame is too long: 338.01M/100.00M` error.\n1. Login to Cloudera Manager in DR site with your personal administrative account:\n`Kudu > Instances > Click on Master > Select Tab Configuration`\n2. In `Search` box write safety valvue and at `Master Advanced Configuration Snippet (Safety Valve for gflagfile)` add th following flag:\n```bash\n--hive_metastore_max_message_size_bytes=858993459\n```\n>**Important Ndef**:  The above step with flag must be set at all three masters\n3. Restart the three kudu masters (one at a time)\n4. After rerunning the query the table is not still created but this time we get the following error: \n```bash\nSASL decode failed: SASL(-1): generic failure:\nwO706 15:44:11.242372 109675 hms_notification_log_listener.cc:130] Hive Metastore notification log listener poll failed: Not authorized: failed to ret\nrieve notification log events: failed to get Hive Metastore next notification: SASL decode failed: SASL(-1): generie failure:\nw0706 15:44:35.127687 109673 hms_client.cc:345] Time spent get HMS notification events: real 8.885s user 0.000s sys 0.228s\n```\n5. Restarting all Tablet Servers (dr1node01-10),one at a time, fixed the problem. \n**Before Restarting Tablets the following Flows must be stopped !!!**\n```\nPROD_IBANK_IngestStream_Visible\nPROD_Online_IngestStream\nPROD_IBank_IngestStream\n```\nStop the flows:\n>Ndef: We used following command because flows were working fine. Otherwise, we you should kill the application.\n```bash\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown _marker/\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\n```\nWhen Tablets are all up and running make sure you start again the flows. \nVerify that Tablets and Kudu is up and running by checking graphs and CM UI (CM -> Kudu -> Charts Library)\nInformation about how to start flows can be found [here](http://https://metis.ghi.com/obss/oss/sysadmin-group/support/-/tree/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows \"here\")\nWe verified that the problem is fixed by running the querry and got Table has been created message.\n## Root Cause Analysis\nThis problem occurred due to dr1node07 disk replacement.\nPlease refer to *IM1893876* for more information.\nThe fact that kudu tablets were offline for more than 1 days resulted in networking issues between Tablets.\n## Affected Systems\nDisaster Site",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "IM1908793.md"
        }
    },
    "467": {
        "page_content": "---\ntitle: Navigator Metadata Server Fails to Connect on PR Site Due to HAProxy Errors\ndescription: The Navigator Metadata Server on PR site intermittently failed to connect, with HAProxy showing 'no server available'; resolved by restarting the service via Cloudera Manager after a script-based restart proved ineffective.\ntags:\n  - mno\n  - bigstreamer\n  - navigator\n  - cloudera\n  - haproxy\n  - metadata server\n  - navigator connection\n  - pr1node03\n  - api cluster\n  - navigator_restart\n  - cloudera manager\n  - im2271635\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2271635\n  system: mno BigStreamer PR Site\n  root_cause: Cloudera Navigator Metadata Server became unresponsive, with HAProxy reporting no available backends\n  resolution_summary: Manual restart via Cloudera Manager restored connectivity, whereas the scripted restart failed\n  affected_component: Navigator Metadata Server\n  workaround_script: /opt/navigator_restart/apicluster.py on pr1node03\n---\n# mno - BigStreamer - IM2271635 - PR NAVIGATOR CONNECTION ISSUES\n## Description\nNavigator on the PR cluster was repeatedly inaccessible due to backend server errors reported by HAProxy. Attempts to restore functionality using the restart script failed. Restarting the Navigator Metadata Server via Cloudera Manager resolved the issue.\nPlease check if the Navigator in PR is functional because we often see messages like\nhaproxy[23876]: proxy cn_vip has no server available!\nWe also noticed that most of the time, when entering the Navigator web application, we receive a message:\n```\nFailed to connect to the Navigator server, check your network connection and try again.\n```\n## Actions Taken\n1. We executed the script located at `pr1node03.mno.gr:/opt/navigator_restart/apicluster.py` as `root`, documented at [Script for navigator metadata restart](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/200) which is used to restart the Cloudera Navigator Metadata server. However, the errors remained and we could not connect to the Navigator Metadata Server.\n2. We connected to Cloudera Manager and used it to restart the Navigator Metadata Service. This restored the connection to the service.",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20240129-IM2271635.md"
        }
    },
    "468": {
        "page_content": "---\ntitle: DR Cluster Service Disruptions Due to High-Complexity Impala Query\ndescription: DR Cloudera services appeared unhealthy, causing mass alerts and crashing some applications. Root cause was a high-complexity Impala query that overloaded threads and caused timeouts in Hive, Kudu, and Sentry.\ntags:\n  - mno\n  - bigstreamer\n  - dr cluster\n  - cloudera\n  - impala\n  - high complexity query\n  - threads cpu time\n  - sentry\n  - hive timeout\n  - kudu timeout\n  - grafana\n  - service disruption\n  - resource spike\n  - SD1949713\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD1949713\n  system: mno BigStreamer - DR Cluster\n  root_cause: A complex Impala query with excessive regex and conversions spiked resource consumption and triggered cascading timeouts across Hive, Kudu, and Sentry\n  user_visible_error: All Cloudera services reported unhealthy (red) status, with ~120 alert emails received\n  detection_method:\n    - Grafana alert\n    - Cloudera Manager Impala queries tab\n    - Service logs showing timeouts and resource exhaustion\n  action_taken:\n    - Validated alerts in Grafana\n    - Identified problem query in Cloudera Manager\n    - Investigated thread and resource usage\n    - No manual action taken; services recovered after query completed\n  outcome: Root cause identified and customer informed; system stabilized automatically after resource usage dropped\n---\n# mno - BigStreamer - SD1949713 - DR Cluster Service Disruptions\n## Description\nThe DR cluster triggered ~120 alert emails reporting unhealthy service status across the board. On logging into Cloudera Manager, all services were marked red. Some dependent applications had crashed due to this. Some of the applications crashed as well.\n## Actions Taken\n1. Login to grafana to make sure that the alert is about DR SITE. We noticed that there were alerts for IBANK Spark Waiting Batches but not for Visible which predisposes us for an issue with Kudu.\n![ibank_kudu_problem](.media/SD1949713/ibank_kudu_problem.PNG)\n2. Login to Cloudera UI for the DR Site.\n3. From `Charts>Impala Perf` we noticed increased resource commitment through Impala Pool Reserved and Threads charts.\n![Impala Pool Reserved](.media/Impala_pool_reserved.PNG)\n![Threads](.media/threads.PNG)\n4. From `Cloudera Manager>Impala>Queries` we searched for queries that took place at the time the problem raised. We found that the query with ID 6d44d9525a681fb8:5e536ffc00000000 had Threads:CPU Time 10.7h. Upon inspection through `Query Details` we saw that the query was of high complexity with conversions and comparisons with regex.\n![Query](.media/query.PNG)\n5. Through Cloudera logs, we noticed that the query impacted the services in the form of timeouts for Kudu and Hive due to slow communication with Sentry Service.\n![hive_problem](.media/SD1949713/hive_problem.PNG)\n![timeouts_kudu](.media/SD1949713/timeouts_kudu.PNG)\n![sentry_problem](.media/SD1949713/sentry_problem.PNG)\n6. The issue resolved itself once the query completed execution and the resource usage dropped. No manual recovery steps were required. We informed the client that it was due to a high complexity query ran by a normal user that resulted in an increased undertaking of resources.\n## Affected Systems\nDR Site\n",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20220617-SD1949713.md"
        }
    },
    "469": {
        "page_content": "---\ntitle: Online_Ingestion MergeBatch Failure on DR Due to Sentry Permission Check Failure\ndescription: The MergeBatch job for Online_Ingestion on the DR site failed due to a Permission Denied error in HDFS caused by Sentry failing to validate permissions during MySQL unavailability; issue resolved by rerunning the job manually and reporting to Graphite.\ntags:\n  - mno\n  - bigstreamer\n  - online_ingestion\n  - merge batch\n  - spark\n  - grafana\n  - hdfs\n  - sentry\n  - mysql\n  - graphite\n  - dr site\n  - batch failure\n  - permission denied\n  - sd2146917\n  - im1996192\n  - im2083185\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD2146917-IM1996192-IM2083185\n  system: mno BigStreamer DR Site\n  root_cause: Sentry failed to verify permissions due to temporary MySQL unavailability, resulting in HDFS Permission Denied error during MergeBatch job\n  resolution_summary: Verified that target par_dt had no records, manually reran the MergeBatch script, and executed Report to Graphite\n  impacted_component: Online_Ingestion MergeBatch\n  hdfs_dir_issue: true\n  permissions_check_failed: true\n---\n# mno - BigStreamer - SD2146917-IM1996192-IM2083185 - Online_Ingestion MergeBatch Failure on DR\n## Description\nWe have the following alert msg on Grafana:\n```\n[DR][IBANK] Online_Ingestion MergeBatch Failed\n```\n## Actions Taken\nLogin to `dr1edge01` with your acount\n```bash\nsu - PRODREST\n```\nWe look at the script log:\n```bash\n/var/log/ingestion/PRODREST/online/logonExecutor_OnlineBatch_full.log\n```\nThe problem was :\n`Permission Denied on hdfs dir. Due to unavailability of mysql it could not check the sentry permissions which are certain`\nThe main problem was due to some tasks running `de\u03bd` there was communication with the server to get the correct Permission.\nwe will have to rerun the script manually. Before running the script we will see if there are records in the table for each `par_dt`.\nEnsure that no records are present in prod_trlog_online.service_audit (eg 23_03_2023)\n```bash\nimpala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select count(*) from prod_trlog_online.service_audit where par_dt='20230223';\"\n```\nIf there is no record for the above `par_dt` then we run the script again.\n```bash\n/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_Online_MergeBatch LOCK_ONLINE_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_Online_MergeBatch /opt/ingestion/PRODREST/online/lock/ FULL 1800 \"`date --date='-1 day' '+%Y-%m-%d 00:00:00'`\" \"`date '+%Y-%m-%d 00:00:00'`\" >> /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log 2>&1 &\n```\nBecause it had failed for the previous day, we ran the script for the previous day.\nThe script ran for over 4 hours.\nAfter the above script successfully executed we ran the step [Report Start to Graphite](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/online.md#report-stats-to-graphite)\n## Root Cause\nThe MergeBatch job failed due to an HDFS permission denied error, which was caused by Sentry being unable to validate access rules due to MySQL unavailability. This prevented Spark from accessing required directories during job execution.",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "SD2146917_IM2085094_Merge_Branch_Online.md"
        }
    },
    "470": {
        "page_content": "---\ntitle: DWH_IBank EXTRACT Job for MY BANK Failed Due to Timeout in Monitoring Script\ndescription: The EXTRACT batch job for the MY BANK component failed with code 6 because the monitoring script timed out before the Spark application started; the Spark job succeeded and re-execution was successful.\ntags:\n  - mno\n  - bigstreamer\n  - dwh_ibank\n  - my_bank\n  - extract job\n  - spark\n  - yarn\n  - grafana\n  - timeout\n  - monitoring\n  - code 6\n  - batch failure\n  - impala\n  - job rerun\n  - sched_extract\n  - im2072206\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2072206\n  system: mno BigStreamer DWH\n  root_cause: Monitoring script timed out after 1.5 minutes while waiting for Spark application to start, though job itself eventually succeeded\n  resolution_summary: Customer reran the job, which succeeded; no abnormal delay was detected in Spark startup\n  affected_component: MY BANK\n  failure_code: 6\n  monitoring_script: sched_extract.sh\n  spark_status: SUCCEEDED\n  solution_reference: https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/101#ndef_94836\n---\n# mno - BigStreamer - IM2072206 - Batch Job Failed\n## Description\nOn 27/01/2023, the `EXTRACT` batch job for the `MY BANK` component of the `DWH_IBank` application failed with **Code 6** in Grafana.\n```\nApplication: DWH_IBank\nJob Name: EXTRACT\nComponent: MY BANK\nStatus: Failed\nDescription: Code 6\n```\n## Root Cause Analysis\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account and confirm that Datawarehouse Flow failed from `Monitoring/Monitoring PR/DR` dashboard.\nThe flow failed with `Code: 6` which means that the control script has timed-out while monitoring the `EXTRACT` script.\n2. Check logs:\nFrom `dr1edge01.mno.gr` with personal account:\n``` bash\nless /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\n```\n![IM2072206_extract_logs](.media/IM2072206_extract_logs.png)\nThe monitoring database was updated with status FAILED due to `Check if app is running` timeout.\n3. Check the Spark application status from YARN UI\n![IM2072206_yarn_app](.media/IM2072206_yarn_app.png)\nSpark App Status: SUCCEEDED.\nThe script waited for only 1,5min and updated the monitoring database with Failed Status. Spark app began its execution after almost 2,5 minutes.\n> 2,5min is not considered as a noticeable or abnormal delay time, so we did not investigate further.\n4. Customer reran the job\n5. Check logs and YARN UI of second application\n![IM2072206_yarn_rerun](.media/IM2072206_yarn_rerun.png)\n![IM2072206_rerun_logs](.media/IM2072206_rerun_logs.png)\nIn this case the Spark app started immediately and the script updated the monitoring app with Running Status.\n## Action Points\nSolution has been given with [this](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/101#ndef_94836) issue.",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230127-IM2072206.md"
        }
    },
    "471": {
        "page_content": "---\ntitle: IBank_Ingestion MergeBatch Job Failure Due to Memory Error on DR Site\ndescription: The IBank MergeBatch job failed in the DR cluster due to memory exhaustion during Spark execution; resolved by increasing coalesce value and rerunning all batch sub-steps manually.\ntags:\n  - mno\n  - bigstreamer\n  - spark\n  - yarn\n  - ibank\n  - merge batch\n  - dr site\n  - grafana\n  - hdfs\n  - memory error\n  - service audit\n  - kudu\n  - hbase\n  - sd2146915\n  - im2085092\n  - ingestion\n  - graphite\n  - kerberos\n  - batch failure\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD2146915-IM2085092\n  system: mno BigStreamer DR Site\n  root_cause: Spark memory limit exceeded during MergeBatch execution in DR, likely caused by heavy service_audit dataset load\n  resolution_summary: Increased coalesce value from 6 to 12, reran distinct join and all batch sub-steps manually; job succeeded afterward\n  impacted_component: IBank_Ingestion MergeBatch\n  corrective_actions:\n    - log review\n    - coalesce tuning\n    - manual execution of historical and reporting scripts\n    - graphite report resubmission\n    - duplicate check in Kudu/HBase\n---\n# mno - BigStreamer - SD2146915-IM2085092 [DR][IBANK] Application : IBank_Ingestion MergeBatch Failed\n## Description\nWe have the following alert msg on Grafana.\n```\n[DR][IBANK] IBank_Ingestion MergeBatch Failed\n```\n## Actions Taken\n1. Login to `dr1edge01` with your account\n```bash\nsu - PRODREST\n```\n2. We check the script log:\nScript Logs: `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`\nError code: `Log messages was for memory fault.`\nWe also look at the Spark logs:\nUse Firefox on dr1edge01.mno.gr/pr1edge01.mno.gr to access the logs via YARN Resource Manager UI\nScript: `/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh` on dr1edge01.mno.gr/pr1edge01.mno.gr (each   edge server submits to a different cluster)\n### Troubleshooting\n- Use the script logs `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log` to identify the cause of the failure\nIf we have mentioned `error code` then:\n```bash\nvi /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh\n```\nChange `colaesce` from `6` to `12` and save changes. \n> Ndef: Inform the next day developers in order to update the git repo with the new value\n- Ensure that no records are present in prod_trlog_ibank.service_audit_old\n``` bash\n# eg. 09-11-2019\nimpala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\"\n```\n- If no records exist and no other process is up, you can run the script again.\n- For the previous day:\n``` bash\n/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n```\n- For a specified date:\n``` bash\n# e.g. 09-11-2019\n/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n```\nThe process runs for well over an hour under normal circumstances or even longer for heavy load. Use of screen command advised.\nAfter the above script completed we ran the next sub-steps manually:\n1. `Distinct join to Service Audit` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\n2.  `Report stats to Graphite` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#report-stats-to-graphite)\n3. `Drop hourly partitions` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#drop-hourly-partitions)\n4. `Upsert to HBase (Migration)` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\n5. `Send reports to business users` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#send-reports-to-business-users)\n6. `Duplicates between Impala and Kudu/HBase` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#duplicates-between-impala-and-kuduhbase)\n## Root Cause\nThe MergeBatch job on the DR site failed due to Spark memory exhaustion while processing `prod_trlog_ibank.service_audit_old`. Logs confirmed a memory fault error, and coalesce was initially set too low for the workload.",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "SD2146915-IM2085092.md"
        }
    },
    "472": {
        "page_content": "---\ntitle: Impala Connection Pool Exhaustion During HBase Upsert Job Due to Serial Execution on Coordinator\ndescription: The Impala daemon (pr1node04) was overloaded during execution of the Upsert to HBase stage for the IBANK Visible table due to forced single-node execution, resulting in blocked queries, high concurrent connections, and system health alerts; resolved by reverting script changes and disabling HBase quotas.\ntags:\n  - mno\n  - bigstreamer\n  - impala\n  - impala daemon\n  - impala client connections\n  - impala coordinator\n  - hbase\n  - hbase quotas\n  - upsert to hbase\n  - ibank visible\n  - prodrest\n  - spark\n  - concurrency\n  - impala query failure\n  - cloudera manager\n  - pr1node04\n  - pr1node01\n  - resource bottleneck\n  - impala query logs\n  - impala role restart\n  - IM2097021\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2097021\n  system: mno BigStreamer PR Site\n  root_cause: Forced single-node execution of an Impala upsert query exhausted coordinator (pr1node04), blocking concurrent queries and triggering health alerts\n  resolution_summary: Cancelled the long-running query, restarted Impala daemons, removed serial execution flag, and reran the job with parallel execution enabled\n  affected_daemons:\n    - pr1node01\n    - pr1node04\n  related_jobs:\n    - /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh\n  customer_impact: live stream ingestion and production queries were stalled\n---\n# mno - BigStreamer - IM2097021 - Multiple Health issues on PR Impala\n## Description\nOn 02/03/2023 at 23:30, multiple Impala daemons across PR Site triggered health alerts in Cloudera Manager due to high concurrent client connections and pause duration warnings. The issue originated from a long-running query used in the IBANK Visible table upsert job, which was executed in single-node mode (via `set num_nodes = 1`). This forced the query to overload the coordinator daemon (pr1node04), which caused query blocking, client connection accumulation, and stalled streaming services.\nToday 02/03/2023 & 23:30 the Following alarms appeared on Cloudera:\n```\n[Impala: Daemon (pr1node01)]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node02)]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node03)]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node04)]\n[Pause Duration]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node05)]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node06)]\n[Impala Concurrent Client Connections]\n```\n## Actions Taken\n### Investigation\n1. Login to Cloudera for PR Site \n2. To identify the Impala query from `Upsert to HBase` we can see logs from the script at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log` as `PRODREST` user. We cite a screenshot that shows the query. Also, we see the url where we can monitor the query progress (paste this url on a firefox opened through terminal), as well as the coordinator.\n![logs_screenshot](.media/upsert_to_hbase_logs_query.PNG)\n> Ndef: These are not logs from that specific script execution, just a sample to see where you can find the query information you need.\n3. From `Cloudera > Impala > Queries` we identified the query and noticed that it had stopped getting processed. In addition, we noticed that Impala had stopped processing other queries as well.\n### Mitigation\n4. We cancelled the query that ran for `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` execution. We can cancel the query in two ways.\n1st way: From `Cloudera > Impala > Queries` you can click `cancel` at the dropdown next to the query\n2nd way: From the url that we monitor the query.\n5. We restarted Impala daemon role for pr1node01. This solved the problem with this specific node, however the service did not correspond.\n### Resolution\n6. We restarted Impala daemon role for pr1node04 that was the coordinator for the query. This solved the problem and recovered the service functionality.\n7. Upon investigation, we concluded that the change to `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` that stops the parallel execution of the query by Impala daemons (set num_nodes = 1) was the cause of the problem.\n8. We scheduled to rerun the `Upsert to HBase` stage the following day after reverting the script to use all Impala daemons for parallel execution.\n### Long-Term Fix\n9.  On 03/03/2023 \n- we disabled HBase quotas for ` PROD_IBANK` namespace on PR Site according to [this](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/22263_mno_HBASE_TUNING.docx) MoP\n- we removed `set num_nodes = 1` from `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\n- reran the script\n10. The script ran successfully\n## Action Points\n1. We opened [this](https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/67) issue to investigate and deploy a permanent fix for running `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` alongside HBase quotas.\n## Our Ticket Response\n```\n03/03/23 00:49:29 Europe/Eastern (MASTROKOSTA MARIA):\nThe root cause is the same as ticket SD2158913. The job that populates the visible table was canceled after consultation with the customer as it was affecting the live streams.\nThe job will be scheduled to be rerun after consultation.\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230301-IM2097021.md"
        }
    },
    "473": {
        "page_content": "---\ntitle: Spark Waiting Batches Alert on DR Online Topology Due to Kerberos Failure\ndescription: Spark Waiting Batches alert triggered on DR Online topology due to Kerberos authentication failure caused by domain controller downtime; topology was resubmitted and affected data was restored via distcp.\ntags:\n  - mno\n  - bigstreamer\n  - spark\n  - spark waiting batches\n  - online topology\n  - kerberos\n  - grafana\n  - prodrest\n  - dr1edge01\n  - hbase\n  - kudu\n  - distcp\n  - yarn\n  - cluster monitoring\n  - ticket im2165930\n  - sd2228613\n  - authentication error\n  - domain controller\n  - merge batch\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2165930\n  related_issue: SD2228613\n  system: mno BigStreamer DR Site\n  root_cause: Spark Streaming job failed due to Kerberos errors caused by domain controller being down for patching\n  resolution_summary: Resubmitted Spark topology, avoided duplicate merge batch inserts by transferring Hive data from PR via distcp and cleaning Kudu manually\n  affected_components:\n    - Spark Streaming Online\n    - Kerberos\n    - prod_trlog_online.service_audit\n    - DR Site Merge Batch\n  remediation_actions:\n    - Resubmitted topology\n    - Avoided duplicate inserts\n    - Manual Kudu cleanup\n    - Verified HBase cleared via CleanupHBaseSAS\n---\n# mno - BigStreamer - IM2165930 - Alert at Grafana\n## Description\nA Spark Waiting Batches alert was triggered on the DR Online topology due to a Kerberos failure caused by a domain controller outage. The topology was restarted and missing data was synced from PR via distcp, avoiding duplication in downstream merge batch jobs.\nThe following alert appeared in Grafana:\n```\n[DR][ONLINE] Spark Waiting Batches alert\n```\n## Actions Taken\n1. Login to grafana at https://dr1edge01.mno.gr:3000 with personal account\n2. Inspected Monitoring Alerts and Monitoring DR\\PR to confirm which topology has the issue. We saw that the alert line for online topology had no line.\n3. SSH to dr1nodeedge01 and open firefox with `firefox` without root privilege.\n4. From `Yarn` tab on the browser we ensured that the application had failed\n5. Resubmitted topology with `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` script as `PRODREST` user.\n6. From application's logs we concluded it was a kerberos issue.\n ![image](.media/kerberos_error_stream_online.PNG)\n7. The topology was down from ~8.30 pm on Sunday 11/06 and was resubmitted on Monday 12/06. Therefore the merge batch job for 11/06 had completed with lack of data. The devs' team checked that there were 9 transactions that were interrupted during the failure of the spark job so if we reran the script for merge batch there would be 9 double records in hive.\n8. We decided to transfer the prod_trlog_online.service_audit table for par_dt=20230611 from PR to DR Site with distcp according to the procedure described [here](./20201218-IM1389913.md)\n> Ndef: The data from kudu for the specific partition needed manual deletion on 15/06 by devs and HBase needs no manual action since it clears by CleanupHBaseSAS spark job.\n## Our Ticket Response\n```\n12/06/23 17:13:35 Europe/Eastern (MASTROKOSTA MARIA):\nThe root cause is the same as in SD2228613.\n12/06/23 11:18:40 Europe/Eastern (MASTROKOSTA MARIA):\nThe topology crashed due to a problem with kerberos as mentioned in SD2228613. It has been resubmitted.\n```\nThe client confirmed that the issue with kerberos was because one of the two domain controllers was out due to patching\n## Affected Systems\nDR Site Online\n[def]: ./media/kerberosthis",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230611-IM2165930.md"
        }
    },
    "474": {
        "page_content": "---\ntitle: Navigator Metadata Server Not Displaying Analytics Data on Primary Site\ndescription: Navigator on the Primary Site was not showing analytics data due to invalid column errors in the Metadata Server logs. The issue was resolved by purging and reinitializing the Navigator Metadata Server Storage directory and manually resetting the NAV_UPGRADE_ORDINAL table in the metadata database.\ntags:\n  - bigstreamer\n  - navigator\n  - cloudera manager\n  - metadata server\n  - navigator metadata server\n  - nav_upgrade_ordinal\n  - analytics tab\n  - purge navigator\n  - mysql\n  - solr\n  - PR site\n  - cloudera navigator restart\n  - cloudera bug fix\n  - storage dir\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: SD1752317\n  system: MNO BigStreamer Primary Site\n  root_cause: Corrupted/inconsistent metadata and schema issues in Navigator Metadata Server Solr data\n  user_visible_error: Navigator analytics tab not displaying data\n  action_taken:\n    - Checked Navigator UI and logs for permission and backend issues\n    - Found invalid column errors in `/var/log/cloudera-scm-navigator`\n    - Purged metadata storage directory\n    - Reset database state via NAV_UPGRADE_ORDINAL\n    - Restarted metadata server and verified UI functionality\n  outcome: Navigator analytics functionality restored successfully\n---\n# mno - BigStreamer - SD1752317 - Cloudera Navigator \u03c3\u03c4\u03bf primary site\n## Description\nNavigator analytics tab was not displaying data on the primary site due to metadata server issues. Logs revealed schema inconsistency errors related to invalid columns. Issue resolved by purging the metadata storage directory and resetting the NAV_UPGRADE_ORDINAL table in the Navigator DB.\n## Actions Taken\n1. Login to Cloudera Navigator on PR Site (https://xxxx:7187) with your Exxx account\n2. Check analytics tab if you have permissions to see the graphs (If you are in WBDADMIN group you should see)\n3. ssh Exxx@pr1edge01;ssh Exxx@pr1node03;\n4. less /var/log/cloudera-scm-navigator/ & check navigator logs for errors\n5. Errors for invalid columns appeared.\n6. To solve the issue we made the below steps:\nPurging the Navigator Metadata Server Storage Directory\n- Identify and take ndef of the Navigator Metadata Server Storage Dir directory by Logging into Cloudera Manager and browsing to Cloudera Management Services > Configuration > Navigator Metadata Server > Navigator Metadata Server Storage Dir. Ndef the storage directory location.\n- Stop Navigator Metadata Server by navigating to Cloudera Management Service > Instances, selecting Navigator Metadata Server > Actions > Stop # First Infrom Monitoring and after stop the role.\n- Backup the storage Directory. Example:\n```bash\n# sudo tar -cpzf /<backup-location/navms_data_backup-`date +%Y%m%d-%H%M`.gz /<Navigator Metadata Server Storage Dir location>\n```\n- Remove the Navigator Metadata Server Storage Dir location. Example:\n```bash\n# sudo mv /<Navigator Metadata Server Storage Dir location>/ /root/<Navigator Metadata Server Storage Dir location>_bkp\n```\n- Starting 2.9 release whenever NMS Solr data directory is purged, database table need to be modified to make sure that state is in sync. To do this run following SQL command in NMS database:\n```bash\nssh pr1edge01;\nssh pr1node03;\nmysql -uroot -p\nuse navigator_metadata;\n# Required for NMS >=2.9 to sync schema state after purge\ncreate table date_temp_nav_upgrade_ordinal as select * from NAV_UPGRADE_ORDINAL;\ndelete from NAV_UPGRADE_ORDINAL;\ninsert into NAV_UPGRADE_ORDINAL values(-1, -1);\n```\n- Start Navigator Metadata Server and it will recreate data directory and Solr schema on the first run. Cloudera Management Service > Instances, selecting Navigator Metadata Server > Actions > Start\n7. Reproduce steps 1 & 2 to check if issue solved.\n8. Infrom Monitoring that all tasks completed.\n## Affected Systems\nmno Bigstreamer Navigator",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20211105-SD1752317.md"
        }
    },
    "475": {
        "page_content": "---\ntitle: Impala Query Details Missing in Cloudera Manager - Session Timeout Fix\ndescription: Query details were not visible during execution in Cloudera Manager due to Impala query and session timeouts. Issue was resolved by configuring timeouts in both Hue and Impala and restarting the services.\ntags:\n  - mno\n  - bigstreamer\n  - cloudera manager\n  - impala\n  - hue\n  - query details\n  - query timeout\n  - session timeout\n  - idle_session_timeout\n  - idle_query_timeout\n  - waiting on client\n  - cmert\n  - sr 3-29589386011\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1840323\n  system: mno BigStreamer - Disaster Site\n  root_cause: Impala queries remained in \"Waiting on Client\" state due to session and query timeouts\n  user_visible_error: Query details not available in Cloudera Manager for executing Impala queries\n  resolution_method:\n    - Adjusted session and query timeout settings in Hue and Impala configuration\n    - Restarted both Hue and Impala services\n  oracle_case_reference: SR 3-29589386011\n---\n# mno - BigStreamer - IM1840323 - Issue with query details CMert\n## Description\nIn some queries we cannot see the details from cloudera manager > impala > queries while they are in the executing phase.\nExample\nhttps://pr1node03.mno.gr:7183/cmf/impala/queryDetails?queryId=e7441b27715b1699%3Ad3527df300000000&serviceName=impala\n## Actions Taken\n1. Check that that the problem truly occurs:\n- Login to CM DR with your pesonal account and go to `Cloudera Manager > impala > queries`\n- Select a query that it on executing phase and click on `Query details`\nWhen accessing Impala query details during execution, the interface displayed only \u2018Waiting on Client\u2019 with no detail view.\n2. As a part of the investigation we created `SR 3-29589386011`\n3. According to Oracle\u2019s response, the issue was caused by session and query timeouts in Impala and Hue. So we had to set the below values: \n- Login to CM DR with your pesonal account and go to `Cloudera Manager > Hue >  Configuration > Hue Service Advanced Configuration Snippet (Safety Valve) for hue_safety_valve.in` and set the below values: \n```bash\n[impala]\nquery_timeout_s=60\nsession_timeout_s=60\nclose_queries =true\n[desktop]\n[[auth]]\nidle_session_timeout=300\n```\n4. `Restart` Hue Service\n5. From `Cloudera Manager > Impala > Configuration and change below values:\n- idle_query_timeout: 1 min\n- idle_session_timeout: 1 min\n6. `Restart` Impala Service\n## Affected Systems\nDisaster Site",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20220415-SD1897262.md"
        }
    },
    "476": {
        "page_content": "---\ntitle: [DR][IBANK] Spark Waiting Batches Alert and IngestStream Topology Recovery\ndescription: Alert from Grafana about waiting Spark streaming batches on DR site for IBANK and ONLINE systems. Root cause traced to a bottleneck in the PROD_Online_IngestStream topology. Issue resolved by checking Kudu memory usage and restarting the faulty Spark topology.\ntags:\n  - bigstreamer\n  - mno\n  - ibank\n  - dr site\n  - grafana\n  - spark\n  - spark streaming\n  - spark waiting batches\n  - ingeststream\n  - kudu\n  - memory usage\n  - topology restart\n  - yarn\n  - cloudera manager\n  - application id\n  - streaming recovery\n  - spark troubleshooting\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1361249\n  system: MNO BigStreamer - DR IBANK\n  root_cause: High memory usage in Kudu tablets and backlog in the `PROD_Online_IngestStream` topology led to delayed Spark batches\n  alert_source: Grafana Monitoring Alerts dashboard\n  triggered_alerts:\n    - [DR][IBANK] Spark Waiting Batches\n    - [DR][ONLINE] Spark Waiting Batches\n  action_taken:\n    - Investigated Grafana dashboards for delayed topologies\n    - Checked Kudu tablet memory usage and disk utilization\n    - Restarted `PROD_Online_IngestStream` Spark topology\n    - Verified resolution through Spark UI and Grafana batch metrics\n  outcome: IngestStream topology recovered and Spark batches processed normally\n---\n# mno - BigStreamer - IM1361249 - Spark Waiting Batches Alert - Grafana\n## Description\nGrafana raised critical alerts for `[DR][IBANK]` and `[DR][ONLINE]` due to prolonged Spark waiting batches. Investigation revealed issues with the `PROD_Online_IngestStream` topology on the DR site, likely caused by resource bottlenecks in Kudu or Kafka ingestion delays.\n## Actions Taken\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account\n2. Inspected `Monitoring Alerts` and `Monitoring DR\\PR`\n3. From the `Monitoring Alerts` check the graphs `spark waiting batches` to find which topology has delays`\n4. Open MobaXterm `dr1edge01` ssh with your personal account\n5. Execute `firefox`\n6. Click the `DR` bookmark\n7. Check the logs of failed spark topology.\n8. Login to `DR cloudera manager` with your personal account\n9. Go to `CHARTS-->KAFKA_KUDU_DISK_UTIL` and see if abnormal rates on disk util exists\n10. Using Firefox on `dr1edge01`, navigate to the Kudu tablet Web UI for each DR node and inspect the **Memory Usage (detailed)** section. If any tablet exceeds 90% memory usage, restart the affected Kudu tablet via Cloudera Manager or systemd.\n11. Check again the graphs `Monitoring Alerts` and `Monitoring DR\\PR`\n12. If the alert appeared `(in our case appeared on PROD_Online_IngestStream topology)` restart the specific topology\n13. Open a new tab on MobaXterm `dr1edge01` ssh with your personal account\n14. sudo -iu `PRODREST`\n15. yarn application -list | grep \"PROD_Online_IngestStream\"\n16. yarn application -kill `<application_id>`\n17. Start again the topology `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh`\n18. Check from Graphana again the graphs `Monitoring Alerts` and `Monitoring DR\\PR`.\n19. Go to spark jobs history and click running.\n20. Click `Running-->Online_Ibank_IngestStream-->Streaming`\n21. On the Spark Job UI for `Online_Ibank_IngestStream`, monitor the **Active Batches** section and ensure the count drops to 0, indicating no backlog.\n## Affected Systems\nDisaster Site IBANK",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20201120-IM1361249.md"
        }
    },
    "477": {
        "page_content": "---\ntitle: HBase RegionServer OOM and Spark Waiting Batches Alert on DR Site\ndescription: Spark streaming job in DR cluster failed due to HBase RegionServer OOM and high per-region traffic in PROD:BANK namespace; issue mitigated by restarting streaming job, root cause identified as uneven HBase load.\ntags:\n  - mno\n  - bigstreamer\n  - hbase\n  - regionserver\n  - dr1edge01\n  - spark\n  - streaming\n  - online ingestion\n  - grafana\n  - spark waiting batches\n  - prodrest\n  - hbase hotspotting\n  - workload imbalance\n  - workaround\n  - hbase performance\n  - region load\n  - memory issue\n  - im1896751\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM1896751\n  system: mno BigStreamer - DR Site\n  root_cause: Spark job failure caused by HBase RegionServer Out-of-Memory errors due to hotspotting in PROD:BANK HBase namespace\n  resolution_summary: Streaming job resubmitted; HBase cluster stabilized; client advised to tune table design to reduce per-region load\n  workaround: Job manually restarted\n  downstream_risks: Potential recurrence unless application logic or table distribution is improved\n---\n# mno - BigStreamer - IM1896751 - Alert in Grafana Application\n## Description\nA [DR][ONLINE] Spark Waiting Batches alert appeared in Grafana, indicating a failure in the Spark streaming job on the DR site.\n## Actions Taken\n1. Login to `dr1edge01` and open firefox\n2. At the YARN UI search for `PRODREST` and sort by End date. You will find the failed application.\n3. From the UI we saw that Spark exited due to errors related to HBase timeouts.\n4. From DR site's Cloudera Manager we saw that HBase Regionservers restarted due to Out-Of-Memory errors and are now healthy again.\n5. Using this [document](/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/online.md#spark-streaming) we re-submitted the failed topology and the alarm was cleared:\n```bash\n/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\n```\n6. We informed the customer that a **workaround** has been implemented. The ticket status from this point is \"Workaround Provided\".\n7. The logs and resources charts from Cloudera Manager did not indicate a specific reason for the restarts.\n8. After investigating HBase tables we identified that 2 tables from the `PROD_BANK` namespace although they have low traffic overall, they have high traffic per region. Since each region is hosted on only one Regionserver, so this application pattern creates hotspots on specific servers and does not utilize the whole cluster properly. Below you can find the charts that show this behavior.\nWorkload per region\n```sql\nselect delete_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'\n```\n![DR_DELETE.PNG](.media/DR_DELETE.PNG)\n![DR_DELETE_12H.PNG](.media/DR_DELETE_12H.PNG)\n```sql\nselect get_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'\n```\n![DR_GET.PNG](.media/DR_GET.PNG)\n![DR_GET_12H.PNG](.media/DR_GET_12H.PNG)\n```sql\nselect increment_rate_across_hregions + append_rate_across_hregions + mutate_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'\n```\n![DR_WRITE.PNG](.media/DR_WRITE.PNG)\n![DR_WRITE_12H.PNG](.media/DR_WRITE_12H.PNG)\nUneven workload between Regionservers\n```sql\nselect read_requests_rate, write_requests_rate\n```\n![DR_LOAD_REGIONS.PNG](.media/DR_LOAD_REGIONS.PNG)\n![DR_LOAD_REGIONS_12H.PNG](.media/DR_LOAD_REGIONS_12H.PNG)\n## Our Ticket Response\n9. Inform the customer of the findings\n```\nThe problem did not occur today. Looking at the overall picture of HBase, we notice that its usage has increased significantly. Specifically, we see tables in the Namespace \"PROD:BANK\" with a high workload throughout the day and with an application-level configuration that creates uneven load distribution between Region Servers.\nYou should immediately proceed with the actions that have been suggested for benchmarking and tuning HBase as a service and the applications that use it in order to optimize its use.\n```\n## Affected Systems\nDisaster Site HBase",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20220630-IM1896751.md"
        }
    },
    "478": {
        "page_content": "---\ntitle: Merge Batch Job for 29/02 Fails Due to Record Volume, Kerberos Timeouts, and Partition Overload\ndescription: Merge Batch on 29/02 failed due to high record volume, Kerberos expiration, dynamic date issues, and uncleaned Kudu partitions; resolved through manual execution of batch steps, data replication, and range partition management across PR/DR clusters.\ntags:\n  - mno\n  - bigstreamer\n  - merge batch\n  - ibank\n  - kerberos\n  - kudu\n  - kudu partition\n  - spark\n  - yarn\n  - service_audit\n  - service_audit_old\n  - pr1edge01\n  - dr1edge01\n  - kudu range cleanup\n  - replication\n  - visible table\n  - hbase upsert\n  - dynamic date bug\n  - manual override\n  - feb29\n  - im2285747\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2285747\n  system: mno BigStreamer PR & DR Site\n  root_cause: MergeBatch failed on Feb 29 due to excessive data volume, Kerberos ticket expiration after 8 hours, dynamic date scripting bugs, and leftover Kudu partitions\n  resolution_summary: Re-executed merge steps manually across sites, performed HDFS replication, adjusted Spark resources, and cleaned up Kudu partitions for continuity\n  manual_intervention: true\n  kudu_partitions_modified: true\n  replication_method: HDFS copy (Cloudera Manager)\n  impacted_tables:\n    - prod_trlog_ibank.service_audit\n    - prod_trlog_ibank.service_audit_old\n    - prod_trlog_ibank.service_audit_stream\n---\n# mno - BigStreamer - IM2285747 - merge batch 29/2\n## Description\nThe Merge Batch for 29/02 failed to complete due to a combination of Kerberos ticket expiration, exceptionally large input data (~115 GB), multiple leftover Kudu partitions, and dynamic date logic bugs. Recovery required manual reruns, HDFS replication, Spark resource tuning, and Kudu partition management.\nCheck the merge batch in DR/pr for 2/29. It seems to be still running. We have disabled it on 1/31 until this is finished.\n## Investigation\n> Ndef: All log paths and query executions are found/executed from pr1edge01/dr1edge01.\n1. Login to Grafana and make sure that the failed step is the Merge Batch.\n2. Login to DR/PR edge nodes and through the node's firefox check YARN at https://dr1node03:8090 and https://pr1node03:8090 for the PROD_Ibank merge batch job.\n3. Check the stages tab for stages that have been completed for this job. At PR the `insert into` stages had completed after 8h. The RDD stages had failed and continued failing.\n![Yarn UI](.media/IM2285747_1.png)\n![Spark Stages](.media/IM2285747_2.png)\n4. The 8 hour mark gives us some clues as to the failure. In essence after 8 hours kerberos tickets are dropped, leading to continuous authentication failures and timeouts as shown in the logs `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`.\n5. Checked the merge batch logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log` and `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log` and indeed we found authorization errors.\n6. Checked the size and count of the `service_audit_old` table through Impala and HDFS and it was found to be among the largest ever both in size (over 115 GB) and count (60 mil). The query used is `select  count(*) from prod_trlog_ibank.service_audit_old where par_dt=20240229`.\n7. Inspecting the submition script we can see that for each individual sub-script the date is generated anew in the sub-script invocation. This means that since the merge in question spanned multiple dates, all steps started after the day's end wouldn't have completed succesfully.\n8. Inspecting the range partitions through Impala with the query `show range partitions prod_trlog_ibank.service_audit_stream`, we can find multiple leftover range partitions since November. This added further computation time for the already large table.\n### Investigation for 05/03:\nFor this day, while the size and count were not unusually large the last step of the process, ie. the\nhbase upsert took unusually long and couldn't complete on its own. The process seemed to get stuck on 1\nsingular task as shown by the logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`, which leads us to conclude that a certain record was problematic because all tasks completed\nnormally except a specific one that kept failling even on re-runs.\n### Investigation - Summary:\n- Greater than most end of the month dates size(over 115 GB) and count (60 mil)\n- Multiple leftover range partitions\n- Execution spanning multiple days, triggering the dynamic date issue with the submition script\n- Stage execution surpassing the 8 hour mark, triggering a known kerberos authorization bug\n- Problematic record for 05/03\n## Resolution\n1. Since the data was already loaded into PR for 29/2, the rest of the steps were executed manually on it according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#batch instructions for steps beyond the Merge Batch .\n2. For all the remaining dates since 29/2 and because the cron jobs were stopped, each day was executed manually in full in each site, half of them (days) in PR and half of them (days) in DR following all the steps in the sub-steps guide from https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md .\n3. For each job completed in one site it was replicated over to the other using HDFS replication, through the destination's Cloudera Manager for both `service_audit` and `service_audit_old` tables. **A similar procedure for table replication exists in [Table Replication](./20201218-IM1389913.md) but not for HDFS replication.**\n- /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit_old/par_dt=$date\n- /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit/par_dt=$date\n4. Once replicated the HBase Upsert step was run on the destination site according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .\n5. HBase markers were manually set for each job that didn't complete automatically according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .\n6. Kudu range partitions were cleaned up manually for all previous days and 3 new partitions were created for the 3 previous days, in order for the cleanup script to continue functioning as nornal. More details on this can be found at the end of this document.\n### Resolution for 05/03 HBase Upsert\nFor this operation specifically more resources had to be allocated to the spark-submit job at `/opt/ingestion/PRODREST/ibank/spark/submit/visible_trn_hbase_daily_upsert/submitVisibleTrnToHbaseIndexesDailyUpsert_STABLE.sh`. After its completion resources were reverted back to normal:\n```\nIncrease node count to 12\nDecrease core count to 1\nIncrease tasks to 40\n```\nKudu Range Partitions:\nRange partitions are created for the `service_audit_stream` table on the `u_timestamp` column. The commands to create and delete them\ncan be found below for some example dates. Ndef that those partitions are in UTC time, so the time to\ncreate/drop must be converted to local time, taking into account DST. For winter we are at GMT+2 so in order\nto include a full day it must range for 22:00 of the previous to 22:00 of the current (where current is the day you want to delete).\n```sh\n# Drop\nalter table prod_trlog_ibank.service_audit_stream drop range partition '2023-02-26T22:00:00.000000Z' <= VALUES < '2023-02-27T22:00:00.000000Z' ;\n\n# Create\nalter table prod_trlog_ibank.service_audit_stream add range partition '1970-01-01T00:00:00.000000Z' <= VALUES < '2023-02-28T22:00:00.000000Z' ;\n```\nThe example above includes the first partition, which tracks from 1970 to the date in question. When deleting previous partitions, the first one\nmust be recreated in order to include the time from 1970 to the first date, and then you can create the daily partitions. For example, lets say we have\nthe following partitions:\n```\n| 1970-01-01T00:00:00.000000Z <= VALUES < 2023-11-29T22:00:00.000000Z | # We want to drop this one in order to include a bigger range\n| 2023-11-29T22:00:00.000000Z <= VALUES < 2023-11-30T22:00:00.000000Z | # We want to drop this one due to policy\n| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\n| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\n| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\n| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This must not be touched\n| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |\n| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |\n| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |\n```\nThe above will become:\n```\n| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # This was created by including all dates from 1970 to this one\n| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # This was dropped and recreated\n| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # This was dropped and recreated\n| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This was not touched\n| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |\n| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |\n| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20240302-IM2285747.md"
        }
    },
    "479": {
        "page_content": "---\ntitle: MergeBatch Job Failure Due to Increased JSON Payload in response_text_data\ndescription: The IBank_Ingestion MergeBatch job failed after running over 14 hours due to a spike in data volume caused by large JSON payloads in the response_text_data field; the issue was mitigated by splitting the job into three time-based segments and performing distcp and HBase upsert operations afterward.\ntags:\n  - mno\n  - bigstreamer\n  - ibank\n  - mergebatch\n  - spark\n  - job failure\n  - response_text_data\n  - json size\n  - campaignmanagement\n  - getcampaigns\n  - distcp\n  - hdfs\n  - impala\n  - yarn\n  - grafana\n  - hbase\n  - kudu\n  - sd2221480\n  - im2158906\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2158906\n  related_issue: SD2221480\n  system: mno BigStreamer PR Site\n  root_cause: The service_name /CAMPAIGNMANAGEMENT/GETCAMPAIGNS caused JSON payloads in response_text_data to nearly double, resulting in prolonged Spark execution times\n  resolution_summary: Job was manually terminated, then re-executed in 3 time-based chunks over 3 days; followed by distcp sync to the second site, upsert to HBase, and manual cleanup of old data in Kudu\n  affected_component: IBank_Ingestion MergeBatch\n  spark_chunk_strategy: 00:00\u201312:00, 12:00\u201318:00, 18:00\u201300:00\n  spark_issue_type: long-running job\n---\n# mno - BigStreamer - IM2158906 - Failed job at Grafana\n## Description\nOn 30/05/2023, the IBank_Ingestion MergeBatch Spark job failed due to large JSON payloads returned by the /CAMPAIGNMANAGEMENT/GETCAMPAIGNS service, doubling data sizes and exceeding resource limits. The issue was resolved by running the job in chunks and syncing results via distcp.\nFollowing ticket SD2221480, the failed job [IBank_Ingestion]-[MergeBatch]-[JOB] appeared again but for d1edge01.mno.gr\n## Actions Taken\nAfter communicating with the customer, we proceeded to manually kill the job as it was running for over 14 hours and was affecting live production flows.\nProceeding to investigate the issue, we saw the following:\n1. Going to Cloudera Manager => Yarn => Applications =>\n``\nname RLIKE '.*PROD_IBank_MergeBatch' and application_duration > 3h\n``\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/Yarn_Applications_Merge_Batch_Prod_Ibank.png)\nWe notice that the Merge batch for `20230531` was running `14 hours` without finishing.\n2. As a second step, let's check the number of records in `impala` and the space occupied in `hdfs` by each `par_dt` from `20230509` to `20230530`\nBelow we see the space at the `hdfs` level:\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/HDFS_du.png)\nThe number of records in `impala` as an example for the `par_dt` `20230511` and `20230512`:\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/impala_query_par_dt_count.PNG)\nWhat we observe above is that while the `par_dt` `20230511` and `20230512` they have no difference in the number of records, they are twice as large. Where did this come from and what impact does it have? We will analyze it in the next steps.\n3. Let's see how this increase came about:\nAnalyzing the sum of length for response_text_data for each service, we notice that from `12/05/2023` onwards the service_name `'/CAMPAIGNMANAGEMENT/GETCAMPAIGNS'` takes up much more space as shown below compared to previous days.\nAfter `20230512`\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/After_20230512.png)\nBefore `20230512`\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/Before_20230512.png)\nUntil 11/05/2023 the average avg(length(response_text_data)) of prod_trlog_ibank.service_audit is stable at ~12K while from 12/05/2023 we see it approximately doubling.\nThis resulted in the `Merge Batch` not finishing as since the size of each `json` has doubled it takes much longer to execute the spark job.\n4. How did we handle it to get it running?\nAs described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#merge-batch) in the subchapter `If the problem is with resources (out-of-memory errors):` we ran the `Merge Batch` in separate chunks of the day.\nThe process took 3 days to complete as each chunk of the day took ~9 hours.\n5. After it was completed on one site, `distcp` was performed as described in [issue](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/issues/20201218-IM1389913.md) for the `prod_trlog_ibank.service_audit` and `prod_trlog_ibank.service_audit_old` tables with `par_dt` `20230530` on the other site. Finally, on the other site, after `distcp` was completed, we ran [upsert-to-hbase-migration](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\n6. Finally, after all the steps were completed on both sites, the developers manually deleted the old data in `kudu`.\n> Ndef: After all the above was completed, we should proceed with the consultation with the bank, to execute the `DWH` flows for the days that did not run due to the above issue. The `DWH` flow was removed, so we did not need to take any action.",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230531-IM2158906.md"
        }
    },
    "480": {
        "page_content": "---\ntitle: Clock Offset Alerts in Cloudera Manager Due to NTP Drift on Edge Nodes\ndescription: Clock offset alerts appeared in Cloudera Manager for nodes pr1edge01, pr1edge02, and dr1edge02, likely due to domain controller downtime; the issue was resolved by restarting the ntpd service.\ntags:\n  - mno\n  - bigstreamer\n  - clock offset\n  - cloudera manager\n  - ntpd\n  - time sync\n  - systemctl\n  - pr1edge01\n  - pr1edge02\n  - dr1edge02\n  - domain controller\n  - time server\n  - drift\n  - im2165544\n  - sd2228610\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2165544\n  related_issue: SD2228610\n  system: mno BigStreamer PR & DR Sites\n  root_cause: NTP time drift due to one domain controller being down for patching\n  resolution_summary: Restarted `ntpd` service on affected nodes, which cleared the alerts\n  affected_nodes:\n    - pr1edge01\n    - pr1edge02\n    - dr1edge02\n---\n# mno - BigStreamer - IM2165544 - Alert at Cloudera Manager (DR/PR)\n## Description\nClock Offset alerts were triggered in Cloudera Manager across multiple edge nodes due to NTP time drift. The issue was traced to a domain controller outage and resolved by restarting the ntpd service.\nThe following alert appeared in Cloudera Manager (PR) in Hosts:\n```\n-pr1edge01.mno.g\n-Clock Offset\n```\nFollowing ticket SD2228610, the following alerts also appeared in Hosts for Cloudera Manager DR and PR respectively:\n```\n-dr1edge02.mno.gr\n-Clock Offset\n```\n```\n-pr1edge02.mno.gr\n-Clock Offset\n```\n## Actions Taken\n1. Login to Cloudera Manager UI for both Sites\n2. We did not notice any other issues on either Site after checking services' charts and did not have any other alerts\n3. The clock offset relates to the ntpd service, so on the affected nodes (dr1edge02, pr1edge01, pr1edge02) we inspected the service's status:\n```bash\nsystemctl status ntpd\n```\nThe service was running.\n4. We restarted the service on the affected nodes\n```bash\nsystemctl restart ntpd\n```\n5. After a while the alert cleared up\n6. We guessed that there were actions at the time servers\n## Our Ticket Response\n```\n11/06/23 05:04:49 Europe/Eastern (MASTROKOSTA MARIA):\nWe have restarted the ntpd service on dr1edge02, pr1edge01 and pr1edge02 that displayed the clock offset alert. After the restart, the alert cleared.\n```\nDuring communication with the customer it was confirmed that one of the two domain controllers was out due to patches.\n## Affected Systems\nDR Site, PR Site",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230611-IM2165544.md"
        }
    },
    "481": {
        "page_content": "---\ntitle: DWH LoanPayment Export Failed Due to Duplicate Primary Key in SQL Server\ndescription: The `EXPORT` job for the `LOAN_PAYMENT` component in DWH_IBank failed due to duplicate IDs in the staging table, violating SQL Server's primary key constraint during Sqoop export; resolved by reconstructing the partition and keeping only the valid record.\ntags:\n  - mno\n  - bigstreamer\n  - dwh_ibank\n  - loan_payment\n  - export failure\n  - duplicate id\n  - sqoop\n  - sqoop export\n  - impala\n  - sql server\n  - primary key violation\n  - dwh_details_loan_payment\n  - staging table\n  - batch job\n  - impala shell\n  - yarn\n  - grafana\n  - im2070630\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2070630\n  system: mno BigStreamer DWH\n  root_cause: Two records with the same `id` and different timestamps were exported, violating SQL Server's unique constraint during Sqoop export\n  resolution_summary: A temporary table was created with deduplicated entries for `par_dt=20230125`, and the target table was overwritten; job was re-executed successfully\n  duplicate_id: 0E86AF89-F15C-4B78-8925-08ED8D237805\n  conflict_table: prod_trlog_ibank_analytical.dwh_details_loan_payment\n  export_script: sched_export_to_dwh.sh -t loanPayment\n  export_component: LOAN_PAYMENT\n  failure_code: 1\n---\n# mno - BigStreamer - IM2070630 - Failed batch Job on Grafana\n## Description\nThe `EXPORT` batch job for `DWH_IBank \u2192 LOAN_PAYMENT` failed on 26/01/2023 with code `1`, due to a `PRIMARY KEY` violation in the external SQL Server system.\n```\nApplication: DWH_IBank\nJob Name: EXPORT\nComponent: LOAN_PAYMENT\nStatus: Failed\nDescription: Code:1\nThanks\n```\n## Actions Taken\n### Root Cause Analysis\nAnalysis was performed in collaboration with @lmn and @iaravant\n1. Check MapReduce logs from YARN UI - App Name: PROD_IBank_DWH_EXPORT_LoanPaymentDetails_*\n![IM2070630_yarn_app](.media/IM2070630_yarn_app.png)\n![IM2070630_yarn_mapreduce](.media/IM2070630_yarn_mapreduce.png)\nThere was a duplicate entry in `prod_trlog_ibank_analytical.dwh_details_loan_payment_stg`:\n```\nDuplicate Key Value: 0E86AF89-F15C-4B78-8925-08ED8D237805\n```\n2. Check the tables from Impala Shell with PRODUSER\n```bash\n[PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\n```\nCheck for duplicates in dwh_details_loan_payment_stg and dwh_details_loan_payment with id=0E86AF89-F15C-4B78-8925-08ED8D237805\n![IM2070630_details_duplicates](.media/IM2070630_details_duplicates.PNG)\nCheck service_audit specifically for id=0E86AF89-F15C-4B78-8925-08ED8D237805\n![IM2070630_service_audit](.media/IM2070630_service_audit.PNG)\nCheck service_audit for duplicates\n![IM2070630_service_audit_monthly](.media/IM2070630_service_audit_monthly.PNG)\nService_audit contained two entries with the same id=0E86AF89-F15C-4B78-8925-08ED8D237805 and different timestamps.\n### Resolution\nSolution provided by @fgh and @adrint\n```bash\n[PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\n```\n```sql\n--- == DWH LoanPayment ==\n --Create table with original data\ncreate table prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig like prod_trlog_ibank_analytical.dwh_details_loan_payment;\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt=20230125;\n-- Create table and insert only required data\ncreate table prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp like prod_trlog_ibank_analytical.dwh_details_loan_payment;\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt = 20230125 and id != '0E86AF89-F15C-4B78-8925-08ED8D237805';\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt = 20230125 and id = '0E86AF89-F15C-4B78-8925-08ED8D237805' and tr_timestamp = '202301025 09:38:13.072489000' limit 1;\n-- Overwrite normal table with correct data\ninsert overwrite prod_trlog_ibank_analytical.dwh_details_loan_payment partition(par_dt) select * from prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp where par_dt = 20230125;\n```\n```bash\n# Run Export procedure \n/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\n```\n```bash\n[PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\n```\n```bash\n# Drop temporary table\ndrop prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp purge\n# TODO Check data and drop backup table with initial orginal data\ndrop prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig purge\n```\n## Our Ticket Response\n```text\nAfter investigation, it was determined that there is a problem with two different transactions that occurred on 25/01/2023 which were declared with the same id.\nThe external SQL server LoanPaymentDetails restricts the id field to be UNIQUE, hence the expected error displayed by sqoop-export.\nSQLException: Violation of PRIMARY KEY constraint 'PK_LoanPaymentDetails'.\nCannot insert duplicate key in object 'srcib.LoanPaymentDetails'.\nThe duplicate key value is (0e86af89-f15c-4b78-8925-08ed8d237805)\nThe id of the problematic transaction is 0E86AF89-F15C-4B78-8925-08ED8D237805 and the first transaction has a timestamp of 09:38:13.072489000 while the second one has a timestamp of 2023-01-25 2023-01-25 09:38:13.476066000.\nWe suggest deleting one of the two transactions so that we can proceed with the export of the loan payment data for the remaining transactions. The information for both transactions remains in the big data environment\nso it can be retrieved if needed later.\n```\n```text\nAfter communication via email, we proceeded to reconstruct the table, keeping only the record with timestamp 09:38:13.072489000 and reran the job. Please, investigate this on your part and take the necessary actions so that duplicate records are not sent to us.\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230126-IM2070630.md"
        }
    },
    "482": {
        "page_content": "---\ntitle: DWH_IBank LOAN_PAYMENT EXPORT Failed Due to SQL Server Timeout in Sqoop Eval\ndescription: The EXPORT batch job for LOAN_PAYMENT failed with code 6 because the `sqoop-eval` step timed out while connecting to SQL Server, preventing the export from starting; no MapReduce job was submitted.\ntags:\n  - mno\n  - bigstreamer\n  - dwh_ibank\n  - loan_payment\n  - export job\n  - sqoop\n  - sqoop-eval\n  - grafana\n  - code 6\n  - batch failure\n  - sqlserver\n  - network io exception\n  - connection timed out\n  - yarn\n  - impala\n  - im2074270\n  - monitoring\n  - rerun required\n  - sched_export\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  issue_id: IM2074270\n  system: mno BigStreamer DWH\n  root_cause: sqoop-eval failed due to SQL Server connection timeout, causing the batch job to exit with code 6 before export could be launched\n  resolution_summary: Job was not executed; customer instructed to rerun after confirming SQL Server availability\n  failure_stage: sqoop-eval\n  sqoop_export_status: not executed\n  yarn_job_submitted: false\n  affected_component: LOAN_PAYMENT\n---\n# mno - BigStreamer - IM2074270 - Failed Batch Job on Grafana\n## Description\nOn 30/01/2023, the EXPORT batch job for the `LOAN_PAYMENT` component of the `DWH_IBank` application failed with **Code 6**, as seen in Grafana.\n```\napplication : DWH_IBank\njob_name : EXPORT\ncomponent : LOAN_PAYMENT\ndate : 30-01-2023\nstatus : FAILED\ndescription : code 6\nhost : -\n```\n## Actions Taken\n1. Check Loan Payment - Export Status from Grafana\n```bash\nLOCAL MONITOR -> Batch Jobs DR -> DW Jobs\n```\nThe script excited with Code 6, which means that it timed-out, waiting for the Sqoop MapReduce job to be in running state.\n2. Check Impala Query Status\n```bash\nCloudera Manager -> Impala -> Queries -> statement RLIKE '.*details_loan_payment.*' -> Search\n```\n![IM2074270_impala_queries](.media/IM2074270_impala_queries.png)\nImpala query ran successfully.\n3. Check MapReduce job from YARN\n![IM2074270_yarn_apps](.media/IM2074270_yarn_apps.png)\nThere wasn't any job for Loan Payment,so it had never been submitted.\n4. Check export logs\n```bash\n#from dr1edge01\nless /var/log/datawarehouse-ibank/PRODUSER/sched_export.log\n```\n![IM2074270_export_logs](.media/IM2074270_export_logs.png)\nThere was a Connection Timed Out error from sqoop-eval which indicates a problem with the SQLServer.\n5. Check export script\n```bash\n#from dr1edge01\nless /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_to_dwh.sh\n```\n![IM2074270_export_script](.media/IM2074270_export_script.png)\nAs we can see in the image above, sqoop-export runs after sqoop-eval. In our case sqoop-export did not run because sqoop-eval exited with error.\n6. Inform Customer and ask for a rerun\n## Our Ticket Response\n```\nfrom the analysis of the logs we see that sqoop was not submitted because the evaluation for SQL Server was run first, which crashed with a Network I/O exception. (Connection Timed Out)\nPlease rerun the job.\nWe also see that the DWH started today at 12. Because at this time we have an increased chance of the SQL Server evaluation crashing as the traffic on it is increased, could you inform us why the execution of the DWH was delayed?\n```",
        "metadata": {
            "category": "issues",
            "client": "Client_mno",
            "name": "20230131-IM2074270.md"
        }
    }
}