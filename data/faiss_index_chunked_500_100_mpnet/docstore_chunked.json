{
    "0": {
        "page_content": "# Prometheus\n## Useful Links\n[Infrastructure](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/Infastructure)  \n[Monitoring](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/home#monitoring)  \n## Oozie workflow\n``` mermaid\n  graph TD\n    A[Oracle DB table DWSRC.DWH22  <br> via Port Forward 999.999.999.999:6634] -->|Sqoop Import| B[HDFS Staging Directory]\n    B -->|Hive Load| C[Hive: prometheus.dwh22]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 1"
        }
    },
    "1": {
        "page_content": "B -->|Hive Load| C[Hive: prometheus.dwh22]\n    C -->|Impala Refresh| D[Impala: prometheus.dwh22]\n```\nRuns every day at `06:30 AM UTC`\n**User**: `prometheus`  \n**Coordinator**: `Prometheus-Coordinator`  \n**Workflow**: `Prometheus-Import-Workflow`  \n**Source Database**:  \n- **Host**: `999.999.999.999`  \n- **Port**: `1521`  \n- **SID**: `A7`\n- **User**: `bigstreamer`  \n**Target Table**: `prometheus.dwh22`  \n**HDFS Installation Directory**: `/user/prometheus/flows`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 2"
        }
    },
    "2": {
        "page_content": "**Target Table**: `prometheus.dwh22`  \n**HDFS Installation Directory**: `/user/prometheus/flows`  \n**HDFS Staging Directory**: `/ez/warehouse/prometheus.db/tmp_sqoop_jobs/`\n**Alerts**:\n- Mail with subject: `Prometheus Flow failed`\n**Troubleshooting Steps**:\n- Check messages written to Monitoring App\n    - Check monitoring app for successful executions:  \n        - From `un2` with personal account:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 3"
        }
    },
    "3": {
        "page_content": "- Check monitoring app for successful executions:  \n        - From `un2` with personal account:\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'`\n \n    - Check monitoring app for failed executions:  \n  \n        - From `un2` with personal account:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 4"
        }
    },
    "4": {
        "page_content": "- Check monitoring app for failed executions:  \n  \n        - From `un2` with personal account:\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=FAILED&operativePartition=<timestamp e.g.:20220518>'`\n    - Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 5"
        }
    },
    "5": {
        "page_content": "- Grafana link: `https://unc1.bigdata.abc.gr:3000/d/PcKYyfTVz/prometheus-dashboard?orgId=1&from=now-2d&to=now`\n- Check if partition is loaded:\n  From `Hue` as `prometheus` in `Impala Editor`:\n  ``` sql\n  SHOW PARTITIONS prometheus.dwh22;\n  SELECT COUNT(*) FROM prometheus.dwh22 WHERE par_dt='<par_dt>';\n  ```\n- Check logs for failed steps:  \n  From `Hue` as `prometheus` in `Workflows`:\n  - Search for `Prometheus-Import-Workflow` and filter for failed",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 6"
        }
    },
    "6": {
        "page_content": "- Search for `Prometheus-Import-Workflow` and filter for failed\n  - Go to logs and check both stdout and stderr\n- In case a partition has partially been inserted into the final table `prometheus.dwh22` and the error that caused the failure has been resolved:\n  From `Hue` as `prometheus` in `Impala Editor`:\n    ``` sql\n    ALTER TABLE prometheus.dwh22 DROP IF EXISTS PARTITION (par_dt='<par_dt>');\n    ```\n  - For the previous day:\n    From `Hue` as `prometheus` in `Workflows`:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 7"
        }
    },
    "7": {
        "page_content": "```\n  - For the previous day:\n    From `Hue` as `prometheus` in `Workflows`:\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n  - For the previous day:\n    From `Hue` as `prometheus` in `Workflows`:\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n  - For partitions older than yesterday:\n    From `Hue` as `prometheus` in `File Browser`:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 8"
        }
    },
    "8": {
        "page_content": "- For partitions older than yesterday:\n    From `Hue` as `prometheus` in `File Browser`:\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and set `days_back` to the number of days back needed to reach the partition\n    From `Hue` as `prometheus` in `Workflows`:\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n    From `Hue` as `prometheus` in `File Browser`:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 9"
        }
    },
    "9": {
        "page_content": "- Re-run it\n    From `Hue` as `prometheus` in `File Browser`:\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and restore `days_back` to `1`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "prometheus.md - Part 10"
        }
    },
    "10": {
        "page_content": "# Brond ADSL/VDSL Flow\n## Installation info\n### Data Source File\n- Source system: FTP Server  \n  - host :`999.999.999.999`\n  - port :`22`\n  - protocol :`SFTP`\n  - user : `bigd`\n  - spool area : `/ADSL_Brond_DWH`\n  - file_type : `DWH_ADSL*.csv.gz` and `DWH_VDSL*.csv.gz`\n  - load_suffix : `LOADED`\n- Local FileSystem Directories\n\t- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n\t- landing_zone : `/data/1/brond_dsl_stats_LZ`\n\t- archive_dir= : `/data/1/brond_dsl_stats_LZ/archives`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 1"
        }
    },
    "11": {
        "page_content": "- archive_dir= : `/data/1/brond_dsl_stats_LZ/archives`\n\t- work_dir= : `/shared/abc/brond_dsl_stats/repo`\n- HDFS Directories\n\t- hdfs_Bin : `/user/brond`\n\t- hdfs_dir : `/ez/warehouse/brond.db/landing_zone/brond_dsl_stats`\n\t- hdfs_pending_dir : `/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/not_loaded`\n\t- hdfs_stats_dir : `/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/stats`\n### Scripts-Configuration Location\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 2"
        }
    },
    "12": {
        "page_content": "- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- scripts path : `/shared/abc/brond_dsl_stats/DataParser/scripts`\n-\tconfigurations path : `/shared/abc/brond_dsl_stats/DataParser/scripts/transferlist/*.trn` (i.e. brond_retrains.trn)\n### Logs Location\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- path : `/shared/abc/brond_dsl_stats/DataParser/scripts/log`\n- log file: `002.Brond_xDSL_Load.<YYYYMMDD>.log`\n### Oozie Scheduling\n- user : `brond`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 3"
        }
    },
    "13": {
        "page_content": "- log file: `002.Brond_xDSL_Load.<YYYYMMDD>.log`\n### Oozie Scheduling\n- user : `brond`\n- Coordinator :`Brond_Load_xDSL_Coord_NEW`  \n\truns at : `04:00, 05:00, 06:00, 10:00 UTC`\n- Workflow : `Brond_Load_xDSL_WF_NEW`  \n- Main script : `HDFS:/user/brond/000.Brond_xDSL_Oozie_Main.sh`\n- SSH Identity file : `HDFS:/user/brond/id_rsa`\nNdef: **Main Script** runs `oozie_brond_xdsl.sh` located on `un-vip.bigdata.abc.gr` using **ssh** as user **brond**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 4"
        }
    },
    "14": {
        "page_content": "`$ ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa brond@un-vip.bigdata.abc.gr \"/shared/abc/brond_dsl_stats/DataParser/scripts/oozie_brond_xdsl.sh\"`\n### Hive Tables\n- Target Database: `brond`\n- Staging Tables: `brond.brond_adsl_stats_daily_stg, brond.brond_vdsl_stats_daily_stg`\n- Target Tables: `brond.brond_adsl_stats_daily, brond.brond_vdsl_stats_daily`\n### Beeline-Impala Shell commands",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 5"
        }
    },
    "15": {
        "page_content": "### Beeline-Impala Shell commands\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/default;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala-shell: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k`\n## Data process\n1. sftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 6"
        }
    },
    "16": {
        "page_content": "```\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond_DWH\n-rw-r--r--    0 507      500      35399779 Nov 27 06:19 ADSL_Brond_DWH/DWH_ADSL.327_2022_11_27.csv.gz.LOADED\n-rw-r--r--    0 507      500      35440542 Nov 28 06:57 ADSL_Brond_DWH/DWH_ADSL.328_2022_11_28.csv.gz.LOADED\n-rw-r--r--    0 507      500      35360378 Nov 29 06:20 ADSL_Brond_DWH/DWH_ADSL.329_2022_11_29.csv.gz.LOADED",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 7"
        }
    },
    "17": {
        "page_content": "-rw-r--r--    0 507      500      35415258 Nov 30 06:48 ADSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz\n-rw-r--r--    0 507      500      150757798 Nov 27 05:33 ADSL_Brond_DWH/DWH_VDSL.327_2022_11_27.csv.gz.LOADED\n-rw-r--r--    0 507      500      150728306 Nov 28 06:26 ADSL_Brond_DWH/DWH_VDSL.328_2022_11_28.csv.gz.LOADED\n-rw-r--r--    0 507      500      150589497 Nov 29 05:34 ADSL_Brond_DWH/DWH_VDSL.329_2022_11_29.csv.gz.LOADED",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 8"
        }
    },
    "18": {
        "page_content": "-rw-r--r--    0 507      500      150823890 Nov 30 06:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_30.csv.gz\n```\n2. rename the raw file(s) in remdef SFTP server by adding the suffix .LOADED\n\t`echo \"rename /ADSL_Brond_DWH/ADSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz /ADSL_Brond_DWH/ADSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz.LOADED\" | sftp -oport=22 bigd@999.999.999.999`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 9"
        }
    },
    "19": {
        "page_content": "`echo \"rename /ADSL_Brond_DWH/VDSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz /ADSL_Brond_DWH/VDSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz.LOADED\" | sftp -oport=22 bigd@999.999.999.999`\n3. parsing raw files in `/data/1/brond_dsl_stats_LZ`\n\t- removes the headers (1st line)\n\t- removes double-qudefs chars\n\t- defines the PAR_DT value from the filename (i.e. DWH_ADSL.330_2022_11_30.csv.gz convert to 20221130)\n\t- add the prefix `HDFS___` to raw file\n\t- add the suffix `<load time>` to raw file",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 10"
        }
    },
    "20": {
        "page_content": "- add the prefix `HDFS___` to raw file\n\t- add the suffix `<load time>` to raw file  \n\t\tLoad time format:`<YYYYMMDD_HHMISS>`  \n\t\ti.e. `HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz`\n4. put raw files into HDFS landingzone\n\t```\n\thdfs dfs -put /data/1/brond_dsl_stats_LZ/HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz /ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 11"
        }
    },
    "21": {
        "page_content": "hdfs dfs -put /data/1/brond_dsl_stats_LZ/HDFS___DWH_VDSL.330_2022_11_30.csv.gz.20221201_060005.gz /ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_VDSL.330_2022_11_30.csv.gz.20221201_060005.gz`\n\t```\n5. clean-up any copy of the raw files from local filesystem  \n\t`/data/1/brond_dsl_stats_LZ`  \n\t`/shared/abc/brond_dsl_stats/repo`  \n6. load HDFS files into hive staging tables  \n\t`brond.brond_adsl_stats_daily_stg` and `brond.brond_vdsl_stats_daily_stg`  \n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 12"
        }
    },
    "22": {
        "page_content": "`brond.brond_adsl_stats_daily_stg` and `brond.brond_vdsl_stats_daily_stg`  \n\t```\n\tbeeline -e \"LOAD DATA INPATH '/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz' OVERWRITE INTO TABLE brond.brond_adsl_stats_daily_stg PARTITION (par_dt='20221130')\"",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 13"
        }
    },
    "23": {
        "page_content": "beeline -e \"LOAD DATA INPATH '/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_VDSL.330_2022_11_30.csv.gz.20221201_060005.gz' OVERWRITE INTO TABLE brond.brond_vdsl_stats_daily_stg PARTITION (par_dt='20221130')\"\n\t```\n\t*Ndef: Once the load completed, the staging tables should contain no data.*\n\t\n7. update hive tables with filtered columns  \n\tscript: `/shared/abc/brond_dsl_stats/DataParser/scripts/003.Brond_xDSL_Post.sh`\n\t\n\t- `brond_adsl_stats_daily`\n\t\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 14"
        }
    },
    "24": {
        "page_content": "- `brond_adsl_stats_daily`\n\t\t```\n\t\tset hive.exec.dynamic.partition.mode=nonstrict;\n\t\tinsert overwrite table brond.brond_adsl_stats_daily partition (par_dt) \n\t\tselect \n\t\t\tserv_tel,\n\t\t\tserv_siid,\n\t\t\tne_name,\n\t\t\tne_port,\n\t\t\tcard_tehn,\n\t\t\tcard_type,\n\t\t\tinv_port,\n\t\t\tmeasure_date,\n\t\t\tlast_change,\n\t\t\taif_adm,\n\t\t\taif_oper,\n\t\t\topmod_annex,\n\t\t\tup_sign_attn,\n\t\t\tup_snr,\n\t\t\tup_crt_rate,\n\t\t\tup_max_rate,\n\t\t\tdn_sign_attn,\n\t\t\tdn_snr,\n\t\t\tdn_crt_rate,\n\t\t\tdn_max_rate,\n\t\t\tprf_name,\n\t\t\tradius,\n\t\t\tsproto,\n\t\t\tpar_dt",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 15"
        }
    },
    "25": {
        "page_content": "dn_snr,\n\t\t\tdn_crt_rate,\n\t\t\tdn_max_rate,\n\t\t\tprf_name,\n\t\t\tradius,\n\t\t\tsproto,\n\t\t\tpar_dt\n\t\tfrom brond.brond_adsl_stats_daily_stg \n\t\twhere 1=1\n\t\t;\n\t\t```\n\t- `brond_vdsl_stats_daily`\n\t\t```\n\t\tset hive.exec.dynamic.partition.mode=nonstrict;\n\t\tinsert overwrite table brond.brond_vdsl_stats_daily partition (par_dt) \n\t\tselect \n\t\t\tserv_tel,\n\t\t\tserv_siid,\n\t\t\tne_name,\n\t\t\tne_port,\n\t\t\tcard_tehn,\n\t\t\tcard_type,\n\t\t\tinv_port,\n\t\t\tmeasure_date,\n\t\t\tcustid,\n\t\t\tlast_change,\n\t\t\taif_adm,\n\t\t\taif_oper,\n\t\t\topmod_annex,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 16"
        }
    },
    "26": {
        "page_content": "inv_port,\n\t\t\tmeasure_date,\n\t\t\tcustid,\n\t\t\tlast_change,\n\t\t\taif_adm,\n\t\t\taif_oper,\n\t\t\topmod_annex,\n\t\t\tup_sign_attn,\n\t\t\tup_snr,\n\t\t\tup_max_rate,\n\t\t\tup_crt_rate,\n\t\t\tdn_sign_attn,\n\t\t\tdn_snr,\n\t\t\tdn_max_rate,\n\t\t\tdn_crt_rate,\n\t\t\tprf_name,\n\t\t\tradius,\n\t\t\tsproto,\n\t\t\tpar_dt\n\t\tfrom brond.brond_vdsl_stats_daily_stg \n\t\twhere 1=1\n\t\t;\n\t\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 17"
        }
    },
    "27": {
        "page_content": "radius,\n\t\t\tsproto,\n\t\t\tpar_dt\n\t\tfrom brond.brond_vdsl_stats_daily_stg \n\t\twhere 1=1\n\t\t;\n\t\t```\nIn the event where multiple files are transfered (files refer to the same data), we proceed with overwriting data in brond table. Deletion of those multiple files is abc's responsibility.\n## Monitoring\n### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 18"
        }
    },
    "28": {
        "page_content": "|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \nConnection command: `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n### Monitoring Message list\nFor each type of load (ADSL or VDSL) the following set of messages will be recorded in the Monitoring database.\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 19"
        }
    },
    "29": {
        "page_content": "```\nid    | execution_id | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n------+--------------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 20"
        }
    },
    "30": {
        "page_content": "15675 | 1659931204   | BROND       | BROND_ADSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:00:04 | 2022-08-08 07:01:38 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n15677 | 1659931204   | BROND       | BROND_ADSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:00:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 21"
        }
    },
    "31": {
        "page_content": "15679 | 1659931204   | BROND       | BROND_ADSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n15681 | 1659931204   | BROND       | BROND_ADSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 22"
        }
    },
    "32": {
        "page_content": "15683 | 1659931204   | BROND       | BROND_ADSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr\n15685 | 1659931204   | BROND       | BROND_ADSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 23"
        }
    },
    "33": {
        "page_content": "15687 | 1659931204   | BROND       | BROND_ADSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:00:26 |                     |                        | brond | un2.bigdata.abc.gr\n15689 | 1659931204   | BROND       | BROND_ADSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:01:38 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 24"
        }
    },
    "34": {
        "page_content": "id    | execution_id | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n------+--------------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 25"
        }
    },
    "35": {
        "page_content": "15691 | 1659931204   | BROND       | BROND_VDSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:01:38 | 2022-08-08 07:03:51 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n15693 | 1659931204   | BROND       | BROND_VDSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:01:40 |                     | Single raw file found  | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 26"
        }
    },
    "36": {
        "page_content": "15695 | 1659931204   | BROND       | BROND_VDSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n15697 | 1659931204   | BROND       | BROND_VDSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 27"
        }
    },
    "37": {
        "page_content": "15699 | 1659931204   | BROND       | BROND_VDSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr\n15701 | 1659931204   | BROND       | BROND_VDSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 28"
        }
    },
    "38": {
        "page_content": "15703 | 1659931204   | BROND       | BROND_VDSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:02:05 |                     |                        | brond | un2.bigdata.abc.gr\n15705 | 1659931204   | BROND       | BROND_VDSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:03:51 |                     |                        | brond | un2.bigdata.abc.gr\n```\n### Monitoring Component list",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 29"
        }
    },
    "39": {
        "page_content": "```\n### Monitoring Component list\n|Component | Description \n|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED| \n|GET RAW XDSL FILES|sftp-get the raw files from the remdef server.<br />i.e.<br />DWH_ADSL.197_2022_07_18.csv.gz<br />DWH_VDSL.197_2022_07_18.csv.gz\n|RENAME FILES @SFTP SERVER| Rename the raw files in remdef server by adding the suffix .LOADED<br />i.e.<br />DWH_ADSL.197_2022_07_18.csv.gz.LOADED<br />DWH_VDSL.197_2022_07_18.csv.gz.LOADED",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 30"
        }
    },
    "40": {
        "page_content": "|PARSING FILES| removes any control chars (if any) from the raw files\n|LOAD HDFS LANDINGZONE|PUT the parsing files into HDFS\n|CLEAN-UP THE INPUT FILES|Clean-up any copy of the raw files from the filesystem\n|LOAD HDFS FILES INTO HIVE STAGING TABLES| Load raw data (files) into the staging tables<br />`brond.brond_adsl_stats_daily_stg`<br />`brond.brond_vdsl_stats_daily_stg`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 31"
        }
    },
    "41": {
        "page_content": "|UPDATE HIVE TABLES WITH FILTERED COLUMNS| Update final tables with the necessary columns only.<br />`brond.brond_adsl_stats_daily`<br />`brond.brond_vdsl_stats_daily`\n### Monitoring database Queries\n- List messages of the last load  \n\t`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n```\nselect \n  execution_id, id, application, job, component, operative_partition,  \n  status, system_ts, system_ts_end, message, user,host   \nfrom jobstatus a where upper(job) like 'BROND__DSL%'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 32"
        }
    },
    "42": {
        "page_content": "from jobstatus a where upper(job) like 'BROND__DSL%'   \nand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'BROND__DSL%')  \n;\nexecution_id | id    | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 33"
        }
    },
    "43": {
        "page_content": "-------------+-------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n1659931204   | 15675 | BROND       | BROND_ADSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:00:04 | 2022-08-08 07:01:38 | Succesfully Completed. | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 34"
        }
    },
    "44": {
        "page_content": "1659931204   | 15677 | BROND       | BROND_ADSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:00:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n1659931204   | 15679 | BROND       | BROND_ADSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 35"
        }
    },
    "45": {
        "page_content": "1659931204   | 15681 | BROND       | BROND_ADSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15683 | BROND       | BROND_ADSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 36"
        }
    },
    "46": {
        "page_content": "1659931204   | 15685 | BROND       | BROND_ADSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15687 | BROND       | BROND_ADSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:00:26 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 37"
        }
    },
    "47": {
        "page_content": "1659931204   | 15689 | BROND       | BROND_ADSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:01:38 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15691 | BROND       | BROND_VDSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:01:38 | 2022-08-08 07:03:51 | Succesfully Completed. | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 38"
        }
    },
    "48": {
        "page_content": "1659931204   | 15693 | BROND       | BROND_VDSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:01:40 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n1659931204   | 15695 | BROND       | BROND_VDSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 39"
        }
    },
    "49": {
        "page_content": "1659931204   | 15697 | BROND       | BROND_VDSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15699 | BROND       | BROND_VDSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 40"
        }
    },
    "50": {
        "page_content": "1659931204   | 15701 | BROND       | BROND_VDSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15703 | BROND       | BROND_VDSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:02:05 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 41"
        }
    },
    "51": {
        "page_content": "1659931204   | 15705 | BROND       | BROND_VDSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:03:51 |                     |                        | brond | un2.bigdata.abc.gr\n-------------+-------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n```\n### Monitoring Health-Check",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 42"
        }
    },
    "52": {
        "page_content": "```\n### Monitoring Health-Check\n  - Check Monitoring status.  \n\t```\t\n\t$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\n\t\n\t{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 43"
        }
    },
    "53": {
        "page_content": "{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n\t```  \n\t\n\t- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\n- An email will be sent by the system with the point of failure.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 44"
        }
    },
    "54": {
        "page_content": "## Troubleshooting\n- An email will be sent by the system with the point of failure.\n- Check the log file for errors/exceptions  \n\t`egrep -i 'error|fail|exception|problem' /shared/abc/brond_dsl_stats/DataParser/scripts/log/002.Brond_xDSL_Load.YYYYMMDD.log`\n- List Failed Monitoring messages of the last load\n\t```\n\t/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n\tselect * from jobstatus where upper(job) like 'BROND__DSL' \n\tand status='FAILED'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 45"
        }
    },
    "55": {
        "page_content": "select * from jobstatus where upper(job) like 'BROND__DSL' \n\tand status='FAILED'\n\tand operative_partition=(select max(operative_partition) from jobstatus where upper(job) like 'BROND__DSL' and operative_partition regexp '[0-9]{8}')\n\torder by id\n\t;\n\texecution_id | id    | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 46"
        }
    },
    "56": {
        "page_content": "-------------+-------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n\t1659946615   | 15825 | BROND       | BROND_ADSL_STATS | MAIN                                     | 20220808            | FAILED  | 2022-08-08 11:16:55 | 2022-08-08 11:16:55 | No raw files found     | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 47"
        }
    },
    "57": {
        "page_content": "1659946615   | 15827 | BROND       | BROND_ADSL_STATS | GET RAW XDSL FILES                       | 20220808            | FAILED  | 2022-08-08 11:16:55 |                     | No raw files found     | brond | un2.bigdata.abc.gr\n\t1659946615   | 15829 | BROND       | BROND_VDSL_STATS | MAIN                                     | 20220808            | FAILED  | 2022-08-08 11:16:56 | 2022-08-08 11:16:56 | No raw files found     | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 48"
        }
    },
    "58": {
        "page_content": "1659946615   | 15831 | BROND       | BROND_VDSL_STATS | GET RAW XDSL FILES                       | 20220808            | FAILED  | 2022-08-08 11:16:56 |                     | No raw files found     | brond | un2.bigdata.abc.gr\n\t```\n### Common errors  \n- `No raw files found`, there are no raw files available for loading at remdef server.  \nA WARNING message will be sent to abc by email.\nNo actions required from OBSS. Responsible abc. \n- Other factors not related to the specific flow",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 49"
        }
    },
    "59": {
        "page_content": "No actions required from OBSS. Responsible abc. \n- Other factors not related to the specific flow\n\t- impala/hive availability\n\t- Kerberos authentication (A.  \n\t*Ndef: The flow checks if the ticket is still active before any HDFS action.  \n\tIn case of expiration the flow performs a `kinit` command*\n## Manually triggering the workflow\nThere are cases where abc might upload new files after the scheduled workflow timing, and request these files to be",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 50"
        }
    },
    "60": {
        "page_content": "processed in the same day. This can only be done by manually triggering the workflow. Before doing so, you need to make some checks first:\n### Check workflow logs\n1. Login to https://999.999.999.999:8888/hue/accounts/login?next=/hue using the brond account\n2. Go to \"Jobs\" > \"Workflows\"\nIf all workflow executions (\"Brond_Load_xDSL_WF_NEW\") were successful, you can proceed by checking that the file(s)\nabc added, were copied after the scheduled timings of the workflow\n### Check added files",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 51"
        }
    },
    "61": {
        "page_content": "abc added, were copied after the scheduled timings of the workflow\n### Check added files\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond\n-rw-r--r-- 0 507 500 21925249 Nov 29 06:22 ADSL_Brond/Counter_Collection_24H.329_2022_11_29.csv.gz.LOADED\n-rw-r--r-- 0 507 500 22107252 Nov 29 13:52 ADSL_Brond/Counter_Collection_24H.330_2022_11_29.csv.gz\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 52"
        }
    },
    "62": {
        "page_content": "```\nThe second file was indeed added after the scheduled time and has not been picked up by the workflow.\n### Trigger workflow\nYou can now proceed to manually trigger the workflow:\n1. Go to HUE and select \"Jobs\"\n2. Go to \"Workflow\" and select \"Brond_Load_xDSL_WF_NEW\"\n3. In the next screen, select \"Rerun\"\n4. Wait for the workflow to successfully end\n5. If no errors occur, proceed with:\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 53"
        }
    },
    "63": {
        "page_content": "sftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond\n-rw-r--r-- 0 507 500 21925249 Nov 29 06:22 ADSL_Brond/Counter_Collection_24H.329_2022_11_29.csv.gz.LOADED\n-rw-r--r-- 0 507 500 22107252 Nov 29 13:52 ADSL_Brond/Counter_Collection_24H.330_2022_11_29.csv.gz.LOADED\n```\n## Data Check\n- **Check final tables for new partitions**:\n  - Impala-shell: \n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 54"
        }
    },
    "64": {
        "page_content": "```\n## Data Check\n- **Check final tables for new partitions**:\n  - Impala-shell: \n\t```\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\n\trefresh brond.brond_adsl_stats_daily;  \n\tshow partitions brond.brond_adsl_stats_daily;  \n\t\n\tpar_dt   | #Rows  | #Files | Size     | Bytes Cached | Cache Replication | Format | Incremental stats | Location",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 55"
        }
    },
    "65": {
        "page_content": "---------+--------+--------+----------+--------------+-------------------+--------+-------------------+---------------------------------------------------------------------------------\n\t20221130 | 629397 |      1 | 155.09MB | NOT CACHED   | NOT CACHED        | TEXT   | false             | hdfs://nameservice1/ez/warehouse/brond.db/brond_adsl_stats_daily/par_dt=20220808",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 56"
        }
    },
    "66": {
        "page_content": "Total    |     -1 |      1 | 155.09MB | 0B           |                   |        |                   |                                                                                 \n\trefresh brond.brond_vdsl_stats_daily;\n\tshow partitions brond.brond_vdsl_stats_daily\n\t\n\tpar_dt   | #Rows   | #Files | Size     | Bytes Cached | Cache Replication | Format | Incremental stats | Location",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 57"
        }
    },
    "67": {
        "page_content": "---------+---------+--------+----------+--------------+-------------------+--------+-------------------+---------------------------------------------------------------------------------\n\t20221130 | 2157413 |      1 | 588.26MB | NOT CACHED   | NOT CACHED        | TEXT   | false             | hdfs://nameservice1/ez/warehouse/brond.db/brond_vdsl_stats_daily/par_dt=20220808",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 58"
        }
    },
    "68": {
        "page_content": "Total    |      -1 |      1 | 588.26MB | 0B           |                   |        |                   |                                                                                 \n\t```\n- **Check the amount of data in final tables**:\n  - Impala-shell: \n\t```\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\n\tSELECT par_dt, count(*) as cnt from brond.brond_adsl_stats_daily group by par_dt order by 1;\n\tpar_dt   | cnt   \n\t---------+-------\n\t20221130 | 629397",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 59"
        }
    },
    "69": {
        "page_content": "par_dt   | cnt   \n\t---------+-------\n\t20221130 | 629397\n\tSELECT par_dt, count(*) as cnt from brond.brond_vdsl_stats_daily group by par_dt order by 1;\n\tpar_dt   | cnt    \n\t---------+--------\n\t20221130 | 2157413\n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_xDSL_Stats_Flow.md - Part 60"
        }
    },
    "70": {
        "page_content": "# def_NETWORK_MAP Flow (OneTicket)\n## Installation info\n### Data Source Tables\n- Source system: Oracle Database 11g Enterprise Edition (999.999.999.999.0)  \n\t- Server:`999.999.999.999:1521`\n\t- Port Forward:`999.999.999.999:1521`\n\t- User:`def_network_map`\n\t- SID:`defsblf_rw`\n\t- Oracle Tables: \n\t\t- def_NETWORK_MAP.ACTIVITY\n\t\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t\t- def_NETWORK_MAP.OPEN_MW\n\t\t- def_NETWORK_MAP.OPEN_NTT",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 1"
        }
    },
    "71": {
        "page_content": "- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t\t- def_NETWORK_MAP.OPEN_MW\n\t\t- def_NETWORK_MAP.OPEN_NTT\n\t\t- def_NETWORK_MAP.OPEN_OCT\n\t\t- def_NETWORK_MAP.OPEN_WTT\n\t\t- def_NETWORK_MAP.EXPORT_CTL\n\t\n### Scripts-Configuration Location in HDFS\n- user : `def_network_maps`\n- scripts path : `HDFS:/user/def_network_maps/`\n-\tconfiguration path : `HDFS:/user/def_network_maps/`\n\t- `oneTicket_env.sh`, The environment of the flow",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 2"
        }
    },
    "72": {
        "page_content": "- `oneTicket_env.sh`, The environment of the flow\n\t- `OraExpData.tables`, List of tables which are going to be exported/imported from Oracle to Hive\n\t- `monitoring.config`, The `Monitoring` connection details\n\t- `oraclecmd.config`, The Oracle connection details\n\t- `oneticket.keystore`, The Oracle password file\n- Temp dir : `HDFS:/ez/landingzone/tmp/oneTicket`\n### Export Data Location\n- node : Dynamically defined by the Oozie service  \n\ti.e. `sn95.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 3"
        }
    },
    "73": {
        "page_content": "- node : Dynamically defined by the Oozie service  \n\ti.e. `sn95.bigdata.abc.gr`\n- Directory : Dynamically defined by the Oozie service\n\ti.e. `/data/2/yarn/nm/usercache/def_network_maps/appcache/application_1668434520231_277391/container_e276_1668434520231_277391_01_000001`\n### Logs Location\n- user : `def_network_maps`\n- logs path : `/user/def_network_maps/log`\n- log files: \n\t- `101.OneTicket_OraMetaData.<YYYYMM>.log`\n\t- `102.OneTicket_OraData_CTRL.<YYYYMM>.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 4"
        }
    },
    "74": {
        "page_content": "- `101.OneTicket_OraMetaData.<YYYYMM>.log`\n\t- `102.OneTicket_OraData_CTRL.<YYYYMM>.log`\n\t- `103.OneTicket_OraData_Export_Import.<TABLE_NAME>.<UNIX-TIME>.log`\n\t- `104.OneTicket_OraData_Import_Hive.<UNIX-TIME>.log`\n\t`<UNIX-TIME>` is the timestamp of the load in unix-epoch format  \n\t`<TABLE_NAME>` list of values:  \n\t- def_NETWORK_MAP.ACTIVITY\n\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t- def_NETWORK_MAP.OPEN_MW",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 5"
        }
    },
    "75": {
        "page_content": "- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t- def_NETWORK_MAP.OPEN_MW\n\t- def_NETWORK_MAP.OPEN_NTT\n\t- def_NETWORK_MAP.OPEN_OCT\n\t- def_NETWORK_MAP.OPEN_WTT\n\ti.e.  \n\t```\n\t$ hdfs dfs -ls -t -r /user/def_network_maps/log\n\t\n\t101.OneTicket_OraMetaData.202302.log\n\t102.OneTicket_OraData_CTRL.202302.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.ACTIVITY.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.AFFECTED_CUSTOMERS.1675939511.log",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 6"
        }
    },
    "76": {
        "page_content": "103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.AFFECTED_CUSTOMERS.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.AFFECTED_OCT_WTT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_MW.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_NTT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_OCT.1675939511.log",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 7"
        }
    },
    "77": {
        "page_content": "103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_OCT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_WTT.1675939511.log\n\t104.OneTicket_OraData_Import_Hive.1675939511.log\t\n\t```\n### Oozie Scheduling\n- user : def_network_maps\n- Coordinator :`def_NETWORK_MAP_Coordinator`  \n\truns at : every 5 minutes on a Daily basis  \n\t\t`0,5,10,15,20,25,30,35,40,45,50,55 * * * *` \n- Workflow : `def_NETWORK_MAP_Workflow`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 8"
        }
    },
    "78": {
        "page_content": "`0,5,10,15,20,25,30,35,40,45,50,55 * * * *` \n- Workflow : `def_NETWORK_MAP_Workflow`  \n\tBash script : `HDFS:/user/def_network_maps/100.OneTicket_Main.sh`\n### Hive Tables\n- Target Database: `def_network_map`\n- Target Tables: \n\t- def_NETWORK_MAP.ACTIVITY\n\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t- def_NETWORK_MAP.OPEN_MW\n\t- def_NETWORK_MAP.OPEN_NTT\n\t- def_NETWORK_MAP.OPEN_OCT\n\t- def_NETWORK_MAP.OPEN_WTT",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 9"
        }
    },
    "79": {
        "page_content": "- def_NETWORK_MAP.OPEN_NTT\n\t- def_NETWORK_MAP.OPEN_OCT\n\t- def_NETWORK_MAP.OPEN_WTT\n### Database CLI commands\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/def_network_map;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr -d def_network_map --ssl -k`\n- Oracle*: `sqlplus -s def_network_map/<PASSWORD>@999.999.999.999:1521/defsblf_rw`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 10"
        }
    },
    "80": {
        "page_content": "- Oracle*: `sqlplus -s def_network_map/<PASSWORD>@999.999.999.999:1521/defsblf_rw`\n- MySql*: `mysql -u monitoring -p -h 999.999.999.999 monitoring`\n*\\*The passwords for the Oracle and MySql databases can be found [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)*\n## Data process\n### In General\nThe flow consist of two basic procedures and one control Oracle table.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 11"
        }
    },
    "81": {
        "page_content": "### In General\nThe flow consist of two basic procedures and one control Oracle table.  \n\t- the **Export** procedure, which is running at the remdef Oracle server (Responsible def/abc),  \n\t- the **Import** procedure, which is running at the BigStreamer cluster,  \n\t- the `def_NETWORK_MAP.EXPORT_CTL` table, used to synchronize the **Export** procedure with the **Import** procedure.  \nThe data in `def_NETWORK_MAP.EXPORT_CTL` is similar to the following",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 12"
        }
    },
    "82": {
        "page_content": "The data in `def_NETWORK_MAP.EXPORT_CTL` is similar to the following\nConnect to Oracle (see [Database CLI commands](#database-cli-commands))  \n```\nselect * from def_NETWORK_MAP.EXPORT_CTL;  \nEXPORT_SEQUENCE | TARGET                 | EXPORT_START_DT     | EXPORT_END_DT       | ROW_COUNT | IMPORT_START_DT     | IMPORT_END_DT      \n----------------+------------------------+---------------------+---------------------+-----------+---------------------+--------------------",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 13"
        }
    },
    "83": {
        "page_content": "0 | TOTAL                  | 2022-11-15 16:50:01 | 2022-11-15 17:07:09 |           | 2022-11-22 09:28:24 | 2022-11-22 09:29:12\n              5 | ACTIVITY               | 2022-11-15 16:51:27 | 2022-11-15 17:04:36 |     73211 |                     |                    \n              6 | AFFECTED_CUSTOMERS     | 2022-11-15 17:04:36 | 2022-11-15 17:04:54 |     14438 |                     |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 14"
        }
    },
    "84": {
        "page_content": "7 | OPEN_OCT               | 2022-11-15 17:04:54 | 2022-11-15 17:07:05 |     58338 |                     |                    \n              8 | OPEN_WTT               | 2022-11-15 17:07:05 | 2022-11-15 17:07:09 |      3690 |                     |                    \n              1 | OPEN_MW                | 2022-11-15 17:10:05 | 2022-11-15 17:10:42 |       249 |                     |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 15"
        }
    },
    "85": {
        "page_content": "2 | OPEN_NTT               | 2022-11-15 17:10:42 | 2022-11-15 17:11:03 |      6957 |                     |                    \n              3 | AFFECTED_OCT_WTT       | 2022-11-15 17:11:03 | 2022-11-15 17:11:20 |      1782 |                     |                    \n              4 | DEFECTIVE_NETW_ELEMENT | 2022-11-15 17:11:20 | 2022-11-15 17:11:21 |      6236 |                     |                    \n```\n  Ndef: We are interesting for the 1st row only `EXPORT_SEQUENCE=0`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 16"
        }
    },
    "86": {
        "page_content": "```\n  Ndef: We are interesting for the 1st row only `EXPORT_SEQUENCE=0`  \n### Phased\n1. The **Export** procedure is implemented by def/abc.  \n\tIt is responsible to prepare the data in Oracle tables (see Oracle Table list in [Data Source Tables](#data-source-tables))  \n\tOnce completed, it updates the `def_NETWORK_MAP.EXPORT_CTL.EXPORT_START_DT` column with the current system's timestamp.   \n\tNdef: It is not known how often the **Export** procedure runs.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 17"
        }
    },
    "87": {
        "page_content": "Ndef: It is not known how often the **Export** procedure runs.\n2.  The **Import** procedure is implemented by jkl.  \n\tIt checks periodically if the value of `def_NETWORK_MAP.EXPORT_CTL.EXPORT_START_DT` has been updated.  \n\t- In case of new value, the procedure exports the data from the Oracle tables  \n\t`./oracle_cmd.sh \"select * from <table>\" > ./<table>.exp`  \n\t- stores them into HDFS  \n\t`hdfs dfs -moveFromLocal ./<table>.exp .`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 18"
        }
    },
    "88": {
        "page_content": "- stores them into HDFS  \n\t`hdfs dfs -moveFromLocal ./<table>.exp .`\n\t- and, consequently, load them into the corresponding [Hive Tables](#hive-tables).  \n\t*Connect to Beeline (see [Database CLI commands](#database-cli-commands))*\n\t`Beeline <connection> -e \"load data inpath './<table>.exp' overwrite into table <table>;\"`\n\t- Once the Import procedure completed, the `IMPORT_START_DT` column will be updated with the current system's timestamp.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 19"
        }
    },
    "89": {
        "page_content": "*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*\n\t`update def_NETWORK_MAP.EXPORT_CTL.EXPORT_START_DT`\n\t- Compute table statistics using impala-shell\t \n\t*Connect to Impala (see [Database CLI commands](#database-cli-commands))*\n\t`compute incremental stats brond.brond_retrains_hist;`\n3. In case the `EXPORT_START_DT` value isn't updated, the **Import** procedure exists doing nothing.\n## Monitoring\n### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 20"
        }
    },
    "90": {
        "page_content": "## Monitoring\n### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \nConnection command: `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n### Monitoring Message list\nFor each load for each TABLE the following set of messages will be recorded in the Monitoring database.\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 21"
        }
    },
    "91": {
        "page_content": "```\nexecution_id | id     | application | job             | component                 | operative_partition | status  | system_ts           | system_ts_end       | param0 | message                                                              | user             | host",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 22"
        }
    },
    "92": {
        "page_content": "-------------+--------+-------------+-----------------+---------------------------+---------------------+---------+---------------------+---------------------+--------+----------------------------------------------------------------------+------------------+-------------------------",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 23"
        }
    },
    "93": {
        "page_content": "1670509202   | 402171 | ONETICKET   | def_NETWORK_MAP | MAIN                      | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 | 2022-12-08 16:22:05 |        | Procedure Started                                                    | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 24"
        }
    },
    "94": {
        "page_content": "1670509202   | 402173 | ONETICKET   | def_NETWORK_MAP | CHECK_FOR_AVAILABLE_DATA  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 |                     |        | New data found: 2022-12-08 15:50:04                                  | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 25"
        }
    },
    "95": {
        "page_content": "1670509202   | 402207 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-<TABLE-NAME>  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     | 6987   | Oracle export def_NETWORK_MAP.OPEN_NTT data. Rows:6987               | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 26"
        }
    },
    "96": {
        "page_content": "1670509202   | 402209 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-<TABLE-NAME> | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 27"
        }
    },
    "97": {
        "page_content": "1670509202   | 402211 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-<TABLE-NAME>    | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     |        | Move def_NETWORK_MAP.OPEN_NTT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 28"
        }
    },
    "98": {
        "page_content": "1670509202   | 402241 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-<TABLE-NAME>    | 20221208_1620       | SUCCESS | 2022-12-08 16:21:32 |                     |        | Load def_NETWORK_MAP.OPEN_NTT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 29"
        }
    },
    "99": {
        "page_content": "1670509202   | 402247 | ONETICKET   | def_NETWORK_MAP | COMPUTE_STATS             | 20221208_1620       | SUCCESS | 2022-12-08 16:22:04 |                     |        | Compute statistics                                                   | def_network_maps | un-vip.bigdata.abc.gr\n```\n### Monitoring Component list\n|Component | Description \n|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED|",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 30"
        }
    },
    "100": {
        "page_content": "|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED| \n|CHECK_FOR_AVAILABLE_DATA| Check the Oracle table EXPORT_CTL if there are new data to export\n|EXPORT_DATA-\\<TABLE-NAME\\>| Exports data from Oracle to `/shared/abc/oneTicket/exp`\n|DATA_PARSING-\\<TABLE-NAME\\>| Change column separator and remove the string \"null\"\n|HDFS_MOVE-\\<TABLE-NAME\\>| Move export file from local file system to HDFS `/ez/landingzone/tmp/oneTicket`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 31"
        }
    },
    "101": {
        "page_content": "|LOAD_DATA-\\<TABLE-NAME\\>| Load export file from HDFS `/ez/landingzone/tmp/oneTicket` into the HIVE table (i.e. `def_NETWORK_MAP.OPEN_NTT`)\n|COMPUTE_STATS| performs compute statistics on HIVE tables using impala-shell\n### Monitoring database Queries\n- List messages of the last load  \n  `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n  ```\n  select \n    execution_id, id, application, job, component, operative_partition,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 32"
        }
    },
    "102": {
        "page_content": "```\n  select \n    execution_id, id, application, job, component, operative_partition,  \n    status, system_ts, system_ts_end, param0, message, user,  host\n  from jobstatus a where 1=1\n  and upper(application)='ONETICKET' and upper(job) like 'def_NETWORK_MAP'   \n  and execution_id=(\n    select max(execution_id) execution_id from jobstatus where 1=1\n    and upper(application)='ONETICKET' and upper(job) like 'def_NETWORK_MAP'   \n    and lower(message) not like '%no new export found%'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 33"
        }
    },
    "103": {
        "page_content": "and lower(message) not like '%no new export found%' \n    and component='CHECK_FOR_AVAILABLE_DATA'\n    and system_ts>=date(now())-30)\n  order by id\n  ;\n  execution_id | id     | application | job             | component                                           | operative_partition | status  | system_ts           | system_ts_end       | param0 | message                                                              | user             | host",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 34"
        }
    },
    "104": {
        "page_content": "-------------+--------+-------------+-----------------+-----------------------------------------------------+---------------------+---------+---------------------+---------------------+--------+----------------------------------------------------------------------+------------------+-------------------------",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 35"
        }
    },
    "105": {
        "page_content": "1670509202   | 402171 | ONETICKET   | def_NETWORK_MAP | MAIN                                                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 | 2022-12-08 16:22:05 |        | Procedure Started                                                    | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 36"
        }
    },
    "106": {
        "page_content": "1670509202   | 402173 | ONETICKET   | def_NETWORK_MAP | CHECK_FOR_AVAILABLE_DATA                            | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 |                     |        | New data found: 2022-12-08 15:50:04                                  | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 37"
        }
    },
    "107": {
        "page_content": "1670509202   | 402183 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.AFFECTED_OCT_WTT        | 20221208_1620       | SUCCESS | 2022-12-08 16:20:12 |                     | 1867   | Oracle export def_NETWORK_MAP.AFFECTED_OCT_WTT data. Rows:1867       | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 38"
        }
    },
    "108": {
        "page_content": "1670509202   | 402185 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.AFFECTED_OCT_WTT       | 20221208_1620       | SUCCESS | 2022-12-08 16:20:12 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 39"
        }
    },
    "109": {
        "page_content": "1670509202   | 402187 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.AFFECTED_CUSTOMERS      | 20221208_1620       | SUCCESS | 2022-12-08 16:20:15 |                     | 17397  | Oracle export def_NETWORK_MAP.AFFECTED_CUSTOMERS data. Rows:17397    | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 40"
        }
    },
    "110": {
        "page_content": "1670509202   | 402189 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.AFFECTED_OCT_WTT          | 20221208_1620       | SUCCESS | 2022-12-08 16:20:15 |                     |        | Move def_NETWORK_MAP.AFFECTED_OCT_WTT data in HDFS                   | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 41"
        }
    },
    "111": {
        "page_content": "1670509202   | 402191 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.AFFECTED_CUSTOMERS     | 20221208_1620       | SUCCESS | 2022-12-08 16:20:15 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 42"
        }
    },
    "112": {
        "page_content": "1670509202   | 402193 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_MW                 | 20221208_1620       | SUCCESS | 2022-12-08 16:20:17 |                     | 238    | Oracle export def_NETWORK_MAP.OPEN_MW data. Rows:238                 | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 43"
        }
    },
    "113": {
        "page_content": "1670509202   | 402195 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_MW                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:17 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 44"
        }
    },
    "114": {
        "page_content": "1670509202   | 402197 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:18 |                     | 6035   | Oracle export def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT data. Rows:6035 | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 45"
        }
    },
    "115": {
        "page_content": "1670509202   | 402199 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT | 20221208_1620       | SUCCESS | 2022-12-08 16:20:18 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 46"
        }
    },
    "116": {
        "page_content": "1670509202   | 402201 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.AFFECTED_CUSTOMERS        | 20221208_1620       | SUCCESS | 2022-12-08 16:20:18 |                     |        | Move def_NETWORK_MAP.AFFECTED_CUSTOMERS data in HDFS                 | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 47"
        }
    },
    "117": {
        "page_content": "1670509202   | 402203 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_MW                   | 20221208_1620       | SUCCESS | 2022-12-08 16:20:21 |                     |        | Move def_NETWORK_MAP.OPEN_MW data in HDFS                            | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 48"
        }
    },
    "118": {
        "page_content": "1670509202   | 402205 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT    | 20221208_1620       | SUCCESS | 2022-12-08 16:20:21 |                     |        | Move def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT data in HDFS             | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 49"
        }
    },
    "119": {
        "page_content": "1670509202   | 402207 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_NTT                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     | 6987   | Oracle export def_NETWORK_MAP.OPEN_NTT data. Rows:6987               | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 50"
        }
    },
    "120": {
        "page_content": "1670509202   | 402209 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_NTT               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 51"
        }
    },
    "121": {
        "page_content": "1670509202   | 402211 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_NTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     |        | Move def_NETWORK_MAP.OPEN_NTT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 52"
        }
    },
    "122": {
        "page_content": "1670509202   | 402213 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_WTT                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     | 3621   | Oracle export def_NETWORK_MAP.OPEN_WTT data. Rows:3621               | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 53"
        }
    },
    "123": {
        "page_content": "1670509202   | 402215 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_WTT               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 54"
        }
    },
    "124": {
        "page_content": "1670509202   | 402217 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_WTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:30 |                     |        | Move def_NETWORK_MAP.OPEN_WTT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 55"
        }
    },
    "125": {
        "page_content": "1670509202   | 402219 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.ACTIVITY                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:36 |                     | 74433  | Oracle export def_NETWORK_MAP.ACTIVITY data. Rows:74433              | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 56"
        }
    },
    "126": {
        "page_content": "1670509202   | 402221 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.ACTIVITY               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:37 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 57"
        }
    },
    "127": {
        "page_content": "1670509202   | 402223 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.ACTIVITY                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:40 |                     |        | Move def_NETWORK_MAP.ACTIVITY data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 58"
        }
    },
    "128": {
        "page_content": "1670509202   | 402225 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_OCT                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:40 |                     | 60164  | Oracle export def_NETWORK_MAP.OPEN_OCT data. Rows:60164              | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 59"
        }
    },
    "129": {
        "page_content": "1670509202   | 402227 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_OCT               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:40 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 60"
        }
    },
    "130": {
        "page_content": "1670509202   | 402229 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_OCT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:43 |                     |        | Move def_NETWORK_MAP.OPEN_OCT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 61"
        }
    },
    "131": {
        "page_content": "1670509202   | 402231 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.ACTIVITY                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:57 |                     |        | Load def_NETWORK_MAP.ACTIVITY data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 62"
        }
    },
    "132": {
        "page_content": "1670509202   | 402233 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.AFFECTED_CUSTOMERS        | 20221208_1620       | SUCCESS | 2022-12-08 16:21:04 |                     |        | Load def_NETWORK_MAP.AFFECTED_CUSTOMERS data into Hive               | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 63"
        }
    },
    "133": {
        "page_content": "1670509202   | 402235 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.AFFECTED_OCT_WTT          | 20221208_1620       | SUCCESS | 2022-12-08 16:21:11 |                     |        | Load def_NETWORK_MAP.AFFECTED_OCT_WTT data into Hive                 | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 64"
        }
    },
    "134": {
        "page_content": "1670509202   | 402237 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT    | 20221208_1620       | SUCCESS | 2022-12-08 16:21:18 |                     |        | Load def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT data into Hive           | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 65"
        }
    },
    "135": {
        "page_content": "1670509202   | 402239 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_MW                   | 20221208_1620       | SUCCESS | 2022-12-08 16:21:25 |                     |        | Load def_NETWORK_MAP.OPEN_MW data into Hive                          | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 66"
        }
    },
    "136": {
        "page_content": "1670509202   | 402241 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_NTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:21:32 |                     |        | Load def_NETWORK_MAP.OPEN_NTT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 67"
        }
    },
    "137": {
        "page_content": "1670509202   | 402243 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_OCT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:21:39 |                     |        | Load def_NETWORK_MAP.OPEN_OCT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 68"
        }
    },
    "138": {
        "page_content": "1670509202   | 402245 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_WTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:21:46 |                     |        | Load def_NETWORK_MAP.OPEN_WTT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 69"
        }
    },
    "139": {
        "page_content": "1670509202   | 402247 | ONETICKET   | def_NETWORK_MAP | COMPUTE_STATS                                       | 20221208_1620       | SUCCESS | 2022-12-08 16:22:04 |                     |        | Compute statistics                                                   | def_network_maps | un-vip.bigdata.abc.gr\n  ```\n### Monitoring Health-Check\n  - Check Monitoring status.  \n  ```  \n  $ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 70"
        }
    },
    "140": {
        "page_content": "$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\n  \n  {\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n  ```  \n  \n  - In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 71"
        }
    },
    "141": {
        "page_content": "## Troubleshooting\nAn email will be sent by the system with the point of failure.  \ni.e.\n<pre>\nFrom: abc_bigd@abc.gr  \nSubject: ONETICKET - def_NETWORK_MAP: FAILED  \nLoad <b>def_NETWORK_MAP.ACTIVITY</b> data into Hive (1673849411)  \n<b>Exec_id:1673849411</b>  \nThis is an automated e-mail.  \nPlease do not reply.  \n</pre>\n**Actions**  \n1. Write down the values of the `Table name` and `Exec_id` described in the alert email  \n\ti.e. \n\t- Table name: `def_NETWORK_MAP.ACTIVITY`\n\t- Exec_id:`1673849411`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 72"
        }
    },
    "142": {
        "page_content": "i.e. \n\t- Table name: `def_NETWORK_MAP.ACTIVITY`\n\t- Exec_id:`1673849411`\n2. Copy from HDFS the folowing log files which contains the specific `Table name` and `Exec_id` in its filename.\n- 103.OneTicket_OraData_Export_Import.\\<Table name\\>.\\<Exec_id\\>.log\n- 104.OneTicket_OraData_Import_Hive.\\<Exec_id\\>.log\n<pre>\nhdfs dfs -get /user/def_network_maps/hdfs dfs -get 103.OneTicket_OraData_Export_Import.<b>def_NETWORK_MAP.ACTIVITY.1673849411</b>.log",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 73"
        }
    },
    "143": {
        "page_content": "hdfs dfs -get /user/def_network_maps/hdfs dfs -get 104.OneTicket_OraData_Import_Hive.<b>1673849411</b>.log\n</pre>\n3. Searches for Exception messages in log files  \n`egrep '(Exception:|Coused by)' 10[1-4].OneTicket_OraData*.log`  \ni.e.\n<pre>\n$ egrep '(Exception:|Coused by)' 10[1-4].OneTicket_OraData*.log\n104.OneTicket_OraData_Import_Hive.1673849411.log:javax.security.sasl.SaslException: GSS initiate failed",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 74"
        }
    },
    "144": {
        "page_content": "104.OneTicket_OraData_Import_Hive.1673849411.log:Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find\n</pre>\n### Common errors  \n  - impala/hive availability\n  - Kerberos authentication\n  *Ndef: The flow checks if the ticket is still active before any HDFS action.  \n  In case of expiration the flow performs a `kinit` command*\n\t\n## Data Check\nThe data checks below are provided for informational purposes only.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 75"
        }
    },
    "145": {
        "page_content": "## Data Check\nThe data checks below are provided for informational purposes only.  \nIf any of them returns wrong data, then no actions need to be taken from the support team.  \nThe flow runs periodically over the day and every time overwrites the data.   \n### Check Load Status.\nif the difference between `EXPORT_START_DT` and `IMPORT_START_DT` is greater than 2 hours it is considered as a problem in loading procedure.  \n*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 76"
        }
    },
    "146": {
        "page_content": "*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*  \n\t<pre>\n\tselect \n\t  EXPORT_START_DT, IMPORT_START_DT,\n\t  case when 24*(EXPORT_START_DT-IMPORT_START_DT)>2 then 'ERROR' else 'OK' end Load_Status\n\tfrom EXPORT_CTL where EXPORT_SEQUENCE=0;\n\t</pre>\n\t<pre>\n\tEXPORT_START_DT     | IMPORT_START_DT     | LOAD_STATUS\n\t--------------------+---------------------+------------\n\t<b>2022-12-02 10:46:11 | 2022-12-02 07:48:26 | ERROR      </b>#in case of load issue",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 77"
        }
    },
    "147": {
        "page_content": "<b>2022-12-02 10:46:11 | 2022-12-02 07:48:26 | ERROR      </b>#in case of load issue\n\t\n\tEXPORT_START_DT     | IMPORT_START_DT     | LOAD_STATUS\n\t--------------------+---------------------+------------\n\t2022-12-02 10:46:11 | 2022-12-02 10:48:26 | OK         #under normal circumstances\n\t</pre>\n### Check data in Hive-Impala tables\n*Connect to Impala (see [Database CLI commands](#database-cli-commands))*  \n```\nselect * from (",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 78"
        }
    },
    "148": {
        "page_content": "*Connect to Impala (see [Database CLI commands](#database-cli-commands))*  \n```\nselect * from (\n  select distinct  'activity' tbl, upd_ts from def_network_map.activity union all\n  select distinct  'affected_customers', upd_ts from def_network_map.affected_customers union all\n  select distinct  'affected_oct_wtt', upd_ts from def_network_map.affected_oct_wtt union all\n  select distinct  'defective_netw_element', upd_ts from def_network_map.defective_netw_element union all",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 79"
        }
    },
    "149": {
        "page_content": "select distinct  'open_mw', upd_ts from def_network_map.open_mw union all\n  select distinct  'open_ntt', upd_ts from def_network_map.open_ntt union all\n  select distinct  'open_oct', upd_ts from def_network_map.open_oct union all\n  select distinct  'open_wtt', upd_ts from def_network_map.open_wtt\n)a order by tbl\n;\n+------------------------+---------------------+\n| tbl                    | upd_ts              |\n+------------------------+---------------------+",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 80"
        }
    },
    "150": {
        "page_content": "| tbl                    | upd_ts              |\n+------------------------+---------------------+\n| activity               | 2022-12-16 10:50:18 |\n| affected_customers     | 2022-12-16 10:50:18 |\n| affected_oct_wtt       | 2022-12-16 10:50:18 |\n| defective_netw_element | 2022-12-16 10:50:18 |\n| open_mw                | 2022-12-16 10:50:18 |\n| open_ntt               | 2022-12-16 10:50:18 |\n| open_oct               | 2022-12-16 10:50:18 |\n| open_wtt               | 2022-12-16 10:50:18 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 81"
        }
    },
    "151": {
        "page_content": "| open_oct               | 2022-12-16 10:50:18 |\n| open_wtt               | 2022-12-16 10:50:18 |\n+------------------------+---------------------+\nFetched 8 row(s) in 6.10s\n```\n`upd_ts` should have the same value *(+- 10 seconds)* as the one in `IMPORT_START_DT` from Oracle table `EXPORT_CTL`  \ni.e.  \n*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*  \n`select IMPORT_START_DT from EXPORT_CTL where EXPORT_SEQUENCE=0;`\n```\nIMPORT_START_DT     \n--------------------",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 82"
        }
    },
    "152": {
        "page_content": "```\nIMPORT_START_DT     \n--------------------\n2022-12-16 10:50:19\n```\n*Conclusion: Hive/Impala tables contain the correct data according to the control table*\n```\n     upd_ts     = 2022-12-16 10:50:18 #Hive/Impala tables\nIMPORT_START_DT = 2022-12-16 10:50:19 #Oracle Control Table\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "NETWORK_MAP_Support_Notes.md - Part 83"
        }
    },
    "153": {
        "page_content": "# Monitoring application\n[[_TOC_]]\n## Scope\nThe purpose of application is to monitor all streamming and batch jobs through HTTP calls. Requests are written to database ``monitoring`` and table ``jobstatus``. Metrics about application performance are written to Graphite.\n## Setup\n### App\nDeployed on nodes **un5**, **un6**  \n- Host: `un5.bigdata.abc.gr`, `un6.bigdata.abc.gr`\n- Port: 12800\nReached via HAProxy  \n- Host: **un-vip.bigdata.abc.gr**\n- Port: 12800\n### Config Dir",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "manage-monitoring-app.md - Part 1"
        }
    },
    "154": {
        "page_content": "- Port: 12800\nReached via HAProxy  \n- Host: **un-vip.bigdata.abc.gr**\n- Port: 12800\n### Config Dir\n `/opt/monitoring_app/monitoring_config`\n### Logs\n- application logs: `/opt/monitoring-app/logs/monitoring-api.log`\n  - older logs: `/opt/monitoring-app/logs/2022-11`\n- access logs:  `/opt/monitoring-app/logs/tomcat/access_log.log`\n### MySQL\n- Host: db-vip.bigdata.abc.gr:3306\n- User: monitoring\n- Schema: monitoring\n- Table: jobstatus\n### Graphite:\n- Host: un-vip.bigdata.abc.gr\n- Port: 2004",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "manage-monitoring-app.md - Part 2"
        }
    },
    "155": {
        "page_content": "- Schema: monitoring\n- Table: jobstatus\n### Graphite:\n- Host: un-vip.bigdata.abc.gr\n- Port: 2004\n## Procedure\n### Check service status\n#### Container\n| Description| Command |\n| ----------- | ----------- |\n|check container is running| `sudo docker ps --filter=\"name=monitoring-app-{version}\" --filter=\"status=running\"`|\n|check container is stopped| `sudo docker ps --filter=\"name=monitoring-app-{version}\" --filter=\"status=exited\"`|",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "manage-monitoring-app.md - Part 3"
        }
    },
    "156": {
        "page_content": "|check container status| `sudo docker ps --filter=\"name=monitoring-app-{version}\"`|\n|stop monitoring-app| `sudo docker stop monitoring-app-{version}`|\n|start monitoring-app|`sudo docker start monitoring-app-{version}`|\n|run monitoring-app if container is killed & removed|[use start app script](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/Deployment-PROD#run-start-up-script)|\n### Stop/Start procedures\n#### Connect to nodes\n1. SSH to un2   \n`ssh root@un2`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "manage-monitoring-app.md - Part 4"
        }
    },
    "157": {
        "page_content": "### Stop/Start procedures\n#### Connect to nodes\n1. SSH to un2   \n`ssh root@un2`\n2. SSH to un5/un6 nodes via personal sudoer user.  \n(superuser privileges to perform Docker operations)\n   1. `ssh <user>@<un5/un6>`\n   1. `sudo su`\n#### Stop current container\n1. Get container name with docker ps as described above\n2. Stop container  \n`docker stop monitoring-app-{version}`\n#### Start stopped container",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "manage-monitoring-app.md - Part 5"
        }
    },
    "158": {
        "page_content": "2. Stop container  \n`docker stop monitoring-app-{version}`\n#### Start stopped container\nTo start the container using the latest version (the latest container version is the one that was stopped with the previous command) use,\n`docker start monitoring-app-{version}`\n#### API calls\n- Using un5 IP\n| Description | Command |\n| ----------- | ----------- |\n| Check app is running | `curl --location --request GET 'http://999.999.999.999:12800/monitoring/app/status'` |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "manage-monitoring-app.md - Part 6"
        }
    },
    "159": {
        "page_content": "| Check Load Balancer is enabled | `curl --location --request GET 'http://999.999.999.999:12800/monitoring/app/lb/check' `|\n| Enable Load Balancer | `curl --location --request PUT 'http://999.999.999.999:12800/monitoring/app/lb/enable'` |\n| Disable Load Balancer  | `curl --location --request PUT 'http://999.999.999.999:12800/monitoring/app/lb/disable'`|\n### Troubleshooting Step\n- Check [logs](#logs) to identify the problem \n### Deployment steps",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "manage-monitoring-app.md - Part 7"
        }
    },
    "160": {
        "page_content": "### Troubleshooting Step\n- Check [logs](#logs) to identify the problem \n### Deployment steps\n- [Deployment guide](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/Deployment-PROD)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "manage-monitoring-app.md - Part 8"
        }
    },
    "161": {
        "page_content": "[[_TOC_]]\n# Introduction\nThis support document outlines the steps that compose the IPVPN-SM replacement process that computes Key Performance Indicators (KPIs) regarding the network quality of abc. The ETL process involves a series of scripts and a Spring Boot application that work together to retrieve, transform, and load data from Impala tables into a remdef server.\nThere are three major KPI categories, *Components*, *SLA* and *Interfaces*, which consist of the following subcategories:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 1"
        }
    },
    "162": {
        "page_content": "|KPI category|Metrics involved| Relevant Impala tables|\n|---|---|---|\n|Components|CPU, Memory (MEM)|bigcust.nnm_ipvpn_componentmetrics_hist,<br>  nnmnps.nms_node |\n|Interfaces|Interfaces (IF)|bigcust.perf_interfacemetrics_ipvpn_hist,<br>  bigcust.pe_interfaces,<br>  nnmnps.nms_node|\n|SLA|Availability (AV), Quality of Service (QOS)|bigcust.nnmcp_ipvpn_slametrics_hist,<br> bigcust.customer_pl,<br> bigcust.customer_sla_config_ipvpn,<br> bigcust.sla_configurations|\n# Application Flow",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 2"
        }
    },
    "163": {
        "page_content": "# Application Flow\n## Scripts & IPVPN-SM App operations\nThe metrics computation of each of the three categories (sla, components, interfaces) is triggered by the `un2:/shared/abc/ip_vpn/run/initiate_export_*.sh` script that runs as part of the IPVPN-SLA [cronjob](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/blob/master/PROD/crontab/crontab_per_user.txt), scheduled to run every 5 minutes, and the computation refers to a full five-minute interval.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 3"
        }
    },
    "164": {
        "page_content": "The ETL process follows the following flow:\n### IPVPN-SLA\n- [IPVPN-SLA Support Doc](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ip_vpn.md)\n### IPVPN-SM\n- [IPVPN-SM Dev Wiki](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/home)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 4"
        }
    },
    "165": {
        "page_content": "3. The `export_*_kpis.sh` calls the IPVPN-SM App to execute the Impala queries. The curl request to the IPVPN-SM App is executed asynchronously, allowing the IPVPN-SM pipeline to continue with other scheduled 5 min intervals without waiting for the query results.\n4. The IPVPN-SM App executes the Impala queries, applies transformations on the result set based on the agreed schema specifications, and sends the results to the SQM server.\n## IPVPN-SM Application",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 5"
        }
    },
    "166": {
        "page_content": "## IPVPN-SM Application\nThe IPVPN-SM Application executes the Impala queries specified in the `/shared/abc/ip_vpn/sm-app/deployment/config/field-mappings.yml`, applies transformations on the result set based on the agreed [schema specifications](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Data-Transmission-Schema) , and sends the results to the SQM server.\n### Ingestion Endpoint",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 6"
        }
    },
    "167": {
        "page_content": "### Ingestion Endpoint\nThe endpoint for the IPVPN-SM application is `/ingest-query`. This endpoint should be called with a POST request.\nDetails on the request body fields and examples [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Application-details#request-body-1)\n### Check application status\n```\ncurl --location --request GET 'un-vip:13001/ipvpn-sm/app/status'\n```\n## Application Flow Diagram\n```mermaid\nflowchart TD;",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 7"
        }
    },
    "168": {
        "page_content": "```\n## Application Flow Diagram\n```mermaid\nflowchart TD;\n  A(\"/shared/abc/ip_vpn/run/initiate_export_*.sh <br> ipvpn@un2\") --> B(\"/shared/abc/ip_vpn/run/export_*_kpis.sh <br> ipvpn@un2\")\n  B -- 1. Calls Sync ----> C(\"/shared/abc/ip_vpn/sm-replacement/scripts/refresh_*_kpis.sh <br> ipvpn@un2\")\n  B -- 2. Calls Async ---> I(\"/shared/abc/ip_vpn/sm-replacement/scripts/compute_metrics_via_sm_app.sh <br> ipvpn@un2\")--> J(\"/shared/abc/ip_vpn/sm-replacement/scripts/query_sm.sh <br> ipvpn@un2\")",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 8"
        }
    },
    "169": {
        "page_content": "J -- /ingest-query request--->IPVPN-SM_Replacement_Application\n  subgraph IPVPN-SM_Replacement_Application[\"root@un1/un2\"]\n\t E(Impala Queries)\n\t F(Data with agreed schema)\n  end\n  E -- data transformation --> F\n  F -- POST --> G[SQM Server]\n  style A fill:#F8F8F8\n  style B fill:#F8F8F8\n  style IPVPN-SM_Replacement_Application fill:#DCDCDC\n  style I fill:#DCDCDC\n  style C fill:#DCDCDC\n  style E fill:#DCDCDC\n  style F fill:#DCDCDC\n  style J fill:#DCDCDC\n  style G fill:darkgray\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 9"
        }
    },
    "170": {
        "page_content": "style E fill:#DCDCDC\n  style F fill:#DCDCDC\n  style J fill:#DCDCDC\n  style G fill:darkgray\n```\n# Infrastructure\nThe ETL pipeline infrastructure includes the following components:\n## Application\n- un1/un2 files\n\t- **Deployment path**: `/shared/abc/ip_vpn/sm-app/deployment`\n\t- **Configuration**: `/shared/abc/ip_vpn/sm-app/deployment/config`\n\t\t- [application.yml](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/blob/main/PROD/deployment/config/application.yml)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 10"
        }
    },
    "171": {
        "page_content": "- **Logs**: `/shared/abc/ip_vpn/sm-app/deployment/logs`\n- Data sources:\n\t- **Impala**:  `un-vip.bigdata.abc.gr:21050`\n\t- **hive database**: `bigcust`\n- Authentication:\n  The authentication to Impala is done with Kerberos. The files used are:\n    - `/etc/krb5.conf`\n    - `/shared/abc/ip_vpn/sm-app/deployment/config/jaas.conf`\n    - `/home/users/ipvpn/ipvpn.keytab`\n- IPVPN-SM app:\n  The application uses HAProxy, a load balancer that distributes incoming requests to the deployed application nodes.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 11"
        }
    },
    "172": {
        "page_content": "- **HAProxy**:\n  Host: `un-vip` ,\n  Port: `13001`\n  - **Application**:\n    Hosts: `un1/un2`,\n    Active Ports:\n      - HTTP port : `13000`\n      - JMX port : `13800`\n- External SQM server:\n\t- **host**: `gw-prod-sqm.aksprd.abc.gr`\n\t- **port**: `443`\n\t- **endpoint**: `/inventory/measurements`\n## Bash scripts",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 12"
        }
    },
    "173": {
        "page_content": "- **port**: `443`\n\t- **endpoint**: `/inventory/measurements`\n## Bash scripts\nIn addition to the IPVPN-SM application, IPVPN-SM utilizes certain bash scripts. Their primary purpose is to generate the appropriate input data that is subsequently passed as the body parameter in the POST requests to the IPVPN-SM application.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 13"
        }
    },
    "174": {
        "page_content": "They are invoked by the IPVPN-SLA `export_*_kpis.sh` [scripts](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/blob/master/PROD/nnmnps/bin/export_component_kpis.sh).\n- **Node**: `un2`\n- **User**: `ipvpn`\n- **Scripts**: `/shared/abc/ip_vpn/sm-replacement/scripts`\n- **IPVPN-SM App host & port configured in file** : `/shared/abc/ip_vpn/sm-replacement/scripts/config.properties`\n- **Logs**: `/shared/abc/ip_vpn/sm-replacement/log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 14"
        }
    },
    "175": {
        "page_content": "- **Logs**: `/shared/abc/ip_vpn/sm-replacement/log`\nThe specific order in which they are called is the following:\n```mermaid\ngraph LR\nA[\"export_*_kpis.sh<br><i>IPVPN-SLA</i>\"] -->B[compute_metrics_via_sm_app.sh]\nB -->C[query_sm.sh]\nC --/ingest-query-->D(SM Application)\nstyle A fill:darkgray\nstyle B fill:#ECECEC\nstyle C fill:#ECECEC\n```\n## Deployment Instructions",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 15"
        }
    },
    "176": {
        "page_content": "style A fill:darkgray\nstyle B fill:#ECECEC\nstyle C fill:#ECECEC\n```\n## Deployment Instructions\nStop/Start guide on [abc_BigStreamer_Apps_MoP_Restart.docx](https://ghi.sharepoint.com/:w:/t/abcBigData/EZsK97GxFyVHkeBoFTFjuRYBtPF7lK86cTA44wMhayNlNA?e=Gjh0ZO)  \nCheck project wiki [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Application-details#deployment-instructions)\n## SQM Token Authentication",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 16"
        }
    },
    "177": {
        "page_content": "## SQM Token Authentication\nThe authentication on the SQM server is done with the HTTP Token Auth in IPVPN-SM application. It requires providing the keystore file, where the token is stored.\n# Failure Handling\n## Logs\n#### Script Logs on un2\nThe scripts which refresh the Impala tables and which call the IPVPN-SM application write logs under the directories:\n1. The `refresh_*_.sh script` Logs\n`/shared/abc/ip_vpn/sm-replacement/log/refresh_*_kpis`\n2. The `compute_metrics_via_sm_app.sh` Logs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 17"
        }
    },
    "178": {
        "page_content": "`/shared/abc/ip_vpn/sm-replacement/log/refresh_*_kpis`\n2. The `compute_metrics_via_sm_app.sh` Logs\n`/shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*`\nThe asterisk is used to dendef the type of the particular category.\n#### Application Logs on un1/un2\nIn case of failure, the logs related to the IPVPN-SM app are the following:\n1. **Access Logs**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/tomcat/access_log.log`\n2. **Application Logs**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 18"
        }
    },
    "179": {
        "page_content": "`/shared/abc/ip_vpn/sm-app/deployment/logs/tomcat/access_log.log`\n2. **Application Logs**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/application.log`\n3. **Requests sent to SQM Server**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/sm-server-requests.log`\n#### IPVPN-SLA Logs on `un2`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 19"
        }
    },
    "180": {
        "page_content": "`/shared/abc/ip_vpn/sm-app/deployment/logs/sm-server-requests.log`\n#### IPVPN-SLA Logs on `un2`\nAside from the above logs, the operation of the IPVPN-SLA scripts is also monitored by the original implementation. Details about the logging of IPVPN-SLA can be found [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ip_vpn.md)\n### Auto-retry mechanism",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 20"
        }
    },
    "181": {
        "page_content": "### Auto-retry mechanism\nDuring the schedules runs, if the call to the IPVPN-SM app fails, the call to the application is repeated. If the application fails on all 5 attemps, the script exits. On each failed attempt, an email alert is sent via the monitoring app. This operation is done inside the `un2:/shared/abc/ip_vpn/sm-replacement/scripts/compute_metrics_via_sm_app.sh` script.\n# Support\n## Check request status via Monitoring\n### Grafana\nCheck grafana dashboard for failed requests:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 21"
        }
    },
    "182": {
        "page_content": "## Check request status via Monitoring\n### Grafana\nCheck grafana dashboard for failed requests:\n`https://unc1.bigdata.abc.gr:3000/d/HiuaKlU4z/ipvpn-sm-replacement-monitoring?orgId=1&refresh=5s`\n### Monitoring DB\nIPVPN-SM App uses Monitoring App in order to record statistics regarding the status of each metric computation request. Querying monitoring DB allows us to investigate pdefntial failed requests.\n1. Connect to monitoring DB\n`mysql -umonitoring -p -h 999.999.999.999`\n2. `use monitoring;`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 22"
        }
    },
    "183": {
        "page_content": "1. Connect to monitoring DB\n`mysql -umonitoring -p -h 999.999.999.999`\n2. `use monitoring;`\n#### See failed requests in the past 24h excluding EmptyQuery Exceptions\nOften persistent EmptyQuery alerts demand further investigation in the CustomPoller component of IPVPN-SLA application and the associated metrics cannot be retrieved from the Impala tables.   \nTo identify failed metrics in the past 24h due to errors other than EmptyQuery exceptions, we can use the following query\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 23"
        }
    },
    "184": {
        "page_content": "```\nmysql> select distinct x.job, x.5min_interval_ts, x.sent_to_SQM, y.request_repetitions, x.exception from ( select d.job, DATE_FORMAT( STR_TO_DATE( operative_partition, '%Y%m%d%H%i%s' ), '%Y-%m-%d %H:%i' ) as 5min_interval_ts, c.operative_partition, c.status AS 'sent_to_S",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 24"
        }
    },
    "185": {
        "page_content": "QM', DATE_FORMAT( metric_ts_start, '%Y-%m-%d %H:%i:%s' ) as metric_ts_start, DATE_FORMAT( c.system_ts, '%Y-%m-%d %H:%i:%s' ) as metric_ts_end, SUBSTRING_INDEX( c.message, 'com.jkl.bigstreamer.abc.ipvpn.sm.exception.', -1 ) as exception from ( SELECT job, compone",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 25"
        }
    },
    "186": {
        "page_content": "nt as start_comp, status as started, operative_partition as start_oppart, system_ts as metric_ts_start FROM jobstatus WHERE application = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_START' and message = 'n_attempt=0' order by system_ts desc,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 26"
        }
    },
    "187": {
        "page_content": "job ) d inner join ( select b.* from ( SELECT MAX(system_ts) as max_ts FROM jobstatus WHERE application = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_END' group by operative_partition, job ) a inner join ( SELECT * FROM jobstatus WHERE applic",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 27"
        }
    },
    "188": {
        "page_content": "ation = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_END' ) b on a.max_ts = b.system_ts ) c on d.job = c.job and d.start_oppart = c.operative_partition order by 5min_interval_ts desc ) x left join ( select count(*)-1 as 'request_repetitions',",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 28"
        }
    },
    "189": {
        "page_content": "job, operative_partition from jobstatus where application = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_START' group by operative_partition, job ) y on x.operative_partition = y.operative_partition and x.job = y.job where sent_to_SQM=\"FAILED\"\n and SUBSTRING_INDEX( exception, ':', 1 )!=\"AppEmptyQueryException\" order by 5min_interval_ts desc;\n+-----+------------------+-------------+---------------------+-------------------+",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 29"
        }
    },
    "190": {
        "page_content": "+-----+------------------+-------------+---------------------+-------------------+\n| job | 5min_interval_ts | sent_to_SQM | request_repetitions | exception         |\n+-----+------------------+-------------+---------------------+-------------------+\n| AV  | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| CPU | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| MEM | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 30"
        }
    },
    "191": {
        "page_content": "| MEM | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| IF  | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| QOS | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| AV  | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| CPU | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| IF  | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 31"
        }
    },
    "192": {
        "page_content": "| IF  | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| MEM | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| QOS | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n+-----+------------------+-------------+---------------------+-------------------+\n```\nThese are the requests that should be manually handled following the actions described next.\n#### Inspect specific metric request",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 32"
        }
    },
    "193": {
        "page_content": "#### Inspect specific metric request\nIn case of alerts, we should first check the status of the particular requests on the monitoring DB.\nUsing the metric category and the reference interval associated with a request to IPVPN-SM App we can query the Monitoring DB\nby passing the corresponding job and operative_partition values.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 33"
        }
    },
    "194": {
        "page_content": "by passing the corresponding job and operative_partition values.\n- `select application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\"IPVPN-SM\" and job=\"<category>\" and operative_partition=\"<YYYYMMDDhhmmss>\" order by system_ts;`\nFor example. if we want to check the status of a request for QOS metric category and `2023-05-10 06:55:00` reference interval:\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 34"
        }
    },
    "195": {
        "page_content": "```\nmysql> select application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\"IPVPN-SM\" and job=\"QOS\" and operative_partition=\"20230510065500\" order by system_ts;\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 35"
        }
    },
    "196": {
        "page_content": "| application | job | component  | operative_partition | status  | message                                                                    | system_ts               |\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 36"
        }
    },
    "197": {
        "page_content": "| IPVPN-SM    | QOS | MAIN_START | 20230510065500      | SUCCESS | n_attempt=0                                                                | 2023-05-10 07:04:51.000 |\n| IPVPN-SM    | QOS | MAIN_END   | 20230510065500      | FAILED  | com.jkl.bigstreamer.abc.ipvpn.sm.exception.AppEmptyQueryException | 2023-05-10 07:05:02.000 |\n| IPVPN-SM    | QOS | MAIN_START | 20230510065500      | SUCCESS | n_attempt=1                                                                | 2023-05-10 07:05:22.000 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 37"
        }
    },
    "198": {
        "page_content": "| IPVPN-SM    | QOS | MAIN_END   | 20230510065500      | SUCCESS |                                                                            | 2023-05-10 07:05:35.000 |\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\n4 rows in set (0.01 sec)\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 38"
        }
    },
    "199": {
        "page_content": "4 rows in set (0.01 sec)\n```\nAccording to the above results, we conclude that the computation of the QOS metrics for the selected 5min interval failed on the first attempt with AppEmptyQueryException error. However the request was successful on the second attempt.\n#### Inspect overall requests' status\nIf we want to check if any of the requests has failed, for instance in the past 24h\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 39"
        }
    },
    "200": {
        "page_content": "If we want to check if any of the requests has failed, for instance in the past 24h\n```\nselect application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\"IPVPN-SM\" and component=\"MAIN_END\" and status=\"FAILED\" and system_ts >= now() - INTERVAL 1 DAY;\n```\n##  Pdefntial Error Cases\n### AppEmptyQueryException\nThis exception occurs when the executed Impala query fails. The underlying reason behind this message can be one the following:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 40"
        }
    },
    "201": {
        "page_content": "1. The query did not retrieve any records for the `specified 5min interval`. For category **IF**, this type of failure of a 5min interval may cause the failure of `the following 5min interval` as well. This occurs because an aggregation between two consecutive intervals is performed. \n2. Inability to open one of the HDFS files that constitute a table associated with the particular Impala query.\n3. Unreachable Impala deamon.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 41"
        }
    },
    "202": {
        "page_content": "3. Unreachable Impala deamon.\n4. Inability to allocate Memory (OutOfMemoryError: unable to create new native thread).\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 42"
        }
    },
    "203": {
        "page_content": "**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\nIf the `bigcust` tables are empty, manually adding metrics won't be effective. In such cases, it's crucial to conduct a detailed investigation to pinpoint the specific step within the IPVPN-SLA process where the failure occurred. Such an investigation is documented [here](../procedures/ipvpn_sm_AppEmptyQuery_resolution_MoP.md)\n### SMSystemException",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 43"
        }
    },
    "204": {
        "page_content": "### SMSystemException\nThe above exception is solely due to SQM server's inability to ingest the information sent by IPVPN-SM App.\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n### SMValidationException\nThis occurs in the case when the schema of the data sent does not comply with the agreed schema.\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n### AppQueryIngestionException",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 44"
        }
    },
    "205": {
        "page_content": "### AppQueryIngestionException\nThis can occur due to inability to apply the particular transformation on the retrieved data.\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n### SMAuthException\nThis occurs if the token used as basic HTTP authentication to the SQM endpoint is rejected.\n**Support Actions:**\n1. [Update keystore with the new credentials](#update-http-auth-token)\n2. [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n## Actions",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 45"
        }
    },
    "206": {
        "page_content": "2. [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n## Actions\n### Update HTTP Auth Token\n1. Create keystore\n```\nkeytool -importpass -alias sm_token -storetype JCEKS -validity 365000 -keystore credentials.keystore\n```\n2. Enter <keystore_pw>\n2. Save the given credentials in the format: `username:password`\n#### Instructions to update the file if given new credentials\n1. Update keystore\n`keytool -keypasswd -keystore credentials.keystore -alias sm_token`\n2. Enter <keystore_pw>",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 46"
        }
    },
    "207": {
        "page_content": "`keytool -keypasswd -keystore credentials.keystore -alias sm_token`\n2. Enter <keystore_pw>\n3. Enter new credentials in the format: `username:password`\n### Call the IPVPN-SM App manually on un2",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 47"
        }
    },
    "208": {
        "page_content": "### Call the IPVPN-SM App manually on un2\nIn case we want to bypass the scripts running on un2 we can post a request directly on the IPVPN-SM application, either by performing a curl request specifying the [request body](#request-body) details or by running the script `/shared/abc/ip_vpn/sm-replacement/scripts/sm-replacement-call-repeater.sh` providing the arguments:\n- `category`, one the valid category types {AV,QOS,CPU,MEM,IF}\n- `reference-start-time` in the format yyyyMMddHHmm",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 48"
        }
    },
    "209": {
        "page_content": "- `reference-start-time` in the format yyyyMMddHHmm\n- `reference-end-time` in the format yyyyMMddHHmm (inclusive)\nThe script will successively call the `refresh_*_.sh` and  `compute_metrics_via_sm_app.sh` scripts for the given category and the period specified by the end and start arguments.\n1. In case we want perform the execution for just **one 5m interval**, we provide the same argument for `reference-start-time` and `reference-end-time` fields. E.g. for 5m period of 18/12/2022 10:10:\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 49"
        }
    },
    "210": {
        "page_content": "```\n./sm-replacement-call-repeater.sh CPU 202212181010 202212181010\n```\n2. If we want to perform requests for **5 multiple consecutive** 5min intervals E.g. for 5m periods of 18/12/2022 10:10 to 10:30 (inclusive):\n```\n./sm-replacement-call-repeater.sh CPU 202212181010 202212181030\n```\nThe logs of this operation are appended to the log files in `/shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ipvpn_sm_replacement.md - Part 50"
        }
    },
    "211": {
        "page_content": "# Streamsets\n**Utility Node / Server:** `un2.bigdata.abc.gr`  \n**User:** `sdc`  \n**[Password](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)**   \n**Logs:** `/shared/sdc/log/sdc.log`  \n**Log Retention:** `10 days`  \n**Configuration:** `/shared/sdc/configuration/pipelines.properties`  \n**Streamsets:** `https://un2.bigdata.abc.gr:18636`  \n**Hue:** `https://un-vip.bigdata.abc.gr:8888`\n## Streamsets Flows",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 1"
        }
    },
    "212": {
        "page_content": "**Hue:** `https://un-vip.bigdata.abc.gr:8888`\n## Streamsets Flows\n`Streamsets Flows` are used for getting files from sftp remdef resources, processing them, storing them into HDFS directories and loading the file data into Hive and Impala tables. The tables are partitioned based on the file name which contain a timestamp (e.g. \\*\\_20181121123916.csv -> par_dt='20181121'). \n``` mermaid\n  graph TD\n    C[Remdef Sftp Server]\n    A[SFTP] --> |Transform and Place Files|B[HDFS]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 2"
        }
    },
    "213": {
        "page_content": "``` mermaid\n  graph TD\n    C[Remdef Sftp Server]\n    A[SFTP] --> |Transform and Place Files|B[HDFS]\n    B --> |Transform and Run Query|D[Hive]\n    style C fill:#5d6d7e\n```\n### AUMS\n| Pipelines | Status |\n| --------- | ------ |\n| AUMS Data File Feed | Running |\n| AUMS Metadata File Feed | Running |\n#### AUMS Data File Feed\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`   \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/aums/`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 3"
        }
    },
    "214": {
        "page_content": "**SFTP Password:** `passwordless`   \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/aums/`  \n**SFTP Compressed File:** `aems_data_*.zip` containing `data_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/aums/archive_data` \n**Hive Database:** `aums`  \n**Hive Table Name:** `archive_data`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `AUMS Data File Feed`\n#### AUMS Metadata File Feed\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`    \n**SFTP Server:** `999.999.999.999`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 4"
        }
    },
    "215": {
        "page_content": "**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`    \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/aums/`  \n**SFTP Compressed File:** `aems_data_*.zip` containing `metadata_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/aums/archive_metadata` \n**Hive Database:** `aums`  \n**Hive Table Name:** `archive_metadata`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `AUMS Metadata File Feed`\n### EEMS\n| Pipelines | Status |\n| --------- | ------ |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 5"
        }
    },
    "216": {
        "page_content": "### EEMS\n| Pipelines | Status |\n| --------- | ------ |\n| EEMS Data File Feed | Running |\n| EEMS Metadata File Feed | Running |\n#### EEMS Data File Feed\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`   \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/aums_eems/`  \n**SFTP Compressed File:** `aems_data_*.zip` containing `data_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/aums/eems_archive_data/` \n**Hive Database:** `aums`  \n**Hive Table Name:** `eems_archive_data`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 6"
        }
    },
    "217": {
        "page_content": "**Hive Database:** `aums`  \n**Hive Table Name:** `eems_archive_data`  \n**Hive Retention:** `2 years`\n**Logs `grep` keyword**: `EEMS Data File Feed`\n#### EEMS Metadata File Feed\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/aums_eems/`  \n**SFTP Compressed File:** `aems_data_*.zip` containing `metadata_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/aums/eems_archive_metadata/` \n**Hive Database:** `aums`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 7"
        }
    },
    "218": {
        "page_content": "**HDFS Path:** `/ez/landingzone/StreamSets/aums/eems_archive_metadata/` \n**Hive Database:** `aums`  \n**Hive Table Name:** `eems_archive_metadata`  \n**Hive Retention:** `2 years`\n**Logs `grep` keyword**: `EEMS Metadata File Feed`\n### Energy-Efficiency\n| Pipelines | Status |\n| --------- | ------ |\n| energy_efficiency enodeb_auxpiu | Running |\n| energy_efficiency enode_boards | Running |\n| energy_efficiency enodeb_vswr | Running |\n| energy_efficiency nodeb_auxpiu | Running |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 8"
        }
    },
    "219": {
        "page_content": "| energy_efficiency enodeb_vswr | Running |\n| energy_efficiency nodeb_auxpiu | Running |\n| energy_efficiency nodeb_boards | Running |\n| energy_efficiency nodeb_vswr | Running |\n| energy_efficiency tcu_temperatures | Running | \n| energy_efficiency cells | Running |\n| energy_efficiency Huawei_potp_sdh_hour | _Stopped_ |\n| energy_efficiency Huawei_potp_wdm_hour | _Stopped_ |\n| energy_efficiency baseband FAN TEST | Running |\n| energy_efficiency baseband RET TEST | Running |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 9"
        }
    },
    "220": {
        "page_content": "| energy_efficiency baseband FAN TEST | Running |\n| energy_efficiency baseband RET TEST | Running |\n| energy_efficiency baseband SFP TEST | Running |\n| energy_efficiency baseband TEMP SERIAL TEST | Running |\n| energy_efficiency baseband VSWR TEST | Running |\n| energy_efficiency basebandsouth FAN TEST | Running |\n| energy_efficiency basebandsouth RET TEST | Running |\n| energy_efficiency basebandsouth SFP TEST | Running |\n| energy_efficiency basebandsouth TEMP SERIAL TEST | Running |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 10"
        }
    },
    "221": {
        "page_content": "| energy_efficiency basebandsouth TEMP SERIAL TEST | Running |\n| energy_efficiency basebandsouth VSWR TEST | Running |\n#### Energy Efficiency enodeb_auxpiu\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `eNodeB_AuxPIU_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/enodeb_auxpiu/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `enodeb_auxpiu`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 11"
        }
    },
    "222": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `enodeb_auxpiu`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enodeb_auxpiu`\n#### Energy Efficiency enode_boards\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `eNodeB_boards_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/enodeb_board/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 12"
        }
    },
    "223": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `enodeb_board`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enode_boards`\n#### Energy Efficiency enodeb_vswr\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `eNodeB_VSWR_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/enodeb_vswr` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 13"
        }
    },
    "224": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `enodeb_vswr`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enodeb_vswr`\n#### Energy Efficiency nodeb_auxpiu\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `NodeB_AuxPIU_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/nodeb_auxpiu/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 14"
        }
    },
    "225": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `nodeb_auxpiu`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enodeb_auxpiu`\n#### Energy Efficiency nodeb_boards\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `NodeB_boards_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/nodeb_board/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 15"
        }
    },
    "226": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `nodeb_board`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency enode_boards`\n#### Energy Efficiency nodeb_vswr\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `NodeB_VSWR_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/nodeb_vswr/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 16"
        }
    },
    "227": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `nodeb_vswr`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency nodeb_vswr`\n#### Energy Efficiency tcu_temperatures\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `TCU_tempratures_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/tcu_temperatures/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 17"
        }
    },
    "228": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `tcu_temperatures`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency tcu_temperatures`\n#### Energy Efficiency cells\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `NodeB_Cells_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/cell/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 18"
        }
    },
    "229": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `cell`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency cells`\n#### Energy Efficiency Huawei_potp_sdh_hour\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `none`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/huawei_potp_sdh_hour/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 19"
        }
    },
    "230": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `huawei_potp_sdh_hour`  \n**Hive Retention:** `none`\n#### Energy Efficiency Huawei_potp_wdm_hour\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `none`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/huawei_potp_wdm_hour/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `huawei_potp_wdm_hour`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 20"
        }
    },
    "231": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `huawei_potp_wdm_hour`  \n**Hive Retention:** `none`\n#### Energy Efficiency baseband FAN TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_FAN_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_fan/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_fan`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 21"
        }
    },
    "232": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_fan`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband FAN TEST`\n#### Energy Efficiency baseband RET TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_RET_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_ret/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 22"
        }
    },
    "233": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_ret`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband RET TEST`\n#### Energy Efficiency baseband SFP TEST \n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_SFP_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_sfp/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 23"
        }
    },
    "234": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_sfp`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband SFP TEST`\n#### Energy Efficiency baseband TEMP SERIAL TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_TEMP_SERIAL_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_temp_serial/`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 24"
        }
    },
    "235": {
        "page_content": "**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_temp_serial/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_temp_serial`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth TEMP SERIAL TEST`\n#### Energy Efficiency baseband VSWR TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_VSWR_*.csv`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 25"
        }
    },
    "236": {
        "page_content": "**SFTP Path:** `/ossrc/`  \n**SFTP File:** `baseband_VSWR_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandnorth_vswr/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandnorth_vswr`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth VSWR TEST`\n#### Energy Efficiency basebandsouth FAN TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 26"
        }
    },
    "237": {
        "page_content": "**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_FAN_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandsouth_fan/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_fan`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth FAN TEST`\n#### Energy Efficiency basebandsouth RET TEST\n**SFTP User:** `bigd`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 27"
        }
    },
    "238": {
        "page_content": "#### Energy Efficiency basebandsouth RET TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_RET_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandsouth_ret/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_ret`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband RET TEST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 28"
        }
    },
    "239": {
        "page_content": "**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband RET TEST`\n#### Energy Efficiency basebandsouth SFP TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_SFP_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandsouth_sfp/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_sfp`  \n**Hive Retention:** `none`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 29"
        }
    },
    "240": {
        "page_content": "**Hive Table Name:** `basebandsouth_sfp`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth SFP TEST`\n#### Energy Efficiency basebandsouth TEMP SERIAL TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_TEMP_SERIAL_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/basebandsouth_temp_serial/` \n**Hive Database:** `energy_efficiency`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 30"
        }
    },
    "241": {
        "page_content": "**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_temp_serial`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency baseband TEMP SERIAL TEST`\n#### Energy Efficiency basebandsouth VSWR TEST\n**SFTP User:** `bigd`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/ossrc/`  \n**SFTP File:** `basebandsouth_VSWR_*.csv`  \n**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/' basebandsouth_vswr'/`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 31"
        }
    },
    "242": {
        "page_content": "**HDFS Path:** `/ez/landingzone/StreamSets/energy_efficiency/' basebandsouth_vswr'/` \n**Hive Database:** `energy_efficiency`  \n**Hive Table Name:** `basebandsouth_vswr`  \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `energy_efficiency basebandsouth VSWR TEST`\n### Nemo\n| Pipelines | Status |\n| --------- | ------ |\n| Nemo Network Connectivity | Running |\n| Nemo Video | _Stopped_ |\n| Nemo Voice | Running |\n| Nemo Signal Coverage | Running |\n| Nemo Datahttp | Running |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 32"
        }
    },
    "243": {
        "page_content": "| Nemo Voice | Running |\n| Nemo Signal Coverage | Running |\n| Nemo Datahttp | Running |\n| Nemo Web | _Stopped_ |\n| Nemo Data Session v2 | Running | \n| Nemo Streaming Session | Running |\n| Nemo Call Session | Running |\n#### Nemo Network Connectivity\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `netcon__1_*.gz`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 33"
        }
    },
    "244": {
        "page_content": "**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `netcon__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/network_connectivity_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `network_connectivity_details_investigation`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Network Connectivity`\n#### Nemo Video\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 34"
        }
    },
    "245": {
        "page_content": "**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `video__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/video_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `video_details_investigation`  \n**Hive Retention:** `60 partitions`\n#### Nemo Voice\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 35"
        }
    },
    "246": {
        "page_content": "**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `voice__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/voice_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `voice_details_investigation`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Voice`\n#### Nemo Signal Coverage\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 36"
        }
    },
    "247": {
        "page_content": "#### Nemo Signal Coverage\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `cov__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/signal_coverage_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `signal_coverage_details_investigation`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Signal Coverage`\n#### Nemo Datahttp",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 37"
        }
    },
    "248": {
        "page_content": "**Logs `grep` keyword**: `Nemo Signal Coverage`\n#### Nemo Datahttp\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `datahttp__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/datahttp_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `datahttp_details_investigation`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Datahttp`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 38"
        }
    },
    "249": {
        "page_content": "**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Datahttp`\n#### Nemo Web\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/`  \n**SFTP Compressed File:** `web__1_*.gz`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/web_details_investigation/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `web_details_investigation`  \n**Hive Retention:** `60 partitions`\n#### Nemo Data Session v2",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 39"
        }
    },
    "250": {
        "page_content": "**Hive Retention:** `60 partitions`\n#### Nemo Data Session v2\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/Handy_Files/`  \n**SFTP File:** `DATA_*.csv`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/data_session/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `data_session`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Data Session v2`\n#### Nemo Streaming Session",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 40"
        }
    },
    "251": {
        "page_content": "**Logs `grep` keyword**: `Nemo Data Session v2`\n#### Nemo Streaming Session\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/Handy_Files/`  \n**SFTP File:** `STREAMING_*.csv`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/streaming_session/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `streaming_session`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Streaming Session`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 41"
        }
    },
    "252": {
        "page_content": "**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Streaming Session`\n#### Nemo Call Session\n**SFTP User:** `nbi`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/data/ftp/sftp/uploads/Handy_Files/`  \n**SFTP File:** `CALL_*.csv`\n**HDFS Path:** `/ez/landingzone/StreamSets/nemo/call_session/`\n**Hive Database:** `nemo`  \n**Hive Table Name:** `call_session`  \n**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Call Session`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 42"
        }
    },
    "253": {
        "page_content": "**Hive Retention:** `60 partitions`\n**Logs `grep` keyword**: `Nemo Call Session`\n### Open Weather Map\n| Pipelines | Status |\n| --------- | ------ |\n| open_weather_map_pipeline | Running |\n#### Open weather map pipeline\n**SFTP User:** `ipvpn`  \n**SFTP Password:** `passwordless`  \n**SFTP Server:** `999.999.999.999`  \n**SFTP Path:** `/shared/vantage_ref-data/REF-DATA/OpenWeatherMap/`  \n**SFTP File:** `OpenWeatherMap_*`\n**HDFS Paths:**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 43"
        }
    },
    "254": {
        "page_content": "**SFTP File:** `OpenWeatherMap_*`\n**HDFS Paths:**\n- `/ez/landingzone/StreamSets/open_weather_map/openweathermap_final/{pardt}/{weather}`  \n- `/ez/landingzone/StreamSets/open_weather_map/openweathermap_forecast/{pardt}/{weather}`\n**Hive Database:** `open_weather_map`  \n**Hive Table Names:**\n- `openweathermap_forecast`  \n- `openweathermap_final`    \n**Hive Retention:** `none`\n**Logs `grep` keyword**: `open_weather_map_pipeline`\n## Monitoring \n_Connection Details_\n**Database Type:** `mysql`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 44"
        }
    },
    "255": {
        "page_content": "## Monitoring \n_Connection Details_\n**Database Type:** `mysql`   \n**Host:** `db-vip.bigdata.abc.gr:3306`  \n**DB Name:** `monitoring`  \n**DB User:** `monitoring`  \n**DB Password:** `https://metis.ghi.com/obss/bigdata/abc/devops/devops-projects/-/blob/master/System_Users/abc_dev.kdbx`  \n**Table:** `jobstatus`  \n**Connection command:** `/usr/bin/mysql -u monitoring -p -h db-vip.bigdata.abc.gr:3306 monitoring`\n_General details_\n**Requests:**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 45"
        }
    },
    "256": {
        "page_content": "_General details_\n**Requests:**\n- **Add:** Monitoring `add http requests` for **only** `SUCCESS` status. (FAILED status is not handled)\n- **Email:** If the pipeline `fails` to execute at any stage, an email alert is sent through the Streamsets UI.  \n**operativePartition:** is created from the filename *_YYYYMMDD\\*.csv\n### EEMS\n#### EEMS Data File Feed\n##### Components\n| Component | Status | Description |\n| ------ | ------ | ------ |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 46"
        }
    },
    "257": {
        "page_content": "##### Components\n| Component | Status | Description |\n| ------ | ------ | ------ |\n| SFTP_HDFS | SUCCESS | Sftp get the raw files from the remdef server, process them and put the parsing files into HDFS landingzone |\n| MAIN | SUCCESS | Indicates the status of the whole load |\nFor `FAILED` components an `email` is sent through `Streamsets UI`.\n##### Records\nFor each execution the following set of messages will be recorded in the Monitoring database.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 47"
        }
    },
    "258": {
        "page_content": "For each execution the following set of messages will be recorded in the Monitoring database.\n| id | execution_id | application | job | component | operative_partition | status | system_ts | \n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n| 813749 | 7db009eb-e2b7-4379-8c00-393ac732b66e | EEMS | EEMS_DATA_FILE_FEED | SFTP_HDFS | 20230104 | SUCCESS | 2023-01-05T01:20:23.000Z |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 48"
        }
    },
    "259": {
        "page_content": "| 813750 | 7db009eb-e2b7-4379-8c00-393ac732b66e | EEMS | EEMS_DATA_FILE_FEED | MAIN | 20230104 | SUCCESS | 2023-01-05T01:20:28.000Z |\n##### Database Queries\n###### MySQL: List details of the last load\n```\nselect \nexecution_id, id, application, job, component, operative_partition,  \nstatus, system_ts, system_ts_end, message   \nfrom jobstatus a where upper(job) like 'EEMS_DATA_FILE_FEED%'   \nand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'EEMS_DATA_FILE_FEED%');",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 49"
        }
    },
    "260": {
        "page_content": "```\n###### Application: List details of specific load\n```\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=EEMS&job=EEMS_DATA_FILE_FEED&component=SFTP_HDFS&operativePartition=20230104'\n```\n#### EEMS Metadata File Feed\n##### Components\n| Component | Status | Description |\n| ------ | ------ | ------ |\n| SFTP_HDFS | SUCCESS | Sftp get the raw files from the remdef server, process them and put the parsing files into HDFS landingzone |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 50"
        }
    },
    "261": {
        "page_content": "| MAIN | SUCCESS | Indicates the status of the whole load |\nFor `FAILED` components an `email` is sent through `Streamsets UI`.\n##### Records\nFor each pipeline execution the following set of messages will be recorded in the Monitoring database.\n| id | execution_id | application | job | component | operative_partition | status | system_ts | \n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 51"
        }
    },
    "262": {
        "page_content": "| ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n| 808931 | 7bfb8fda-1573-46d0-be7f-a9f297538042 | EEMS | EEMS_METADATA_FILE_FEED | SFTP_HDFS | 20230104 | SUCCESS | 2023-01-04T17:28:03.000Z |\n| 808932 | 7bfb8fda-1573-46d0-be7f-a9f297538042 | EEMS | EEMS_METADATA_FILE_FEED | MAIN | 20230104 | SUCCESS | 2023-01-04T17:28:07.000Z |\n##### Database Queries\n###### MySQL: List details of the last load\n```\nselect",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 52"
        }
    },
    "263": {
        "page_content": "##### Database Queries\n###### MySQL: List details of the last load\n```\nselect \nexecution_id, id, application, job, component, operative_partition,  \nstatus, system_ts, system_ts_end, message   \nfrom jobstatus a where upper(job) like 'EEMS_METADATA_FILE_FEED%'   \nand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'EEMS_METADATA_FILE_FEED%');\n```\n###### Application: List details of specific load\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 53"
        }
    },
    "264": {
        "page_content": "```\n###### Application: List details of specific load\n```\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=EEMS&job=EEMS_METADATA_FILE_FEED&component=SFTP_HDFS&operativePartition=20230104'\n```\n## Troubleshooting\nFollowing troubleshooting steps apply to all pipelines.\n**Step 1:** Log in to [Stremsets](https://un2.bigdata.abc.gr:18636/) with `sdc` user.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 54"
        }
    },
    "265": {
        "page_content": "**Step 1:** Log in to [Stremsets](https://un2.bigdata.abc.gr:18636/) with `sdc` user.\n1. Check that the specific pipeline has the status `RUNNING`. If it has any other status continue the investigation.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 55"
        }
    },
    "266": {
        "page_content": "1. Open a pipeline. Check the generic summary (click anywhere in the board and select summary) to get an overview through dashboards on the data processed and errors. Also check if any job of the pipeline has errors (click on an action box and select tab Errors). Streamset will provide a specific ERROR message.\n**Step 2:** Look at the `logs`.\n**TO VIEW THE LOG FILE FOR A SPECIFIC PIPELINE:** `cat sdc.log | grep -i '<pipeline-grep-keyword>'`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 56"
        }
    },
    "267": {
        "page_content": "**TO VIEW THE LOG FILE FOR A SPECIFIC PIPELINE:** `cat sdc.log | grep -i '<pipeline-grep-keyword>'`\n1. Through the `Stramsets UI`, `logs` can be viewed by pressing the paper icon (second) on the top right corner of the selected pipeline environment.\n1. Open `logs` and apply some filters in order to retrieve the information related to the specific pipeline. Logs can be found at `/shared/sdc/log/sdc.log`. To search logs use `grep` or open log file with `less sdc.log` and search `'/'`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 57"
        }
    },
    "268": {
        "page_content": "1. Grep for `Started reading file` to see when a new file is parsed successfully:\n        ```\n        cat sdc.log | grep -i 'Started reading file'\n        ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 58"
        }
    },
    "269": {
        "page_content": "```\n        cat sdc.log | grep -i 'Started reading file'\n        ```\n        > 2022-03-22 14:00:03,419 [user:\\*sdc] [pipeline:energy_efficiency basebandsouth RET TEST/energyeffd112ecef-f20d-45ff-bef4-b88f2117e3d7] [runner:] [thread:ProductionPipelineRunnable-energyeffd112ecef-f20d-45ff-bef4-b88f2117e3d7-energy_efficiency basebandsouth RET TEST] INFO  RemdefDownloadSource - **Started reading file**: /basebandsouth_RET_20220322-092713.csv",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 59"
        }
    },
    "270": {
        "page_content": "1. Grep `Error while attempting to parse file` for error while parsing files:\n        ```\n        cat sdc.log | grep -i 'Error while attempting to parse file'\n        ```\n        > ERROR RemdefDownloadSource - **Error while attempting to parse file**: /baseband_TEMP_SERIAL_20220322-081534.csv\n        > java.io.IOException: (line 3331) invalid char between encapsulated token and delimiter\n    1. Grep `A JVM error occurred while running the pipeline` for JVM errors:\n        ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 60"
        }
    },
    "271": {
        "page_content": "1. Grep `A JVM error occurred while running the pipeline` for JVM errors:\n        ```\n        cat sdc.log | grep -i 'A JVM error occurred while running the pipeline'\n        ```\n        > ERROR ProductionPipelineRunnable - A JVM error occurred while running the pipeline, java.lang.OutOfMemoryError: Java heap     space java.lang.OutOfMemoryError: Java heap space\n    1. Grep `ERROR` for any errors that might occur.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 61"
        }
    },
    "272": {
        "page_content": "1. Grep `ERROR` for any errors that might occur.\n    1. Following `WARN` message with exceptions does not affect the insertion of data:\n        ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 62"
        }
    },
    "273": {
        "page_content": "```\n        2022-03-31 00:20:14,058 [user:*sdc] [pipeline:AUMS Metadata File Feed/AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2] [runner:0] [thread:ProductionPipelineRunnable-AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2-AUMS Metadata File Feed] WARN  UserGroupInformation - PriviledgedActionException as:sdc/un2.bigdata.abc.gr@CNE.abc.GR (auth:KERBEROS) cause:java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 63"
        }
    },
    "274": {
        "page_content": "2022-03-31 00:20:14,058 [user:*sdc] [pipeline:AUMS Metadata File Feed/AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2] [runner:0] [thread:ProductionPipelineRunnable-AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2-AUMS Metadata File Feed] INFO  HiveConfigBean - Connection to Hive become stale, reconnecting.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 64"
        }
    },
    "275": {
        "page_content": "2022-03-31 00:20:14,058 [user:*sdc] [pipeline:AUMS Metadata File Feed/AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2] [runner:0] [thread:ProductionPipelineRunnable-AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2-AUMS Metadata File Feed] WARN  TIOStreamTransport - Error closing output stream.\n        java.net.SocketException: Socket is closed\n        ......",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 65"
        }
    },
    "276": {
        "page_content": "java.net.SocketException: Socket is closed\n        ......\n        2022-03-31 00:20:14,059 [user:*sdc] [pipeline:AUMS Metadata File Feed/AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2] [runner:0] [thread:ProductionPipelineRunnable-AUMSMetadf3f82d52-871f-4df9-b9e1-c5e8180971c2-AUMS Metadata File Feed] INFO  HiveConfigBean - Error closing stale connection Error while cleaning up the server resources\n        java.sql.SQLException: Error while cleaning up the server resources\n        ......",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 66"
        }
    },
    "277": {
        "page_content": "java.sql.SQLException: Error while cleaning up the server resources\n        ......\n        Caused by: org.apache.thrift.transport.TTransportException: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe (Write failed)\n        ......\n        Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe (Write failed)\n        ......",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 67"
        }
    },
    "278": {
        "page_content": "......\n        Caused by: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe (Write failed)\n        ......\n        Caused by: java.net.SocketException: Broken pipe (Write failed)\n        ......\n        ```\n**Step 3:** Log in to [Hue](https://un-vip.bigdata.abc.gr:8888/hue/editor/?type=impala) with `intra` user, and check the status of loaded partitions of the tables which correspond to the pipeline.\n1. Run query `show partitions <database>.<table>`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 68"
        }
    },
    "279": {
        "page_content": "1. Run query `show partitions <database>.<table>`. \n1. If the table has no partitions or no stats you can use following query to check the partitions under investigation:  \n`select count(*), par_dt from <database>.<table> where par_dt > '<partition>' group by par_dt order by par_dt desc;`\n  - Ndef: Execute `REFRESH <table_name>` if Hive and Impala tables have different data.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 69"
        }
    },
    "280": {
        "page_content": "- Ndef: Execute `REFRESH <table_name>` if Hive and Impala tables have different data.\n**Step 4:** Check if there are files in sftp remdef directory, which haven't been processed and loaded into hive and impala tables. This is accomplished through comparing the file in the remdef directory and the partitions found in the hdfs directory.\n1. Access and view SFTP files in remdef directory\n    1. Login to `un2` and change to `sdc` user.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 70"
        }
    },
    "281": {
        "page_content": "1. Access and view SFTP files in remdef directory\n    1. Login to `un2` and change to `sdc` user. \n    1. From there execute `sftp <sftp-user>@<sftp-server>:<sftp-path>`.\n    1. Run `ls -ltr` to view the latest files in the remdef directory.\n    1. Check that the files have the correct credential permissions and rights, `sftp user` and at least `-rw-r-----` permission. \n1. Access and view partitions in hdfs directory\n    1. Login to `un2` and change to `sdc` user.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 71"
        }
    },
    "282": {
        "page_content": "1. Access and view partitions in hdfs directory\n    1. Login to `un2` and change to `sdc` user.\n    1. From there execute `hdfs dfs -ls <hdfs-path>`.\n    1. Make sure partitions are created in the correct hdfs path.\nFinally, check for each file if there is an equivalent partition created. The partition's format is `YYYYMMDD` and it derives from the file name.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 72"
        }
    },
    "283": {
        "page_content": "**Step 5:** If the errors has been resolved and the pipeline status is (`EDITED` or `STOPPED`), start the pipeline and wait to see if the errors have been indeed fixed and no other errors have occurred due to the latest changes.\n---\n---\n### Common Problems and Ways to Fix them",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 73"
        }
    },
    "284": {
        "page_content": "---\n---\n### Common Problems and Ways to Fix them\nThere are some issues that occur quite often while using `Streamsets` flows. If any of the issues mentioned below happen, firstly follow the general troubleshooting steps to identify the problem that occurred and then follow the steps which correspond to fixing the issue. \n---\n#### Check file(s) has been loaded correctly\nLogin to `un2.bigdata.abc.gr` and change to `sdc` user.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 74"
        }
    },
    "285": {
        "page_content": "Login to `un2.bigdata.abc.gr` and change to `sdc` user. \n**Step 1:** Execute `sftp <sftp-user>@<sftp-server>:<sftp-path>`, run `ls -ltr` to view the latest files in the remdef directory and check that the files have the correct credential permissions and rights. The user must be the `sftp user` and permissions be at least `-rw-r----- `.\nExample:\n```\n-rw-r-----    1 nbi      nbi         87987 Jan  4 11:57 STREAMING_W52_2022.csv",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 75"
        }
    },
    "286": {
        "page_content": "Example:\n```\n-rw-r-----    1 nbi      nbi         87987 Jan  4 11:57 STREAMING_W52_2022.csv\n-rw-r-----    1 nbi      nbi       1795960 Jan  4 11:57 DATA_SESSIONS_W52_2022.csv\n-rw-r-----    1 nbi      nbi        284724 Dec 22 11:56 CALL_SESSIONS_W50_2022.csv\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 76"
        }
    },
    "287": {
        "page_content": "-rw-r-----    1 nbi      nbi        284724 Dec 22 11:56 CALL_SESSIONS_W50_2022.csv\n```\n**Step 2:** Get the file(s) with \"YYYYMMDD-HHMMss\" as the datetime of the file you want to check its integrity from the sftp remdef directory by running `get <filaname> <local-path>`. The file will be copied in `un2` at the location `<local-path>`. A usefull local path is `/tmp/streamsets`. If does not exist just create it `mkdir /tmp/streamsets`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 77"
        }
    },
    "288": {
        "page_content": "**Step 3:** Execute `wc -l <filename>` to count the number of lines the file has.\n**Step 4:** Compare the number of lines with the equivalent value of records in the corresponding partition by running (in [Hue](https://999.999.999.999:8888/hue/editor/?type=impala) or with secimp) \n```\nselect count(*), par_dt from <database>.<table> where par_dt='<partition>' group by par_dt;\n```\nCommand `refresh tables` might be needed.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 78"
        }
    },
    "289": {
        "page_content": "```\nCommand `refresh tables` might be needed.\n**Step 5:** If the `returned result of records of the impala query` is equal to the `returned result of 'wc -l \\*\\_YYYYMMDD-HHMMss.csv' -1`, then the flow was executed correctly for the csv file for the examined date.\n**Step 6:** Clear the local directory from the unnecessary fetched data.\n---\n#### Manually inserting missing data in Hive and Impala\n---\nTo manually insert missing data in Hive and Impala there are two ways.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 79"
        }
    },
    "290": {
        "page_content": "---\nTo manually insert missing data in Hive and Impala there are two ways.\nA. Manually get and put the files with the missing data from the Streamsets pipeline remdef directory **(Suggested Way)**\n  For each file:\n  1. From `un2.bigdata.abc.gr` with user `sdc` execute `sftp <sftp-user>@<sftp-server>:<sftp-path>`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 80"
        }
    },
    "291": {
        "page_content": "1. From `un2.bigdata.abc.gr` with user `sdc` execute `sftp <sftp-user>@<sftp-server>:<sftp-path>`.\n  1. From the sftp remdef directory, fetch locally the missing data by running `get <filename>.csv/zip <local-path>`. The file will be copied in `un2.bigdata.abc.gr` at `<local-path>`. A usefull local path is `/tmp/streamsets`. If does not exist just create it `mkdir /tmp/streamsets`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 81"
        }
    },
    "292": {
        "page_content": "1. From the sftp remdef directory, put the recently fetched data in the directory again with a different name by executing `put <local-path>/<filename>.csv/zip <filename>_tmp.csv/zip`. This will result with the new file having a different name and timestamp (last modified) so the pipeline can see it and process it.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 82"
        }
    },
    "293": {
        "page_content": "1. When the Streamset pipeline has finished processing the data, remove the `<filename>_tmp.csv/zip` file from the remdef sftp directory with the sftp command `rm <filename>_tmp.csv/zip`. `!IMPORTANT`\n  1. Clear the local directory from the unnecessary fetched data.\nB. Configure the offset of the Streamset \n  1. Select the wanted pipeline and `Stop` it\n  1. Select from the top right toolbar of the pipeline the `...` option and press `Reset Origin`\n  1. Select the component `SFTP FTP Client 1`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 83"
        }
    },
    "294": {
        "page_content": "1. Select the component `SFTP FTP Client 1`\n  1. Go to Configuration panel and select the `SFTP/FTP/FTPS` and take the value found in the `File Name Pattern` field\n  1. Change the property `File Name Pattern` with the exact file name you want the stream to start processing. This sets the `offset` to the name of the file you set in the above field.\n  1. `Wait` for the file to be processed\n  1. `Stop` the pipeline again\n  1. `Reset` the `File Name Pattern` to its previous value",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 84"
        }
    },
    "295": {
        "page_content": "1. `Stop` the pipeline again\n  1. `Reset` the `File Name Pattern` to its previous value\n  1. `Start` the pipeline  \n  This will make the Streamset pipeline to process only the specific files. For more information about offset and origin press [here](https://metis.ghi.com/obss/bigdata/documentation/-/wikis/dev/frameworks/streamsets#offset-through-streamset-gui).\n---\n#### Manually correct faulty data in Hive and Impala\n---",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 85"
        }
    },
    "296": {
        "page_content": "---\n#### Manually correct faulty data in Hive and Impala\n---\n**Step 1:** Check the faulty partitions by following the procedure found [here](#check-files-has-been-loaded-correctly).\n**Step 2:** In `Hue` as `intra`, delete existing wrong partitions that overlap with the required interval from kudu table and/or from impala table.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 86"
        }
    },
    "297": {
        "page_content": "- If it is in kudu (10 most recent days are in kudu), do: `ALTER table <database>.<table> DROP IF EXISTS RANGE PARTITION 'v1'<= values < 'v2';`, where v1 and v2 the range of partitions. \n  - If it is in impala, do: `ALTER table <database>.<table> DROP IF EXISTS PARTITION (par_dt='v1');`, where v1 the wanted partition.\n**Step 3:** Follow the instruction [here](#manually-inserting-data-missing-in-hive-and-impala) to load the recently deleted data.\n### Exceptions and Possible Root Causes",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 87"
        }
    },
    "298": {
        "page_content": "### Exceptions and Possible Root Causes\n1.  `CONTAINER_0001 - net.schmizz.sshj.connection.ConnectionException: Stream closed`  \n    - SFTP Server side issue which results to missing data.\n1. `CONTAINER_0001 - net.schmizz.sshj.sftp.SFTPException: Permission denied`  \n    - Files are put in sftp directory with wrong user and file permissions and later changed to the correct ones\n    - Password and user were changed at the SFTP server but not updated in streamsets\n    - SFTP Server side issue",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 88"
        }
    },
    "299": {
        "page_content": "- SFTP Server side issue\n  \n1. `A JVM error occurred while running the pipeline, java.lang.OutOfMemoryError: Java heap space`\n    - SFTP Server read file issue. Logs will have \"Broken transport; encoutered EOF\" errors. This could happen as a result of issues with SFTP Server which causes Java heap space errors.\n1. `TTransportException: java.net.SocketException: Connection closed by remdef host`\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 89"
        }
    },
    "300": {
        "page_content": "1. `TTransportException: java.net.SocketException: Connection closed by remdef host`\n```\n2023-01-12 11:50:21,208 [user:*sdc] [pipeline:EEMS Data File Feed/EEMSData7adbe2c9-4c70-425b-a475-fc766cd02ada] [runner:0] [thread:ProductionPipelineRunnable-EEMSData7adbe2c9-4c70-425b-a475-fc766cd02ada-EEMS Data File Feed] INFO\u00a0 HiveConfigBean - Error closing stale connection Error while cleaning up the server resources\njava.sql.SQLException: Error while cleaning up the server resources\n...",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 90"
        }
    },
    "301": {
        "page_content": "java.sql.SQLException: Error while cleaning up the server resources\n...\nCaused by: org.apache.thrift.transport.TTransportException: java.net.SocketException: Connection closed by remdef host\n2023-01-15 15:00:09,403 [user:sdc] [pipeline:energy_efficiency baseband VSWR TEST/energyeffa1e8ea2e-6e09-4529-889e-740157783fd8] [runner:0] [thread:ProductionPipelineRunnable-energyeffa1e8ea2e-6e09-4529-889e-740157783fd8-energy_efficienc",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 91"
        }
    },
    "302": {
        "page_content": "y baseband VSWR TEST] WARN\u00a0 UserGroupInformation - PriviledgedActionException as:sdc/un2.bigdata.abc.gr@CNE.abc.GR (auth:KERBEROS) cause:java.sql.SQLException: [Simba][ImpalaJDBCDriver](500593) Communication link failure. Failed\nto connect to server. Reason: java.net.SocketException: Broken pipe (Write failed).",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 92"
        }
    },
    "303": {
        "page_content": "to connect to server. Reason: java.net.SocketException: Broken pipe (Write failed).\n2023-01-15 15:00:09,403 [user:sdc] [pipeline:energy_efficiency baseband VSWR TEST/energyeffa1e8ea2e-6e09-4529-889e-740157783fd8] [runner:0] [thread:ProductionPipelineRunnable-energyeffa1e8ea2e-6e09-4529-889e-740157783fd8-energy_efficienc\ny baseband VSWR TEST] INFO\u00a0 HiveConfigBean - Connection to Hive become stale, reconnecting.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 93"
        }
    },
    "304": {
        "page_content": "y baseband VSWR TEST] INFO\u00a0 HiveConfigBean - Connection to Hive become stale, reconnecting.\n2023-01-15 15:00:09,408 [user:sdc] [pipeline:energy_efficiency baseband VSWR TEST/energyeffa1e8ea2e-6e09-4529-889e-740157783fd8] [runner:0] [thread:ProductionPipelineRunnable-energyeffa1e8ea2e-6e09-4529-889e-740157783fd8-energy_efficienc\ny baseband VSWR TEST] INFO\u00a0 HiveConfigBean - Error closing stale connection [Simba][JDBC](10060) Connection has been closed.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 94"
        }
    },
    "305": {
        "page_content": "java.sql.SQLNonTransientConnectionException: [Simba][JDBC](10060) Connection has been closed.\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "streamsets.md - Part 95"
        }
    },
    "306": {
        "page_content": "# CSI-Redis Flow\n## Installation info\n### Data Source\n- Source system: HDFS  \n  - user : `rediscsi`\n  - Parquet files:  \n\t\t- `/ez/warehouse/npce.db/yak_cells/*`  \n\t\t- `/ez/warehouse/csi.db/csi_cell_dashboard_primary_dly/*`  \n\t\t- `/ez/warehouse/csi.db/csi_cell_daily_v3/*`  \n- Local FileSystem Directories\n  - user : `rediscsi`\n\t- exec node : defined by Oozie\n\t- work dir : defined by Oozie\n\t- export dir: `/csiRedis_exp_data`\n- HDFS Directories\n\t- Export dir : `/user/rediscsi/docx-data/csi/parquet/`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 1"
        }
    },
    "307": {
        "page_content": "- HDFS Directories\n\t- Export dir : `/user/rediscsi/docx-data/csi/parquet/`\n\t- Status dir : `/user/rediscsi/docx-data/metatdata/checkpoints`\n#### Scripts-Configuration Location\n- node : `HDFS`\n- user : `rediscsi`\n- scripts path : `hdfs:/user/rediscsi`\n-\tconfigurations path : `hdfs:/user/rediscsi`\n#### Logs Location\n- node : `HDFS`\n- user : `rediscsi`\n- path : `/user/rediscsi/log`\n- log file: `csiRedis.<partition data>.<execution ID>.tar.gz`  \n\t*i.e. `csiRedis.20230420.20230420_230010.tar.gz`*",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 2"
        }
    },
    "308": {
        "page_content": "*i.e. `csiRedis.20230420.20230420_230010.tar.gz`*\n#### Oozie Scheduling\n- user : rediscsi\n- Coordinator :`Redis-CSI_Coordinator`  \n- Workflow : `Redis-CSI_Workflow`  \n- Shell : `/user/rediscsi/100.CSI_Main.sh`\n- runs at : `20:00 UTC Daily`\n#### Database CLI commands\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/def_network_map;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 3"
        }
    },
    "309": {
        "page_content": "- Impala: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr -d def_network_map --ssl -k`\n- MySql*: `mysql -u monitoring -p -h 999.999.999.999 monitoring`\n*\\*The password for the MySql database can be found [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)*\n### Data target\n- Redis VM:`999.999.999.999`\n- Port Forward:`un-vip.bigdata.abc.gr:2223`\n- user: `bigstreamer`\n- scripts path: `/home/bigstreamer/bin`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 4"
        }
    },
    "310": {
        "page_content": "- user: `bigstreamer`\n- scripts path: `/home/bigstreamer/bin`\n-\tLoad Script: `102.CSI_Redis_Load_Data.sh`\n## Data process\n### Set HDFS Export Path\nDefines the export path in HDFS and updates the json configuration files.  \nReplaces the key-word `HDFS_PATH_YYYYMMDD` with the `/user/rediscsi/docx-data/csi/parquet/<execution ID>`  \ni.e. `/user/rediscsi/docx-data/csi/parquet/20230401_102030`  \n### Data Preparation\nExecute the Data preparation Spark jobs\n- AggregateRdCells",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 5"
        }
    },
    "311": {
        "page_content": "### Data Preparation\nExecute the Data preparation Spark jobs\n- AggregateRdCells  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.aggregator.AggregateRdCells cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./aggregate_rd_cells_full.json`\n- AggregateCsiPrimary",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 6"
        }
    },
    "312": {
        "page_content": "- AggregateCsiPrimary  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.aggregator.AggregateCsiPrimary cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar aggregate_csi_primary_inc.json`  \n**IMPORTANT: if any of the above Spark jobs fails then the procedure stops.**\n### Data Aggregation\nExecute the Data Aggregation Spark jobs\n- CSIAveragePerCellId",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 7"
        }
    },
    "313": {
        "page_content": "### Data Aggregation\nExecute the Data Aggregation Spark jobs\n- CSIAveragePerCellId  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.csiarea.CSIAveragePerCellId ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./csi_average_per_cell_id_metrics_predef_all.json`\n- AverageCsi",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 8"
        }
    },
    "314": {
        "page_content": "- AverageCsi  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.avgcsi.AverageCsi ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./avg_csi_metrics_predef_all.json`\n- PLMNCsiCellDistri",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 9"
        }
    },
    "315": {
        "page_content": "- PLMNCsiCellDistri  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.plmncsicelldistribution.PLMNCsiCellDistri ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./plmn_csi_cell_distri_metrics_predef_all.json`\n- TopWorstCsiCellTableAndMap",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 10"
        }
    },
    "316": {
        "page_content": "- TopWorstCsiCellTableAndMap  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.topworstcsi.TopWorstCsiCellTableAndMap ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./top_worst_csi_metrics_predef_all.json`\n- CSIPerLocTimeCharts",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 11"
        }
    },
    "317": {
        "page_content": "- CSIPerLocTimeCharts  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.csibyloc.CSIPerLocTimeChartsToMongo ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./avg_csi_by_loc_metrics_predef_all_mongo.json`\n- TopWorstDeltaCsiCellTableAndMap",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 12"
        }
    },
    "318": {
        "page_content": "- TopWorstDeltaCsiCellTableAndMap  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.topworstdeltacsi.TopWorstDeltaCsiCellTableAndMap ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./top_worst_delta_csi_metrics_inc.json`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 13"
        }
    },
    "319": {
        "page_content": "**Ndef: The Spark jobs above create the export file in HDFS under `/user/rediscsi/docx-data/csi/parquet/<execution ID>`**\n### Get export files from HDFS\nCopy the export files from HDFS to the slave node's local filesystem  \nThe working slave node is defined by Oozie\n\t`hdfs dfs -get /user/rediscsi/docx-data/csi/parquet/<execution ID>/* ./csiRedis_exp_data/<execution ID>`  \n\ti.e. `hdfs dfs -get /user/rediscsi/docx-data/csi/parquet/20230401_102030/* ./csiRedis_exp_data/20230401_102030`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 14"
        }
    },
    "320": {
        "page_content": "### Archive export files\ncreates a compressed tar file which contains all the log files\n\t`tar cvfz ./csiRedis_exp_data/<execution ID>/redisCSI.<execution ID>.tar.gz ./csiRedis_exp_data/<execution ID>`  \n\ti.e. `tar cvfz ./csiRedis_exp_data/<execution ID>/redisCSI.20230401_102030.tar.gz ./csiRedis_exp_data/20230401_102030`\n### Tranfer Archived file to Redis VM\nTranfers the Archived file to Redis VM using `SFTP PUT`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 15"
        }
    },
    "321": {
        "page_content": "### Tranfer Archived file to Redis VM\nTranfers the Archived file to Redis VM using `SFTP PUT`  \n`echo \"put ./csiRedis_exp_data/<execution ID>/redisCSI.<execution ID>.tar.gz <redis_LZ>\" | sftp -o \"StrictHostKeyChecking no\" -i ./id_rsa -P$<redis_Port> $<redis_User>@$<redis_Node>`  \ni.e. `echo \"put ./csiRedis_exp_data/20230401_102030/redisCSI.20230401_102030.tar.gz ./CSI_LZ\" | sftp -o \"StrictHostKeyChecking no\" -i ./id_rsa -P2223 bigstreamer@un-vip.bigdata.abc.gr`\n### Load Data to Redis DB",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 16"
        }
    },
    "322": {
        "page_content": "### Load Data to Redis DB\nExtracts the parquet files from the Archived file and load them into the Redis database  \nExecute the load script `Redis VM:/home/bigstreamer/bin/102.CSI_Redis_Load_Data.sh` remdefly.\n\t`ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa -P2223 bigstreamer@un-vip.bigdata.abc.gr \"/home/bigstreamer/bin/102.CSI_Redis_Load_Data.sh`\n## Monitoring\n### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 17"
        }
    },
    "323": {
        "page_content": "|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \n### Monitoring Message list\nFor each load the following set of messages will be recorded in the Monitoring database.\n```\n+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 18"
        }
    },
    "324": {
        "page_content": "| execution_id    | component                       | job              | operative_partition | status  | system_ts               |\n+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+\n| 20230420_230010 | MAIN_START                      | JOB_BEGIN        | 20230420            | SUCCESS | 2023-04-20 23:00:10.000 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 19"
        }
    },
    "325": {
        "page_content": "| 20230420_230010 | UPDATE_HDFS_EXPORT_PATH         | PRE_TASK         | 20230420            | SUCCESS | 2023-04-20 23:00:10.000 |\n| 20230420_230010 | AGGREGATERDCELLS                | DATA_PREPARATION | 20230420            | SUCCESS | 2023-04-20 23:02:14.000 |\n| 20230420_230010 | AGGREGATECSIPRIMARY             | DATA_PREPARATION | 20230420            | SUCCESS | 2023-04-20 23:06:30.000 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 20"
        }
    },
    "326": {
        "page_content": "| 20230420_230010 | COREKPIANDCSIBYLEVEL            | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:09:29.000 |\n| 20230420_230010 | CSIAVERAGEPERCELLID             | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:12:14.000 |\n| 20230420_230010 | AVERAGECSI                      | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:15:49.000 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 21"
        }
    },
    "327": {
        "page_content": "| 20230420_230010 | PLMNCSICELLDISTRI               | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:17:17.000 |\n| 20230420_230010 | TOPWORSTCSICELLTABLEANDMAP      | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:24:42.000 |\n| 20230420_230010 | CSIPERLOCTIMECHARTSTOMONGO      | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:27:58.000 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 22"
        }
    },
    "328": {
        "page_content": "| 20230420_230010 | TOPWORSTDELTACSICELLTABLEANDMAP | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:29:34.000 |\n| 20230420_230010 | GET_EXP_FILES_FROM_HDFS         | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:29:55.000 |\n| 20230420_230010 | TAR_EXP_FILES                   | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:30:09.000 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 23"
        }
    },
    "329": {
        "page_content": "| 20230420_230010 | SFTP_PUT_EXP_FILE_TO_REDIS      | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:30:13.000 |\n| 20230420_230010 | LOAD_DATA_TO_REDIS_DB           | LOAD_REDIS       | 20230420            | SUCCESS | 2023-04-20 23:33:52.000 |\n| 20230420_230010 | MAIN_END                        | JOB_END          | 20230420            | SUCCESS | 2023-04-20 23:36:01.000 |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 24"
        }
    },
    "330": {
        "page_content": "+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+\n```\n### Monitoring Component list\n```\n+---------------------------------+------------------+--------------------------------------------------------------------------------------------------\n| Component                       | Job              | Description",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 25"
        }
    },
    "331": {
        "page_content": "| Component                       | Job              | Description\n+---------------------------------+------------------+--------------------------------------------------------------------------------------------------\n| MAIN_START                      | JOB_BEGIN        | Procedure Started\n| UPDATE_HDFS_EXPORT_PATH         | PRE_TASK         | Set the HDFS path in Json Configuration files",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 26"
        }
    },
    "332": {
        "page_content": "| UPDATE_HDFS_EXPORT_PATH         | PRE_TASK         | Set the HDFS path in Json Configuration files\n| AGGREGATERDCELLS                | DATA_PREPARATION | Data preparation: `spark-submit` using `aggregate_rd_cells_full.json` config file\n| AGGREGATECSIPRIMARY             | DATA_PREPARATION | Data preparation: `spark-submit` using `aggregate_csi_primary_inc.json` config file",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 27"
        }
    },
    "333": {
        "page_content": "| COREKPIANDCSIBYLEVEL            | DATA_AGGREGATION | Data aggregation: `spark-submit` using `core_kpi_and_csi_by_level_metrics_predef_all` config file\n| CSIAVERAGEPERCELLID             | DATA_AGGREGATION | Data aggregation: `spark-submit` using `csi_average_per_cell_id_metrics_predef_all` config file\n| AVERAGECSI                      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `avg_csi_metrics_predef_all` config file",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 28"
        }
    },
    "334": {
        "page_content": "| PLMNCSICELLDISTRI               | DATA_AGGREGATION | Data aggregation: `spark-submit` using `plmn_csi_cell_distri_metrics_predef_all` config file\n| TOPWORSTCSICELLTABLEANDMAP      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `top_worst_csi_metrics_predef_all` config file\n| CSIPERLOCTIMECHARTSTOMONGO      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `avg_csi_by_loc_metrics_predef_all_mongo` config file",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 29"
        }
    },
    "335": {
        "page_content": "| TOPWORSTDELTACSICELLTABLEANDMAP | DATA_AGGREGATION | Data aggregation: `spark-submit` using `top_worst_delta_csi_metrics_inc` config file\n| GET_EXP_FILES_FROM_HDFS         | POST_TASK        | hdfs copyToLocal the export files\n| TAR_EXP_FILES                   | POST_TASK        | Archive the export files\n| SFTP_PUT_EXP_FILE_TO_REDIS      | POST_TASK        | Tranfers the Archived file to Redis VM",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 30"
        }
    },
    "336": {
        "page_content": "| SFTP_PUT_EXP_FILE_TO_REDIS      | POST_TASK        | Tranfers the Archived file to Redis VM\n| LOAD_DATA_TO_REDIS_DB           | LOAD_REDIS       | Upload the Archived file into Redis database \n| MAIN_END                        | JOB_END          | Procedure Completed\n+---------------------------------+------------------+--------------------------------------------------------------------------------------------------\n```\n### Monitoring database Queries\n- List messages of the last load",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 31"
        }
    },
    "337": {
        "page_content": "```\n### Monitoring database Queries\n- List messages of the last load  \n\t`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n\t```\n    select \n      execution_id, component, job, operative_partition,  \n      status, system_ts, substr(message,1,50) msg\n    from jobstatus a where 1=1\n    and upper(application)='CSI'\n    and execution_id in (select max(execution_id) from jobstatus where upper(application)='CSI' and upper(job)='DATA_PREPARATION')\n    order by a.id\n    ;\n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 32"
        }
    },
    "338": {
        "page_content": "order by a.id\n    ;\n\t```\n### Monitoring Health-Check\n  - Check Monitoring status.  \n\t```\t\n\t$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\n\t\n\t{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 33"
        }
    },
    "339": {
        "page_content": "{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n\t```  \n\t\n\t- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\nAn email will be sent by the system with the point of failure.  \ni.e.\n<pre>",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 34"
        }
    },
    "340": {
        "page_content": "## Troubleshooting\nAn email will be sent by the system with the point of failure.  \ni.e.\n<pre>\nFrom: abc_bigd@abc.gr  \nSubject: CSI - DATA_AGGREGATION: FAILED  \n<b>\nData preparation:top_worst_delta_csi_metrics_inc\nExec_id:20230401_102030\n</b>\nThis is an automated e-mail.  \nPlease do not reply.  \n</pre>\n**Actions**  \n1. Write down the value of `Exec_id` described in the alert email  \n\ti.e. Exec_id:`1673849411`\n2. Log files are stored in HDFS in archived files.  \n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 35"
        }
    },
    "341": {
        "page_content": "i.e. Exec_id:`1673849411`\n2. Log files are stored in HDFS in archived files.  \n```\n$ hdfs dfs -ls /user/rediscsi/log/\n-rw-r--r--   3 rediscsi rediscsi  366865842 2023-04-20 23:35 /user/rediscsi/log/csiRedis.20230420.20230420_230010.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  361801963 2023-04-21 23:38 /user/rediscsi/log/csiRedis.20230421.20230421_230010.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  358913487 2023-04-22 23:41 /user/rediscsi/log/csiRedis.20230422.20230422_230013.tar.gz",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 36"
        }
    },
    "342": {
        "page_content": "-rw-r--r--   3 rediscsi rediscsi  364564867 2023-04-23 23:42 /user/rediscsi/log/csiRedis.20230423.20230423_230010.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  359603322 2023-04-24 23:38 /user/rediscsi/log/csiRedis.20230424.20230424_230009.tar.gz\n```\nEach archived file contains a set of logs related to the specific flow run (execution ID).  \nThe filename contains info about the `<partition data>` and the `<execution ID>`  \n`csiRedis.<partition data>.<execution ID>.tar.gz`\n3. Get the log file",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 37"
        }
    },
    "343": {
        "page_content": "`csiRedis.<partition data>.<execution ID>.tar.gz`\n3. Get the log file  \n- create a new dir to store the log file\n\t```\n\tmkdir -p /tmp/csi_redis_log\n\tcd /tmp/csi_redis_log\n\t```\n- Copy from the HDFS log dir the proper log file according to the `<execution ID>` mentioned in the alert email\n`hdfs dfs -get /user/rediscsi/log/csiRedis.20230401.20230401_102030.tar.gz`\n- Extract the archived log file\n`tar xvfz ${archFile} --strip-components 9 -C .`\n- Searches for Exception messages in log files",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 38"
        }
    },
    "344": {
        "page_content": "`tar xvfz ${archFile} --strip-components 9 -C .`\n- Searches for Exception messages in log files  \n`egrep -i '(Exception:|Coused by)' *.log`  \n4. In case of failure, the flow will try to load the data in the next run.  \n\tjkl-Telecom is not aware of how the data files are produced or the contents in them.  \n\tThe Spark jobs that are used by the flow, have been developed by a partner of abc (an India company).  \n\tjkl-Telecom is responsible for",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 39"
        }
    },
    "345": {
        "page_content": "jkl-Telecom is responsible for \n\t- the execution of Spark jobs to produce the export data files, \n\t- the collection of the export data files (if any), \n\t- the transfer of them in Redis node \n\t- and finally the loading of the export files into the Redis database (using specific Spark jobs).",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "CSI-Redis_Flow.md - Part 40"
        }
    },
    "346": {
        "page_content": "# Brond Retrains Flow\n## Installation info\n### Data Source File\n- Source system: FTP Server  \n  - host :`999.999.999.999`\n  - port :`22`\n  - protocol :`SFTP`\n  - user : `bigd`\n  - spool area : `/ADSL_Brond`\n  - file_type : `Counter_Collection_24H.*.csv.gz`\n  - load_suffix : `LOADED`\n- Local FileSystem Directories\n\t- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n\t- landing_zone : `/data/1/brond_retr_LZ`\n\t- archive_dir : `/data/1/brond_retr_LZ/archives`\n\t- work_dir : `/shared/brond_retr_repo`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 1"
        }
    },
    "347": {
        "page_content": "- archive_dir : `/data/1/brond_retr_LZ/archives`\n\t- work_dir : `/shared/brond_retr_repo`\n- HDFS Directories\n\t- hdfs_Bin : `/user/brond`\n\t- hdfs_dir : `/ez/warehouse/brond.db/landing_zone/brond_retrains`\n\t- hdfs_pending_dir : `/ez/warehouse/brond.db/landing_zone/brond_retrains/not_loaded`\n\t- hdfs_stats_dir : `/ez/warehouse/brond.db/landing_zone/brond_retrains/stats`\n### Scripts-Configuration Location\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 2"
        }
    },
    "348": {
        "page_content": "- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- scripts path : `/shared/abc/brond/DataParser/scripts`\n-\tconfigurations path : `/shared/abc/brond/DataParser/scripts/transferlist/*.trn` (i.e. brond_retrains.trn)\n### Logs Location\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- path : `/shared/abc/brond/DataParser/scripts/log`\n- log file: `002.Brond_Retrains_Load.<YYYYMMDD>.log`\n### Oozie Scheduling\n- user : `brond`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 3"
        }
    },
    "349": {
        "page_content": "- log file: `002.Brond_Retrains_Load.<YYYYMMDD>.log`\n### Oozie Scheduling\n- user : `brond`\n- Coordinator :`Brond_Load_Retrains_Coord_NEW`  \n\truns at : `04:10, 05:10, 06:10, 10:10 UTC`\n- Workflow : `Brond_Load_Retrains_WF_NEW`  \n- Main script : `HDFS:/user/brond/000.Brond_Retrains_Oozie_Main.sh`\n- SSH Identity file : `HDFS:/user/brond/id_rsa`\nNdef: **Main Script** runs `oozie_brond_retrains.sh` located on `un-vip.bigdata.abc.gr` using **ssh** as user **brond**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 4"
        }
    },
    "350": {
        "page_content": "`$ ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa brond@un-vip.bigdata.abc.gr \"/shared/abc/brond/DataParser/scripts/oozie_brond_retrains.sh\"`\n### Hive Tables\n- Target Database: `brond`\n- Target Tables: `brond.brond_retrains_hist`\n### Beeline-Impala Shell commands\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/default;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 5"
        }
    },
    "351": {
        "page_content": "- Impala-shell: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k`\n## Data process\n1. sftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond\n-rw-r--r-- 0 507 500 21902115 Nov 28 07:02 ADSL_Brond/Counter_Collection_24H.328_2022_11_28.csv.gz.LOADED\n-rw-r--r-- 0 507 500 21925249 Nov 29 06:22 ADSL_Brond/Counter_Collection_24H.329_2022_11_29.csv.gz.LOADED",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 6"
        }
    },
    "352": {
        "page_content": "-rw-r--r-- 0 507 500 22107252 Nov 30 06:52 ADSL_Brond/Counter_Collection_24H.330_2022_11_30.csv.gz\n```\n2. rename the raw file(s) in remdef SFTP server by adding the suffix .LOADED\n\t`echo \"rename /ADSL_Brond/Counter_Collection_24H.330_2022_11_30.csv.gz /ADSL_Brond/Counter_Collection_24H.330_2022_11_30.csv.gz.LOADED\" | sftp -oport=22 bigd@999.999.999.999`\n3. unzip raw files using `gzip -d` command in `/data/1/brond_retr_LZ`\n4. parsing raw files in `/data/1/brond_retr_LZ`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 7"
        }
    },
    "353": {
        "page_content": "4. parsing raw files in `/data/1/brond_retr_LZ`\n\t- removes the headers (1st line)\n\t- removes double-qudefs chars\n\t- defines the PAR_DT value from the filename (i.e. Counter_Collection_24H.330_2022_11_30.csv.gz convert to 20221130)\n\t- add the prefix `RETR___` to raw file\n\t- add the suffix `<load time>.parsed` to raw file  \n\t\tLoad time format:`<YYYYMMDD_HHMISS>`  \n\t\ti.e. `RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed`\n\t\t\n5. put raw files into HDFS landingzone",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 8"
        }
    },
    "354": {
        "page_content": "5. put raw files into HDFS landingzone\n\t`hdfs dfs -put /data/1/brond_retr_LZ/RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed /ez/warehouse/brond.db/landing_zone/brond_retrains/RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed`\n6. clean-up any copy of the raw files from local filesystem  \n\t`/data/1/brond_retr_LZ`  \n\t`/shared/brond_retr_repo`  \n7. load HDFS files into hive table `brond.brond_retrains_hist`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 9"
        }
    },
    "355": {
        "page_content": "`/shared/brond_retr_repo`  \n7. load HDFS files into hive table `brond.brond_retrains_hist`\n\t`beeline -e \"LOAD DATA INPATH '/ez/warehouse/brond.db/landing_zone/brond_retrains/RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed' OVERWRITE INTO TABLE brond.brond_retrains_hist PARTITION (par_dt='20221130')\"`\n8. execute compute stats using impala-shell  \n\t```\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\n\t\n\tcompute incremental stats brond.brond_retrains_hist;\n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 10"
        }
    },
    "356": {
        "page_content": "compute incremental stats brond.brond_retrains_hist;\n\t```\nIn the event where multiple files are transfered (files refer to the same data), we proceed with overwriting data in brond table. Deletion of those multiple files is abc's responsibility.\n## Monitoring\n### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 11"
        }
    },
    "357": {
        "page_content": "|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \nConnection command: `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n### Monitoring Message list\nFor each load the following set of messages will be recorded in the Monitoring database.\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 12"
        }
    },
    "358": {
        "page_content": "For each load the following set of messages will be recorded in the Monitoring database.\n```\nid    | execution_id | application | job            | component                        | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 13"
        }
    },
    "359": {
        "page_content": "------+--------------+-------------+----------------+----------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n15807 | 1659939004   | BROND       | BROND_RETRAINS | MAIN                             | 20220808            | SUCCESS | 2022-08-08 09:10:04 | 2022-08-08 09:10:48 | Succesfully Completed. | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 14"
        }
    },
    "360": {
        "page_content": "15809 | 1659939004   | BROND       | BROND_RETRAINS | GET_RAW_RETRAIN_FILES            | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n15811 | 1659939004   | BROND       | BROND_RETRAINS | RENAME_FILES_@SFTP_SERVER        | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 15"
        }
    },
    "361": {
        "page_content": "15813 | 1659939004   | BROND       | BROND_RETRAINS | UNZIP_FILES                      | 20220808            | SUCCESS | 2022-08-08 09:10:07 |                     |                        | brond | un2.bigdata.abc.gr\n15815 | 1659939004   | BROND       | BROND_RETRAINS | PARSING_FILES                    | 20220808            | SUCCESS | 2022-08-08 09:10:13 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 16"
        }
    },
    "362": {
        "page_content": "15817 | 1659939004   | BROND       | BROND_RETRAINS | LOAD_HDFS_LANDINGZONE            | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n15819 | 1659939004   | BROND       | BROND_RETRAINS | CLEAN-UP_THE_INPUT_FILES         | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 17"
        }
    },
    "363": {
        "page_content": "15821 | 1659939004   | BROND       | BROND_RETRAINS | LOAD_HDFS_FILES_INTO_HIVE_TABLES | 20220808            | SUCCESS | 2022-08-08 09:10:37 |                     |                        | brond | un2.bigdata.abc.gr\n15823 | 1659939004   | BROND       | BROND_RETRAINS | POST_SCRIPT                      | 20220808            | SUCCESS | 2022-08-08 09:10:48 |                     |                        | brond | un2.bigdata.abc.gr\n```\n### Monitoring Component list\n|Component | Description \n|-|-|",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 18"
        }
    },
    "364": {
        "page_content": "```\n### Monitoring Component list\n|Component | Description \n|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED| \n|GET_RAW_RETRAIN_FILES|sftp-get the raw files from the remdef server.<br />i.e.<br />Counter_Collection_24H.218_2022_08_08.csv.gz\n|RENAME_FILES_@SFTP_SERVER| Rename the raw files in remdef SFTP server by adding the suffix .LOADED<br />i.e.<br />Counter_Collection_24H.218_2022_08_08.csv.gz.LOADED",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 19"
        }
    },
    "365": {
        "page_content": "|UNZIP_FILES| unzip the raw files using `gzip -d` command\n|PARSING_FILES| removes any control chars (if any) from the raw files\n|LOAD_HDFS_LANDINGZONE|PUT the parsing files into HDFS landingzone `/ez/warehouse/brond.db/landing_zone/brond_retrains`\n|CLEAN-UP_THE_INPUT_FILES|Clean-up any copy of the raw files from the filesystem (`/data/1/brond_retr_LZ`, `/shared/brond_retr_repo`)\n|LOAD_HDFS_FILES_INTO_HIVE_TABLE| Load raw data (files) into the tables<br />`brond.brond_retrains_hist`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 20"
        }
    },
    "366": {
        "page_content": "|POST_SCRIPT| Execute Compute Statistics using impala-shell.<br />`compute incremental stats brond.brond_retrains_hist;`\n### Monitoring database Queries\n- List messages of the last load  \n\t`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n\t```\n\tselect \n\t\texecution_id, id, application, job, component, operative_partition,  \n\t\tstatus, system_ts, system_ts_end, message, user,host   \n\tfrom jobstatus a where upper(job) like 'BROND_RETRAINS%'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 21"
        }
    },
    "367": {
        "page_content": "from jobstatus a where upper(job) like 'BROND_RETRAINS%'   \n\tand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'BROND_RETRAINS%')  \n\t;\n\texecution_id | id    | application | job            | component                        | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 22"
        }
    },
    "368": {
        "page_content": "-------------+-------+-------------+----------------+----------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n\t1659939004   | 15807 | BROND       | BROND_RETRAINS | MAIN                             | 20220808            | SUCCESS | 2022-08-08 09:10:04 | 2022-08-08 09:10:48 | Succesfully Completed. | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 23"
        }
    },
    "369": {
        "page_content": "1659939004   | 15809 | BROND       | BROND_RETRAINS | GET RAW RETRAIN FILES            | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n\t1659939004   | 15811 | BROND       | BROND_RETRAINS | RENAME FILES @SFTP SERVER        | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 24"
        }
    },
    "370": {
        "page_content": "1659939004   | 15813 | BROND       | BROND_RETRAINS | UNZIP FILES                      | 20220808            | SUCCESS | 2022-08-08 09:10:07 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15815 | BROND       | BROND_RETRAINS | PARSING FILES                    | 20220808            | SUCCESS | 2022-08-08 09:10:13 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 25"
        }
    },
    "371": {
        "page_content": "1659939004   | 15817 | BROND       | BROND_RETRAINS | LOAD HDFS LANDINGZONE            | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15819 | BROND       | BROND_RETRAINS | CLEAN-UP THE INPUT FILES         | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 26"
        }
    },
    "372": {
        "page_content": "1659939004   | 15821 | BROND       | BROND_RETRAINS | LOAD HDFS FILES INTO HIVE TABLES | 20220808            | SUCCESS | 2022-08-08 09:10:37 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15823 | BROND       | BROND_RETRAINS | POST SCRIPT                      | 20220808            | SUCCESS | 2022-08-08 09:10:48 |                     |                        | brond | un2.bigdata.abc.gr\n\t```\n### Monitoring Health-Check\n  - Check Monitoring status.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 27"
        }
    },
    "373": {
        "page_content": "```\n### Monitoring Health-Check\n  - Check Monitoring status.  \n\t```\t\n\t$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\n\t\n\t{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 28"
        }
    },
    "374": {
        "page_content": "{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n\t```  \n\t\n\t- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\n- An email will be sent by the system with the point of failure.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 29"
        }
    },
    "375": {
        "page_content": "## Troubleshooting\n- An email will be sent by the system with the point of failure.\n- Check the log file for errors/exceptions  \n\t`egrep -i 'error|fail|exception|problem' /shared/abc/brond/DataParser/scripts/log/002.Brond_Retrains_Load.<YYYYMMDD>.log`\n- List Failed Monitoring messages of the last load  \n\t```\n\t/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n\tselect * from jobstatus where upper(job) like 'BROND_RETRAINS%' \n\tand status='FAILED'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 30"
        }
    },
    "376": {
        "page_content": "select * from jobstatus where upper(job) like 'BROND_RETRAINS%' \n\tand status='FAILED'\n\tand operative_partition=(select max(operative_partition) from jobstatus where upper(job) like 'BROND_RETRAINS%' and operative_partition regexp '[0-9]{8}')\n\torder by id\n\t;\n\tid    | execution_id | application | job            | component             | operative_partition | status | system_ts           | system_ts_end       | message            | user  | host",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 31"
        }
    },
    "377": {
        "page_content": "------+--------------+-------------+----------------+-----------------------+---------------------+--------+---------------------+---------------------+--------------------+-------+-----------------------\n\t14621 |              | BROND       | BROND_RETRAINS | MAIN                  | 20220801            | FAILED | 2022-08-01 16:13:13 | 2022-08-01 16:13:14 | No raw files found | brond | un2.bigdata.abc.gr",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 32"
        }
    },
    "378": {
        "page_content": "14623 |              | BROND       | BROND_RETRAINS | GET RAW RETRAIN FILES | 20220801            | FAILED | 2022-08-01 16:13:14 |                     | No raw files found | brond | un2.bigdata.abc.gr\n\t```\n### Common errors  \n- `No raw files found`, there are no raw files available for loading at remdef server.  \nA WARNING message will be sent to abc by email.\nNo actions required from OBSS. Responsible abc. \n- Other factors not related to the specific flow\n\t- impala/hive availability",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 33"
        }
    },
    "379": {
        "page_content": "- Other factors not related to the specific flow\n\t- impala/hive availability\n\t- Kerberos authentication (A.  \n\t*Ndef: The flow checks if the ticket is still active before any HDFS action.  \n\tIn case of expiration the flow performs a `kinit` command*\n## Manually triggering the workflow\nThere are cases where abc might upload new files after the scheduled workflow timing, and request these files to be",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 34"
        }
    },
    "380": {
        "page_content": "processed in the same day. This can only be done by manually triggering the workflow. Before doing so, you need to make some checks first:\n### Check workflow logs\n1. Login to https://999.999.999.999:8888/hue/accounts/login?next=/hue using the brond account\n2. Go to \"Jobs\" > \"Workflows\"\nIf all workflow executions (\"Brond_Load_Retrains_WF_NEW\") were successful, you can proceed by checking that the file(s)\nabc added, were copied after the scheduled timings of the workflow\n### Check added files",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 35"
        }
    },
    "381": {
        "page_content": "abc added, were copied after the scheduled timings of the workflow\n### Check added files\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond_DWH\n-rw-r--r--    0 507      500      150589497 Nov 29 05:34 ADSL_Brond_DWH/DWH_VDSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      150823890 Nov 29 13:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_29.csv.gz\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 36"
        }
    },
    "382": {
        "page_content": "```\nThe second file was indeed added after the scheduled time and has not been picked up by the workflow.\n### Trigger workflow\nYou can now proceed to manually trigger the workflow:\n1. Go to HUE and select \"Jobs\"\n2. Go to \"Workflow\" and select \"Brond_Load_Retrains_WF_NEW\"\n3. In the next screen, select \"Rerun\"\n4. Wait for the workflow to successfully end\n5. If no errors occur, proceed with:\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 37"
        }
    },
    "383": {
        "page_content": "sftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond_DWH\n-rw-r--r--    0 507      500      150589497 Nov 29 05:34 ADSL_Brond_DWH/DWH_VDSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      150823890 Nov 29 13:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_29.csv.gz\n```\n## Data Check\n- **Check final tables for new partitions**:\n  - Impala-shell: \n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 38"
        }
    },
    "384": {
        "page_content": "```\n## Data Check\n- **Check final tables for new partitions**:\n  - Impala-shell: \n\t```\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\n\trefresh brond.brond_retrains_hist;  \n\tshow partitions brond.brond_retrains_hist;  \n\t\n\tpar_dt   | #Rows   | #Files | Size     | Bytes Cached | Cache Replication | Format | Incremental stats | Location",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 39"
        }
    },
    "385": {
        "page_content": "---------+---------+--------+----------+--------------+-------------------+--------+-------------------+------------------------------------------------------------------------------\n\t20221130 | 2784494 |      1 | 146.16MB | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/brond.db/brond_retrains_hist/par_dt=20221130",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 40"
        }
    },
    "386": {
        "page_content": "Total    | 5569421 |      1 | 146.16MB | 0B           |                   |        |                   |                                                                              \n\t```\n- **Check the amount of data in final tables**:\n  - Impala-shell: \n\t```\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\n\tselect par_dt, count(*) as cnt from brond.brond_retrains_hist group by par_dt order by 1;\n\t\n\tpar_dt   | cnt    \n\t---------+--------\n\t20221130 | 2784494\n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Brond_Retrains_Flow.md - Part 41"
        }
    },
    "387": {
        "page_content": "# Traffica Flow\n## Useful links\n- [Wiki](https://metis.ghi.com/obss/bigdata/abc/etl/traffica/traffica-devops/-/wikis/home)\n- [Infrastructure](https://metis.ghi.com/obss/bigdata/abc/etl/traffica/traffica-devops/-/wikis/Infrastructure)\n## SMS\n``` mermaid\n     graph TD\n      A0[\"abc Flow <br> User: trafficaftp\"]\n      A1[\"Host: cne.def.gr <br> Path: /data/1/trafficaftp/Traffica_XDR\"]\n      A2[\"Staging Directory <br> Path: /data/1/traffica_LZ/sms\"]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 1"
        }
    },
    "388": {
        "page_content": "A2[\"Staging Directory <br> Path: /data/1/traffica_LZ/sms\"]\n      A3(\"Staging HDFS Directory <br> Path: /ez/warehouse/sai.db/landing_zone/sms\")\n      A4(\"Staging Table <br> Hive: sai.sms_load\")\n      A5(\"Staging Table <br> Hive: sai.sms_raw_text\")\n      A6(\"Table <br> Impala: sai.sms_raw\")\n      A7(\"Cleanup <br> Add .LOADED suffix to local raw files <br> Clean local staging directories <br> Clean HDFS tables/directories\")\n    \n      A0 -->|SFTP| A1\n      A1 --> |Merge files| A2",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 2"
        }
    },
    "389": {
        "page_content": "A0 -->|SFTP| A1\n      A1 --> |Merge files| A2\n      A2 --> |HDFS Load| A3\n      A3 --> |Hive Load| A4\n      A4 --> |Hive Insert| A5\n      A5 --> |Impala Insert| A6\n      A6 --> |Successful loaded files only| A7\n```\n**Schedule**: `every 35 minutes`  \n**Scheduler**: `Java Springboot Application`  \n**User**: `traffica`  \n**Active Node**: `unc2.bigdata.abc.gr`  \n**Backup Node**: `unc1.bigdata.abc.gr`  \n**Installation directory**: `/shared/abc/traffica`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 3"
        }
    },
    "390": {
        "page_content": "**Backup Node**: `unc1.bigdata.abc.gr`  \n**Installation directory**: `/shared/abc/traffica`  \n**Logs**: `/shared/abc/traffica/logs`  \n**Configuration File**: `/shared/abc/traffica/config/application.yml`\n**Start command**: `supervisorctl start traffica_sms`  \n**Stop command**: `supervisorctl stop traffica_sms`  \n**Enable command (un-pause)**: `curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\"`\n**Alerts**:\n- Mail with subject: `Traffica Application failed`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 4"
        }
    },
    "391": {
        "page_content": "**Alerts**:\n- Mail with subject: `Traffica Application failed`\nPossible messages:\n1. `Traffica sms main flow failed.`\n2. `Traffica sms application has been paused due to multiple sequential failures. Manual actions are needed.`\n3. `Traffica sms application has been paused due to inability to rename local files. Manual actions are needed.`\n4. `Traffica sms application has been paused due to inability to clean up files. Manual actions are needed.`\n**Troubleshooting steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 5"
        }
    },
    "392": {
        "page_content": "**Troubleshooting steps**:\n- Check to see if the application is running:\n  \n  From `unc2` with personal account:\n  ``` bash\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\"\n  ```\n- Check the logs for errors to identify the root cause\n  From `unc2` as `traffica`:\n  ``` bash\n  # For the current log file\n  grep -i -e error -e exception /shared/abc/traffica/logs/traffica-sms.log\n  # For older compressed files",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 6"
        }
    },
    "393": {
        "page_content": "# For older compressed files\n  zgrep -i -e error -e exception /shared/abc/traffica/logs/<yearMonthFolder>/<name_of_logfile>\n  ```\n- Check metrics and error rates from Grafana\n  Open Firefox using VNC and go to `https://unc1.bigdata.abc.gr:3000/d/qIM5rod4z/traffica`\n  Use panels ending in `Err` to identify problematic components and steps.\n  Use `Files`,`Size`,`Rows` to identify if input has changed\n- If there is a problem renaming files with the `.LOADED` suffix\n  From `unc2` as `traffica`:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 7"
        }
    },
    "394": {
        "page_content": "- If there is a problem renaming files with the `.LOADED` suffix\n  From `unc2` as `traffica`:\n  ``` bash\n  # Get files that where processed correctly\n  grep 'filelist=' /shared/abc/traffica/logs/traffica-sms.log \n  # Move files pending rename from the list above\n  cd /data/1/trafficaftp/Traffica_XDR\n  mv <file>{,.LOADED}\n  ```\n- If the root cause is resolved resume normal operation.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 8"
        }
    },
    "395": {
        "page_content": "mv <file>{,.LOADED}\n  ```\n- If the root cause is resolved resume normal operation.\n  The flow has been designed with auto-recovery. It marks only successfully loaded files as `.LOADED` and handles/cleans up all staging directories on each run.\n  From `unc2` with personal user:\n  ``` bash\n  # Check if scheduling is enabled \n  curl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disabled\"\n  # If the above command returns true",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 9"
        }
    },
    "396": {
        "page_content": "# If the above command returns true\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\"\n  ```\n  - If `unc2` is down, then manually start the application on `unc1` this requires that VIP `cne.def.gr` has been migrated and SFTP is working on `unc1`\n**Ndefs**:\n  From `unc2` with personal account:\n  ``` bash\n  curl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/info/health\" # HTTP 200 if app is up",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 10"
        }
    },
    "397": {
        "page_content": "curl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/info/health\" # HTTP 200 if app is up\n  curl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/info/check\" # returns message if up\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/shutdown/gracefully\" # shutdown application. If flow is running, then wait to finish. App should terminate ONLY with this method.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 11"
        }
    },
    "398": {
        "page_content": "curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disable\" # enable flow scheduling\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable\" # enable flow scheduling\n  curl -X GET \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/disabled\" # true if disabled, else false\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/cleanup/all\" # Run cleanup on demand",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 12"
        }
    },
    "399": {
        "page_content": "curl -X PUT \"http://unc2.bigdata.abc.gr:11483/traffica/app/operations/main/run\" # Run flow on demand\n  # UI with endpoints available from VNC: http://unc2.bigdata.abc.gr:11483/traffica/swagger-ui/index.html#/\n  ```\n## VOICE\n``` mermaid\n     graph TD\n      A0[\"abc Flow <br> User: trafficaftp\"]\n      A1[\"cne.def.gr <br> Path: /data/1/trafficaftp/Traffica_XDR\"]\n      A2[\"Staging Directory <br> Path: /data/1/traffica_LZ/voice\"]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 13"
        }
    },
    "400": {
        "page_content": "A2[\"Staging Directory <br> Path: /data/1/traffica_LZ/voice\"]\n      A3(\"Staging HDFS Directory <br> Path: /ez/warehouse/sai.db/landing_zone/voice\")\n      A4(\"Staging Table <br> Hive: sai.voice_load\")\n      A5(\"Staging Table <br> Hive: sai.voice_raw_text\")\n      A6(\"Staging Table <br> Hive: sai.voice_raw_text_c2c\")\n      A7(\"Table <br> Impala: sai.voice_raw\")\n      A8(\"Cleanup <br> Add .LOADED suffix to local raw files <br> Clean local staging directories <br> Clean HDFS tables/directories\")",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 14"
        }
    },
    "401": {
        "page_content": "A0 -->|SFTP| A1\n      A1 --> |Merge files| A2\n      A2 --> |HDFS Load| A3\n      A3 --> |Hive Load| A4\n      A4 --> |Hive Insert| A5\n      A4 --> |Hive Insert| A6\n      A5 --> |Impala Insert| A7\n      A6 --> |Impala Insert| A7\n      A7 --> |Successful loaded files only| A8\n```\n**Schedule**: `every 20 minutes`  \n**Scheduler**: `Java Springboot Application`  \n**User**: `traffica`  \n**Active Node**: `unc2.bigdata.abc.gr`  \n**Backup Node**: `unc1.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 15"
        }
    },
    "402": {
        "page_content": "**Active Node**: `unc2.bigdata.abc.gr`  \n**Backup Node**: `unc1.bigdata.abc.gr`  \n**Installation directory**: `/shared/abc/traffica`  \n**Logs**: `/shared/abc/traffica/logs`  \n**Configuration File**: `/shared/abc/traffica/config/application.yml`\n**Start command**: `supervisorctl start traffica_voice`  \n**Stop command**: `supervisorctl stop traffica_voice`  \n**Enable command (un-pause)**: `curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/enable\"`\n**Alerts**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 16"
        }
    },
    "403": {
        "page_content": "**Alerts**:\n- Mail with subject: `Traffica Application failed`\nPossible messages:\n1. `Traffica voice main flow failed.`\n2. `Traffica voice application has been paused due to multiple sequential failures. Manual actions are needed.`\n3. `Traffica voice application has been paused due to inability to rename local files. Manual actions are needed.`\n4. `Traffica voice application has been paused due to inability to clean up files. Manual actions are needed.`\n**Troubleshooting steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 17"
        }
    },
    "404": {
        "page_content": "**Troubleshooting steps**:\n- Check to see if the application is running:\n  \n  From `unc2` with personal account:\n  ``` bash\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\"\n  ```\n- Check the logs for errors to identify the root cause\n  From `unc2` as `traffica`:\n  ``` bash\n  # For the current log file\n  grep -i -e error -e exception /shared/abc/traffica/logs/traffica-voice.log\n  # For older compressed files",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 18"
        }
    },
    "405": {
        "page_content": "# For older compressed files\n  zgrep -i -e error -e exception /shared/abc/traffica/logs/<yearMonthFolder>/<name_of_logfile>\n  ```\n- Check metrics and error rates from Grafana\n  Open Firefox using VNC and go to `https://unc1.bigdata.abc.gr:3000/d/qIM5rod4z/traffica`\n  Use panels ending in `Err` to identify problematic components and steps.\n  Use `Files`,`Size`,`Rows` to identify if input has changed\n- If there is a problem renaming files with the `.LOADED` suffix\n  From `unc2` as `traffica`:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 19"
        }
    },
    "406": {
        "page_content": "- If there is a problem renaming files with the `.LOADED` suffix\n  From `unc2` as `traffica`:\n  ``` bash\n  # Get files that where processed correctly\n  grep 'filelist=' /shared/abc/traffica/logs/traffica-voice.log \n  # Move files pending rename from the list above\n  cd /data/1/trafficaftp/Traffica_XDR\n  mv <file>{,.LOADED}\n  ```\n- If the root cause is resolved resume normal operation.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 20"
        }
    },
    "407": {
        "page_content": "mv <file>{,.LOADED}\n  ```\n- If the root cause is resolved resume normal operation.\n  The flow has been designed with auto-recovery. It marks only successfully loaded files as `.LOADED` and handles/cleans up all staging directories on each run.\n  From `unc2` with personal user:\n  ``` bash\n  # Check if scheduling is enabled \n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/disabled\"\n  # If the above command returns true",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 21"
        }
    },
    "408": {
        "page_content": "# If the above command returns true\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/enable\"\n  ```\n  - If `unc2` is down, then manually start the application on `unc1` this requires that VIP `cne.def.gr` has been migrated and SFTP is working on `unc1`\n**Ndefs**:\n  From `unc2` with personal account:\n  ``` bash\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/health\" # HTTP 200 if app is up",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 22"
        }
    },
    "409": {
        "page_content": "curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/health\" # HTTP 200 if app is up\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/info/check\" # returns message if up\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/shutdown/gracefully\" # shutdown application. If flow is running, then wait to finish. App should terminate ONLY with this method.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 23"
        }
    },
    "410": {
        "page_content": "curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/disable\" # enable flow scheduling\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/enable\" # enable flow scheduling\n  curl -X GET \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/lifecycle/disabled\" # true if disabled, else false\n  curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/cleanup/all\" # Run cleanup on demand",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 24"
        }
    },
    "411": {
        "page_content": "curl -X PUT \"http://unc2.bigdata.abc.gr:11482/traffica/app/operations/main/run\" # Run flow on demand\n  # UI with endpoints available from VNC: http://unc2.bigdata.abc.gr:11482/traffica/swagger-ui/index.html#/\n  ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "traffica.md - Part 25"
        }
    },
    "412": {
        "page_content": "[[_TOC_]]\n# 1. Full\n- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - Full Workflow`, \n- **COORDINATOR**: `DWHFixed - Full Coordinator`\n- **HDFS path**: `/user/dwhfixed/full`\n- **Runs**: `15:30, 18:30 (UTC)` \n- **Config file**: `hdfs: /user/dwhfixed/dwhfixed.config`\n- **Tables config/list**: `hdfs: /user/dwhfixed/full/tables_full.config`\n- **Oracle user**: `dm_sas_va`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 1"
        }
    },
    "413": {
        "page_content": "- **Oracle user**: `dm_sas_va`\n- **Oracle pass**: `https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx`. You need password to open this file and get the user's password.\nMoves data from the source tables to a `yesterday` partition of the target tables.\n## 1.1. Oracle Tables (source)\n- `SAS_VA_VIEW.V_BOX_DIM`\n- `SAS_VA_VIEW.V_FIXED_CABLE_DIM`\n- `SAS_VA_VIEW.V_KV_DIM`\n- `SAS_VA_VIEW.V_DSLAM_DIM`\n- `SAS_VA_VIEW.V_SR_TYPE_DIM`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 2"
        }
    },
    "414": {
        "page_content": "- `SAS_VA_VIEW.V_KV_DIM`\n- `SAS_VA_VIEW.V_DSLAM_DIM`\n- `SAS_VA_VIEW.V_SR_TYPE_DIM`\n- `SAS_VA_VIEW.V_SRIA_ADDINFO_DIM`\n- `SAS_VA_VIEW.V_SRIA_STATUS_DIM`\n- `SAS_VA_VIEW.V_SRIA_SERV_PRODUCT_CAT_DIM`\n- `SAS_VA_VIEW.V_SRIA_PRIORITY_DIM`\n- `SAS_VA_VIEW.V_PROVIDER_DIM`\n- `SAS_VA_VIEW.V_def_NETWORK_DIM`\n- `SAS_VA_VIEW.V_def_DIVISION_DIM`\n- `SAS_VA_VIEW.V_SRIA_TYPE_DIM`\n- `SAS_VA_VIEW.V_SRIA_ACT_STATUS_DIM`\n- `SAS_VA_VIEW.V_SRIA_SUBAREA_DIM`\n- `SAS_VA_VIEW.V_POSITION_DIM`\n- `SAS_VA_VIEW.V_CAUSE_DIM`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 3"
        }
    },
    "415": {
        "page_content": "- `SAS_VA_VIEW.V_SRIA_SUBAREA_DIM`\n- `SAS_VA_VIEW.V_POSITION_DIM`\n- `SAS_VA_VIEW.V_CAUSE_DIM`\n- `SAS_VA_VIEW.V_ACTION_DIM`\n- `SAS_VA_VIEW.V_ADSL_DIM`\n- `SAS_VA_VIEW.V_SRIA_AREA_DIM`\n## 1.2. Hive - Impala Tables (target)\n- `dwhfixed.v_box_dim_hist`\n- `dwhfixed.v_fixed_cable_dim_hist`\n- `dwhfixed.v_kv_dim_hist`\n- `dwhfixed.v_dslam_dim_hist`\n- `dwhfixed.v_sr_type_dim_hist`\n- `dwhfixed.v_sria_addinfo_dim_hist`\n- `dwhfixed.v_sria_status_dim_hist`\n- `dwhfixed.v_sria_serv_product_cat_dim_hist`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 4"
        }
    },
    "416": {
        "page_content": "- `dwhfixed.v_sria_status_dim_hist`\n- `dwhfixed.v_sria_serv_product_cat_dim_hist`\n- `dwhfixed.v_sria_priority_dim_hist`\n- `dwhfixed.v_provider_dim_hist`\n- `dwhfixed.v_def_network_dim_hist`\n- `dwhfixed.v_def_division_dim_hist`\n- `dwhfixed.v_sria_type_dim_hist`\n- `dwhfixed.v_sria_act_status_dim_hist`\n- `dwhfixed.v_sria_subarea_dim_hist`\n- `dwhfixed.v_position_dim_hist`\n- `dwhfixed.v_cause_dim__hist`\n- `dwhfixed.v_action_dim_hist`\n- `dwhfixed.v_adsl_dim_hist`\n- `dwhfixed.v_sria_area_dim_hist`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 5"
        }
    },
    "417": {
        "page_content": "- `dwhfixed.v_action_dim_hist`\n- `dwhfixed.v_adsl_dim_hist`\n- `dwhfixed.v_sria_area_dim_hist`\n## 1.3. Data Flow\n``` mermaid\n    graph TD\n        A1(<b>Oracle Table</b>)\n        A2(\"<b>HDFS</b> <br> /user/hive/warehouse/dwhfixed.db/landing_zone/{table_name}\")\n        A3(<b>Hive table</b>)\n        A4(\"<b>Impala table</b>\")\n        A1 --> |\"\"\"<b>sqoop: import</b> <br> \n                jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD <br>\n                user: dm_sas_va <br>",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 6"
        }
    },
    "418": {
        "page_content": "user: dm_sas_va <br>\n                password file jceks://hdfs/user/dwhfixed/dwhfixed <br>\n                password-alias: dm_sas_va.pass\"\"\"| A2\n        A2 --> |\"<b>beeline: load data inpath</b>  <br>\n                 un-vip.bigdata.abc.gr:10000/default\"| A3\n        A3 --> |\"<b>impala-shell: refresh</b> <br>\n                 un-vip.bigdata.abc.gr\"| A4\n```\n## 1.4. Logs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 7"
        }
    },
    "419": {
        "page_content": "un-vip.bigdata.abc.gr\"| A4\n```\n## 1.4. Logs\nApplication logs can be found in each Workflow in Hue as user `dwhfixed`. Oozie Coordinator result can be seen into HUE (login as `dwhfixed` user) and go to `Jobs -> Workflows` and look for `DWHFixed - Full Workflow`\n## 1.5. Monitoring messages\n- All monitoring messages have the following constant values:\\\n  **application=DWHFIXED**\\\n  **job=FULL**\n- All monitoring messages of the same execution have a **unique executionId**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 8"
        }
    },
    "420": {
        "page_content": "**job=FULL**\n- All monitoring messages of the same execution have a **unique executionId**\n- Every component of one execution has a unique row that is updated between the following status values: RUNNING to SUCCESS or FAILED.\n```mermaid\n    graph TD\n        A1(\"component=MAIN_START_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 9"
        }
    },
    "421": {
        "page_content": "A2(\"component=SQOOP_IMPORT_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A3(\"component=BEELINE_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A4(\"component=IMPALA_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A5(\"component=MAIN_END_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 10"
        }
    },
    "422": {
        "page_content": "A1 --> A2\n        A2 --> A3\n        A3 --> A4\n        A4 --> A5\n```\n- Check monitoring app for successful executions:  \n  \n  From `un2` with personal account:\n ``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=FULL$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'\n  ```\n- Check monitoring app for failed executions:  \n  \n  From `un2` with personal account:\n ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 11"
        }
    },
    "423": {
        "page_content": "- Check monitoring app for failed executions:  \n  \n  From `un2` with personal account:\n ``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=FULL$status=FAILED&operativePartition=<timestamp e.g.:20220518>'\n  ```\n - Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n### 1.5.1. Grafana dashboard",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 12"
        }
    },
    "424": {
        "page_content": "### 1.5.1. Grafana dashboard\nGrafana: `https://unc1.bigdata.abc.gr:3000/d/DNM-sBo4z/dwhfixed-dashboard?orgId=1&from=now-2d&to=now`\n## 1.6. Alerts (Mail)\n**Subject**: `DWHFIXED - FULL: FAILED`  \n**Body**: \n- `${ORACLE_TABLE}: failed to perform Oracle query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to fetch data through Sqoop.\\nTime: ${datetime_now}`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 13"
        }
    },
    "425": {
        "page_content": "- `${ORACLE_TABLE}: failed to fetch data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: fetched no data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to execute beeline query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n`${ORACLE_TABLE}`: Can be any of the tables from section `1.1. Oracle (source)`  \n`${datetime_now}`: Current datetime in `%Y-%m-%d %H:%M` format",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 14"
        }
    },
    "426": {
        "page_content": "`${datetime_now}`: Current datetime in `%Y-%m-%d %H:%M` format\nThe application sends an email in each case of the following failures (for each table):\n### 1.6.1 Oracle failure\n- Sqoop eval failure.\n- Sqoop import failure.\n- Sqoop has fetched no data.\nHow to check Oracle:\n```\nssh <username>@un2\nsh -l intra\njsec_file=jceks://hdfs/ez/intra.Sqoop.Creds.jceks\nsqoop eval \\\n-Dhadoop.security.credential.provider.path=${jsec_file} \\\n--password-alias dm_sas_va.pass \\",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 15"
        }
    },
    "427": {
        "page_content": "-Dhadoop.security.credential.provider.path=${jsec_file} \\\n--password-alias dm_sas_va.pass \\\n--connect jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD \\\n--username dm_sas_va \\\n--query \"select * FROM SAS_VA_VIEW.V_DW_CONTROL_TABLE where 1=1\" ;\n```\nAlternative way of checking Oracle:\n```\nssh <username>@undt1\nsu -l intra\nexport ORACLE_HOME=/usr/lib/oracle/11.2/client64\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:${LD_LIBRARY_PATH}\nexport PATH=$ORACLE_HOME/bin:${PATH}",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 16"
        }
    },
    "428": {
        "page_content": "export LD_LIBRARY_PATH=$ORACLE_HOME/lib:${LD_LIBRARY_PATH}\nexport PATH=$ORACLE_HOME/bin:${PATH}\nexport SQLPATH=$ORACLE_HOME/lib:${SQLPATH}\nexport NLS_DATE_FORMAT='yyyy-mm-dd hh24:mi:ss'\nexport HISTSIZE=0\nsqlplus dm_sas_va/<Oracle-pass>@999.999.999.999:6644/DWHPRD\n```\n### 1.6.2 Hive/Impala failure\n- Beeline load data inpath has failed.\n- Impala refresh has failed.\nHow to check Hive/Impala:\n- In the Hue UI mentioned above, click on editor -> Hive/Impala",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 17"
        }
    },
    "429": {
        "page_content": "How to check Hive/Impala:\n- In the Hue UI mentioned above, click on editor -> Hive/Impala\n- Run the following query on each:`select * from dwhfixed.v_sria_subarea_dim_hist limit 3;`\n### 1.6.3 Actions\n//TODO\n# 2. Delta\n- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - Delta Workflow`, \n- **COORDINATOR**: `DWHFixed - Delta Coordinator`\n- **HDFS path**: `/user/dwhfixed/delta`\n- **Runs**: `1:30,3:30,5:30,7:30,9:30,11:30,13:30,15:30,19:30,21:30,23:30 (UTC)`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 18"
        }
    },
    "430": {
        "page_content": "- **Runs**: `1:30,3:30,5:30,7:30,9:30,11:30,13:30,15:30,19:30,21:30,23:30 (UTC)`\n- **Config file**: `hdfs: /user/dwhfixed/dwhfixed.config`\n- **Tables config/list**: `hdfs: /user/dwhfixed/delta/tables_delta.config`\n- **Oracle user**: `dm_sas_va`\n- **Oracle pass**: `https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx`. You need password to open this file and get the user's password.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 19"
        }
    },
    "431": {
        "page_content": "Runs every 2h, checks control table: `SAS_VA_VIEW.V_DW_CONTROL_TABLE` in order to evaluate if a new partition has been added to the source tables. After that, it runs data flow.\n## 2.1. Oracle (source)\n- `SAS_VA_VIEW.V_SRIA_SERVICE_REQUESTS_FCT`\n- `SAS_VA_VIEW.V_ACS_MODEMS_ACTIVE_FCT`\n- `SAS_VA_VIEW.V_LL_DIM`\n- `SAS_VA_VIEW.V_SRIA_INTERACT_ACTIVITY_FCT`\n- `SAS_VA_VIEW.V_DW_CONTROL_TABLE`\n- `SAS_VA_VIEW.V_FAULT_NTT_NETWORK_ELEM_FCT`\n- `SAS_VA_VIEW.V_SRIA_SITE_DIM`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 20"
        }
    },
    "432": {
        "page_content": "- `SAS_VA_VIEW.V_FAULT_NTT_NETWORK_ELEM_FCT`\n- `SAS_VA_VIEW.V_SRIA_SITE_DIM`\n- `SAS_VA_VIEW.V_SR_AFF_CUST_FCT`\n## 2.2. Hive - Impala (target)\n- `dwhfixed.v_sria_service_requests_fct_hist`\n- `dwhfixed.v_acs_modems_active_fct_hist`\n- `dwhfixed.v_ll_dim_hist`\n- `dwhfixed.v_sria_interact_activity_fct_hist`\n- `dwhfixed.v_fault_ntt_network_elem_fct_hist`\n- `dwhfixed.v_sria_site_dim_hist`\n- `dwhfixed.v_sr_aff_cust_fct_hist`\n## 2.3. Data Flow\n``` mermaid\n    graph TD\n        A1(<b>Oracle Table</b>)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 21"
        }
    },
    "433": {
        "page_content": "## 2.3. Data Flow\n``` mermaid\n    graph TD\n        A1(<b>Oracle Table</b>)\n        A2(\"<b>HDFS</b> <br> /user/hive/warehouse/dwhfixed.db/landing_zone/{table_name}\")\n        A3(<b>Hive table</b>)\n        A4(\"<b>Impala table</b>\")\n        A1 --> |\"\"\"<b>sqoop: import</b> <br> \n                jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD <br>\n                user: dm_sas_va <br>\n                password file jceks://hdfs/user/dwhfixed/dwhfixed <br>",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 22"
        }
    },
    "434": {
        "page_content": "password file jceks://hdfs/user/dwhfixed/dwhfixed <br>\n                password-alias: dm_sas_va.pass\"\"\"| A2\n        A2 --> |\"<b>beeline: load data inpath</b>  <br>\n                 un-vip.bigdata.abc.gr:10000/default\"| A3\n        A3 --> |\"<b>impala-shell: refresh</b> <br>\n                 un-vip.bigdata.abc.gr\"| A4\n```\n## 2.4. Logs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 23"
        }
    },
    "435": {
        "page_content": "un-vip.bigdata.abc.gr\"| A4\n```\n## 2.4. Logs\nApplication logs can be found in each Workflow in Hue as user `dwhfixed`. Oozie Coordinator result can be seen into HUE (login as `dwhfixed` user) and go to `Jobs -> Workflows` and look for `DWHFixed - Delta Workflow`\n## 2.5. Monitoring messages\n- All monitoring messages have the following constant values:\\\n  **application=DWHFIXED**\\\n  **job=DELTA**\n- All monitoring messages of the same execution have a **unique executionId**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 24"
        }
    },
    "436": {
        "page_content": "**job=DELTA**\n- All monitoring messages of the same execution have a **unique executionId**\n- Every component of one execution has a unique row that is updated between the following status values: RUNNING to SUCCESS or FAILED.\n```mermaid\n    graph TD\n        A1(\"component=MAIN_START_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 25"
        }
    },
    "437": {
        "page_content": "A2(\"component=SQOOP_IMPORT_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A3(\"component=BEELINE_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A4(\"component=IMPALA_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A5(\"component=MAIN_END_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 26"
        }
    },
    "438": {
        "page_content": "A1 --> A2\n        A2 --> A3\n        A3 --> A4\n        A4 --> A5\n```\n- Check monitoring app for successful executions:  \n  \n  From `un2` with personal account:\n ``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=DELTA$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'\n  ```\n- Check monitoring app for failed executions:  \n  \n  From `un2` with personal account:\n ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 27"
        }
    },
    "439": {
        "page_content": "- Check monitoring app for failed executions:  \n  \n  From `un2` with personal account:\n ``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=DELTA$status=FAILED&operativePartition=<timestamp e.g.:20220518>'\n  ```\n - Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n \n### 2.5.1. Grafana dashboard",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 28"
        }
    },
    "440": {
        "page_content": "### 2.5.1. Grafana dashboard\nGrafana: `https://unc1.bigdata.abc.gr:3000/d/DNM-sBo4z/dwhfixed-dashboard?orgId=1&from=now-2d&to=now`\n## 2.6. Alerts (Mail)\n**Subject**: `DWHFIXED - DELTA: FAILED`  \n**Body**: \n- `${ORACLE_TABLE}: failed to perform Oracle query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to fetch data through Sqoop.\\nTime: ${datetime_now}`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 29"
        }
    },
    "441": {
        "page_content": "- `${ORACLE_TABLE}: failed to fetch data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: fetched no data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to execute beeline query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n`${ORACLE_TABLE}`: Can be any of the tables from section `1.1. Oracle (source)`  \n`${datetime_now}`: Current datetime in `%Y-%m-%d %H:%M` format",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 30"
        }
    },
    "442": {
        "page_content": "`${datetime_now}`: Current datetime in `%Y-%m-%d %H:%M` format\nThe application sends an email in each case of the following failures (for each table):\n### 2.6.1 Oracle failure\n- Sqoop eval failure.\n- Sqoop import failure.\n- Sqoop has fetched no data.\nHow to check Oracle:\n```\nssh <username>@un2\nsh -l intra\njsec_file=jceks://hdfs/ez/intra.Sqoop.Creds.jceks\nsqoop eval \\\n-Dhadoop.security.credential.provider.path=${jsec_file} \\\n--password-alias dm_sas_va.pass \\",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 31"
        }
    },
    "443": {
        "page_content": "-Dhadoop.security.credential.provider.path=${jsec_file} \\\n--password-alias dm_sas_va.pass \\\n--connect jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD \\\n--username dm_sas_va \\\n--query \"select * FROM SAS_VA_VIEW.V_DW_CONTROL_TABLE where 1=1\" ;\n```\nAlternative way of checking Oracle:\n```\nssh <username>@undt1\nsu -l intra\nexport ORACLE_HOME=/usr/lib/oracle/11.2/client64\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:${LD_LIBRARY_PATH}\nexport PATH=$ORACLE_HOME/bin:${PATH}",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 32"
        }
    },
    "444": {
        "page_content": "export LD_LIBRARY_PATH=$ORACLE_HOME/lib:${LD_LIBRARY_PATH}\nexport PATH=$ORACLE_HOME/bin:${PATH}\nexport SQLPATH=$ORACLE_HOME/lib:${SQLPATH}\nexport NLS_DATE_FORMAT='yyyy-mm-dd hh24:mi:ss'\nexport HISTSIZE=0\nsqlplus dm_sas_va/<Oracle-pass>@999.999.999.999:6644/DWHPRD\n```\n### 2.6.2 Hive/Impala failure\n- Beeline load data inpath has failed.\n- Impala refresh has failed.\nHow to check Hive/Impala:\n- In the Hue UI mentioned above, click on editor -> Hive/Impala",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 33"
        }
    },
    "445": {
        "page_content": "How to check Hive/Impala:\n- In the Hue UI mentioned above, click on editor -> Hive/Impala\n- Run the following query on each:`select * from dwhfixed.v_sria_subarea_dim_hist limit 3;`\n### 2.6.3 Actions\nIn case of any of alert, do nothing. The flow will try to re-run in 2 hours. If everything is OK, then it will load successfully the partitions. After 2-2:30 hours check the status of the next run. \nIf the error persists:\n- In case of infastructure error (ex. HDFS, Hive are down), do nothing.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 34"
        }
    },
    "446": {
        "page_content": "If the error persists:\n- In case of infastructure error (ex. HDFS, Hive are down), do nothing.\n- In case of other error, contact BigData Developer team.\n# 3. HDFS Log Files Retention \nArchiving of the log files produced in every run of the flow and store them in HDFS directory.  \nIt is useful for investigation in case of errors.  \nThe retention of the log files is configurable. Default value: 9 days  \n- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - HDFS_Log_Retention_Workf`,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 35"
        }
    },
    "447": {
        "page_content": "- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - HDFS_Log_Retention_Workf`, \n- **COORDINATOR**: `DWHFixed - HDFS_Log_Retention_Coord`\n- **Runs**: `${coord:days(1)} 23:00 (Europe/Athens)`\n- **HDFS Retention path**: `/user/dwhfixed/HDFS_LOG_Retention`\n- **HDFS Log path**: `/user/dwhfixed/log`\n- **HDFS Log Files**: `DWHFIXED.*.Archive.tar.gz`\n# 4. Useful Links\n[Home](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/dwhfixed/-/wikis/home)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 36"
        }
    },
    "448": {
        "page_content": "[Home](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/dwhfixed/-/wikis/home)\n[Infrastructure](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/dwhfixed/-/wikis/Infrastructure)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "dwhfixed.md - Part 37"
        }
    },
    "449": {
        "page_content": "# TrustCenter Flows\nThis is a document that will assist on support. Business documents can be found [here](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/tree/master/mini%20projects/location_mobility/docs).\n## Location Mobility\nLocation Mobility (LM) reffers to extraction of data from BigStreamer into files.  \nThe output files are transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them.  \nThese files are:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 1"
        }
    },
    "450": {
        "page_content": "These files are:\n- `LM_02_lte_yyyyMMdd_xxx.txt`\n- `LM_03_smsIn_yyyyMMdd_xxx.txt`\n- `LM_04_smsOut_yyyyMMdd_xxx.txt`\n- `LM_05_voiceInOut_yyyyMMdd_xxx.txt`\n- `LM_06_voiceIn_yyyyMMdd_xxx.txt`\n- `LM_07_voiceOut_yyyyMMdd_xxx.txt`\n- `LM_08_cellHist_yyyyMMdd_xxx.txt`\nAlong with those, the reconciliation files are produced and sent for each one.  \nThey give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\n``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 2"
        }
    },
    "451": {
        "page_content": "``` bash\ncat /shared/abc/location_mobility/logging/LM_05_voiceInOut_reconciliation.log\n#e.g for LM_05_voiceInOut and 1st of February 2022\n2022-02-01 08:06:33 LM_05_voiceInOut_20220201_00001.txt 20220201 20906\n2022-02-01 10:02:36 LM_05_voiceInOut_20220201_00002.txt 20220201 23810\n2022-02-01 12:02:45 LM_05_voiceInOut_20220201_00003.txt 20220201 179719\n2022-02-01 14:03:45 LM_05_voiceInOut_20220201_00004.txt 20220201 876051\n2022-02-01 16:05:13 LM_05_voiceInOut_20220201_00005.txt 20220201 1581201",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 3"
        }
    },
    "452": {
        "page_content": "2022-02-01 16:05:13 LM_05_voiceInOut_20220201_00005.txt 20220201 1581201\n2022-02-01 20:02:00 LM_05_voiceInOut_20220201_00006.txt 20220201 1606966\n```\n**Reconcilication Files**:  \n`/shared/abc/location_mobility/logging/LM_*` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check to see if the file was produced at the right time and contained the expected number of rows.\n### LM_02_lte",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 4"
        }
    },
    "453": {
        "page_content": "### LM_02_lte\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala table `eea.eea_hour`.  \nThe filename format is `LM_02_lte_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_02_lte_20220301_00002.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 5"
        }
    },
    "454": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 6"
        }
    },
    "455": {
        "page_content": "**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\n  graph TD \n  A[Impala: eea.eea_hour] -->| Impala Query | B1[location_mobility.lm_lte_exp]\n  B1 --> |Merge HDFS files to a single file | B2[File: LM_02_lte_yyyyMMdd_000xx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B2 --> |SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 7"
        }
    },
    "456": {
        "page_content": "B2 --> |SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` on `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 8"
        }
    },
    "457": {
        "page_content": "**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/eea_hour.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    # e.g for 2021-02-22\n    [...] - INFO: end_date=2021-02-22 09:00:00",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 9"
        }
    },
    "458": {
        "page_content": "``` logs\n    # e.g for 2021-02-22\n    [...] - INFO: end_date=2021-02-22 09:00:00\n    [...] - INFO: max_date=2021-02-22 09:00:00\n    ```\n    If end_date is newer or equal to max_date, it means that table `eea.eea_hour` does not contain new data and therefore there is nothing to be done during this execution.  \n\t\tabc should load data in `eea.eea_hour` table first and then execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 10"
        }
    },
    "459": {
        "page_content": "abc should load data in `eea.eea_hour` table first and then execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 11"
        }
    },
    "460": {
        "page_content": "- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \nFor example if 6 files were not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 12"
        }
    },
    "461": {
        "page_content": "For example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 13"
        }
    },
    "462": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_03_smsIn\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.sms_raw_v, osix.osix_sms_raw` that fulfill some conditions.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 14"
        }
    },
    "463": {
        "page_content": "The filename format is `LM_03_smsIn_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_03_smsIn_20220301_00002.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 15"
        }
    },
    "464": {
        "page_content": "**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 16"
        }
    },
    "465": {
        "page_content": "The master script triggers the export procedure.\n``` mermaid\ngraph TD\n  A[Impala: sai.sms_raw_v] --> |union all | D[Impala: osix.osix_sms_raw ] --> | Impala Query | B[File: LM_03_smsIn_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 17"
        }
    },
    "466": {
        "page_content": "**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_in_v2.sql` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/sms_in.lock`\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 18"
        }
    },
    "467": {
        "page_content": "**Lock file**: `/shared/abc/location_mobility/run/sms_in.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 19"
        }
    },
    "468": {
        "page_content": "- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that tables `sai.sms_raw_v` or `osix.osix_sms_raw` do not contain new data and therefore there is nothing to be done during this execution.  \n    New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.sms_raw`, updated by TRAFFICA flow (`sai.sms_raw_v` is a view on `sai.sms_raw` table).  \n\t\t- `osix.osix_sms_raw`, responsible abc",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 20"
        }
    },
    "469": {
        "page_content": "- `osix.osix_sms_raw`, responsible abc\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_sms_in_v2_mon.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/sms_in.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 21"
        }
    },
    "470": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 22"
        }
    },
    "471": {
        "page_content": "This will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \nFor example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 23"
        }
    },
    "472": {
        "page_content": "```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag.  \nFor example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_04_smsOut",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 24"
        }
    },
    "473": {
        "page_content": "```\n### LM_04_smsOut\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.sms_raw_v, osix.osix_sms_raw` that fulfill some conditions.  \nThe filename format is `LM_04_smsOut_yyyyMMdd_xxx.txt` where `xx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_04_smsOut_20220301_00002.txt`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 25"
        }
    },
    "474": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 26"
        }
    },
    "475": {
        "page_content": "**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.sms_raw_v] --> |union all | D[Impala: osix.osix_sms_raw ] -->| Impala Query | B[File: LM_04_smsOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 27"
        }
    },
    "476": {
        "page_content": "```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_out_v2.sql` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/sms_out.lock`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 28"
        }
    },
    "477": {
        "page_content": "**Lock file**: `/shared/abc/location_mobility/run/sms_out.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 29"
        }
    },
    "478": {
        "page_content": "- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that table `sai.sms_raw_v` or `osix.osix_sms_raw` do not contain new data and therefore there is nothing to be done during this execution.  \n    New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.sms_raw`, updated by TRAFFICA flow (`sai.sms_raw_v` is a view on `sai.sms_raw` table).  \n\t\t- `osix.osix_sms_raw`, responsible abc",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 30"
        }
    },
    "479": {
        "page_content": "- `osix.osix_sms_raw`, responsible abc\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_sms_out_v2_mon.sh` return no process means the previous execution was forcefully stopped.  \n\t\tDelete the lock file `/shared/abc/location_mobility/run/sms_out.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 31"
        }
    },
    "480": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 32"
        }
    },
    "481": {
        "page_content": "This will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \nFor example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d') 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 33"
        }
    },
    "482": {
        "page_content": "```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_05_voiceInOut",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 34"
        }
    },
    "483": {
        "page_content": "```\n### LM_05_voiceInOut\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions. \nThe filename format is `LM_05_voiceInOut_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_05_voiceInOut_20220301_00002.txt`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 35"
        }
    },
    "484": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 36"
        }
    },
    "485": {
        "page_content": "**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] --> | Impala Query | B[File: LM_05_voiceInOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 37"
        }
    },
    "486": {
        "page_content": "```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2.sql` on `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 38"
        }
    },
    "487": {
        "page_content": "**Lock file**: `/shared/abc/location_mobility/run/voice_inout.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 39"
        }
    },
    "488": {
        "page_content": "- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution.  \n    New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \n\t\t- `osix.osix_voice_raw`, responsible abc",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 40"
        }
    },
    "489": {
        "page_content": "- `osix.osix_voice_raw`, responsible abc\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_voice_inout_v2_mon.sh` return no process means the previous execution was forcefully stopped.  \n\t\tDelete the lock file `/shared/abc/location_mobility/run/voice_inout.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 41"
        }
    },
    "490": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 42"
        }
    },
    "491": {
        "page_content": "This will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \nFor example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 43"
        }
    },
    "492": {
        "page_content": "```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_06_voiceIn",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 44"
        }
    },
    "493": {
        "page_content": "```\n### LM_06_voiceIn\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions.  \nThe filename format is `LM_06_voiceIn_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_06_voiceIn_20220301_00002.txt`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 45"
        }
    },
    "494": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 46"
        }
    },
    "495": {
        "page_content": "**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] -->| Impala Query | B[File: LM_06_voiceIn_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 47"
        }
    },
    "496": {
        "page_content": "```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_in_v2.sql` on `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 48"
        }
    },
    "497": {
        "page_content": "**Lock file**: `/shared/abc/location_mobility/run/voice_in.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 49"
        }
    },
    "498": {
        "page_content": "- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution. \n    New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \n\t\t- `osix.osix_voice_raw`, responsible abc",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 50"
        }
    },
    "499": {
        "page_content": "- `osix.osix_voice_raw`, responsible abc\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_voice_in.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/voice_in.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 51"
        }
    },
    "500": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 52"
        }
    },
    "501": {
        "page_content": "This will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up. For example if 6 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 53"
        }
    },
    "502": {
        "page_content": "```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_07_voiceOut",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 54"
        }
    },
    "503": {
        "page_content": "```\n### LM_07_voiceOut\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions.  \nThe filename format is `LM_07_voiceOut_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_07_voiceOut_20220301_00002.txt`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 55"
        }
    },
    "504": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 2 hours`  \n**Coordinator**: `Location_Mobility_2Hour_CO`\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 56"
        }
    },
    "505": {
        "page_content": "**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] -->| Impala Query | B[File: LM_07_voiceOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 57"
        }
    },
    "506": {
        "page_content": "```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh` on `un2.bigdata.abc.gr`\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_out_v2.sql` on `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 58"
        }
    },
    "507": {
        "page_content": "**Lock file**: `/shared/abc/location_mobility/run/voice_out.lock`\n**Troubleshooting Steps**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- Identify system or service errors in the log file e.g failed Impala query.\n- Check if this message exists in the failed execution's log:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 59"
        }
    },
    "508": {
        "page_content": "- Check if this message exists in the failed execution's log:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: Nothing to export.\n    ```\n    This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution. \n    New data should be loaded in the following tables and then execute the script.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 60"
        }
    },
    "509": {
        "page_content": "New data should be loaded in the following tables and then execute the script.  \n\t\t- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \n\t\t- `osix.osix_voice_raw`, responsible abc\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 61"
        }
    },
    "510": {
        "page_content": "``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_voice_out_v2_mon.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/voice_out.lock` and execute the script.\n**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 62"
        }
    },
    "511": {
        "page_content": "Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N 2-hour intervals. This is not needed if 4 or less files were missed in which case the procedure will automatically catch up. For example if 6 files were not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 63"
        }
    },
    "512": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 64"
        }
    },
    "513": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\n    ```\n### LM_08_cellHist\nUnder normal circumstances this file is produced every day and contains yesterday's data from the Impala table `refdata.rd_cells_v`.  \nThe filename format is `LM_08_cellHist_yyyyMMdd_00001.txt`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 65"
        }
    },
    "514": {
        "page_content": "The filename format is `LM_08_cellHist_yyyyMMdd_00001.txt`.  \nFor example, if the file contains data for the 1st of March 2022 the filename will be `LM_08_cellHist_20220301_00001.txt`.\n``` mermaid\n  graph TD\n  A[Oozie Coord: Location_Mobility_Daily_CO] -->|SHELL| B[Master Script ]\n  B --> C[ Remdef Script ]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 07:00`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 66"
        }
    },
    "515": {
        "page_content": "**User**: `mtuser`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 07:00`  \n**Coordinator**: `Location_Mobility_Daily_CO`\n**Master Script**: `000.Location_Mobility_Daily_Oozie_Main.sh`\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_daily.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 67"
        }
    },
    "516": {
        "page_content": "The master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: refdata.rd_cells_v] -->| Impala Query | B[File: LM_08_cellHist_yyyyMMdd_00001.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 68"
        }
    },
    "517": {
        "page_content": "**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/lm`\n**Logs**: ```/shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log```\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/rd_cells.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 69"
        }
    },
    "518": {
        "page_content": "- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:  \n\t- login on `un2.bigdata.abc.gr` with personal account  \n\t- `su - mtuser`\n    ``` logs\n    [...] - INFO: max_date=yyyyMMdd and export_date=yyyyMMdd\n    ```\n    If the desired export_date is newer than max_date, it means that table `refdata.rd_cells_v` does not contain new data and therefore there is nothing to be done during this execution.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 70"
        }
    },
    "519": {
        "page_content": "Load table `refdata.rd_cells` first and then execute the script.\n- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_lm_rd_cells.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/rd_cells.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 71"
        }
    },
    "520": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If 5 or more dates weren't exported execute the script with the `--max-files <N>` flag.  \nThis will instruct the script to catch-up meaning to export files for N dates.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 72"
        }
    },
    "521": {
        "page_content": "This will instruct the script to catch-up meaning to export files for N dates.  \nThis is not needed if 4 or less dates were missed in which case the procedure will automatically catch up.  \nFor example if 6 dates were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 73"
        }
    },
    "522": {
        "page_content": "```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh -t 20220313 >> /shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log 2>&1\n    ```\n## Router Analytics",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 74"
        }
    },
    "523": {
        "page_content": "```\n## Router Analytics\nRouter Analytics (RA) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\n- `RA_01_yyyymmdd_00001_x.gz` \n- `RA_02_yyyymmdd_00001_x.gz`\n- `RA_03_yyyymmdd.gz`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 75"
        }
    },
    "524": {
        "page_content": "- `RA_01_yyyymmdd_00001_x.gz` \n- `RA_02_yyyymmdd_00001_x.gz`\n- `RA_03_yyyymmdd.gz`\nAlong with those, the reconciliation files are produced and sent for each one. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\n``` bash\ncat /shared/abc/location_mobility/logging/RA_BS_01_reconciliation.log\n#e.g for LM_05_voiceInOut and 31st of January 2022\n2022-02-01 09:06:39 RA_01_20220131_00001_[0-5] 20220131 68579162\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 76"
        }
    },
    "525": {
        "page_content": "2022-02-01 09:06:39 RA_01_20220131_00001_[0-5] 20220131 68579162\n```\n**Reconcilication Files**: `/shared/abc/location_mobility/logging/RA_BS_*` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check to see if the file was produced at the right time and contained the expected number of rows.\n### RA_01",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 77"
        }
    },
    "526": {
        "page_content": "### RA_01\nUnder normal circumstances this file is produced every day and contains yesterday's data from the Impala table `npce.device_session`. The filename format is `RA_01_yyyymmdd_00001_x.gz` where `x` is a serial number between `1` and `5` as due to size the file is split into subfiles. For example, the files containing data for the 1st of March 2022 will be `RA_01_20220301_00001_[0-5].gz`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 78"
        }
    },
    "527": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 07:00`  \n**Coordinator**: `export_Router_Analytics_files_daily`\n**Master Script**: `/shared/abc/location_mobility/run/run_ra_exports.sh`\nThe master script triggers the export procedure.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 79"
        }
    },
    "528": {
        "page_content": "The master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: npce.device_session] -->| Impala Query | B[File: RA_01_yyyymmdd_00001_x.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 80"
        }
    },
    "529": {
        "page_content": "**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_01.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/ra_01.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 81"
        }
    },
    "530": {
        "page_content": "- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2021-02-01\n    [...] - INFO: max_date=20220131 and export_date=20220131\n    ```\n    If the desired export_date is newer than max_date, it means that table `npce.device_session` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\n- If failed execution's log contains the message:\n    ``` logs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 82"
        }
    },
    "531": {
        "page_content": "- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_ra_bs_01.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_01.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 83"
        }
    },
    "532": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 84"
        }
    },
    "533": {
        "page_content": "- If 4 or more dates weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N dates. This is not needed if 3 or less dates were missed in which case the procedure will automatically catch up. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 5 dates were not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 85"
        }
    },
    "534": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_01.sh --max-files 6 >> /shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 13th of March 2022 was not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 86"
        }
    },
    "535": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_01.sh -t 20220313 >> /shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n### RA_02",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 87"
        }
    },
    "536": {
        "page_content": "```\n### RA_02\nUnder normal circumstances this file is produced every day and contains yesterday's data from the Impala table `npce.device_traffic`. The filename format is `RA_02_yyyymmdd_00001_x.gz` where `x` is a serial number between `1` and `5` as due to size the file is split into subfiles. For example, the files containing data for the 1st of March 2022 will be `RA_02_20220301_00001_[0-5].gz`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 88"
        }
    },
    "537": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 07:00`  \n**Coordinator**: `export_Router_Analytics_files_daily`\n**Master Script**: `/shared/abc/location_mobility/run/run_ra_exports.sh`\nThe master script triggers the export procedure.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 89"
        }
    },
    "538": {
        "page_content": "The master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: npce.device_traffic] -->| Impala Query | B[File: RA_02_yyyymmdd_00001_x.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 90"
        }
    },
    "539": {
        "page_content": "**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_02.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/ra_02.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 91"
        }
    },
    "540": {
        "page_content": "- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2021-02-01\n    [...] - INFO: max_date=20220131 and export_date=20220131\n    ```\n    If the desired export_date is newer than max_date, it means that table `npce.device_traffic` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\n- If failed execution's log contains the message:\n    ``` logs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 92"
        }
    },
    "541": {
        "page_content": "- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_ra_bs_02.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_02.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 93"
        }
    },
    "542": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 94"
        }
    },
    "543": {
        "page_content": "- If 4 or more dates weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N dates. This is not needed if 3 or less dates were missed in which case the procedure will automatically catch up. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 5 dates were not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 95"
        }
    },
    "544": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_02.sh --max-files 6 >> /shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 13th of March 2022 was not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 96"
        }
    },
    "545": {
        "page_content": "``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_02.sh -t 20220313 >> /shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n### RA_03\nUnder normal circumstances this file is produced every Wednesday and contains past week's data from the Impala table `npce.device_dms`. The filename format is `RA_03_yyyymmdd.gz`. For example, the files containing data up to the 2nd of March 2022 will be `RA_03_20220302.gz`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 97"
        }
    },
    "546": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every Wednesday at 16:00`  \n**Coordinator**: `export_Router_Analytics_files_to_mediation_ra_03_weekly`\n**Master Script**: `/shared/abc/location_mobility/run/run_ra_03_export.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 98"
        }
    },
    "547": {
        "page_content": "**Master Script**: `/shared/abc/location_mobility/run/run_ra_03_export.sh`\nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: npce.device_dms] -->| Impala Query | B[File: RA_03_yyyymmdd.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 99"
        }
    },
    "548": {
        "page_content": "**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/ra`\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_03.sh` on `un2.bigdata.abc.gr`\n**Lock file**: `/shared/abc/location_mobility/run/ra_03.lock`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 100"
        }
    },
    "549": {
        "page_content": "- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2021-01-26\n    [...] - INFO: max_date=20220126 and export_date=20220202\n    ```\n    If the desired export_date is newer than max_date, it means that table `npce.device_dms` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\n- If failed execution's log contains the message:\n    ``` logs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 101"
        }
    },
    "550": {
        "page_content": "- If failed execution's log contains the message:\n    ``` logs\n    [...] - ERROR: Script is being executed by another process. Exiting..\n    ```\n    and `ps -ef | grep export_ra_bs_03.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_03.lock` and execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 102"
        }
    },
    "551": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 103"
        }
    },
    "552": {
        "page_content": "- If 2 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N executions. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 2 files were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_03.sh --max-files 2 >> /shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 104"
        }
    },
    "553": {
        "page_content": "```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 16th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_ra_bs_03.sh -t 20220316 >> /shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n## Application Data Usage Insights",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 105"
        }
    },
    "554": {
        "page_content": "```\n## Application Data Usage Insights\nApplication Data Usage Insights (AUI) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\n- `AUI_01_yyyymmdd_0000x.txt`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 106"
        }
    },
    "555": {
        "page_content": "- `AUI_01_yyyymmdd_0000x.txt`\nAlong with those, a reconciliation file are produced and sent. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\n``` bash\ncat /shared/abc/location_mobility/logging/AUI_BS_01_reconciliation.log\n#e.g for AUI_01 and 21st of February 2022\n2021-02-22 06:00:09 AUI_01_20210221_00005.txt 20210221 15\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 107"
        }
    },
    "556": {
        "page_content": "2021-02-22 06:00:09 AUI_01_20210221_00005.txt 20210221 15\n```\n**Reconcilication File**: `/shared/abc/location_mobility/logging/AUI_BS_01_reconciliation.log` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check to see if the file was produced at the right time and contained the expected number of rows.\n### AUI_01",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 108"
        }
    },
    "557": {
        "page_content": "### AUI_01\nUnder normal circumstances this file is produced every 4 hours and contains data from 6 to 2 hours ago of the Impala table `npce.abc_apps_raw_events`. The filename format is `AUI_01_yyyymmdd_0000x.txt` where `x` is a serial number between `1` and `6`. For example, the files containing data for the 1st of March 2022 from 00:00 to 04:00 will be `AUI_01_20220301_00001.txt`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 109"
        }
    },
    "558": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie: export_Application_Data_Usage_Insights_files_4_hours] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every 4 hours`  \n**Coordinator**: `export_Application_Data_Usage_Insights_files_4_hours`\n**Master Script**: `/shared/abc/location_mobility/run/run_aui_exports.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 110"
        }
    },
    "559": {
        "page_content": "**Master Script**: `/shared/abc/location_mobility/run/run_aui_exports.sh`\n \nThe master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: npce.abc_apps_raw_events] -->| Impala Query | B[File: AUI_01_yyyymmdd_0000x.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /aui]\n```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 111"
        }
    },
    "560": {
        "page_content": "```\n**User**: `mtuser`\n**Local path**: `/data/location_mobility/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/aui`\n**Logs**: ```/shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/location_mobility/run/export_aui.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 112"
        }
    },
    "561": {
        "page_content": "- Find in the failed execution's log the message:\n    ``` logs\n    date: invalid date \u2018NULL 6 hours ago\u2019\n    ```\n    This means that table `npce.abc_apps_raw_events` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 113"
        }
    },
    "562": {
        "page_content": "**Ndefs**:\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 114"
        }
    },
    "563": {
        "page_content": "- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N 4-hour intervals. This is not needed if 4 or less dates were missed in which case the procedure will automatically catch up. For example if 6 dates were not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_aui.sh --max-files 6 >> /shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 115"
        }
    },
    "564": {
        "page_content": "```\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\n    ``` bash\n    /shared/abc/location_mobility/run/export_aui.sh -t 20220313 >> /shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\n    ```\n## Customer Satisfaction Index",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 116"
        }
    },
    "565": {
        "page_content": "```\n## Customer Satisfaction Index\nCustomer Satisfaction Index (CSI) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\n- `CSI_fix_mmddyyyy_wXX.txt`\n- `CSI_mob_mmddyyyy_mmddyyyy.txt`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 117"
        }
    },
    "566": {
        "page_content": "- `CSI_fix_mmddyyyy_wXX.txt`\n- `CSI_mob_mmddyyyy_mmddyyyy.txt`\nAlong with those, a reconciliation file are produced and sent. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\n``` bash\ncat /shared/abc/export_sai_csi/logging/CSI_mob_reconciliation.log\n#e.g for CSI_mob and 30th of January 2022\n2022-01-30 09:02:42  CSI_mob_01242022_01302022.txt  20220124  4223904\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 118"
        }
    },
    "567": {
        "page_content": "2022-01-30 09:02:42  CSI_mob_01242022_01302022.txt  20220124  4223904\n```\n**Reconcilication File**: `/shared/abc/export_sai_csi/logging/CSI_*` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check to see if the file was produced at the right time and contained the expected number of rows.\n### CSI_fix",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 119"
        }
    },
    "568": {
        "page_content": "### CSI_fix\nUnder normal circumstances this file is produced every 4 hours and contains data from 2 days ago ago of the Impala table `sai.cube_indicators_it`. The filename format is `CSI_fix_mmddyyyy_wXX.txt` where `XX` is a serial number between `1` and `52` for the week of the year. For example, the file containing data for the 2nd of February 2022 which belongs to the 5th week of the year, will be `CSI_fix_02042022_w05.txt`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 120"
        }
    },
    "569": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie: export_CSI_fix_and_mobile_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 7:00`  \n**Coordinator**: `export_CSI_fix_and_mobile_daily`\n**Master Script**: `/shared/abc/location_mobility/run/run_csi_exports_daily.sh`\n \nThe master script triggers the export procedure.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 121"
        }
    },
    "570": {
        "page_content": "The master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.cube_indicators_it] -->| Impala Query | B[File: CSI_fix_mmddyyyy_wXX.txt <br> Server: un2.bigdata.abc.gr <br> Path: /shared/abc/export_sai_csi/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /csi]\n```\n**User**: `mtuser`\n**Local path**: `/shared/abc/export_sai_csi/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/csi`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 122"
        }
    },
    "571": {
        "page_content": "**SFTP user**: `trustcenterftp`\n**SFTP path**: `/csi`\n**Logs**: ```/shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/export_sai_csi/run/export_csi_fix.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2022-01-10\n    Problem with 20220108.\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 123"
        }
    },
    "572": {
        "page_content": "``` logs\n    # e.g for 2022-01-10\n    Problem with 20220108.\n    ```\n    This means that table `sai.cube_indicators_it` does not contain new data and therefore there is nothing to be done during this execution. Load table `brond.cube_indicators` first and then execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 124"
        }
    },
    "573": {
        "page_content": "**Ndefs**:\n- If one date was missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If you need to export file for a specific date execute the script with argument `<yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 125"
        }
    },
    "574": {
        "page_content": "``` bash\n    /shared/abc/export_sai_csi/run/export_csi_fix.sh 20220313 >> /shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log 2>&1\n    ```\n### CSI_mob",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 126"
        }
    },
    "575": {
        "page_content": "```\n### CSI_mob\nUnder normal circumstances this file is produced every day and contains data for the current week of the Impala table `sai.sub_aggr_csi_it`. The filename format is `CSI_mob_mmddyyyy_mmddyyyy.txt` where the first date is the last loaded Monday and the second the current date. For example, the file containing data for the 2nd of February 2022 will be `CSI_mob_01312022_02022022.txt`.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 127"
        }
    },
    "576": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie: export_CSI_fix_and_mobile_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to mtuser| C[Master Script]\n```\nThe workflow triggers a master script which in turn executes the substeps\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 7:00`  \n**Coordinator**: `export_CSI_fix_and_mobile_daily`\n**Master Script**: `/shared/abc/location_mobility/run/run_csi_exports_daily.sh`\n \nThe master script triggers the export procedure.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 128"
        }
    },
    "577": {
        "page_content": "The master script triggers the export procedure.\n``` mermaid\ngraph TD \n  A[Impala: sai.sub_aggr_csi_it] -->| Impala Query | B[File: CSI_mob_mmddyyyy_mmddyyyy.txt <br> Server: un2.bigdata.abc.gr <br> Path: /shared/abc/export_sai_csi/out]\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /csi]\n```\n**User**: `mtuser`\n**Local path**: `/shared/abc/export_sai_csi/out`\n**SFTP user**: `trustcenterftp`\n**SFTP path**: `/csi`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 129"
        }
    },
    "578": {
        "page_content": "**SFTP user**: `trustcenterftp`\n**SFTP path**: `/csi`\n**Logs**: ```/shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/export_sai_csi/run/export_csi_mob_daily.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file e.g failed Impala query.\n- Find in the failed execution's log the message:\n    ``` logs\n    # e.g for 2022-01-10\n    Problem with 20220108.\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 130"
        }
    },
    "579": {
        "page_content": "``` logs\n    # e.g for 2022-01-10\n    Problem with 20220108.\n    ```\n    This means that table `sai.sub_aggr_csi_it` does not contain new data and therefore there is nothing to be done during this execution. Load table `sai.sub_aggr_csi_it` first and then execute the script.\n**Ndefs**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 131"
        }
    },
    "580": {
        "page_content": "**Ndefs**:\n- If one date was missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\n- If you need to export file for a specific date execute the script with argument `<yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 132"
        }
    },
    "581": {
        "page_content": "``` bash\n    /shared/abc/export_sai_csi/run/export_csi_mob_daily.sh 20220313 >> /shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "trustcenter_flows.md - Part 133"
        }
    },
    "582": {
        "page_content": "# TeMIP\n## Overview",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 1"
        }
    },
    "583": {
        "page_content": "The `abc TeMIP alarms live feed to BigStreamer` application is a Java application hosted on a Wildfly application server. The objective of the application is to receive and store (in near real time) the TeMIP alarms (from specific TeMIP Operation Contexts) into the BigStreamer\u2122 ecosystem. The `Apache Kudu` storage engine was selected in order to achieve near real time CRUD operations (Create, Read, Update, Delete). The `Apache Impala` is used for extended data retention (6 months). The `Apache Oozie` scheduler  is used in order to automatically run the necessary scripts.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 2"
        }
    },
    "584": {
        "page_content": "- **Ndef:** All the needed **passwords** can be found [**here**](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx).\n## Flows\nThe `TeMIP Flow` consists of 4 components/flows:\n1. Initialization/Synchronization flow\n1. Main Application flow\n1. Move Kudu to Impala flow\n1. Alert Mail flow\n### Main Application",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 3"
        }
    },
    "585": {
        "page_content": "1. Main Application flow\n1. Move Kudu to Impala flow\n1. Alert Mail flow\n### Main Application\nThe `Main Application Flow` contains our `TeMIP application` deployed to the `Wildfly Server` which receives the TeMIP alarms and stores them into Kudu tables.\n``` mermaid\n  flowchart TD\n  A[TeMIP Server] \n  B[Wildfly Server]\n  A --> |Sends TeMIP alarms| B\n  B --> |Stores TeMIP alarms| D[(Kudu Storage Engine)]\n  D --- E[Kudu: temip.temip_kudu_active_alarms]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 4"
        }
    },
    "586": {
        "page_content": "D --- E[Kudu: temip.temip_kudu_active_alarms]\n  D --- Z[Kudu: temip.temip_kudu_terminated_alarms]\n  D --- K[Kudu: temip.temip_kudu_historic_events]\n  style A fill: #45b39d\n```\n- **TeMIP Server**\n  - **Host:** `999.999.999.999`\n  - **Port:** `7180`\n- **Wildfly Server**\n  - **Servers:**\n    - `temip1 (999.999.999.999)` Standby Server\n    - `temip2 (999.999.999.999)` Active Server\n  - **User:** `temip`\n  - **Installation Path:** `/opt/wf_cdef_temip/`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 5"
        }
    },
    "587": {
        "page_content": "- **User:** `temip`\n  - **Installation Path:** `/opt/wf_cdef_temip/`\n  - **Deployments Path:** `/opt/wf_cdef_temip/standalone/deployments`\n  - **Application Logs:** `/opt/wf_cdef_temip/standalone/log/server.log`\n  - **Access Logs:** `/opt/wf_cdef_temip/standalone/log/access.log`\n  - **Configuration:** `/opt/wf_cdef_temip/standalone/configuration/BigStreamer/config/`\n    - **File:** `temip.properties`\n**Alerts:**\n- **Mail executed by [Alert Mail](#alert-mail)**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 6"
        }
    },
    "588": {
        "page_content": "- **File:** `temip.properties`\n**Alerts:**\n- **Mail executed by [Alert Mail](#alert-mail)**\n  - **Subject:** `\"[ Temip ] No alarms available.\"`\n  - **Body:** `\"There are no Temip alarms  available for the last hour. Corrective action may be needed.\"`\n**Troubleshooting Steps:**\n1. Check `logs` (application and access) with `temip-tailog` for any `ERROR` message that can occur.  \nIf TeMIP Server is running correctly, we should see lines like the following:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 7"
        }
    },
    "589": {
        "page_content": "If TeMIP Server is running correctly, we should see lines like the following:  \n`INFO [com.jkl.bigstreamer.abc.temip.core.service.TemipService] (default task-173) Counter= 3064020, handle= 968, batchName= batch_2, timesRestartedFromLastSync= 1, aoExtractDataList size= 1`\n1. Check if `TeMIP Server` is up by executing `ping 999.999.999.999`.\n1. Contact a `TeMIP admin` to see if there are any server side related issues\n### Initialization/Synchronization",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 8"
        }
    },
    "590": {
        "page_content": "### Initialization/Synchronization\nThe `Initialization/Synchronization Flow` consists of an OOZIE Coordinator called `TeMIP_Synchronization_CO`. The coordinator is responsible for establishing the **connection** and **communication** of the `Wildfly Server` (containing our TeMIP Application) with the `TeMIP Server`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 9"
        }
    },
    "591": {
        "page_content": "Every time the `Main Application` is successfully deployed to `Wildfly Server` or gets restarted, this `coordinator` **must be run manually** to initiate the above procedure, it does not happen automatically. If the `Wildfly Server` is up and running, the `coordinator` executes on specific days of every month to perform maintenance tasks.\n``` mermaid\n  flowchart TD\n  A[OOZIE Server] -->|SHELL Action as user temip| B[un-vip.bigdata.abc.gr <br> User: temip]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 10"
        }
    },
    "592": {
        "page_content": "A[OOZIE Server] -->|SHELL Action as user temip| B[un-vip.bigdata.abc.gr <br> User: temip]\n  B -->|REST message protocol| C[Main Application]\n  C <--> |SOAP message protocol| D[TeMIP Server]\n  style C fill: #45b39d\n```\n- **Oozie Coordinator**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_Synchronization_CO`\n    - **Execution:** `2,7,12,17,22,27 of every month at 03:00 local time`\n    - **Approximate Duration:** `45 minutes`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 11"
        }
    },
    "593": {
        "page_content": "- **Approximate Duration:** `45 minutes`\n    - **Workflow:** `TeMIP_Synchronization_WF`\n      - **Master Script:** `hdfs:/user/temip/100.TeMIP_Synchronization_Oozie_Main.sh`\n      - **Remdef Script:** `un-vip:/shared/abc/temip_oozie_production_scripts/101.temip_synchronization_Main.sh`\n      - **Server:** `un-vip.bigdata.abc.gr`\n      - **SSH User:** `temip`\n      - **Logs:** `un-vip:/shared/abc/temip_oozie_production_scripts/log/102.temip_synchronization.$(date '+%Y%m%d').log`\n**Alerts:**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 12"
        }
    },
    "594": {
        "page_content": "**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**\n1. Check logs for any errors\n1. If workflow `TeMIP_Synchronization_WF` has been run manually, login to `Hue` with `temip` user `after 45 minutes` and execute the following `impala query` editor: `select * from temip.temip_kudu_configs`.  \nIt should return `15 rows`. If not, re run the `TeMIP_Synchronization_WF` workflow\n### Move Kudu to Impala",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 13"
        }
    },
    "595": {
        "page_content": "### Move Kudu to Impala\nThe `Move Kudu to Impala` flow consists of a coordinator called `TeMIP_kudu_2_Impala_CO` which executes once a day and is responsible for moving the alarms from kudu to the equivalent impala table.\n``` mermaid\n flowchart TD\n  A[OOZIE Job] \n  Z[Kudu: temip.temip_kudu_terminated_alarms]\n  K[Kudu: temip.temip_kudu_historic_events]\n  A --> Z\n  A --> K\n  Z --> |Move older alarms to impala|H[Impala: temip.temip_impala_terminated_alarms]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 14"
        }
    },
    "596": {
        "page_content": "A --> K\n  Z --> |Move older alarms to impala|H[Impala: temip.temip_impala_terminated_alarms]\n  K --> |Move older events to impala|L[Impala: temip.temip_impala_historic_events]\n```\n- **Oozie Coordinator**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_kudu_2_Impala_CO`\n    - **Execution:** `everyday at 06:00 local time`\n    - **Approximate Duration:** `15 minutes`\n    - **Workflow:** `TeMIP_kudu_2_Impala_WF`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 15"
        }
    },
    "597": {
        "page_content": "- **Approximate Duration:** `15 minutes`\n    - **Workflow:** `TeMIP_kudu_2_Impala_WF`\n      - **Script:** `hdfs:/user/temip/temip_kudu_to_impala.sh`\n      - **Logs:** `Through Oozie Job in HUE`\n**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**\n1. Check that workflow `TeMIP_kudu_2_Impala_WF` runs successfully. This can be done through accessing `Hue` with `temip` user and selecting `Jobs`. Then filter the jobs with `user:temip` and look for a `job` with the below properties:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 16"
        }
    },
    "598": {
        "page_content": "- **Name:**  `oozie:launcher:T=shell:W=temip_kudu_to_impala:A=shell-661a:*`\n    - **Type:** `Oozie Launcher`\n    - **Execution Time:** `06:00 AM`\n    Select the desired shell execution, view its logs and search for any ERRORS.\n1. After the root cause of the problem has been identified, re-rerun the failed execution. This can be done through accessing `Hue` with `temip` user.\n### Alert Mail",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 17"
        }
    },
    "599": {
        "page_content": "The `Alert Mail` flow consists of a coordinator called `TeMIP_Alert_Mail_CO` which runs every hour and checks if the application receives any alarms from the TeMIP Server. The `TeMIP Server` sends alarms continuously. If in the last hour, the application has not detected any new alarms, an email is sent to jkl Engineers to inform that there might be a issue. The check is performed by comparing the number of alarms from the previous execution stored in `temip.temip_alert_table` table with the current one.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 18"
        }
    },
    "600": {
        "page_content": "``` mermaid\n flowchart TD\n  A[OOZIE Server] -->|SHELL Action as user temip| B[un-vip.bigdata.abc.gr <br> User: temip]\n  B --> C[201.temip_alert_mechanism_Main.sh]\n```\n- **Oozie Scheduler**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_Alert_Mail_CO`\n    - **Execution:** `every hour`\n    - **Workflow:** `TeMIP_Alert_Mail_WF`\n      - **Master Script:** `hdfs:/user/temip/200.TeMIP_Alert_Mail_Oozie_Main.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 19"
        }
    },
    "601": {
        "page_content": "- **Master Script:** `hdfs:/user/temip/200.TeMIP_Alert_Mail_Oozie_Main.sh`\n      - **Remdef Script:** `un-vip/shared/abc/temip_oozie_production_scripts/201.temip_alert_mechanism_Main.sh`\n      - **Server:** `un2.bigdata.abc.gr`\n      - **SSH User:** `temip`\n      - **Logs:** `un-vip:/shared/abc/temip_oozie_production_scripts/log/202.temip_alert_mechanism.$(date '+%Y%m%d').log`\n**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 20"
        }
    },
    "602": {
        "page_content": "**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**\n1. Check for any failed executions. This can be done through accessing `Hue` with `temip` user\n1. Check for any cluster related problems during the failed execution.\n## Manual Actions\n### Restart Wildfly Server\n---\n**_Ndef:_** TEMIP runs only in one node. Second node is in standby mode.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 21"
        }
    },
    "603": {
        "page_content": "---\n**_Ndef:_** TEMIP runs only in one node. Second node is in standby mode.\n1. **Only if requested by TeMip Administrators** In `Hue` with `temip` user, clear table `temip.temip_kudu_active_alarms` by executing `delete from temip.temip_kudu_active_alarms;`\n1. `Shutdown Wildfly Server`\n    1. Login as `temip` user in `temip2`.\n    1. Execute `/bin/bash` and then run `temip-stop` to stop wildfly and check logs with `temip-tailog`.\n    1. Suspend the temip Coordinators (`Hue` with `temip` user):",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 22"
        }
    },
    "604": {
        "page_content": "1. Suspend the temip Coordinators (`Hue` with `temip` user):\n        - `TeMIP_kudu_2_Impala_CO`\n        - `TeMIP_Synchronization_CO`\n        - `TeMIP_Alert_Mail_CO`\n    1. Clear table `temip.temip_kudu_active_alarms` by executing `delete from temip.temip_kudu_active_alarms;` in `Hue` as `temip` user \n1. `Startup Wildfly Server`\n    1. Login as `temip` user in `temip2`.\n    1. Start wildfly by executing `/bin/bash` and then running `temip-start` and check logs with `temip-tailog`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 23"
        }
    },
    "605": {
        "page_content": "1. Resume the three temip coordinators (`Hue` with `temip` user):\n        - `TeMIP_kudu_2_Impala_CO`\n        - `TeMIP_Synchronization_CO`\n        - `TeMIP_Alert_Mail_CO`\n    1. Workflows:\n        - The two workflows `TeMIP_kudu_2_Impala_WF` and `TeMIP_Alert_Mail_WF` should run automatically when oozie scheduler detects that it was suspended.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 24"
        }
    },
    "606": {
        "page_content": "- The third workflow `TeMIP_Synchronization_WF` should be run manually. Specifically, `login` as `temip` to `Hue` and run manually with no parameters. Make sure that it will not also be executed by the corresponding coordinator.\n    1. At `HUE` with `temip` user, open the impala editor and execute the following command in order to refresh e-mail alert script:  \n      `insert overwrite temip.temip_alert_table values(1);`\n1. `Sanity Checks`\n    1. Login as `temip` user in `temip2`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 25"
        }
    },
    "607": {
        "page_content": "1. `Sanity Checks`\n    1. Login as `temip` user in `temip2`\n    1. Check `logs` with `temip-tailog` and search for any errors.\n    1. After `45 minutes`, login to `Hue` with `temip` user and execute the following impala query editor:  \n    `select * from temip.temip_kudu_configs`  \n      It should return 15 rows. If not, `re run` the `TeMIP_Synchronization_WF` workflow.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 26"
        }
    },
    "608": {
        "page_content": "It should return 15 rows. If not, `re run` the `TeMIP_Synchronization_WF` workflow.\n    1. Login to `Hue` with `temip` and perform the below impala queries with a temip admin (Ioanna Bekiari) in order to established if everything is running okay. If the results are the same or really similar, the synchronization is considered successful.\n        ``` sql\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ENM_BASEBAND%';",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 27"
        }
    },
    "609": {
        "page_content": "select count(*) from temip_kudu_active_alarms where operation_context like '%ENM_BASEBAND%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ERICOSS2G%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ERICOSS-LTE%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%NOKIA3G%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%.ATHENS-OC%';",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 28"
        }
    },
    "610": {
        "page_content": "select count(*) from temip_kudu_active_alarms where operation_context like '%.ATHENS-OC%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%NNM_FIXED%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%U2000-OC%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%1350OMS%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%HUAWEI_IMS%';",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 29"
        }
    },
    "611": {
        "page_content": "select count(*) from temip_kudu_active_alarms where operation_context like '%HUAWEI_IMS%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%AUMS-OC%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '.def.A5529.A5520_AMS-OC';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%2000_DSLAM%';\n        ```\n### Load Terminated Alarms from TeMIP Oracle Database In case of data loss\n---",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 30"
        }
    },
    "612": {
        "page_content": "```\n### Load Terminated Alarms from TeMIP Oracle Database In case of data loss\n---\nIn case there is a loss of alarms for any reason, eg our application or TeMIP outage, we may be asked to load historical data directly from TeMIP Oracle Database into our terminated alarms table. In order to start this operation we must wait for some days, so that all alarms are transferred to the Oracle table. Whole procedure is described in detail below:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 31"
        }
    },
    "613": {
        "page_content": "1. Wait `7 days` from the day you want to `re-load` in order for terminated alarms to be refreshed in Oracle table.\n1. Connect as `temip` in `un2` and run `ping 999.999.999.999`, in order to see if `Temip Server` is up and running.\n1. From impala shell `secimp`(as `temip` in `un2`) or `Hue`(as `temip`):\n\t1. Check missing partitions in `temip.temip_kudu_terminated_alarms` and `temip.temip_impala_terminated_alarm` by running",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 32"
        }
    },
    "614": {
        "page_content": "`select count(*), par_dt from <database>.<table> where par_dt='<partition>' group by par_dt;` on both tables.  \n\t\tWe receive TeMIP alarms every day. So if there are general ERRORS(logs) or we have partitions containing less alarms than usual(eg. count), it suggests that there might be problems with the TeMIP server or our application and in need of investigating.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 33"
        }
    },
    "615": {
        "page_content": "1. Delete existing wrong partitions that overlap with the required interval, either from kudu table `temip.temip_kudu_terminated_alarms` or from impala table `temip.temip_impala_terminated_alarms`.\n\t\t- If wrong partitions are contained in kudu table (only 10 most recent days are in kudu), do:  \n`ALTER table temip.temip_kudu_terminated_alarms DROP IF EXISTS RANGE PARTITION 'v1'<= values < 'v2';`,   \nwhere v1 and v2 the required interval.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 34"
        }
    },
    "616": {
        "page_content": "where v1 and v2 the required interval.\n\t\t- If wrong partitions are contained in impala table (10 days past the current date), do:  \n`ALTER table temip.temip_impala_terminated_alarms DROP IF EXISTS PARTITION (par_dt='v');`,   \nwhere v is the wrong partition.\n\t1. In order to not tranfer again old data that have remained, run `truncate table temip.temipdb_term_alarms_load_par;`.\n1. As `temip` in `un2` Run the script with arguments\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 35"
        }
    },
    "617": {
        "page_content": "1. As `temip` in `un2` Run the script with arguments\n    ``` bash\n    sh /usr/icom/scripts/Sqoop_Oracle_HDFS_Impala_Load_TeMIP_v832.sh \"temipaharchi.alarmobject0\" identifier 30 <current-pardt> \"terminationtimestamp>='v1' and terminationtimestamp<'v2'\"\n    ```\n    - **current-pardt:** is the `today` par_dt, the day the script is run. Format `YYYYMMDD`. It has no significant value to the internal process.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 36"
        }
    },
    "618": {
        "page_content": "- **v1, v2:** Use values for `terminationtimestamp` that are between the start and end of the interval you want to load from Oracle. Format `01-MAY-22`.\n    Example for day 20220501:\n    ``` bash\n    sh /usr/icom/scripts/Sqoop_Oracle_HDFS_Impala_Load_TeMIP_v832.sh \"temipaharchi.alarmobject0\" identifier 30 20230104 \"terminationtimestamp>='01-MAY-22' and terminationtimestamp<'02-MAY-22'\"\n    ```\n    The data will be **loaded** into table `temip.temipdb_term_alarms_load_par`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 37"
        }
    },
    "619": {
        "page_content": "```\n    The data will be **loaded** into table `temip.temipdb_term_alarms_load_par`.\n1. From impala shell `secimp`(as `temip` in `un2`) or `Hue`(as `temip`):\n    1. Refresh the staging table in the impala shell:  \n      `refresh temip.temipdb_term_alarms_load_par;`\n    1. Run the following sql command, which transfers automatically all data to the right partition (par_dt) of temip.temip_impala_terminated_alarms:\n        ``` sql",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 38"
        }
    },
    "620": {
        "page_content": "``` sql\n        insert overwrite temip.temip_impala_terminated_alarms partition (par_dt)\n        select concat('OPERATION_CONTEXT ',split_part(upper(ocname),':',2),' ALARM OBJECT ', identifier) outentityspec,\n        null last_Modification_Timestamp,\n        split_part(upper(ocname),':',2) operation_context,\n        cast(identifier as bigint) identifier,\n        \"Terminated-Oracle\" state,\n        \"Closed\" problem_status,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 39"
        }
    },
    "621": {
        "page_content": "\"Terminated-Oracle\" state,\n        \"Closed\" problem_status,\n        case when clearancereportflag = \"1\" then true else false end clearance_report_flag,\n        acknowledgementuseride as acknowledgement_user_identifier,\n        handledby as handled_by,\n        closedby as closed_by,\n        handleduseridentifier as handled_user_identifier,\n        releaseuseridentifier as release_user_identifier,\n        closeuseridentifier as close_user_identifier,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 40"
        }
    },
    "622": {
        "page_content": "closeuseridentifier as close_user_identifier,\n        terminationuseridentif as termination_user_identifier,\n        acknowledgementtimesta as acknowledgement_time_stamp,\n        handletimestamp as handle_time_stamp,\n        closetimestamp as close_time_stamp,\n        terminationtimestamp as termination_time_stamp,\n        releasetimestamp as release_time_stamp,\n        null automatic_terminate_on_close,\n        creationtimestamp as creation_timestamp,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 41"
        }
    },
    "623": {
        "page_content": "null automatic_terminate_on_close,\n        creationtimestamp as creation_timestamp,\n        archivetimestamp as archive_time_stamp,\n        clearancetimestamp as clearance_time_stamp,\n        null previous_state,\n        managedobject as managed_object,\n        targetentities as target_entities,\n        --targetentities60512 as target_entities,\n        alarmtype as alarm_type,\n        eventtime as event_time,\n        probablecause as probable_cause,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 42"
        }
    },
    "624": {
        "page_content": "eventtime as event_time,\n        probablecause as probable_cause,\n        securityalarmcause as security_alarm_cause,\n        specificproblems as specific_problems,\n        --specificproblems (id)-8eloume to join kai edw,\n        null backed_up_status,\n        backupobject as backup_object,\n        trendindication as trend_indication,\n        thresholdinfo as threshold_info,\n        cast(notificationidentifier as bigint) notification_identifier,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 43"
        }
    },
    "625": {
        "page_content": "cast(notificationidentifier as bigint) notification_identifier,\n        correlnotifinfo as correl_notif_info,\n        monitoredattributes as monitored_attributes,\n        proposedrepairactions as proposed_repair_actions,\n        null additional_information,\n        domain as domain,\n        securityalarmdetector as security_Alarm_Detector,\n        null service_User,\n        null service_Provider,\n        ocname as oc_Name,\n        cast(parentalarmobject as bigint) parent_alarm_object,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 44"
        }
    },
    "626": {
        "page_content": "ocname as oc_Name,\n        cast(parentalarmobject as bigint) parent_alarm_object,\n        null severity_changed_time_stamp,\n        alarmcomment as alarm_comment,\n        agentalarmidentifier as agent_alarm_identifier,\n        agententity as agent_entity,\n        perceivedseverity as perceived_Severity,\n        additionaltext as additional_Text,\n        alarmobjectoperatorno as alarm_Object_Operator_Ndef,\n        originalseverity as original_Severity,",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 45"
        }
    },
    "627": {
        "page_content": "originalseverity as original_Severity,\n        originaleventtime as original_Event_Time,\n        0 useridentifier,\n        usertext as user_Text,\n        cast(satotal as bigint) sa_total,\n        null deleted,\n        from_timestamp(to_timestamp(terminationtimestamp,'yyyy-MM-dd HH:mm:ss'),'yyyyMMdd') as par_dt\n        --,*\n        from temip.temipdb_term_alarms_load_par a;\n        ```\n        **Ndef:** There are comments that might affect the query if not handled carefully.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 46"
        }
    },
    "628": {
        "page_content": "**Ndef:** There are comments that might affect the query if not handled carefully. \n    1. Check if data transferred successfully by running the command below and comparing the result with the number of retrieved records in the logs produced by the script.  \n      ``` sql\n      SELECT par_dt, count(*) FROM temip.temip_impala_terminated_alarms where par_dt > 'v';\n      Eg. Logs: INFO mapreduce.ImportJobBase: Retrieved 1113488 records.  \n      +----------+  \n      | count(*) |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 47"
        }
    },
    "629": {
        "page_content": "+----------+  \n      | count(*) |  \n      +----------+  \n      | 1113488  |  \n      +----------+  \n      ```\n## TeMIP Wildfly Server\nIn order to change the `logging level` of any of the `categories` of the `TeMIP Wildfly Server` access the `WildFly Management`.\n### Logging\nIn order to change the `logging level` of any of the categories of the `TeMIP Wildfly Server` access the `WildFly Management`.\n- [Home URL](https://999.999.999.999:8888/)\n- **User:** `admin`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 48"
        }
    },
    "630": {
        "page_content": "- [Home URL](https://999.999.999.999:8888/)\n- **User:** `admin`\n- **Password:** [abc-syspasswd.kdbx](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/abc-syspasswd.kdbx)\n#### Change logging level\n1. Login to the `WildFly Management` by following the [home url](https://999.999.999.999:10213/console/App.html#home), select `Configuration` ~> `Subsystems` ~> `Logging` and select `View`.\n1. Select `LOG CATEGORIES`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 49"
        }
    },
    "631": {
        "page_content": "1. Select `LOG CATEGORIES`\n1. From the table select the `category` you desire to change its **logging level** e.g. com.jkl.bigstreamer.abc.temip is our main TeMIP App.\n1. Press the `Edit` option below the table\n1. Select the desired `Level` between the given options. \n1. Click `Save`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 50"
        }
    },
    "632": {
        "page_content": "1. Select the desired `Level` between the given options. \n1. Click `Save`\n1. Ssh from `un2` with `temip` to `temip1` or `temip2` with `temip` user and check that in the file `/opt/wf_cdef_temip/standalone/configuration/standalone-full.xml` the level of the previously configured logger has changed successfully. It should be configured **automatically**. \n## Useful Links\n- [TeMIP Dir](https://metis.ghi.com/obss/bigdata/abc/temip)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 51"
        }
    },
    "633": {
        "page_content": "## Useful Links\n- [TeMIP Dir](https://metis.ghi.com/obss/bigdata/abc/temip)\n- [TeMIP Application Deployment](https://metis.ghi.com/obss/bigdata/abc/temip/temip-devops/-/wikis/Application-Deployment)\n- [TeMIP Wiki](https://metis.ghi.com/obss/bigdata/abc/temip/temip-devops/-/wikis/home)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "temip.md - Part 52"
        }
    },
    "634": {
        "page_content": "[[_TOC_]]\n# IPVPN\nIP VPN is an application that receives metrics about the network quality for the abc VPN Customers and produces Key Performance Indicators (KPIs) regarding Memory Usage, CPU Load, Provider Edge (PE) Interface, PE Branch Availability and PE Branch Quality of Service (QoS), which are collected and processed by the Service Management (SM) system of abc.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 1"
        }
    },
    "635": {
        "page_content": "This is a document that will assist on support. Business documents can be found [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ipvpnsla-customers-devops/-/tree/master/docs).\n## Input Performance Data",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 2"
        }
    },
    "636": {
        "page_content": "## Input Performance Data\nThere are two source systems, HP Network Node Manager (NNM) and SNMP Custom Poller application, that poll periodically network elements and produces raw files with the instantenous metrics. These files are then parsed by one or more procedures. Finally they are loaded to BigStreamer cluster in Impala tables. There are 3 flows of input performance data that are being described in detail below.\n### Component Metrics\n#### Creation of raw files",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 3"
        }
    },
    "637": {
        "page_content": "### Component Metrics\n#### Creation of raw files\nThe source system in this case is NNM. For high availability there are two instances of NNM on two seperate servers and they operate in active-standby fashion. The whole infrastructure is entirely managed by abc. The raw files produced contain component metrics for CPU load and memory usage of the network elements and are stored in local paths on those servers.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 4"
        }
    },
    "638": {
        "page_content": "``` mermaid\n  graph TD\n  A[Service: NNM <br> Host: nnmprd01.abc.gr] --> B[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz <br> Path: /var/opt/OV/shared/nnm/databases/custompoller/export/final <br> Host: nnmprd01.abc.gr]\n  C[Service: NNM <br> Host: nnmdis01.abc.gr] -.->|Stopped| D[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz <br> Path: /var/opt/OV/shared/nnm/databases/custompoller/export/final <br> Host: nnmdis01.abc.gr]\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 5"
        }
    },
    "639": {
        "page_content": "```\n**Path**: `/var/opt/OV/shared/nnm/databases/custompoller/export/final` on `nnmprd01.abc.gr` or `nnmdis01.abc.gr`\n**File**: `BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz`\n**Schedule**: `Every 5 minutes`\n#### Transfer to BigStreamer nodes\nA Perl script executed periodically by user `ipvpn` collects the raw files locally via passwordless SFTP, decompresses them and moves them to a local directory.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 6"
        }
    },
    "640": {
        "page_content": "``` mermaid\n  graph TD\n  A[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz <br> Path: /var/opt/OV/shared/nnm/databases/custompoller/export/final <br> Host: nnmprd01.abc.gr] -->|SFTP| B[Path: /data/1/nnm_components_LZ <br> Host: un2.bigdata.abc.gr <br> User: ipvpn]\n  B -->|Decompress/Move|C[Path: /data/1/nnm_components_LZ/spooldir <br> Host: un2.bigdata.abc.gr <br> User: ipvpn]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 7"
        }
    },
    "641": {
        "page_content": "D[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv.gz <br> Path: /var/opt/OV/shared/nnm/databases/custompoller/export/final <br> Host: nnmdis01.abc.gr] -.->|Stopped| B\n```\n**User**: `ipvpn`\n**Scheduler**: `Cron`\n**Schedule**: `Every minute`\n**SFTP Path**: `/var/opt/OV/shared/nnm/databases/custompoller/export/final`\n**SFTP user**: `custompoller`\n**Intermediate Path**: `/data/1/nnm_components_LZ`\n**Destination Path**: `/data/1/nnm_components_LZ/spooldir`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 8"
        }
    },
    "642": {
        "page_content": "**Destination Path**: `/data/1/nnm_components_LZ/spooldir`\n**Logs**: ```/shared/abc/ip_vpn/log/nnm_component_metrics.cron.`date +%Y%m%d`.log```\n**Configuration**: `/shared/abc/ip_vpn/DataParser/scripts/transferlist/cpu_mem.trn`\n**Script**: `/shared/abc/ip_vpn/DataParser/scripts/load_data.pl` on `un2.bigdata.abc.gr`\n#### Load to BigStreamer cluster",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 9"
        }
    },
    "643": {
        "page_content": "#### Load to BigStreamer cluster\nDecompressed files are read on the spot by the Flume agent running on `un2.bigdata.abc.gr`. It first parses them using Morphline and then loads them into an Impala table.\n``` mermaid\n  graph TD\n  A[File: BIG-CUSTOMERS-CPU-MEM-UTIL_yyyymmddHHMMssSSS.csv <br> Path: /data/1/nnm_components_LZ/spooldir <br> Host: un2.bigdata.abc.gr]\n  B[Morphline Parsing]\n  C[Impala Table: bigcust.nnm_ipvpn_componentmetrics_hist]\n  A -->|Read| B\n  B -->|Load| C\n```\n**User**: `ipvpn`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 10"
        }
    },
    "644": {
        "page_content": "A -->|Read| B\n  B -->|Load| C\n```\n**User**: `ipvpn`\n**Name**: `Flume-IPVPN` on `un2.bigdata.abc.gr`\n**Schedule**: `Always`\n**Source Path**: `/data/1/nnm_components_LZ/spooldir`\n**Morphline JAR**: `/home/users/ipvpn/flume-ipvpn/jars/nnmmetrics/lib/ipvpnsla-customers-abc-flume-2.0.0-SNAPSHOT.jar`\n**Morphline Configuration**: `/shared/abc/ip_vpn/flume/nnm_component_metrics/morphline_nnmMetricsCsvToRecord_ipvpn_user.conf`\n**Impala Table**: `bigcust.nnm_ipvpn_componentmetrics_hist`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 11"
        }
    },
    "645": {
        "page_content": "**Impala Table**: `bigcust.nnm_ipvpn_componentmetrics_hist`\n**Logs**: `/var/log/flume-ng/flume-cmf-flume5-AGENT-un2.bigdata.abc.gr.log*`\n### SLA Metrics\n#### Creation of raw files",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 12"
        }
    },
    "646": {
        "page_content": "### SLA Metrics\n#### Creation of raw files\nThe source system in this case is SNMP Custom Poller application. For high availability there are two deployments of the application on two seperate servers and they operate in active-standby fashion. While the servers are managed by abc, the application is managed by jkl. The raw files produced contain SLA metrics for QoS and availability of the network elements and are stored in local paths on those servers.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 13"
        }
    },
    "647": {
        "page_content": "``` mermaid\n  graph TD\n  A[Service: SNMP Custom Poller <br> Host: nnmprd01.abc.gr] -.->|Stopped| B[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/ipvpn/out <br> Host: nnmprd01.abc.gr]\n  C[Service: SNMP Custom Poller <br> Host: nnmdis01.abc.gr] --> D[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/ipvpn/out <br> Host: nnmdis01.abc.gr]\n```\n**User**: `custompoller`\n**Scheduler**: `Cron`\n**Schedule**: `Every 5 minutes`\n**Path**: `/home/custompoller/ipvpn/out`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 14"
        }
    },
    "648": {
        "page_content": "**Scheduler**: `Cron`\n**Schedule**: `Every 5 minutes`\n**Path**: `/home/custompoller/ipvpn/out`\n**Elements Configuration**: `/home/custompoller/ipvpn/conf/vpn.config`\n**Logs**: ```/home/custompoller/ipvpn/log/ipvpn-`date +%Y%m%d`.log```\n**Script**: `/home/custompoller/ipvpn/run/run_ipvpn.sh` on `nnmprd01.abc.gr` and `nnmdis01.abc.gr`\n#### Transfer to BigStreamer nodes",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 15"
        }
    },
    "649": {
        "page_content": "#### Transfer to BigStreamer nodes\nA Perl script executed periodically by user `ipvpn` collects the raw files locally via passwordless SFTP, concatenates them into one for every 5 minute interval and uploads them to an HDFS directory.\n``` mermaid\n  graph TD\n  A[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/ipvpn/out <br> Host: nnmprd01.abc.gr]\n  E[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/ipvpn/out <br> Host: nnmdis01.abc.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 16"
        }
    },
    "650": {
        "page_content": "B[Path: /data/1/nnm_custompoller_ipvpn_LZ <br> Host: un2.bigdata.abc.gr]\n  C[File: nnm_poller_ipvpn.yyyymmddHHMM.yyyymmdd_HHMMss.group.parsed <br> Path: /data/1/nnm_custompoller_ipvpn_LZ <br> Host: un2.bigdata.abc.gr]\n  D[Path: hdfs://nameservice1/ez/landingzone/nnm_custompoller_ipvpn/raw]\n  A -.->|Stopped|B\n  E --> |SFTP|B\n  B -->|Concat|C\n  C -->|Upload HDFS|D\n```\n**User**: `ipvpn`\n**Scheduler**: `Executed from the previous step`\n**SFTP Path**: `/home/custompoller/ipvpn/out`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 17"
        }
    },
    "651": {
        "page_content": "**Scheduler**: `Executed from the previous step`\n**SFTP Path**: `/home/custompoller/ipvpn/out`\n**SFTP User**: `custompoller`\n**Intermediate Path**: `/data/1/nnm_custompoller_ipvpn_LZ`\n**Destination Path**: `hdfs://nameservice1/ez/landingzone/nnm_custompoller_ipvpn/raw`\n**Logs**: ```/shared/abc/nnm_custompoller_ipvpn/log/nnmcustompoller_ipvpn_cron.`date +%Y%m%d`.log```\n**Configuration**: `/shared/abc/nnm_custompoller_ipvpn/DataParser/scripts_nnmprod/nnm_custompoller_ipvpn.trn`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 18"
        }
    },
    "652": {
        "page_content": "**Script**: `/shared/abc/nnm_custompoller_ipvpn/DataParser/scripts_nnmprod/nnm_custompoller_ipvpn.pl` on `un2.bigdata.abc.gr`\n#### Load to BigStreamer cluster\nA Spark job executed by the previous step parses the concatenated files and loads them into an Impala table.\n>Ndef\n>`spark-submit.sh` script is triggered in the following manner:\n> 1.`run_ipvpn.sh` is run on nnmdis01 which triggers the execution of\n> 2.`nnm_custompoller_ipvpn.pl` on un2 via ssh which runs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 19"
        }
    },
    "653": {
        "page_content": "> 2.`nnm_custompoller_ipvpn.pl` on un2 via ssh which runs\n> 3. `spark_submit.sh` passed via `post_script` variable\n``` mermaid\n  graph TD\n  A[File: nnm_poller_ipvpn.yyyymmddHHMM.yyyymmdd_HHMMss.group.parsed <br> Path: hdfs://nameservice1/ez/landingzone/nnm_custompoller_ipvpn/raw]\n  B[Parsing]\n  C[Impala Table: bigcust.nnmcp_ipvpn_slametrics_hist]\n  A -->|Read| B\n  B -->|Load| C\n```\n**User**: `ipvpn`\n**Scheduler**: `Executed from the previous step`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 20"
        }
    },
    "654": {
        "page_content": "B -->|Load| C\n```\n**User**: `ipvpn`\n**Scheduler**: `Executed from the previous step`\n**Job Name**: `com.jkl.bigstreamer.ipvpnslacustomers.spark.snmp.SnmpETLTopologyRunner`\n**JAR**: `/home/users/ipvpn/run/ipvpnsla-customers-abc-spark.jar`\n**Logs**: ```/shared/abc/nnm_custompoller_ipvpn/log/nnmcustompoller_ipvpn_cron.`date +%Y%m%d`.log```\n**Submit Script**: `/home/users/ipvpn/run/spark-submit.sh` on `un2.bigdata.abc.gr`\n**Impala Table**: `bigcust.nnmcp_ipvpn_slametrics_hist`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 21"
        }
    },
    "655": {
        "page_content": "**Impala Table**: `bigcust.nnmcp_ipvpn_slametrics_hist`\n### Interface Metrics\n#### Creation of raw files\nThe source system in this case is NNM. For high availability there are two instances of NNM on two seperate servers and they operate in active-standby fashion. The whole infrastructure is entirely managed by abc. The raw files produced contain interface metrics for overall interfaces' usage of the network elements and are stored in local paths on those servers.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 22"
        }
    },
    "656": {
        "page_content": "``` mermaid\n  graph TD\n  A[Service: NNM <br> Host: nnmprd01.abc.gr] --> B[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica <br> Host: nnmprd01.abc.gr]\n  C[Service: NNM <br> Host: nnmdis01.abc.gr] -.->|Stopped| D[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica <br> Host: nnmdis01.abc.gr]\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 23"
        }
    },
    "657": {
        "page_content": "```\n**Source Path**: `/files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica` on `nnmprd01.abc.gr` or `nnmdis01.abc.gr`\n**File**: `InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz`\n**Schedule**: `Every 5 minutes`\nThen a shell script running on the same servers, copies the files to the appropriate directory .\n``` mermaid\n  graph TD\n  A[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica <br> Host: nnmprd01.abc.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 24"
        }
    },
    "658": {
        "page_content": "B[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /files/_CUSTOM_POLLER_PROD/nnm_interface_metrics/vertica <br> Host: nnmdis01.abc.gr]\n  C[Script: transfer-new-files.sh <br> Host: nnmprd01.abc.gr <br> User: custompoller]\n  D[Script: transfer-new-files.sh <br> Host: nnmdis01.abc.gr <br> User: custompoller]\n  E[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /home/custompoller/nnm_interface_metrics <br> Host: nnmprd01.abc.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 25"
        }
    },
    "659": {
        "page_content": "F[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /home/custompoller/nnm_interface_metrics <br> Host: nnmdis01.abc.gr]\n  A --> C\n  C --> E\n  B -.->|Stopped| D\n  D -.-> F\n```\n**User**: `custompoller`\n**Scheduler**: `Cron`\n**Schedule**: `Every minute`\n**Path**: `/home/custompoller/nnm_interface_metrics`\n**Logs**: ```/home/custompoller/export_metrics/log/transfer-new-files.`date +%Y%m%d`.log```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 26"
        }
    },
    "660": {
        "page_content": "**Logs**: ```/home/custompoller/export_metrics/log/transfer-new-files.`date +%Y%m%d`.log```\n**Script**: `/home/custompoller/export_metrics/transfer-new-files.sh` on `nnmprd01.abc.gr` or `nnmdis01.abc.gr`\n#### Load to BigStreamer cluster\nA Perl script executed periodically by user `ipvpn` collects the raw files locally via passwordless SFTP, decompresses them and uploads them to an Impala table.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 27"
        }
    },
    "661": {
        "page_content": "``` mermaid\n  graph TD\n  A[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /home/custompoller/nnm_interface_metrics <br> Host: nnmprd01.abc.gr]\n  B[File: InterfaceMetrics_yyyymmddHHMMssSSS.csv.gz <br> Path: /home/custompoller/nnm_interface_metrics <br> Host: nnmdis01.abc.gr]\n  C[Path: /shared/abc/ip_vpn/interfaces_flow/repo <br> Host: un2.bigdata.abc.gr]\n  D[Impala Table: bigcust.perf_interfacemetrics_ipvpn_hist]\n  A -->|SFTP| C\n  B -.->|Stopped|C\n  C -->|Decompress/Load| D\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 28"
        }
    },
    "662": {
        "page_content": "A -->|SFTP| C\n  B -.->|Stopped|C\n  C -->|Decompress/Load| D\n```\n**User**: `ipvpn`\n**Scheduler**: `Cron`\n**Schedule**: `Every 2 minutes`\n**SFTP Path**: `/home/custompoller/nnm_interface_metrics`\n**SFTP user**: `custompoller`\n**Intermediate Path**: `/shared/abc/ip_vpn/interfaces_flow/repo`\n**Impala Table**: `bigcust.perf_interfacemetrics_ipvpn_hist`\n**Logs**: ```/shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/log/nnm_interface_metrics.cron.`date +%Y%m%d`.log```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 29"
        }
    },
    "663": {
        "page_content": "**Configuration**: `/shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/transferlist/config_Interface_metrics.trn`\n**Script**: `/shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/load_data.pl` on `un2.bigdata.abc.gr`\n## Input Configuration Data\n### NNM Postgres",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 30"
        }
    },
    "664": {
        "page_content": "## Input Configuration Data\n### NNM Postgres\nThe active NNM preserves network configuration data of the elements in a Postgres database, which is managed by abc. Every day these data are transferred to BigStreamer with Sqoop and used in computations of output performance data. Since slave nodes cannot connect directly to the Postgres database, firewall rules have been setup to `un1.bigdata.abc.gr` and `un2.bigdata.abc.gr` in order to forward requests to the external database.\n``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 31"
        }
    },
    "665": {
        "page_content": "``` bash\n[root@un2 ~] firewall-cmd --list-all\npublic (active)\n  target: ACCEPT\n  icmp-block-inversion: no\n  interfaces: bond0 bond0.100 bond0.2000 bond0.300 bond0.951 em2 lo p3p2\n  sources:\n  services: dhcpv6-client ssh\n  ports:\n  protocols:\n  masquerade: yes\n  forward-ports:\n    ...\n\tport=6535:proto=tcp:toport=5432:toaddr=999.999.999.999\n  source-ports:\n  icmp-blocks:\n  rich rules:\n```\n#### Table nms_iface\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 32"
        }
    },
    "666": {
        "page_content": "source-ports:\n  icmp-blocks:\n  rich rules:\n```\n#### Table nms_iface\n``` mermaid\n  graph TD\n  A[Postgres <br> Host: nnmprd01.abc.gr <br> Table: nnm.nms_iface ]\n  B[Postgres <br> Host: nnmdis01.abc.gr <br> Table: nnm.nms_iface ]\n  A -->|Sqoop| C[Impala Table: nnmnps.conf_nms_iface ]\n  B -.->|Stopped| C\n```\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 3:00 (UTC)`\n**Coordinator**: `Coord_nnmdb.nms_iface`\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 33"
        }
    },
    "667": {
        "page_content": "**Coordinator**: `Coord_nnmdb.nms_iface`\n**Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.\n- Check if data have been loaded for a specific date using Impala shell or editor.\n    ```bash\n    # e.g for 01-10-2018\n    select count(*) from nnmnps.conf_nms_iface where par_dt='20181001';\n    ```\n- Check if connectivity to Postgres works from `un2.bigdata.abc.gr`.\n#### Table nms_ip_addr\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 34"
        }
    },
    "668": {
        "page_content": "#### Table nms_ip_addr\n``` mermaid\n  graph TD\n  A[Postgres <br> Host: nnmprd01.abc.gr <br> Table: nnm.nms_ip_addr ]\n  B[Postgres <br> Host: nnmdis01.abc.gr <br> Table: nnm.nms_ip_addr ]\n  A -->|Sqoop| C[Impala Table: nnmnps.nms_ip_addr ]\n  B -.->|Stopped| C\n```\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 3:00 (UTC)`\n**Coordinator**: `Coord_nnmdb.nms_ip_addr`\n**Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 35"
        }
    },
    "669": {
        "page_content": "**Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.\n- Check if data have been loaded for a specific date using Impala shell or editor.\n    ```bash\n    # e.g for 01-10-2018\n    select count(*) from nnmnps.nms_ip_addr where par_dt='20181001';\n    ```\n- Check if connectivity to Postgres works from `un2.bigdata.abc.gr`.\n#### Table nms_node\n``` mermaid\n  graph TD\n  A[Postgres <br> Host: nnmprd01.abc.gr <br> Table: nnm.nms_node ]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 36"
        }
    },
    "670": {
        "page_content": "``` mermaid\n  graph TD\n  A[Postgres <br> Host: nnmprd01.abc.gr <br> Table: nnm.nms_node ]\n  B[Postgres <br> Host: nnmdis01.abc.gr <br> Table: nnm.nms_node ]\n  A -->|Sqoop| C[Impala Table: nnmnps.nms_node ]\n  B -.->|Stopped| C\n```\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 3:00 (UTC)`\n**Coordinator**: `Coord_nnmdb.nms_node`\n**Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 37"
        }
    },
    "671": {
        "page_content": "**Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.\n- Check if data have been loaded for a specific date using Impala shell or editor.\n    ```bash\n    # e.g for 01-10-2018\n    select count(*) from nnmnps.nms_node where par_dt='20181001';\n    ```\n- Check that Coordinator runs correctly.  \n     ```sql\n     SELECT MAX(par_dt) FROM nnmnps.nms_node WHERE par_dt>= from_timestamp(now() - interval 15 days,'yyyyMMdd')\n     ```\n    must return the yesterday's date.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 38"
        }
    },
    "672": {
        "page_content": "```\n    must return the yesterday's date. \n- Check if connectivity to Postgres works from `un2.bigdata.abc.gr`.\n### User Interface\n#### Stack\nUsers can manage configurations used during the computation of the output performance data, using the CustomApps UI. The load balancer sends traffic to two Wildfly instances who keep track of users' changes in MySQL. They are updated into BigStreamer daily. The whole stack is managed by jkl.\n``` mermaid\n  graph TD\n  A[Users] --> B[Load Balancer]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 39"
        }
    },
    "673": {
        "page_content": "``` mermaid\n  graph TD\n  A[Users] --> B[Load Balancer]\n  B --> C[Wildfly <br> Host: unekl1.bigdata.abc.gr]\n  B --> D[Wildfly <br> Host: unekl2.bigdata.abc.gr]\n  C --> E[MySQL]\n  D --> E\n  E --> F[Sync MySQL and BigStreamer]\n  F -->G[Impala]\n```\n##### Load Balancer\nFor load balancing the HaProxy service is used.\n**URL**: `https://cne.def.gr:8643/landing/#/login`\n**Controlled By**: `systemctl`\n**Configuration**: `/etc/haproxy/haproxy.cfg`\n**Host**: `unc1.bigdata.abc.gr` and `unc2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 40"
        }
    },
    "674": {
        "page_content": "**Host**: `unc1.bigdata.abc.gr` and `unc2.bigdata.abc.gr`\n##### Wildfly\nWildfly is the application server used for the UI of CustomApps including IP VPN.\n**User**: `trustcenter`\n**Installation Path**: `/opt/trustcenter/wf_cdef_trc` on `unekl1.bigdata.abc.gr` and `unekl2.bigdata.abc.gr`\n**Deployments Path**: `/opt/trustcenter/wf_cdef_trc/standalone/deployments`\n**General Configuration Path**: `/opt/trustcenter/wf_cdef_trc/standalone/configuration/standalone-full.xml`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 41"
        }
    },
    "675": {
        "page_content": "**Application Configuration Path**: `/opt/trustcenter/wf_cdef_trc/standalone/configuration/ServiceWeaver/beanconfig/`\n**Application Logs**: `/opt/trustcenter/wf_cdef_trc/standalone/log/server.log`\n**Access Log**: `/opt/trustcenter/wf_cdef_trc/standalone/log/access_log.log`\n##### MySQL\nMySQL is the database used for the UI. Apart from other things, configuration data managed by users are stored there.\n**MySQL Servers**: `db01.bigdata.abc.gr` and `db02.bigdata.abc.gr` with vIP `999.999.999.999`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 42"
        }
    },
    "676": {
        "page_content": "**MySQL Servers**: `db01.bigdata.abc.gr` and `db02.bigdata.abc.gr` with vIP `999.999.999.999`\n**MySQL Schema**: `trustcenter`\n#### Data Synchronization\nSynchronization of MySQL and BigStreamer's data is done using Sqoop. These are data necessary for computation of the output performance metrics from MySQL used for UI to BigStreamer. Also steps to load configuration tables are done for improving query performance. The master scripts executes jobs in order to update the following Impala tables.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 43"
        }
    },
    "677": {
        "page_content": "``` mermaid\n  graph TD\n  A[Oozie: Coord_IPVPN_load_mysql_to_Impala] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\n  B -->|sudo to ipvpn| C[Master Script]\n```\n**User**: `intra`\n**Scheduler**: `Oozie`\n**Schedule**: `Every day at 4:00 (UTC)`\n**Coordinator**: `Coord_IPVPN_load_mysql_to_Impala`\n**Master Script**: `/shared/abc/ip_vpn/run/run_load_mysql_to_impala.sh`\n**Troubleshooting Steps**:\n- Identify service errors in the Coordinator's logs and tasks.\n##### Table customer_pl",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 44"
        }
    },
    "678": {
        "page_content": "- Identify service errors in the Coordinator's logs and tasks.\n##### Table customer_pl\nCustomer PL indicates which Packet Loss formula type is going to be used for each customer. Default is 1.\n``` mermaid\ngraph TD\nA[MySQL Table: trustcenter.IPVPNSLA_CUSTOMER_PL] -->|Sqoop| B[Impala Table: bigcust.customer_pl]\n```\n**User**: `ipvpn`\n**Logs**: ```/shared/abc/ip_vpn/log/update_pl_customer.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/ip_vpn/run/update_pl_customer.sh` on `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 45"
        }
    },
    "679": {
        "page_content": "**Script**: `/shared/abc/ip_vpn/run/update_pl_customer.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n##### Table net_to_sm_customer\nNet to SM Customer is a translation for customer names from NNM to SM.\n``` mermaid\ngraph TD\nA[MySQL Table: trustcenter.IPVPNSLA_NET_TO_SM_CUSTOMER] -->|Sqoop| B[Impala Table: bigcust.net_to_sm_customer]\n```\n**User**: `ipvpn`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 46"
        }
    },
    "680": {
        "page_content": "```\n**User**: `ipvpn`\n**Logs**: ```/shared/abc/ip_vpn/log/update_net_to_sm_customer.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/ip_vpn/run/update_net_to_sm_customer.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n##### Table customer_sla_config_ipvpn\nSLA configurations specify how each QoS metric is computed.\n``` mermaid\ngraph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 47"
        }
    },
    "681": {
        "page_content": "SLA configurations specify how each QoS metric is computed.\n``` mermaid\ngraph TD\nA[MySQL Table: trustcenter.IPVPNCUUI_CUSTOMER_SLA_CONFIG] -->|Sqoop| B[Impala Table: bigcust.customer_sla_config_ipvpn]\n```\n**User**: `ipvpn`\n**Logs**: ```/shared/abc/ip_vpn/log/update_customer_sla_config.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/ip_vpn/run/update_customer_sla_config.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 48"
        }
    },
    "682": {
        "page_content": "**Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n##### Table pe_interfaces\nPE interfaces specify for which elements interface KPIs will be exported. The MySQL table is transferred to BigStreamer, enriched with NPS data and populates the Impala table. The enriched data are transferred back to MySQL.\n``` mermaid\ngraph TD\nA[MySQL Table: trustcenter.IPVPNSLA_PE_INTERFACES] -->|Sqoop| B[Impala Table: bigcust.pe_interfaces]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 49"
        }
    },
    "683": {
        "page_content": "A[MySQL Table: trustcenter.IPVPNSLA_PE_INTERFACES] -->|Sqoop| B[Impala Table: bigcust.pe_interfaces]\nB -->|Sqoop| C[MySQL Table: trustcenter.IPVPNSLA_PE_INTERFACES_INTERMEDIATE]\nC -->|MySQL Query|D[MySQL Table: trustcenter.IPVPNSLA_PE_INTERFACES]\n```\n**User**: `ipvpn`\n**Logs**: ```/shared/abc/ip_vpn/log/update_pe_interfaces.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/ip_vpn/run/update_pe_interfaces.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 50"
        }
    },
    "684": {
        "page_content": "**Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n##### Table sla_configurations\nData from the above tables are combined and loaded into an Impala table used for queries of output performance metrics.\n``` mermaid\ngraph TD\nA[Impala Table: bigcust.customer_sla_config_ipvpn] --> B[Impala Table: bigcust.sla_configurations]\nC[Impala Table: bigcust.customer_pl] --> B\nD[Impala Table: bigcust.net_to_sm_customer] --> B\n```\n**User**: `ipvpn`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 51"
        }
    },
    "685": {
        "page_content": "D[Impala Table: bigcust.net_to_sm_customer] --> B\n```\n**User**: `ipvpn`\n**Logs**: ```/shared/abc/ip_vpn/log/update_customer_sla_config.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/ip_vpn/run/update_customer_sla_config.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log file.\n## Output Performance Data",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 52"
        }
    },
    "686": {
        "page_content": "- Identify system or service errors in the log file.\n## Output Performance Data\nFor every 5-minute interval 5 HTTP requests containing CPU load, memory utilization, QoS, availability and interface utilization are sent to SQM server containing instanenous metrics for network elements and are moved to an exchange directory. Also these metrics are stored into an Impala table.\n### CPU and Memory\n#### Main script",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 53"
        }
    },
    "687": {
        "page_content": "### CPU and Memory\n#### Main script\nA procedure is executed periodically that triggers IPVPN-SM App to compute and transmit to the SQM server via HTTP the metrics for CPU and memory KPIs of the network elements. The procedure also inserts the KPIs into an Impala table. These three sub-steps are executed in parallel.\n**User**: `ipvpn`\n**Scheduler**: `Cron`\n**Schedule**: `Every 5 minutes`\n**Exchange user**: `saismpm`\n**Exchange path**: `/shared/abc/ip_vpn/out/saismpm`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 54"
        }
    },
    "688": {
        "page_content": "**Exchange user**: `saismpm`\n**Exchange path**: `/shared/abc/ip_vpn/out/saismpm`\n**Logs**: ```/shared/abc/ip_vpn/log/initiate_export_components.cron.`date '+%Y%m%d'`.log```\n**Script**: `/shared/abc/ip_vpn/run/initiate_export_components.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check IPVPN-SM script and app [logs](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ipvpn_sm_replacement.md#logs)\n##### Load component_metrics\n``` mermaid\ngraph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 55"
        }
    },
    "689": {
        "page_content": "##### Load component_metrics\n``` mermaid\ngraph TD\n  A[Impala Table: bigcust.nnm_ipvpn_componentmetrics_hist]\n  B[Computation of CPU & Memory KPIs]\n  C[Impala Table: bigcust.component_metrics]\n  A -->|Query|B\n  B -->|Load|C\n```\n**User**: `ipvpn`\n**Impala Table**: `bigcust.component_metrics`\n**Logs**: ```/shared/abc/ip_vpn/log/populate_components_metrics_table.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/ip_vpn/run/populate_components_metrics_table.sh` on `un2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 56"
        }
    },
    "690": {
        "page_content": "**Script**: `/shared/abc/ip_vpn/run/populate_components_metrics_table.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log files e.g failed Impala query.\n### QoS and Availability\n#### Main script",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 57"
        }
    },
    "691": {
        "page_content": "### QoS and Availability\n#### Main script\nA procedure is executed periodically that triggers IPVPN-SM App to compute and transmit to the SQM server via HTTP the metrics for QoS and availability KPIs of the network elements. The procedure also inserts the KPIs into an Impala table. These three sub-steps are executed in parallel.\n**User**: `ipvpn`\n**Scheduler**: `Cron`\n**Schedule**: `Every 5 minutes`\n**Exchange user**: `saismpm`\n**Exchange path**: `/shared/abc/ip_vpn/out/saismpm`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 58"
        }
    },
    "692": {
        "page_content": "**Exchange user**: `saismpm`\n**Exchange path**: `/shared/abc/ip_vpn/out/saismpm`\n**Logs**: ```/shared/abc/ip_vpn/log/initiate_export_sla.cron.`date '+%Y%m%d'`.log```\n**Script**: `/shared/abc/ip_vpn/run/initiate_export_sla.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Check IPVPN-SM script and app [logs](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ipvpn_sm_replacement.md#logs)\n##### Load sla_metrics\n``` mermaid\ngraph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 59"
        }
    },
    "693": {
        "page_content": "##### Load sla_metrics\n``` mermaid\ngraph TD\n  A[Impala Table: bigcust.nnmcp_ipvpn_slametrics_hist]\n  B[Computation of QoS and AV KPIs]\n  C[Impala Table: bigcust.sla_metrics]\n  A -->|Query|B\n  B -->|Load|C\n```\n**User**: `ipvpn`\n**Impala Table**: `bigcust.sla_metrics`\n**Logs**: ```/shared/abc/ip_vpn/log/populate_sla_metrics_table.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/ip_vpn/run/populate_sla_metrics_table.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 60"
        }
    },
    "694": {
        "page_content": "**Troubleshooting Steps**:\n- Identify system or service errors in the log files e.g failed Impala query.\n#### Support actions\n##### Spark failure\nIf the ingestion of the SLA metrics failed during the spark job execution (meaning that the files are successfully moved to hdfs dir `/ez/landingzone/nnm_custompoller_ipvpn/raw` and they are ready to be loaded on the Impala table) then we can re-submit the spark job in the following manner:\n1. Connect to un2 \n``` \nssh un2\nsu - ipvpn",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 61"
        }
    },
    "695": {
        "page_content": "1. Connect to un2 \n``` \nssh un2\nsu - ipvpn\nkinit -kt /home/users/ipvpn/ipvpn.keytab ipvpn\n```\n2. Execute the spark-submit as seen on file `/home/users/ipvpn/run/spark-submit.sh` with the appropriate startMin and endMin parameters\n   e.g. for loading the metrics that correspond to the 5min interval `2023-11-29 11:20:00` :\n```\nspark-submit \\\n  --verbose  \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --num-executors 4 \\",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 62"
        }
    },
    "696": {
        "page_content": "```\nspark-submit \\\n  --verbose  \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --num-executors 4 \\\n  --files /home/users/ipvpn/ipvpn.keytab#ipvpn.keytab,/etc/hive/conf/hive-site.xml,/home/users/ipvpn/conf/ipvpn-log4j.xml#ipvpn-log4j.xml \\\n  --conf \"spark.executor.extraJavaOptions=-Dlog4j.configuration=/home/users/ipvpn/conf/ipvpn-log4j.xml\" \\\n  --conf \"spark.driver.extraJavaOptions=-Dlog4j.configuration=/home/users/ipvpn/conf/ipvpn-log4j.xml\" \\",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 63"
        }
    },
    "697": {
        "page_content": "--conf \"spark.driver.extraJavaOptions=-DlogFilename=/home/users/ipvpn/log/ipvpn-\" \\\n  --conf \"spark.executor.extraJavaOptions=-DlogFilename=/home/users/ipvpn/log/ipvpn-\" \\\n  --conf \"spark.executor.instances=4\" \\\n  --conf \"spark.executor.cores=1\" \\\n  --conf \"spark.executor.memory=4g\" \\\n  --conf \"spark.executor.memoryOverhead=600\" \\\n  --conf \"spark.driver.cores=1\" \\\n  --conf \"spark.driver.memory=4g\" \\\n  --conf \"spark.driver.memoryOverhead=600\" \\\n  --conf \"spark.dynamicAllocation.enabled=false\" \\",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 64"
        }
    },
    "698": {
        "page_content": "--conf \"spark.driver.memoryOverhead=600\" \\\n  --conf \"spark.dynamicAllocation.enabled=false\" \\\n  --class com.jkl.bigstreamer.ipvpnslacustomers.spark.snmp.SnmpETLTopologyRunner /home/users/ipvpn/run/ipvpnsla-customers-abc-spark.jar \\",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 65"
        }
    },
    "699": {
        "page_content": "-baseDirectory \"hdfs://nameservice1/ez/landingzone/nnm_custompoller_ipvpn/raw/\" -startMin 202311281120 -endMin 202311281120 -impalaTableName \"bigcust.nnmcp_ipvpn_slametrics_hist\" -counter32List \"NumOfRTT,SumOfRTT,PacketLostSD,PacketLostDS,PacketMIA,NumJitOpCompletions,SumOfPosJitterSD,SumOfNegJitterSD,NumOfPosJitterSD,NumOfNegJitterSD,SumOfPosJitterDS,SumOfNegJitterDS,NumOfPosJitterDS,NumOfNegJitterDS,OperationCompletions,OperationTotInitiations\" -totalDatasetPartitions 30\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 66"
        }
    },
    "700": {
        "page_content": "```\n3. Check metrics are loaded\n```\nrefresh nnmcp_ipvpn_slametrics_hist; \nselect count(*) from nnmcp_ipvpn_slametrics_hist where n5_minute='2023-11-28 11:20:00';\n```\n### Interfaces\n#### Main script\nA procedure is executed periodically that triggers IPVPN-SM App to compute and transmit to the SQM server via HTTP the metrics for interface KPIs of the network elements. The procedure also inserts the KPIs into an Impala table. These two sub-steps are executed in parallel.\n**User**: `ipvpn`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 67"
        }
    },
    "701": {
        "page_content": "**User**: `ipvpn`\n**Scheduler**: `Cron`\n**Schedule**: `Every 5 minutes`\n**Exchange user**: `saismpm`\n**Exchange path**: `/shared/abc/ip_vpn/out/saismpm`\n**Logs**: ```/shared/abc/ip_vpn/log/initiate_export_interfaces.cron.`date '+%Y%m%d'`.log```\n**Master Script**: `/shared/abc/ip_vpn/run/initiate_export_interfaces.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 68"
        }
    },
    "702": {
        "page_content": "**Troubleshooting Steps**:\n- Check IPVPN-SM script and app [logs](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ipvpn_sm_replacement.md#logs)\n##### Load interface_metrics\n``` mermaid\ngraph TD\n  A[Impala Table: bigcust.perf_interfacemetrics_ipvpn_hist]\n  B[Computation of IF KPIs]\n  C[Impala Table: bigcust.interface_metrics]\n  A -->|Query|B\n  B -->|Load|C\n```\n**User**: `ipvpn`\n**Impala Table**: `bigcust.interface_metrics`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 69"
        }
    },
    "703": {
        "page_content": "A -->|Query|B\n  B -->|Load|C\n```\n**User**: `ipvpn`\n**Impala Table**: `bigcust.interface_metrics`\n**Logs**: ```/shared/abc/ip_vpn/log/populate_interface_metrics_table.`date +%Y%m%d`.log```\n**Script**: `/shared/abc/ip_vpn/run/populate_interface_metrics_table.sh` on `un2.bigdata.abc.gr`\n**Troubleshooting Steps**:\n- Identify system or service errors in the log files e.g failed Impala query.\n## Output Configuration Data",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 70"
        }
    },
    "704": {
        "page_content": "## Output Configuration Data\nPeriodically an XML file that contains information about VPN customers with configuration changes, is produced by one of the Wildfly instances and is transferred to an exchange directory using a shell script.\n``` mermaid\ngraph TD\n  A[Wildfly <br> Host: unekl1.bigdata.abc.gr <br> Status: Master] -->| Executes | B[Script: /opt/trustcenter/wf_cdef_trc/ipvpnExports/sftp.sh <br> Host: unekl1.bigdata.abc.gr <br> User: trustuser]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 71"
        }
    },
    "705": {
        "page_content": "B -->|Export| C[File: SD_Inventory_DB_yyyy_mm_dd_HH:MM:SS.xml <br> Server: unekl1.bigdata.abc.gr]\n  E[Wildfly <br> Host: unekl2.bigdata.abc.gr <br> Status: Slave] -.->| Executes | F[Script: /opt/trustcenter/wf_cdef_trc/ipvpnExports/sftp.sh <br> Host: unekl2.bigdata.abc.gr <br> User: trustuser]\n  F -.->|Export| G[File: SD_Inventory_DB_yyyy_mm_dd_HH:MM:SS.xml <br> Server: unekl1.bigdata.abc.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 72"
        }
    },
    "706": {
        "page_content": "F -.->|Export| G[File: SD_Inventory_DB_yyyy_mm_dd_HH:MM:SS.xml <br> Server: unekl1.bigdata.abc.gr]\n  C -->|SFTP| D[User: sd <br> Server: eeaadaptprd.def.gr <br> Path: /sd/classic/scripts/install/inventory/export/IPVPN_Inventory]\n  G -.->|SFTP| D\n```\n**User**: `trustuser`\n**Scheduler**: `Wildfly`\n**Schedule**: `Every 4 hours`\n**SFTP user**: `sd`\n**SFTP Server**: `eeaadaptprd.def.gr`\n**SFTP path**: `/sd/classic/scripts/install/inventory/export/IPVPN_Inventory`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 73"
        }
    },
    "707": {
        "page_content": "**SFTP path**: `/sd/classic/scripts/install/inventory/export/IPVPN_Inventory`\n**Logs**: ```/opt/trustcenter/wf_cdef_trc/standalone/log/server.log*```\n**Script**: `/opt/trustcenter/wf_cdef_trc/ipvpnExports/sftp.sh` on `unekl1.bigdata.abc.gr` and `unekl2.bigdata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "ip_vpn.md - Part 74"
        }
    },
    "708": {
        "page_content": "# Syzefxis Flows\n## Useful Links\n- [Business Documents](https://metis.ghi.com/obss/bigdata/abc/sizefxis/bigstreamer-sizefxis-devops/-/tree/master/docs)\n- [MoP documents](https://metis.ghi.com/obss/bigdata/abc/sizefxis/bigstreamer-sizefxis-devops/-/tree/master/MOPs)\n- Users **keePass file**: [abc-devpasswd.kdbx](../../../abc-devpasswd.kdbx)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 1"
        }
    },
    "709": {
        "page_content": "- Users **keePass file**: [abc-devpasswd.kdbx](../../../abc-devpasswd.kdbx)  \n- **Troubleshooting Steps**: Refer to MoPs files in [devops repository](https://metis.ghi.com/obss/bigdata/abc/sizefxis/bigstreamer-sizefxis-devops/-/blob/master/MOPs/README.md?ref_type=heads) of the project\n## Input Performance Data\n### Service Level Agreement (SLA) Metrics\n#### Creation of raw files",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 2"
        }
    },
    "710": {
        "page_content": "### Service Level Agreement (SLA) Metrics\n#### Creation of raw files\nThe source system in this case is SNMP Custom Poller application. For high availability there are two deployments of the application on two seperate servers and they operate in active-standby fashion. While the servers are managed by abc, the application is managed by jkl. The raw files produced contain SLA metrics for QoS and availability of the network elements and are stored in local paths on those servers.\n``` mermaid",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 3"
        }
    },
    "711": {
        "page_content": "``` mermaid\n  graph TD\n  A[Service: SNMP Custom Poller <br> Host: nnmprd01.abc.gr] --> B[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/out <br> Host: nnmprd01.abc.gr]\n  C[Service: SNMP Custom Poller <br> Host: nnmdis01.abc.gr] -.->|Stoped| D[File: nnmcp.*.yyyymmddHHMM.txt <br> Path: /home/custompoller/out <br> Host: nnmdis01.abc.gr]\n```\n**Server**: `nnmprd01.abc.gr` (backup server `nnmdis01.abc.gr`)  \n**User**: `custompoller`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 4"
        }
    },
    "712": {
        "page_content": "```\n**Server**: `nnmprd01.abc.gr` (backup server `nnmdis01.abc.gr`)  \n**User**: `custompoller`  \n**Password**: `Passwordless SSH from intra@un2.bigdata.abc.gr`  \n**Scheduler**: `Cron`  \n**Schedule**: `Every 5 minutes`  \n**Path**: `/home/custompoller/out`  \n**Elements Configuration**: `/home/custompoller/conf/syzeyksis_syze1.config`  \n**Logs**: ```/home/custompoller/log/syzeyksis-`date +%Y%m%d`.log```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 5"
        }
    },
    "713": {
        "page_content": "**Logs**: ```/home/custompoller/log/syzeyksis-`date +%Y%m%d`.log```  \n**Script**: `/home/custompoller/run/run_syzeyksis_standby.sh` on `nnmprd01.abc.gr` and `nnmdis01.abc.gr`  \n**Alerts**:\n- Not monitored\n#### Transfer to BigStreamer nodes\nBash scripts executed periodically by user `intra` collects the raw files locally via passwordless SFTP, concatenates them into one for every 5 minute interval and uploads them to an HDFS directory.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 6"
        }
    },
    "714": {
        "page_content": "``` mermaid\n  graph TD\n  A[SFTP Server: nnmprd01.abc.gr<br>SFTP User: custopoller<br>SFTP Path: ./out] --> |SFTP GET|B[Local Staging Path: /data/1/nnm_custompoller_LZ/archives<br>FILES: nnm_poller.YYYYMMDDhhmm.YYYYMMDD_hhmmss.group.parsed]\n  -->|HDFS PUT| C[HDFS: /ez/landingzone/nnm_custompoller/raw/YYYYMMDDhhmm<br>FILE PATTERN: nnm_poller.YYYYMMDDhhmm.YYYYMMDD_hhmmss.group.parsed ]\n  D[SFTP Server: nnmdis01.abc.gr<br>SFTP User: custopoller<br>SFTP Path: ./out] -.-> |Stopped|B\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 7"
        }
    },
    "715": {
        "page_content": "D[SFTP Server: nnmdis01.abc.gr<br>SFTP User: custopoller<br>SFTP Path: ./out] -.-> |Stopped|B\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Scheduler**: `Cron`  \n**Schedule**: `\u0395very 10 minutes`  \n**SFTP Server**:  `nnmprd01.abc.gr` (backup: `nnmdis01.abc.gr`)  \n**SFTP Path**:  `./out`  \n**SFTP User**: `custompoller`  \n**SFTP Password**: `Passwordless Authentication with SSH Key`  \n**Local Staging Path**: `/data/1/nnm_custompoller_LZ/archives`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 8"
        }
    },
    "716": {
        "page_content": "**Local Staging Path**: `/data/1/nnm_custompoller_LZ/archives`  \n**HDFS Destination Path**: `/ez/landingzone/nnm_custompoller/raw/YYYYMMDDhhmm`  \n**Logs**: `/shared/abc/nnm_custompoller/log/nnmcustompoller_cron.YYYYMMDD.log`  \n**Configuration**: `/shared/abc/nnm_custompoller/DataParser/scripts/transferlist/nnm_custompoller.trn`  \n**Script**:  `/shared/abc/nnm_custompoller/DataParser/scripts/nnm_custompoller.pl`  \n**Alerts**:\n- Not monitored\n#### Load to BigStreamer cluster",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 9"
        }
    },
    "717": {
        "page_content": "**Alerts**:\n- Not monitored\n#### Load to BigStreamer cluster\nSpark Job to pivot raw data and make temporal calculation every 30 minutes to append the `nnmnps.nnmcp_qametrics_hist` impala table. This table will be the source data for further calculations and reports.\n``` mermaid\n  graph TD\nA[HDFS: /ez/landingzone/nnm_custompoller/raw/YYYYMMDDhhmm<br>FILE PATTERN: nnm_poller.YYYYMMDDhhmm.YYYYMMDD_hhmmss.group.parsed]-->B[Spark]-->C[Impala: nnmnps.nnmcp_qametrics_hist]\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 10"
        }
    },
    "718": {
        "page_content": "```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `syzefxis`  \n**Scheduler**: `Cron`  \n**Schedule**: `Every 30 minutes`  \n**Logs**: `/home/users/syzefxis/DataTransformation/log/syzefxis-YYYY-MM-DD.log`  \n**Script**: `/home/users/syzefxis/DataTransformation/run/spark-submit.sh`  \n**Alerts**:\n- Not monitored\n### Form new metrics and categorize KPIs to separate tables\nMaster script that launches sub-scripts in parallel. Each sub-script is a separate section below.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 11"
        }
    },
    "719": {
        "page_content": "``` mermaid\n  graph TD\n  A[001_CP_nnmnps_Metrics.sh]\n  A -.->B[100_Sqoop_MySql_HDFS_Load.sh]\n  A -.->C[401_CP_cpe_metrics.sh]\n  A -.->D[403_CP_cpe_E2E_metrics.sh]\n  A -.->E[402_CP_core_metrics.sh]\n  A -.->F[404_core_E2E_metrics.sh]\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**MySQL User**: `syzeyksis`  \n**MySQL Host**: `db-vip.bidata.abc.gr`  \n**Scheduler**: `Cron`  \n**Schedule**: `Every day at 6:30`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 12"
        }
    },
    "720": {
        "page_content": "**Schedule**: `Every day at 6:30`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`  \n**Script**: `/shared/abc/nnmnps/bin/001_CP_nnmnps_Metrics.sh`  \n**Alerts**:\n- Not monitored\n#### Sqoop Import\n``` mermaid\n  graph TD\n  S[MySQL: syzeyksis.SZXUI_MASTER_ENTRY]-->|Sqoop|T1[HDFS: /ez/tmp/cpe/exp] -->|Impala Insert|T[Impala: refdata.rd_cpe]\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**MySQL User**: `syzeyksis`\n**MySQL Host**: `db-vip.bidata.abc.gr`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 13"
        }
    },
    "721": {
        "page_content": "**User**: `intra`  \n**MySQL User**: `syzeyksis`\n**MySQL Host**: `db-vip.bidata.abc.gr`\n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`\n**Script**: `/shared/abc/nnmnps/bin/100_Sqoop_MySql_HDFS_Load.sh`  \n**Alerts**:\n- Not monitored\n#### CPE Metrics\n``` mermaid\n  graph TD\n  T[Impala: refdata.rd_cpe]\n  A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.cpe_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 14"
        }
    },
    "722": {
        "page_content": "C[SQL Transformation]\n  G[nnmnps.cpe_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`\n**Script**: `/shared/abc/nnmnps/bin/401_CP_cpe_metrics.sh`  \n**Alerts**:\n- Not monitored\n#### CPE E2E Metrics\n``` mermaid\n  graph TD\n  T[Impala: refdata.rd_cpe]\n  A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.cpe_e2e_metrics]\n  T-->C\n  A-->C",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 15"
        }
    },
    "723": {
        "page_content": "B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.cpe_e2e_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`  \n**Script**: `/shared/abc/nnmnps/bin/403_CP_cpe_E2E_metrics.sh`  \n**Alerts**:\n- Not monitored\n#### Core Metrics\n``` mermaid\n  graph TD\n  T[Impala: refdata.rd_cpe]\n  A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]\n  C[SQL Transformation]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 16"
        }
    },
    "724": {
        "page_content": "A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.core_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`  \n**Script**: `/shared/abc/nnmnps/bin/402_CP_core_metrics.sh`  \n**Alerts**:\n- Not monitored\n#### Core E2E Metrics\n``` mermaid\n  graph TD\n  T[Impala: refdata.rd_cpe]\n  A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 17"
        }
    },
    "725": {
        "page_content": "graph TD\n  T[Impala: refdata.rd_cpe]\n  A[nnmnps.nnmcp_qametrics_hist]\n  B[refdata.rd_calendar]\n  C[SQL Transformation]\n  G[nnmnps.core_e2e_metrics]\n  T-->C\n  A-->C\n  B-->C\n  C-->|Impala Insert|G\n```\n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/nnmnps_Metrics.cron.log`\n**Script**: `/shared/abc/nnmnps/bin/404_core_E2E_metrics.sh`  \n**Alerts**:\n- Not monitored\n### Transformation to calculate report daily KPIs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 18"
        }
    },
    "726": {
        "page_content": "**Alerts**:\n- Not monitored\n### Transformation to calculate report daily KPIs\nA Spark application is executed on a daily basis, which calculates the results according to the Metrics tables and stores the data in the output tables\n``` mermaid\n  graph TD\nA[Impala Tables:<br> nnmnps.cpe_metrics<br>nnmnps.core_metrics]-->B[Spark]-->C[Impala Tables:<br>nnmnps.report_daily_core<br>nnmnps.report_daily_cpe]\n```\n**User**: `intra`  \n**Scheduler**: `Oozie`  \n**Schedule**: `Every day at 8:00 (UTC)`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 19"
        }
    },
    "727": {
        "page_content": "```\n**User**: `intra`  \n**Scheduler**: `Oozie`  \n**Schedule**: `Every day at 8:00 (UTC)`  \n**Oozie Coordinator**: `DailySyzefxisCoordinator`  \n**Oozie workflow**: `Syzefxis_Daily_Spark`  \n**Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n**Alerts**:\n- Not monitored\n### Transformation to calculate report monthly KPIs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 20"
        }
    },
    "728": {
        "page_content": "**Alerts**:\n- Not monitored\n### Transformation to calculate report monthly KPIs\nA Spark application is executed on a monthly basis, which calculates the results according to the Metrics tables and stores the data in the output tables\n``` mermaid\n  graph TD\nA[Impala Tables:<br> nnmnps.cpe_metrics<br>nnmnps.core_metrics]-->B[Spark]-->C[Impala Tables:<br>nnmnps.report_monthly_core<br>nnmnps.report_monthly_cpe]\n```\n**User**: `intra`  \n**Scheduler**: `Oozie`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 21"
        }
    },
    "729": {
        "page_content": "```\n**User**: `intra`  \n**Scheduler**: `Oozie`  \n**Schedule**: `Every 1st day of month at 10:00 (UTC)`  \n**Oozie Coordinator**: `MonthlySyzefxisCoordinator`  \n**Oozie workflow**: `Syzefxis_Monthly_Spark`  \n**Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n**Alerts**:\n- Not monitored\n### Mail Report\nOn the 2nd day of each month, a shell script runs via Crontab that exports the data into the necessary csv files and mails them in ZIP format to the customer.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 22"
        }
    },
    "730": {
        "page_content": "```mermaid\ngraph TD\n  A1[Impala Tables<br>nnmnps.report_monthly_core<br>nnmnps.report_monthly_cpe<br>nnmnps.report_monthly_islet]\n  --> B1[Impala SELECT]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 23"
        }
    },
    "731": {
        "page_content": "--> B1[Impala SELECT]\n  --> C1[FILE:<br>network_report_1_monthly_syzefxis.YYYYMM.csv<br>network_report_13_monthly_syzefxis.YYYYMM.csv<br>network_report_14_monthly_syzefxis.YYYYMM.csv<br>network_report_16_monthly_syzefxis.YYYYMM.csv<br>network_report_19_monthly_syzefxis.YYYYMM.csv<br>network_report_20_monthly_syzefxis.YYYYMM.csv<br>Staging Directory:<br>/shared/abc/nnmnps/tmp]\n  \n  A2[Impala Tables:<br>nnmnps.report_daily_core<br>nnmnps.report_daily_cpe]\n  --> B2[Impala SELECT]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 24"
        }
    },
    "732": {
        "page_content": "A2[Impala Tables:<br>nnmnps.report_daily_core<br>nnmnps.report_daily_cpe]\n  --> B2[Impala SELECT]\n  --> C2[FILES:<br>1. network_report_1_daily_syzefxis.YYYYMM.csv<br>2. network_report_14_daily_syzefxis.YYYYMM.csv<br>3. network_report_16_daily_syzefxis.YYYYMM.csv<br><strong>Staging Directory:</strong><br>/shared/abc/nnmnps/tmp]\n  C1-->Z[Zip file:<br>YYYYMM.zip]\n  C2-->Z\n  Z-->M[mail<br>Recipients:<br>dvordonis fa:fa-at def.gr,sla fa:fa-at def.gr,ATraianou fa:fa-at abc.gr]\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 25"
        }
    },
    "733": {
        "page_content": "```\n**Server**: `un2.bigdata.abc.gr`  \n**Scheduler**: `Cron`  \n**Schedule**: `At the second day of each month at 6:00`  \n**Server**: `un2.bigdata.abc.gr`  \n**User**: `intra`  \n**Logs**: `/shared/abc/nnmnps/log/901_Export_CSV_reports.cron.log`  \n**Scripts**:\n- `/shared/abc/nnmnps/bin/901_Export_CSV_reports.sh`  \n- `/shared/abc/nnmnps/bin/902_Mail_Exported_Files.sh`\n**Export folder**: `/shared/abc/nnmnps/tmp`\n**Alerts**:\n- Not monitored\n### Troubleshooting",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 26"
        }
    },
    "734": {
        "page_content": "**Export folder**: `/shared/abc/nnmnps/tmp`\n**Alerts**:\n- Not monitored\n### Troubleshooting\nCheck if the custompoller continuously generates raw files.  \n1. ssh ipvpn@un2\n1. ssh custompoller@nnmprd01 (active node)\n1. cd /home/custompoller/out\n1. ls -ltr nnmcp.saa-syze1.*.txt\n```bash\n-rw-r--r-- 1 custompoller custompoller 32195036 Jan 16 16:40 nnmcp.saa-syze1.202401161640.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32196182 Jan 16 16:45 nnmcp.saa-syze1.202401161645.txt.LOADED",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 27"
        }
    },
    "735": {
        "page_content": "-rw-r--r-- 1 custompoller custompoller 32196182 Jan 16 16:45 nnmcp.saa-syze1.202401161645.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32197733 Jan 16 16:50 nnmcp.saa-syze1.202401161650.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32199664 Jan 16 16:55 nnmcp.saa-syze1.202401161655.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32201772 Jan 16 17:00 nnmcp.saa-syze1.202401161700.txt.LOADED",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 28"
        }
    },
    "736": {
        "page_content": "-rw-r--r-- 1 custompoller custompoller 32201772 Jan 16 17:00 nnmcp.saa-syze1.202401161700.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32203880 Jan 16 17:05 nnmcp.saa-syze1.202401161705.txt.LOADED\n-rw-r--r-- 1 custompoller custompoller 32138587 Jan 16 17:10 nnmcp.saa-syze1.202401161710.txt\n-rw-r--r-- 1 custompoller custompoller 32154540 Jan 16 17:15 nnmcp.saa-syze1.202401161715.txt\n-rw-r--r-- 1 custompoller custompoller 32179152 Jan 16 17:20 nnmcp.saa-syze1.202401161720.txt\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 29"
        }
    },
    "737": {
        "page_content": "-rw-r--r-- 1 custompoller custompoller 32179152 Jan 16 17:20 nnmcp.saa-syze1.202401161720.txt\n```\nNdef: `.LOADED` suffix indicates that the specific raw files has been transfered into cluster `un2:/data/1/nnm_custompoller_LZ/archives`\nUnder normal circumstances, custompoller creates the following files every 5 minutes\n- a new raw file with filename format `nnmcp.saa-syze1.<yyyymmddHHMM>.txt`\n- a lock file with the same filename, `saa-syze1.lock`\ni.e.\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 30"
        }
    },
    "738": {
        "page_content": "- a lock file with the same filename, `saa-syze1.lock`\ni.e.\n```\n-rw-r--r-- 1 custompoller custompoller        0 Jan 16 17:20 saa-syze1.lock\n-rw-r--r-- 1 custompoller custompoller 32179152 Jan 16 17:20 nnmcp.saa-syze1.202401161720.txt\n```\nNdef: Once the raw file completed, the lock file will be removed.  \nIn case an old lock file remains then the custompoller will stop generating new raw files.  \nLog file: `/home/custompoller/log/syzeyksis-2024-01-10.log`\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 31"
        }
    },
    "739": {
        "page_content": "Log file: `/home/custompoller/log/syzeyksis-2024-01-10.log`\n```\n16:10:02.429 ERROR [Thread-1] [saa-syze1] SNMPWalkTool: snmpWalkByOidsException: \njava.lang.IllegalStateException: Lock file /home/custompoller/out/saa-syze1.lock already exists.\n        at com.jkl.bigstreamer.snmp.tools.snmp4jwalk.SNMPWalkTool.createLockFile(SNMPWalkTool.java:198) ~[bigstreamer-snmp-tools-1.1.1-fixed.jar:1.1.1]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 32"
        }
    },
    "740": {
        "page_content": "at com.jkl.bigstreamer.snmp.tools.snmp4jwalk.SNMPWalkTool.snmpWalkByOids(SNMPWalkTool.java:73) [bigstreamer-snmp-tools-1.1.1-fixed.jar:1.1.1]\n        at com.jkl.bigstreamer.snmp.tools.wrapper.runnables.NodeRunner.run(NodeRunner.java:33) [bigstreamer-snmp-tools-1.1.1-fixed.jar:1.1.1]\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]\n```\n<b>Solution: Delete the lock file manually.</b>",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "syzefxis_flows.md - Part 33"
        }
    },
    "741": {
        "page_content": "# PIRAEUS CISCO VDSL2\n## Useful links\n- [Piraeus Cisco VDSL2 App](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-app)\n- [Piraeus Cisco VDSL2 DevOps](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-devops)\n- [Piraeus Cisco VDSL2 Deployment](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-deployment)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 1"
        }
    },
    "742": {
        "page_content": "- [Wiki Page](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-devops/-/wikis/home)\n- [File Definitions](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-devops/-/wikis/File-Definitions)\n- [Monitoring](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/home)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 2"
        }
    },
    "743": {
        "page_content": "- [Deployment Instructions](https://metis.ghi.com/obss/bigdata/abc/ipvpn/piraeus-cisco-vdsl2/piraeus-cisco-vdsl2-deployment/-/blob/main/Readme.md)\n## Overview",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 3"
        }
    },
    "744": {
        "page_content": "## Overview\n`Piraeus Cisco Vdsl2 App` is an application that polls data every 5 minutes using SNMP, transforms the SNMPs output files, concatenates the files to one output file and then places it to an SFTP server and an HDFS directory, in order to be retrieved by the customer. The application runs in a Kubernetes pod. [Monitoring App](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/home#prod) is used for monitoring and health checks.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 4"
        }
    },
    "745": {
        "page_content": "**Pod User:** `root`  \n**Pod Scheduler:** `Cron`  \n**Kubernetes Namespace** `piraeus-cisco-vdsl2-deployment`  \n**Cluster User** `ipvpn`  \n**Container Registry** `kubemaster-vip.bigdata.abc.gr/piraeus-cisco-vdsl2-app`  \n**Schedule:** `Every 5 minutes`  \n**Pod Script:** `/app/run_vdsl2.sh`  \n**Main Configuration File:** `/app/conf/application.yaml`  \n**Logs:** Use `kubectl logs` to view `stdout`  \n**Hadoop Table:** `bigcust.vdsl2`\n## Application Components\n### SNMP Polling of Elements\n``` mermaid",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 5"
        }
    },
    "746": {
        "page_content": "**Hadoop Table:** `bigcust.vdsl2`\n## Application Components\n### SNMP Polling of Elements\n``` mermaid\n  graph TD\n  A[Piraeus Bank VTUs] -->|SNMP Polling| B[File: nnmcp.vdsl-g*.\\*.txt <br> Path: /app/work/data <br> Pod ]\n```\nThe application polls data using SNMP. The raw files produced, contain component metrics ([4 metrics](#metrics) for each Element) of the network elements and are stored in local path inside a Kubernetes pod.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 6"
        }
    },
    "747": {
        "page_content": "**Output Path Configuration:** `/app/conf/application.yaml` -> `snmppoller.dataDir`  \n**Output File Name Pattern:** `nnmcp.vdsl-g*.\\*.txt`  \n**Elements Configuration File:** `/app/conf/application.yaml` -> `snmppoller.endpoints`  \n**Keystore Path Configuration:** `/app/conf/application.yaml` ->  `snmppoller.keyStoreFilePath`\n### Transformation of SNMP files\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 7"
        }
    },
    "748": {
        "page_content": "### Transformation of SNMP files\n``` mermaid\n  graph TD\n  B[File: nnmcp.vdsl-g*.\\*.txt <br> Path: /app/work/data <br> Pod ] --> |Transform|D[File: nnmcp.vdsl-g*.\\*.txt.csv <br> Path: /app/work/data <br> Pod ]\n```\nAfter the data has been polled, the application transforms the output files to respective CSV files while formatting the data to fit the desired formation. The files are stored in local path inside a Kubernetes pod.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 8"
        }
    },
    "749": {
        "page_content": "**Output Path Configuration:** `/app/conf/application.yaml` -> `vdsl2.dataDir`  \n**Output File Name Pattern Configuration:** `/app/conf/application.yaml` -> `vdsl2.filePattern`  \n**Input File Pattern:** `nnmcp.vdsl-g*.\\*.txt.csv`  \n### Merging of transformed files\n``` mermaid\n  graph TD\n  D[File: nnmcp.vdsl-g*.\\*.txt.csv <br> Path: /app/work/data <br> Pod ] --> |Merge|E[File: VDSL2_*.csv <br> Path: /app/work/data <br> Pod ]\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 9"
        }
    },
    "750": {
        "page_content": "```\nThe application then merges all the output csv files, which have the same timestamp, and produce a single deliverable file.\n**Output Path Configuration:** `/app/conf/application.yaml` -> `fmerger.dataDir`  \n**Output File Name Pattern Configuration:** `/app/conf/application.yaml` -> `fmerger.filePattern`  \n**Input File Pattern:** `VDSL2_*.csv`  \n### SFTP Transfer\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 10"
        }
    },
    "751": {
        "page_content": "**Input File Pattern:** `VDSL2_*.csv`  \n### SFTP Transfer\n``` mermaid\n  graph TD\n  E[File: VDSL2_*.csv <br> Path: /app/work/data <br> Pod ] --> |SFTP|F[File: VDSL2_*.csv <br> Path:/shared/abc/ip_vpn/out/vdsl2 <br> Host: un-vip.bigdata.abc.gr/999.999.999.999]\n```\nThe Piraeus Cisco Vdsl2 App places the deliverable file in an sftp server.\n**Input Step Configuration:** `/app/conf/application.yaml` -> `ssh.inputFrom`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 11"
        }
    },
    "752": {
        "page_content": "**Input Step Configuration:** `/app/conf/application.yaml` -> `ssh.inputFrom`\n**Input File Source Path Configuration:** `/app/conf/application.yaml` -> `ssh.source`  \n**SFTP Destination Path Configuration:** `/app/conf/application.yaml` -> `ssh.remdef`  \n**SFTP Host Configuration:** `/app/conf/application.yaml` -> `ssh.host`  \n**SFTP User Configuration:** `/app/conf/application.yaml` -> `ssh.user`  \n**SFTP Key Configuration:** `/app/conf/application.yaml` -> `ssh.prkey`  \n  \n### HDFS transfer",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 12"
        }
    },
    "753": {
        "page_content": "**SFTP Key Configuration:** `/app/conf/application.yaml` -> `ssh.prkey`  \n  \n### HDFS transfer\n``` mermaid\n  graph TD\n  E[File: VDSL2_*.csv <br> Path: /app/work/data <br> Pod ] --> |HDFS|G[File: VDSL2_*.csv <br> Path:/ez/warehouse/bigcust.db/landing_zone/ext_tables/piraeus_vdsl2 <br> HDFS]\n```\nThe Piraeus Cisco Vdsl2 App places the deliverable file in a hdfs directory for archiving.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 13"
        }
    },
    "754": {
        "page_content": "```\nThe Piraeus Cisco Vdsl2 App places the deliverable file in a hdfs directory for archiving.\n**Input Step Configuration:** `/app/conf/application.yaml` -> `hdfsput.inputFrom` //From which step it gets the files to put to hdfs  \n**Input File Path Configuration:** `/app/conf/application.yaml` -> `hdfsput.dataDir`  \n**Output HDFS URL Configuration:** `/app/conf/application.yaml` -> `hdfsput.hdfsURL`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 14"
        }
    },
    "755": {
        "page_content": "**Output HDFS URL Configuration:** `/app/conf/application.yaml` -> `hdfsput.hdfsURL`  \n**Output HDFS Destination Path Configuration:** `/app/conf/application.yaml` -> `hdfsput.hdfsPath`  \n**Hadoop User Configuration:** `/app/conf/application.yaml` -> `hdfsput.hadoopUser`  \n**Hadoop Site Configuration:** `/app/conf/application.yaml` -> `hdfsput.hadoopSite`  \n**Kerberos Configuration File:** `/app/conf/application.yaml` -> `hdfsput.kerberos.krb5conf`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 15"
        }
    },
    "756": {
        "page_content": "**Kerberos Configuration File:** `/app/conf/application.yaml` -> `hdfsput.kerberos.krb5conf`  \n**Kerberos User Keytab Path Configuration:** `/app/conf/application.yaml` -> `hdfsput.kerberos.keytab`  \n**Kerberos Principal Configuration:** `/app/conf/application.yaml` -> `hdfsput.kerberos.principal`\n## Metrics",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 16"
        }
    },
    "757": {
        "page_content": "| Requirement                         | Metric                          | Metric OID                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 17"
        }
    },
    "758": {
        "page_content": "| ----------------------------------- | ------------------------------- | --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 18"
        }
    },
    "759": {
        "page_content": "| Connected (actual) Speed downstream | xdsl2LineStatusAttainableRateDs | transmission.999.999.999.999.1.20 | Maximum Attainable Data Rate Downstream. The maximum downstream net data rate currently attainable by the xTU-C transmitter and the xTU-R receiver, coded in bit/s.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 19"
        }
    },
    "760": {
        "page_content": "| Connected (actual) Speed upstream   | xdsl2LineStatusAttainableRateUs | transmission.999.999.999.999.1.21 | Maximum Attainable Data Rate Upstream. The maximum upstream net data rate currently attainable by the xTU-R transmitter and the xTU-C receiver, coded in bit/s.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 20"
        }
    },
    "761": {
        "page_content": "| Signal Attenuation per band         | xdsl2LineBandStatusSigAtten     | transmission.999.999.999.999.1.3  | When referring to a band in the downstream direction, it is the measured difference in the total power transmitted by the xTU-C and the total power received by the xTU-R over all  subcarriers of that band during Showtime. When referring to a band in the upstream direction, it is the measured difference in the total power transmitted by the xTU-R and the total power received by the xTU-C over all subcarriers of that band during Showtime. Values range from 0 to 1270 in units of 0.1 dB (physical values are 0 to 127 dB). A special value of 0x7FFFFFFF (2147483647) indicates the line attenuation is out of range to be represented. A special value of 0x7FFFFFFE (2147483646) indicates the line attenuation measurement is unavailable. |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 21"
        }
    },
    "762": {
        "page_content": "| Noise margin per band               | xdsl2LineBandStatusSnrMargin    | transmission.999.999.999.999.1.4  | SNR Margin is the maximum increase in dB of the noise power received at the xTU (xTU-R for a band in the downstream direction and xTU-C for a band in the upstream direction), such that the BER requirements are met for all bearer channels received at the xTU.  Values range from -640 to 630 in units of 0.1 dB (physical values are -64 to 63 dB). A special value of 0x7FFFFFFF (2147483647) indicates the SNR Margin is out of range to be represented. A special value of 0x7FFFFFFE (2147483646) indicates the SNR Margin measurement is currently unavailable.                                                                                                                                                                                     |",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "cisco_vdsl2.md - Part 22"
        }
    },
    "763": {
        "page_content": "# Energy-Efficiency Pollaploi\n## Overview\nThis is an `Oozie Flow` responsible to **load data** from **txt files** into **impala tables**. Through the **Oozie Workflow** a **ssh** action is performed which executes the `pollaploi.sh` script. \n- **Utility Node / Server:** `un2.bigdata.abc.gr`\n  - **User:** `intra`\n  - [Password](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 1"
        }
    },
    "764": {
        "page_content": "- **Main File Directory:** `/shared/abc/energy_efficiency/load_pollaploi/`\n- **Oozie Coordinator**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `intra`\n  - **Coordinator:** `coord_energy_efficiency_load_pollaploi`\n    - **Execution:** \n      - **Winter time:** `every day at 21:00 local time (9PM)`\n      - **Daylight saving time:** `every day at 22:00 local time (10PM)`\n    - **Approximate Duration:** `8 seconds`\n    - **Workflow:** `energy_efficiency_load_pollaploi`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 2"
        }
    },
    "765": {
        "page_content": "- **Approximate Duration:** `8 seconds`\n    - **Workflow:** `energy_efficiency_load_pollaploi`\n      - **SSH Server:** `un-vip.bigdata.abc.gr`\n      - **SSH User:** `intra2`\n      - [Script](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/blob/master/FLOWS/energy_efficiency/PROD/load_pollaploi/pollaploi/pollaploi.sh)\n      - **Logs:** `view through job run - NO LOGS`\n## Pollaploi Flow",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 3"
        }
    },
    "766": {
        "page_content": "The `pollaploi flow` gets a .txt file from a remdef sftp directory and moves it to a temporary directory on the utility node. Then it unzips the file that was just transferred and compares it to another.txt file in the curr directory on the utility node. If those files are the same then it does nothing, since it means that the file has already been processed by the flow. If the file names are different then it removes the old file in the curr directory and moves the new file from the temp to the curr directory. After that the new file in the curr directory is put in a hdfs path. From there impala queries are executed clearing the pollaploi table, loading the data from the new file and refreshing the pollaploi table.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 4"
        }
    },
    "767": {
        "page_content": "- **SFTP:** \n   - **Initiator:** `intra` user\n\t- **User:** `bigd`\n\t- **Password:** `passwordless`\n\t- **Server:** `999.999.999.999`\n\t- **Path:** `/energypm/`\n\t- **Compressed File:** `*_pollaploi.zip` containing `*_pollaploi.txt`\n- **Utility Node Directories:**\n\t- **Curr:** `/shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr`\n\t- **Temp:** `/shared/abc/energy_efficiency/load_pollaploi/pollaploi_temp`\n  - **Scripts:** `/shared/abc/energy_efficiency/load_pollaploi/pollaploi`\n- **HDFS:**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 5"
        }
    },
    "768": {
        "page_content": "- **Scripts:** `/shared/abc/energy_efficiency/load_pollaploi/pollaploi`\n- **HDFS:**\n\t- **Path:** `/ez/landingzone/energy_temp/`\n- **Impala:** \n\t- **Database:** `energy_efficiency`\n\t- **Table Name:** `pollaploi`\n\t- **Retention:** `-`\n- **Logs:**\n  - **Path:** `/shared/abc/energy_efficiency/load_pollaploi/log`\n  - **Retention:** `none` (since 15/12/2019)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 6"
        }
    },
    "769": {
        "page_content": "- **Retention:** `none` (since 15/12/2019)\n**_Ndef:_** One of the `impala-shell` queries executed is the `LOAD DATA INPATH <hdfs_path>/<filename>` As seen in this [article](https://impala.apache.org/docs/build/html/topics/impala_load_data.html) the LOAD DATA INPATH command moves the loaded data file (not copies) into the Impala data directory. So the log entry `rm: /ez/landingzone/energy_temp/2023_03_01_pollaploi.txt': No such file or directory` is not something to worry about.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 7"
        }
    },
    "770": {
        "page_content": "**_[Sample data from pollaploi table](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/blob/master/FLOWS/energy_efficiency/TEST%20%CE%91%CE%A1%CE%A7%CE%95%CE%99%CE%91/2019_05_pollaploi.txt)_** \n## Troubleshooting Steps\nDue to the occurance of these tickets (**SD2179931** and **SD2021989**) the below steps should be followed for troubleshooting.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 8"
        }
    },
    "771": {
        "page_content": "1. Check that a new file `*pollaploi.zip` is placed in the `remdef SFTP directory`. Because the `workflow` runs in the evening (9PM or 10PM), if a file is placed earlier in the remdef SFTP directory and the client asks why it hasn't been loaded, wait until the next day and follow the steps mentioned here to see its execution.\n1. Check that a file `*pollaploi.txt` exists in `/shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 9"
        }
    },
    "772": {
        "page_content": "1. Based on the `date` the file has in `/shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr` check the log file `pollaploi.<YYYYMMDD>.log` of that specific day. E.g. \n    ```\n    $ ls -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/\n    > -rw-r--r-- 1 intra intra 1034132960 Mar 22 18:34 2023_03_01_pollaploi.txt\n    $ less /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.20230322.log\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 10"
        }
    },
    "773": {
        "page_content": "$ less /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.20230322.log\n    ```\n1. In `Hue` go to `Jobs` and search `energy` in the search bar. View the last executed `workflow` and see if it has run successfully.   \n### Possible Response to Ticket\n**_Ticket:_**\n``` \nGood morning,\nthe new pollaploi file has been uploaded but the corresponding table has not been updated yet\nThank you.\n```\n**_Response:_** (example)\n```\nGood evening.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 11"
        }
    },
    "774": {
        "page_content": "Thank you.\n```\n**_Response:_** (example)\n```\nGood evening.\nThere seems to be no new file in the sftp directory /energypm. The workflow that loads the table runs every day at 9PM/10PM, so if a file is added today it will be loaded in the evening. The last file that has been loaded is named 2023_03_01_pollaploi and from the logs on 2023-03-22 it seems to have been loaded normally.\n```\n## Useful Links",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 12"
        }
    },
    "775": {
        "page_content": "```\n## Useful Links\n- **[GitLab Repo](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/tree/master/FLOWS/energy_efficiency)**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "energy_efficiency_pollaploi.md - Part 13"
        }
    },
    "776": {
        "page_content": "# Reference Data Flow\n## Installation info\n### Data Source File\n- Local FileSystem Directories\n  - host :`un2.bigdata.abc.gr (999.999.999.999)`\n  - user : `vantagerd`\n  - spool area : `/data/1/vantage_ref-data/REF-DATA/` link points to `/shared/vantage_ref-data/REF-DATA`\n  - file_types : `<refType>_<refDate>.csv.gz`  \n*\\<refType\\>: `cells, crm, devices, services` (i.e. cells_20230530.csv.gz)*\n  - load_suffix : `<YYYYMMDD>.LOADED` *(i.e. cells_20230530.csv.cells_20230531.LOADED)*",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 1"
        }
    },
    "777": {
        "page_content": "- load_suffix : `<YYYYMMDD>.LOADED` *(i.e. cells_20230530.csv.cells_20230531.LOADED)*\n- HDFS Directories\n\t- hdfs landingzone : `/ez/landingzone/REFDATA`\n### Scripts-Logs Locations\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\n- user : `intra`\n- script path : `/shared/abc/refdata/bin`\n- script files: \n\t- `210_refData_Load.sh`\n\t- `220_refData_Daily_Snapshot.sh`\n- log path : `/shared/abc/refdata/log`\n- log files: \n\t- `210_refData_Load.<YYYYMM>.log`\n\t- `220_refData_Daily_Snapshot.<YYYYMM>.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 2"
        }
    },
    "778": {
        "page_content": "- log files: \n\t- `210_refData_Load.<YYYYMM>.log`\n\t- `220_refData_Daily_Snapshot.<YYYYMM>.log`\n### Crontab Scheduling\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\n- user : `intra`  \n\truns at : Daily at 00:05\n\t`5 0 * * * /shared/abc/refdata/bin/210_refData_Load.sh CELLS $(date '+\\%Y\\%m\\%d' -d \"yesterday\")`\n\t\nNdef1: The entry above loads reference data for CELLS.  \nNdef2: For each reference type a new entry of the `210_refData_Load.sh` is required passing the following parameters",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 3"
        }
    },
    "779": {
        "page_content": "\\<reference Type\\> : `cells, crm, devices, services`  \n\t\\<reference Date\\> : `yesterday` is the default value  \n### Hive Tables\n- Target Database: `refdata`\n- Target Tables: \n\t1. `rd_cells_load`\n\t1. `rd_services_load`\n\t1. `rd_crm_load`\n\t1. `rf_devices_load`\n## Data process\n### High Level Overview\n![High_Level_Overview](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/refdata/-/raw/main/docs/ReferenceData.High_Level_Overview.png)\n##### Steps 1-3:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 4"
        }
    },
    "780": {
        "page_content": "##### Steps 1-3: \nabc is responsible for the preparation/creation of the Reference Data flat files.  \nThese files are stored into a specific directory in `UN2` node using the SFTP-PUT method as user `vantagerd`  \n##### Steps 4-5:\nScript `210_refData_Load.sh` is responsible to read, parse and load the contents of reference files into HIVE tables (aka LOAD tables).  \nThese tables keep the data of all completed loads. That is, they contain all the historicity of the reference data.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 5"
        }
    },
    "781": {
        "page_content": "The data of each load is stored in a separate partition identified by the date of the loading (i.e. par_dt=20230530)  \n##### Steps 6-7:\nScript `220_refData_Daily_Snapshot.sh` reads the most recently added data from the LOAD table and store them as a snapshot into a separate table (aka snapshot tables).  \nThe data of these tables are used for the enrichment of various fact data (i.e. Traffica (SAI))\n## Manually Run\n`210_refData_Load.sh` script is responsible for the loading of a reference file.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 6"
        }
    },
    "782": {
        "page_content": "## Manually Run\n`210_refData_Load.sh` script is responsible for the loading of a reference file.\nTo run the script two arguments are required  \n`/shared/abc/refdata/bin/210_refData_Load.sh <refType>  <refDate>`  \n1st: **\\<refType\\>**, the Reference Type\n```\n- CELLS\n- CRM\n- DEVICES\n- SERVICES\n```\n2nd: **\\<refDate\\>**, the date that the flat file contains in its filename  \n\ti.e.\n```\ncells_20220207.csv.gz\ncells_20220208.csv.gz\ncells_20220209.csv.gz\nservices_20220207.csv.gz\ndevices_20220208.csv.gz",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 7"
        }
    },
    "783": {
        "page_content": "cells_20220208.csv.gz\ncells_20220209.csv.gz\nservices_20220207.csv.gz\ndevices_20220208.csv.gz\ncrm_20220209.csv.gz\n```\nIn case of loading the files above we should execute the following commands\n```\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20220207\n/shared/abc/refdata/bin/210_refData_Load.sh cells 20220208\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20220209\n/shared/abc/refdata/bin/210_refData_Load.sh services 20220207\n/shared/abc/refdata/bin/210_refData_Load.sh DEVICES 20220208",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 8"
        }
    },
    "784": {
        "page_content": "/shared/abc/refdata/bin/210_refData_Load.sh DEVICES 20220208\n/shared/abc/refdata/bin/210_refData_Load.sh crm 20220209\n```\nOnce the loads completed, the flat files renamed to <CURRENT_YYYYMMDD>.LOADED\n```\ncells_20220207.csv.20230531.LOADED\ncells_20220208.csv.20230531.LOADED\ncells_20220209.csv.20230531.LOADED\nservices_20220207.csv.20230531.LOADED\ndevices_20220208.csv.20230531.LOADED\ncrm_20220209.csv.20230531.LOADED\n```\n## Troubleshooting",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 9"
        }
    },
    "785": {
        "page_content": "devices_20220208.csv.20230531.LOADED\ncrm_20220209.csv.20230531.LOADED\n```\n## Troubleshooting\n**Currently, the Reference Data flow does not support the `Monitoring` services.**  \n- An email will be sent by the system with the point of failure.\ni.e.\n```\nSubject: ALERT: Reference data Loading, Type:CELL,  File:cells_20220207.csv\nBody: \n\tReference Type  : CELL\n\tReference File  : cells_20220207.csv\n\tReference Scirpt: 210_refData_Load.sh\n\t------------------------------------------",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 10"
        }
    },
    "786": {
        "page_content": "Reference Scirpt: 210_refData_Load.sh\n\t------------------------------------------\n\tERROR:$(date '+%F %T'), ALTER TABLE or LOAD DATA command failed.\n```\n- Check the log files for errors/exceptions  \n```\negrep -i 'error|fail|exception|problem' /shared/abc/refdata/log/210_refData_Load.YYYYMM.log\negrep -i 'error|fail|exception|problem' /shared/abc/refdata/log/220_refData_Daily_Snapshot.YYYYMM.log\n```\nIn case of failure follow the instructions described in **`Manually Run`**\n### Common errors",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 11"
        }
    },
    "787": {
        "page_content": "```\nIn case of failure follow the instructions described in **`Manually Run`**\n### Common errors  \n- Reference data file is empty or the contents of the file is not the expected.  \nIf this is the case, update abc that the file is invalid and ask them to send a new.  \n- Other factors not related to the specific flow\n\t- impala/hive availability\n\t- Kerberos authentication\n\t*Ndef: The flow checks if the ticket is still active before any HDFS action.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 12"
        }
    },
    "788": {
        "page_content": "*Ndef: The flow checks if the ticket is still active before any HDFS action.  \n\tIn case of expiration the flow performs a `kinit` command*\n## Data Check\n- **Check final tables for new partitions**:\n\t```\n\tsu - intra\n\t\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k -q \"refresh refdata.rd_cells_load; show partitions refdata.rd_cells_load;\"\n\t\n\t+----------+-----------+--------+---------+\n\t| par_dt   | #Rows     | #Files | Size    |\n\t+----------+-----------+--------+---------+",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 13"
        }
    },
    "789": {
        "page_content": "| par_dt   | #Rows     | #Files | Size    |\n\t+----------+-----------+--------+---------+\n\t| 20220227 | 98090     | 1      | 41.88MB |\n\t| 20220228 | 98021     | 1      | 41.84MB |\n\t| 20220301 | 97353     | 1      | 41.76MB |\n\t| Total    | 142404322 | 1500   | 59.63GB |\n\t+----------+-----------+--------+---------+\n\t```\n- **Check the amount of data in final tables**:\n\t```\n\tsu - intra",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 14"
        }
    },
    "790": {
        "page_content": "```\n- **Check the amount of data in final tables**:\n\t```\n\tsu - intra\n\t\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k -q \"select par_dt, count(*) as cnt from refdata.rd_cells_load group by par_dt order by 1;\"\n\t\n\tpar_dt   | cnt    \n\t---------+--------\n\t20221130 | 2784494\n\t```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "Reference_Data_Flow.md - Part 15"
        }
    },
    "791": {
        "page_content": "# Radius\n## Main Flow\n``` mermaid\n    graph TD\n    subgraph AA[Startup]\n      direction TB\n      AA1(\"Cleanup HDFS Folder:\" /ez/warehouse/radius.db/tmp)-->\n      AA2(\"Create Local Folder:\" ./sftp_files)-->\n      AA3(\"Create Local Folder:\" ./exported_files)-->\n      AA4(\"Housekeep Trustcenter SFTP server\")\n    end\n    subgraph AB[Load]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 1"
        }
    },
    "792": {
        "page_content": "AA4(\"Housekeep Trustcenter SFTP server\")\n    end\n    subgraph AB[Load]\n      AB1(\"SFTP Server\") -->|Excluding:<br>Non-valid filenames<br>Files with suffix .LOADED<br>Files older than `days_back` according to filename | AB2(\"Temporary Directory on Nodemanager: ./sftp_files\")\n      AB1 -->|Add suffix .LOADED to<br>filenames in HDFS file:<br>/user/radius/unrenamed_files|AB1\n      AB2 -->|Decompress| AB3(\"Temporary Directory on Nodemanager: ./sftp_files\")\n    end\n    subgraph AC[radarchive]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 2"
        }
    },
    "793": {
        "page_content": "end\n    subgraph AC[radarchive]\n      AC1(\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\") -->|Hive LOAD|AC2(\"Hive: radius.radarchive_stg\")\n      AC2 -->|Refresh/Impala Insert|AC3(\"Impala: radius.radarchive\")\n      AC2 -->|Refresh/Impala Upsert<br>Reduced fields|AC4(\"Kudu: radius.radreference\")\n    end\n    subgraph AD[radacct]\n      subgraph ADA[File Export]\n        direction TB",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 3"
        }
    },
    "794": {
        "page_content": "end\n    subgraph AD[radacct]\n      subgraph ADA[File Export]\n        direction TB\n        AD1(\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\") -->|Hive LOAD|AD2(\"Hive: radius.radacct_stg\")\n        AD6(\"npce.fixed_super_repo<br>Responsibility: abc\") -->AD8\n        AD7(\"demo.dummy_radius_dslams<br>Responsibility: abc\") -->AD8\n        AD2 --> AD8(\"Join\")",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 4"
        }
    },
    "795": {
        "page_content": "AD7(\"demo.dummy_radius_dslams<br>Responsibility: abc\") -->AD8\n        AD2 --> AD8(\"Join\")\n        AD8 -->|Refresh/Impala Select|AD9(\"Temporary Directory on Nodemanager:<br> ./exported_files/radacct_enriched_yearMonthDay_startHour-endHour.csv\")\n        AD9 -->|SFTP Put|AD10(\"Trustcenter SFTP\")\n      end\n      subgraph ADB[Populate Table]\n        direction TB\n        AD3(\"Kudu: radius.radreference\") -->AD4\n        AD11(\"Hive: radius.radacct_stg\") -->AD4(\"Join on username\")",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 5"
        }
    },
    "796": {
        "page_content": "AD11(\"Hive: radius.radacct_stg\") -->AD4(\"Join on username\")\n        AD4 -->|Refresh/Impala Insert|AD5(\"Impala: radius.radacct\")\n      end\n      AD10 -.->|On successful SFTP PUT<br>Populate the table| AD3\n    end\n    subgraph AE[Finish]\n      AE1(\"SFTP Server: Add suffix .LOADED to file\")\n    end\n    AA4 --> AB1\n    AB3 -->|Filename: radarchive_yearMonthDay_startHour-endHour.csv|AC1\n    AB3 -->|Filename: radacct_yearMonthDay_startHour-endHour.csv|AD1\n    AC4 -->|On Success|AE1",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 6"
        }
    },
    "797": {
        "page_content": "AB3 -->|Filename: radacct_yearMonthDay_startHour-endHour.csv|AD1\n    AC4 -->|On Success|AE1\n    AD5 -->|On Success|AE1\n```\n- **User**: `radius`  \n- **Coordinator**: `Radius_Load_Coordinator`  \n- **Workflow**: `Radius_Load_Workflow`\n- **HDFS path**: `/user/radius`\n- **Runs**: `every 1h and 30mins`\n- **Config file**: `HDFS: /user/radius/config/settings_prod.ini`\n- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n- **Input SFTP Server**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 7"
        }
    },
    "798": {
        "page_content": "- **Input SFTP Server**:\n  - **Host**: `999.999.999.999`\n  - **Port**: `22`\n  - **User**: `prdts`\n  - **Remdef Files Folder**: `/home/prdts/transfer`\n  - **Port Forward**:\n    - **Host**: `un-vip.bigdata.abc.gr`\n    - **Port**: `2222`\n- **Trustcenter SFTP Server**:\n  - **Host**: `unc2.bigdata.abc.gr`\n  - **Port**: `22`\n  - **User**: `trustcenterftp`\n  - **Remdef Files Folder**: `/rd`\n**Alerts**:\n- Mail\n  - Subject: Radius Flow failed\n  - Alerts that indicate problem with Input SFTP server:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 8"
        }
    },
    "799": {
        "page_content": "- Mail\n  - Subject: Radius Flow failed\n  - Alerts that indicate problem with Input SFTP server:\n    - Body starts with: `No upcoming files for more than 3h`\n    - Body starts with: `Files found with a late timestamp.`\n    - Body starts with: `Could not rename file`\n  - Alerts that indicate problem with Trustcenter SFTP Server:\n    - Body starts with: `Could not perform radacct_enriched housekeeping on sftp server`\n  - Alerts that indicate general failures without specific cause:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 9"
        }
    },
    "800": {
        "page_content": "- Alerts that indicate general failures without specific cause:\n    - Body starts with: `Insert data failed` and then lists the status for each file\n**Troubleshooting Steps**:\n- If an alert has been received, use the message from the e-mail to determine the root cause for the failure.\n  - For failures with Input SFTP server:\n    - Inform abc in order to check the Input SFTP Server\n    - If abc does not detect any problems, check connectivity:\n      From `un2.bigdata.abc.gr` with personal user:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 10"
        }
    },
    "801": {
        "page_content": "From `un2.bigdata.abc.gr` with personal user:\n  \n      ``` bash\n      su - radius\n      sftp prdts@999.999.999.999\n      # Check for files\n      sftp> ls -l\n      ```\n  - For failures with Trustcenter SFTP Server:\n    - Check `unc2.bigdata.abc.gr` for any errors\n- If no alerts have been received or the problem is not specified determine the root cause for the failure with the following steps:\n  - Check for failed executions\n    From `un2.bigdata.abc.gr` with personal user:\n    ```bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 11"
        }
    },
    "802": {
        "page_content": "- Check for failed executions\n    From `un2.bigdata.abc.gr` with personal user:\n    ```bash\n    curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=RADIUS&status=FAILED&operativePartition=<date in YYYYMMDD e.g.:20220518>'\n    ```\n    For additional query parameters check [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 12"
        }
    },
    "803": {
        "page_content": "or check via the corresponding [Grafana dashboard](https://unc1.bigdata.abc.gr:3000/d/J4KPyBoVk/radius-dashboard?orgId=1&from=now-2d&to=now)\n  - Check the logs for the failed execution.\n- After the underlying problem has been corrected the next execution of the flow will load any files that have been delayed or failed to be loaded in a previous execution.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 13"
        }
    },
    "804": {
        "page_content": "- If the customer requests to load specific files after they have been processed by the flow. **In order to avoid duplicate data we have to reload the whole partition.**\n  - Ensure the files are still available on the SFTP server\n    From `un2.bigdata.abc.gr` with personal user:\n  \n    ``` bash\n    su - radius\n    sftp prdts@999.999.999.999\n    # Check for files covering the whole date affected\n    sftp> ls -l *.LOADED\n    ```\n  \n  - Suspend coordinator `Radius_Load_Coordinator`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 14"
        }
    },
    "805": {
        "page_content": "sftp> ls -l *.LOADED\n    ```\n  \n  - Suspend coordinator `Radius_Load_Coordinator`\n  - Drop partitions that were not properly loaded from following tables\n    From `un2.bigdata.abc.gr` with personal user:\n    ``` bash\n    su - radius\n    kinit -kt /home/users/radius/radius.keytab radius\n    impala-shell -i un-vip.bigdata.abc.gr -k --ssl\n    ```\n    - In case of radarchive category of file:\n      ```sql",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 15"
        }
    },
    "806": {
        "page_content": "```\n    - In case of radarchive category of file:\n      ```sql\n      alter table radius.radarchive drop partitions (par_dt=\"<date in YYYYMMDD e.g.: 20220915>\")\n      ```\n    - In case of radacct category of file:\n      ```sql\n      alter table radius.radacct drop partitions (par_dt=\"<date in YYYYMMDD e.g.: 20220915>\")\n      ```\n  - Connect to SFTP server and rename files with filename timestamp equal to above-mentioned partition(s) and the same category:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 16"
        }
    },
    "807": {
        "page_content": "From `un2.bigdata.abc.gr` with personal user:\n    ``` bash\n    su - radius\n    sftp prdts@999.999.999.999\n    # For every file you need to reload\n    sftp> rename <filename>.LOADED <filename>\n    ```\n  - Resume coordinator `Radius_Load_Coordinator`\n## Kudu Housekeeping and Compute Statistics Flow\n```mermaid\n  graph TD\n  A[Apply 5-day rention to<br>Kudu: radius.radreference]\n  B[Compute Impala Statistics for<br>radius.radarchive<br>radius.radacct]\n```\n- **User**: `radius`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 17"
        }
    },
    "808": {
        "page_content": "B[Compute Impala Statistics for<br>radius.radarchive<br>radius.radacct]\n```\n- **User**: `radius`  \n- **Coordinator**: `Radius_Kudu_Retention_Coordinator`  \n- **Workflow**: `Radius_Kudu_Retention_Workflow`\n- **HDFS path**: `/user/radius`\n- **Runs**: `once a day at 2:25 (UTC)`\n- **Config file**: `hdfs: /user/radius/Kudu_Retention_And_Compute_Stats/settings.ini`\n- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n**Alerts**:\n- Not monitored",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 18"
        }
    },
    "809": {
        "page_content": "**Alerts**:\n- Not monitored\n**Troubleshooting Steps**:\n- Check the logs for the failed execution.\n- After the underlying problem has been corrected the next execution of the flow will apply the correct retention to the Kudu table and compute the missing statistics.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientA",
            "name": "radius.md - Part 19"
        }
    },
    "810": {
        "page_content": "# Online\n## Stream\n### Wilfly\nUser generated events are sent to Wildfly instances installed at edge servers as HTTP POST requests. The requests originate from the backend servers of Internet Banking. Requests are loadbalanced by NetScaler (Loadbalancer) that is managed by mno's networking department. The events are then sent to a Kafka topic. Usually only one site is active (Primary Site).\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 1"
        }
    },
    "811": {
        "page_content": "```mermaid\n  graph TD\n  A[Online Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\n  B -->|Active| C[pr1edge01.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]\n  C -->|Kafka Producer| G[Primary Site's Kafka: prod-trlog-online-ingest-stream-mir]\n  D -->|Kafka Producer| G\n  E -.->|Stopped| H[Disaster Site's Kafka: prod-trlog-online-ingest-stream-mir]\n  F -.->|Stopped| H\n```\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 2"
        }
    },
    "812": {
        "page_content": "F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodreston`\n**Deployments Path**: `/opt/wildfly/default/prodreston/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodreston/server.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 3"
        }
    },
    "813": {
        "page_content": "**Application Logs**: `/var/log/wildfly/prodreston/server.log`\n**Access Log**: `/var/log/wildfly/prodreston/access.log`\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\n**Alerts**:\n- [PR][ONLINE] REST Endpoints Average Response Time\n- [DR][ONLINE] REST Endpoints Average Response Time\n- [PR][ONLINE] Ingestion Average Error rate\n- [DR][ONLINE] Ingestion Average Error rate",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 4"
        }
    },
    "814": {
        "page_content": "- [PR][ONLINE] Ingestion Average Error rate\n- [DR][ONLINE] Ingestion Average Error rate\n**Troubleshooting Steps**:\n- Check application logs for error messages.\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\n- If the logs do not indicate a specific problem, check for any problems regarding Kafka performance.\n### Kafka Mirrorring",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 5"
        }
    },
    "815": {
        "page_content": "### Kafka Mirrorring\nAfter being published to Kafka, the traffic is mirrored to the opposite site via Kafka MirrorMaker. In order to be able to have both Sites active at any given time and simplify the failover procedure the each Site produce messages to a topic with the suffix `-mir`. The topics with the `-mir` suffix contain events published to the Site. The `-mir` topics are then copied via Kafka MirrorMakers to the final topics (no `-mir` suffix), that has all the messages from both Sites.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 6"
        }
    },
    "816": {
        "page_content": "#### PR replication\n```mermaid\n  graph TD\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-online-ingest-stream-mir]\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-online-ingest-stream-mir]\n  E[dr1edge02.mno.gr] -.->|Stopped| F\n  C -->|Kafka MirrorMakers <br> pr1node01.mno.gr/pr1node04.mno.gr| G[PR Site Kafka: prod-trlog-online-ingest-stream]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 7"
        }
    },
    "817": {
        "page_content": "F -->|Kafka MirrorMakers <br> pr1node05.mno.gr/pr1node06.mno.gr| G[PR Site Kafka: prod-trlog-online-ingest-stream]\n```\n#### DR replication\n``` mermaid\n  graph TD\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-online-ingest-stream-mir]\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-online-ingest-stream-mir]\n  E[dr1edge02.mno.gr] -.->|Stopped| F",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 8"
        }
    },
    "818": {
        "page_content": "E[dr1edge02.mno.gr] -.->|Stopped| F\n  F -->|Kafka MirrorMakers <br> dr1node01.mno.gr/dr1node04.mno.gr| H[DR Site Kafka: prod-trlog-online-ingest-stream]\n  C -->|Kafka MirrorMakers <br> dr1node05.mno.gr/dr1node06.mno.gr| H[DR Site Kafka: prod-trlog-online-ingest-stream]\n```\n**MirrorMaker User**: `kafka`\n**Configuration**: Cloudera Manager\n**Logs**: Cloudera Manager\n**Instructions for start/stop**: [Here](../procedures/manage_mirrormaker.md)\n**Alerts**:\n- Cloudera Manager alerts regarding Kafka",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 9"
        }
    },
    "819": {
        "page_content": "**Alerts**:\n- Cloudera Manager alerts regarding Kafka\n### Spark Streaming\nAfter being sent to the final Kafka topic, the records are consumed by two separate Spark Streaming topologies\n#### Prod_Online_IngestStream\nThis topology takes all records from the final Kafka topics, extracts some values and writes them to a Kudu table. It also inserts some columns that are too big for Kudu to an HBase table. This topology runs independently **on both clusters**.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 10"
        }
    },
    "820": {
        "page_content": "``` mermaid\n  graph TD\n  A[Kafka: prod-trlog-online-ingest-stream] --> B[Spark: Prod_Online_IngestStream]\n  B --> C[Kudu: prod_trlog_online.service_audit_stream]\n  B --> D[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM]\n```\n**User**: `PRODREST`\n**Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 11"
        }
    },
    "821": {
        "page_content": "**Submit Script**: `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- [PR][ONLINE] Spark Waiting Batches\n- [DR][ONLINE] Spark Waiting Batches\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 12"
        }
    },
    "822": {
        "page_content": "- [PR][ONLINE] Spark Waiting Batches\n- [DR][ONLINE] Spark Waiting Batches\n**Troubleshooting Steps**:\n- If the alert chart has **no line**, ensure that the application is not running (Cloudera Manager > YARN > Applications) and then submit the topology using the **Submit Script**. After resubmitting the application check the old application logs for the root cause that it failed",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 13"
        }
    },
    "823": {
        "page_content": "- If the alert has exceeded the threshold, investigate HBase/Kudu for delays and check the application logs for any errors. You can also try to kill (use Cloudera Manager or yarn command) and then re-submit the topology.\n## Batch\n### Main script",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 14"
        }
    },
    "824": {
        "page_content": "As mentioned before, the information processed by the [Prod_Online_IngestStream](#prod_online_ingeststream) topology is stored in Kudu/HBase tables. At the end of each day (at **4:15 am in PR & DR site** by **Cron**) the records from HBase/Kudu is enriched with additional information and the transfered to a parquet table. After the data are inserted to the parquet table, sequent jobs are triggered to produce aggregations and data for external systems. This procedure runs **independantly on both sites**.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 15"
        }
    },
    "825": {
        "page_content": "**User**: `PRODREST`\n**Logs**: `/var/log/ingestion/PRODREST/online/log/online_daily_batch_jobs.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/online_daily_batch_jobs_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- _See below_\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 16"
        }
    },
    "826": {
        "page_content": "**Alerts**:\n- _See below_\n**Troubleshooting Steps**:\nThis procedure runs multiple steps which we will describe below with seperate troubleshooting steps for each step. Use the following steps for all alarms as a general guide:\n- Identify the failed step using the alarm name\n- Identify the root cause for the failed job based on the logs of the submit script/cluster application/query\n- If the problem is with an external system, ask the customer to inform the owners of the external system",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 17"
        }
    },
    "827": {
        "page_content": "- if the problem is temporary or if you cannot determine the root cause of the problem, try to re-run the failed application\n### Sub-steps\nThe following steps run **on both clusters independently**, unless specified otherwise.\n#### Merge Batch\nThis step transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and deletes the data from the original tables.\n``` mermaid\n  graph TD\n  A[Kudu: prod_trlog_online.service_audit_stream] --> B[Spark: PROD_Online_MergeBatch]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 18"
        }
    },
    "828": {
        "page_content": "graph TD\n  A[Kudu: prod_trlog_online.service_audit_stream] --> B[Spark: PROD_Online_MergeBatch]\n  C[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM] --> B\n  B --> D[Impala: prod_trlog_online.service_audit]\n  ```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/cronExecutor_OnlineBatch_full.log`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 19"
        }
    },
    "829": {
        "page_content": "**Script**: `/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- Online_Ingestion MergeBatch JOB\n**Troubleshooting Steps**:\n- Use the script/spark logs to identify the cause of the failure\n- Ensure that no records are present in prod_trlog_online.service_audit\n  ``` bash\n  # eg. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 20"
        }
    },
    "830": {
        "page_content": "``` bash\n  # eg. 09-11-2019\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_online.service_audit where par_dt='20191109';\"\n  ```\n- If no records exist and no other process is up, you can ran the script again.\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 21"
        }
    },
    "831": {
        "page_content": "/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_Online_MergeBatch LOCK_ONLINE_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_Online_MergeBatch  /opt/ingestion/PRODREST/online/lock/ FULL 1800 \"`date --date='-1 day' '+%Y-%m-%d 00:00:00'`\" \"`date '+%Y-%m-%d 00:00:00'`\"  >> /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log 2>&1 &",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 22"
        }
    },
    "832": {
        "page_content": "```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 23"
        }
    },
    "833": {
        "page_content": "``` bash\n    # eg. 09-11-2019\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_Online_MergeBatch LOCK_ONLINE_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_Online_MergeBatch  /opt/ingestion/PRODREST/online/lock/ FULL 1800 \"2019-11-09 00:00:00\" \"2019-11-10 00:00:00\"   >> /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log 2>&1 &",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 24"
        }
    },
    "834": {
        "page_content": "```\n- The process runs for well 30 minutes under normal circumstances or even longer for heavy load. Use of `screen` command advised\n- If the problem is with resources (out-of-memory errors):\n  - You can adjust the values at `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh`. At the moment these values are as high as they can go.\n  - You can run the MergeBatch for parts of the day\n    ``` bash\n    # eg. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 25"
        }
    },
    "835": {
        "page_content": "- You can run the MergeBatch for parts of the day\n    ``` bash\n    # eg. 09-11-2019\n    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 00:00:00\" \"2019-11-09 12:00:00\"\n    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 12:00:00\" \"2019-11-09 18:00:00\"",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 26"
        }
    },
    "836": {
        "page_content": "/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 18:00:00\" \"2019-11-10 00:00:00\"\n    ```\n- Run the rest of the steps\n#### Report stats to Graphite\nReports statistics about the ingestion process.\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/cron_report_stats.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 27"
        }
    },
    "837": {
        "page_content": "**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/cron_report_stats.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- This process is not monitored\n- You can safely skip this step\n- Sample execution:\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 28"
        }
    },
    "838": {
        "page_content": "- This process is not monitored\n- You can safely skip this step\n- Sample execution:\n  ``` bash\n  nohup  /opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh prod_trlog_online.service_audit online >> /var/log/ingestion/PRODREST/online/log/cron_report_stats.log\n  ```\n- Run the rest of the steps\n#### Drop hourly partitions\nNdef: **ONLY DR SITE**\nDrop partitions from the [Hourly Merge Batch](#hourly-merge-batch) tables\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 29"
        }
    },
    "839": {
        "page_content": "Drop partitions from the [Hourly Merge Batch](#hourly-merge-batch) tables\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/drop_hourly_partitions.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh`\n**Alerts**:\n- Online_Migration Drop hourly partitions JOB\n**Troubleshooting Steps**:\n- Ensure that you are running steps for the DR site\n- If you are running the steps for the Primary skip this step",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 30"
        }
    },
    "840": {
        "page_content": "- If you are running the steps for the Primary skip this step\n- Use the script logs to identify the cause of the failure\n- For the previous day:\n  ``` bash\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \"prod_trlog_online.service_audit_hourly\" \"`date -d '-1 day' '+%Y%m%d'`\" >> /var/log/ingestion/PRODREST/online/log/drop_hourly_partitions.log 2>&1 &\n  ```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 31"
        }
    },
    "841": {
        "page_content": "```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \"prod_trlog_online.service_audit_hourly\" \"20191109\" >> /var/log/ingestion/PRODREST/online/log/drop_hourly_partitions.log 2>&1 &\n  ```\n- Run the rest of the steps\n#### Execute aggregations\nThis flow computes aggregations for use with the [Queries](#queries).\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 32"
        }
    },
    "842": {
        "page_content": "This flow computes aggregations for use with the [Queries](#queries).\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh`\n**Alerts**:\n- Online_Migration Aggregations JOB\n- Online_Migration Aggregation_SA Impala_Insert\n- Online_Migration Aggregation_SA_Index Kudu_Insert\n**Troubleshooting Steps**:\n- For the previous day:\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 33"
        }
    },
    "843": {
        "page_content": "**Troubleshooting Steps**:\n- For the previous day:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh prod_trlog_online.service_audit prod_trlog_online.aggr_service_audit prod_trlog_online.aggr_service_audit_idx  >> /var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log 2>&1 &\n  ```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 34"
        }
    },
    "844": {
        "page_content": "```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019\n  /opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh prod_trlog_online.service_audit prod_trlog_online.aggr_service_audit prod_trlog_online.aggr_service_audit_idx 20191109 >> /var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log 2>&1\n  ```\n- Run the rest of the steps\n#### Send reports to bussiness users",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 35"
        }
    },
    "845": {
        "page_content": "```\n- Run the rest of the steps\n#### Send reports to bussiness users\nSends email reports to users for duplicate transaction IDS and leftover transactions in `prod_trlog_online.service_audit_stream`.\n**User**: `PRODREST`\n**Script Logs**: `-`\n**Script**: `-`\n**Alerts**:\n- Online_Ingestion GUID_Report Impala\n- Online_Ingestion GUID_Report JOB\n**Troubleshooting Steps**:\n- Check `/var/log/ingestion/PRODREST/online/log/online_report_duplicate_identical.log` for errors",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 36"
        }
    },
    "846": {
        "page_content": "- Check `/var/log/ingestion/PRODREST/online/log/online_report_duplicate_identical.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/online_report_duplicate_identical_STABLE.sh  `date -d '-1 day' '+%Y%m%d'`  prod_trlog_online service_audit service_audit_duplicates >> /var/log/ingestion/PRODREST/online/log/online_report_duplicate_identical.log 2>&1 &",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 37"
        }
    },
    "847": {
        "page_content": "/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_online.service_audit_stream prod_trlog_online.service_audit_old online >> /var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log 2>&1 &\n  ```\n#### Duplicates between Impala and Kudu/HBase",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 38"
        }
    },
    "848": {
        "page_content": "```\n#### Duplicates between Impala and Kudu/HBase\nDelete data which exist both Impala and Kudu. Sometimes MergeBatch fails because YARN kills executors for exceeding memory limits. AS a result, transactions are inserted to impala and job fails before drop them from kudu and HBase\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 39"
        }
    },
    "849": {
        "page_content": "**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Check `/var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 40"
        }
    },
    "850": {
        "page_content": "- You can safely skip this step if not running for the previous day\n- Sample execution:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_online.service_audit_stream prod_trlog_online.service_audit_old online >> /var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log 2>&1\n  ```\n### Hourly Merge Batch",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 41"
        }
    },
    "851": {
        "page_content": "```\n### Hourly Merge Batch\nEvery hour a  transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and without deleting the data from the original tables. This table is used by mno to query the latest transactions.\n``` mermaid\n  graph TD\n  A[Kudu: prod_trlog_online.service_audit_stream] --> B[Spark: PROD_Online_MergeBatch_Hourly]\n  C[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM] --> B\n  B --> D[Impala: prod_trlog_online.service_audit_hourly]\n  ```\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 42"
        }
    },
    "852": {
        "page_content": "B --> D[Impala: prod_trlog_online.service_audit_hourly]\n  ```\n**User**: `PRODREST`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Use the spark logs to identify the cause of the failure\n- The data in the Hourly tables is only for the current day. No need to fill missing hours, as the next MergeBatch will insert the data to the final table.\n## Queries",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 43"
        }
    },
    "853": {
        "page_content": "## Queries\nThe ingested data are queried in order to be displayed by the Online application (used by branches). The queries are submitted to the cluster as HTTP requests using the same Wildfly instances as the [Stream](#stream) flow.\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\n  B -->|Active| C[pr1edge01.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 44"
        }
    },
    "854": {
        "page_content": "B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]\n  C -->|Impala JDBC | G[Primary Site]\n  D -->|Impala JDBC | G\n  E -.->|Stopped| H[Disaster Site]\n  F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodreston`\n**Deployments Path**: `/opt/wildfly/default/prodreston/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/standalone.xml`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 45"
        }
    },
    "855": {
        "page_content": "**Application Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodreston/server.log`\n**Access Log**: `/var/log/wildfly/prodreston/access.log`\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\n**Alerts**:\n- [PR][ONLINE] Query Average Response Time",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 46"
        }
    },
    "856": {
        "page_content": "**Alerts**:\n- [PR][ONLINE] Query Average Response Time\n- [DR][ONLINE] Query Average Response Time\n- [PR][ONLINE] Query Average Error rate\n- [DR][ONLINE] Query Average Error rate\n- [PR][ONLINE] REST Endpoints Average Response Time\n- [DR][ONLINE] REST Endpoints Average Response Time\n**Troubleshooting Steps**:\n- Check in Grafana dashboard `MONITORING/Monitoring Alerts` the charts that are in alerting state",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 47"
        }
    },
    "857": {
        "page_content": "- Check in Grafana dashboard `MONITORING/Monitoring Alerts` the charts that are in alerting state\n- Check if the response time is constant, a single query can increase the mean value of the response time. Check the access log for entries with large response time for the specific endpoint. If it is an isolated query the customer is not impacted.\n- Check application logs for error messages.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 48"
        }
    },
    "858": {
        "page_content": "- Check application logs for error messages.\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\n- If the logs do not indicate a specific problem, check for any problems regarding Impala/Kudu\n### Implementation\nQueries regarding Online query Impala tables stored in both HDFS and Kudu.\n**Endpoints**:\n- dynamic search\n- by-id\n- by-core-fields\n- by-application\n- top-by-branchcode-clientusername",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 49"
        }
    },
    "859": {
        "page_content": "- dynamic search\n- by-id\n- by-core-fields\n- by-application\n- top-by-branchcode-clientusername\n- first-by-branchcode-computername\n- aggr-computernum-usernum-transnum-groupby-branchcode\n- aggr-transnum-groupby-branchcode-clientusername-by-branchcode\n- aggr-opcodenum-transnum-groupby-opclass\n- aggr-transnum-groupby-opclass-opcode-by-opclass\n## Retention Mechanism\n### Impala Retention\n#### DEV",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 50"
        }
    },
    "860": {
        "page_content": "## Retention Mechanism\n### Impala Retention\n#### DEV\nEvery day (at **16:05 on DR site only** by **Cron**). This script drops partitions from impala table `dev_trlog_online.service_audit` older than 60 days.\n**User**: `DEVREST`\n**Script Logs**: `/var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log`\n**Script**: `/opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh`\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 51"
        }
    },
    "861": {
        "page_content": "**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, run manually the following command\n  ``` bash  \n  /opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh >> /var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log 2>&1\n  ```\n### Additional Tables",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 52"
        }
    },
    "862": {
        "page_content": "```\n### Additional Tables\nKudu table's `prod_trlog_online.service_audit_stream` partitions are maintained by an Oozie jobs every day. See section [Create next week kudu partitions](#create-next-week-kudu-partitions).\nParquet table's `prod_trlog_online.service_audit_hourly` partitions are dropped after each successful Merge run. See section [Drop hourly partitions](#drop-hourly-partitions).\n### HBase retention",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 53"
        }
    },
    "863": {
        "page_content": "### HBase retention\nEvery day (at **16:15 in both sites** by **Cron**) This script deletes rows from hbase `PROD_ONLINE:SERVICE_AUDIT_STREAM` which refere to transactions inserted 7 days ago. Those rows may have been not deleted properly after merge batch job. To do so the topology queries rows from impala table `prod_trlog_online.service_audit` inside partition with par_dt refering 7 days ago.\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 54"
        }
    },
    "864": {
        "page_content": "**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/daily_tools_cleanupHBaseSAS.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/daily_tools_cleanupHBaseSAS.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr`\n**Alerts**:\n- Retention OnlineCleanupHbaseSAS JOB\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 55"
        }
    },
    "865": {
        "page_content": "**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, check on `/opt/ingestion/PRODREST/no_bkp/duplicate_cid_hbase` if a manual re-run must be done for a specific date\n  - For a specified date eg 2022-06-30:\n    ``` bash\n    /opt/ingestion/PRODREST/common/spark/submit/submitmnoSparkTopology_tools_cleanupHbaseSAS prod_trlog_online.service_audit PROD_ONLINE:SERVICE_AUDIT_STREAM LIST 20220630\n    ```\n##  Oozie Jobs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 56"
        }
    },
    "866": {
        "page_content": "```\n##  Oozie Jobs\n###  Lookup tables\nEvery day (at 07:00 by Oozie on DR & PR site ), we transfers 3 tables with reference data from the legacy MSSQL server, which is managed by mno, to the cluster. We keep only latest version to BigData (no partition).\n**User**: `PRODREST`\n**Coordinator**: `Coord_OnlineLookupTables_PROD`\n**Workflow**: `ImportLookupTables`\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/lookupTables/import_lookup_tables.sh`\n**Logs**: from HUE\n**Alerts**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 57"
        }
    },
    "867": {
        "page_content": "**Logs**: from HUE\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs\n###  Create next week kudu partitions\nEvery day (at 10:10 by Oozie on DR site and 10:30 by Oozie on PR site ), we delete old Kudu partitions and add new Kudu partitions on `prod_trlog_online.service_audit_stream`.\n**User**: `PRODREST`\n**Coordinator**: `Coord_OnlineCreateKuduPartitionsPROD`\n**Workflow**: `CreateKuduPartitionsPROD`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 58"
        }
    },
    "868": {
        "page_content": "**Coordinator**: `Coord_OnlineCreateKuduPartitionsPROD`\n**Workflow**: `CreateKuduPartitionsPROD`\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\n**Logs**: from HUE\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 59"
        }
    },
    "869": {
        "page_content": "- **Not Monitored**\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs\nThere is a case that merge batch has not properly run for a specific date, **even though** the transactions for that specific date are present on impala `service_audit`. For example by moving a service_audit partiton from DR to PR or vice versa, or when merge batch failed **after** deleting transactions from kudu.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 60"
        }
    },
    "870": {
        "page_content": "In that case we can manually mark merge batch as complete for that date, and therefore next oozie job created by `Coord_OnlineCreateKuduPartitionsPROD` will delete that partition from kudu, by manipulating HBase table `PROD_ONLINE:MERGE_BATCH_STATE_INFO`.\nExample for a specific date (10/10/2022):\n - Run HBase shell\n ```\n   hbase shell\n   ```\n - And then inside HBase shell:\n ```\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase', 'true'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 61"
        }
    },
    "871": {
        "page_content": "put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase_time','0'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu', 'true'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu_time', '0'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running', 'false'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 62"
        }
    },
    "872": {
        "page_content": "put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running+time', '0'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala', 'true'\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala_time', '0'\n   ```\n#### DEV",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 63"
        }
    },
    "873": {
        "page_content": "```\n#### DEV\nEvery day (at **11:00 by Oozie** on **DR site only** ), we delete old Kudu partitions and add new Kudu partitions on `dev_trlog_online.service_audit_stream`.\n**User**: `DEVREST`\n**Coordinator**: `Coord_OnlineCreateKuduPartitionsDEV`\n**Workflow**: `CreateKuduPartitionsDEV`\n**Local path**: `/opt/ingestion/DEVREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\n**Logs**: from HUE\n**Alerts**:\n- **Not Monitored**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "online.md - Part 64"
        }
    },
    "874": {
        "page_content": "# Datawarehouse ibank\n## Extract\n**Extraction of detail tables**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 1"
        }
    },
    "875": {
        "page_content": "Our spark application will extract information from prod_trlog_ibank.service_audit table to different detail tables based on different service names with the help of lookup table service_name where needed. The columns of the detail tables are produced either from a non json column of the input table (eg. client_username) or from parsing the json fields request_text_data and response_text_data and extracting specific fields based on their name. When two fields have the same name then we will keep the greater value either for a number field or for a string based on lexicographic order.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 2"
        }
    },
    "876": {
        "page_content": "These jobs are executed by the bank's scheduler. There is an agent installed on the BDA edge node which is called UC4. The UC4 agent executes the below script for every job:\n**UC4 Agent Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh` on `dr1edge01.mno.gr` **OR** `pr1edge01.mno.gr` (according to which site is the active for the UC4 agent)\n**User**: `PRODUSER`\n**Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 3"
        }
    },
    "877": {
        "page_content": "**User**: `PRODUSER`\n**Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\nThe above script executes the below scripts for every job and the below scripts and each of these scripts executes this generic spark-submit script:\n**Generic Spark Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submitter.sh`\nThe jobs which perform the extraction of the details from service_audit are:\n### Transfer Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 4"
        }
    },
    "878": {
        "page_content": "### Transfer Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_transfer_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_transfer]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_transfer.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 5"
        }
    },
    "879": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT TRANSFER\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 6"
        }
    },
    "880": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t transfer\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t transfer -p 20191109\n    ```\n### Payment Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 7"
        }
    },
    "881": {
        "page_content": "```\n### Payment Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_payment_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_payment]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_payment.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 8"
        }
    },
    "882": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 9"
        }
    },
    "883": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t payment\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t payment -p 20191109\n    ```\n### Loan Payment Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 10"
        }
    },
    "884": {
        "page_content": "```\n### Loan Payment Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_loanPayment_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_loan_payment]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_loan_payment.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 11"
        }
    },
    "885": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT LOAN_PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  \n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 12"
        }
    },
    "886": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t loanPayment\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t loanPayment -p 20191109\n    ```\n### Cancel Payment Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 13"
        }
    },
    "887": {
        "page_content": "```\n### Cancel Payment Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_cancelPayment_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_cancel_payment]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_cancel_payment.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 14"
        }
    },
    "888": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT CANCEL_PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 15"
        }
    },
    "889": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t cancelPayment\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t cancelPayment -p 20191109\n    ```\n### Card Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 16"
        }
    },
    "890": {
        "page_content": "```\n### Card Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_card_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_card]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_card.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 17"
        }
    },
    "891": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT CARD\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 18"
        }
    },
    "892": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t card\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t card -p 20191109\n    ```\n### Stock Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 19"
        }
    },
    "893": {
        "page_content": "```\n### Stock Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_stock_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_stock]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_stock.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 20"
        }
    },
    "894": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT STOCK\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 21"
        }
    },
    "895": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t stock\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t stock -p 20191109\n    ```\n### Time Deposit Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 22"
        }
    },
    "896": {
        "page_content": "```\n### Time Deposit Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_timeDeposit_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_time_deposit]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_time_deposit.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 23"
        }
    },
    "897": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT TIME_DEPOSIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 24"
        }
    },
    "898": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t timeDeposit\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t timeDeposit -p 20191109\n    ```\n### Mass Debit Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 25"
        }
    },
    "899": {
        "page_content": "```\n### Mass Debit Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_massDebit_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_mass_debit]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_mass_debit.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 26"
        }
    },
    "900": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT MASS_DEBIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 27"
        }
    },
    "901": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t massDebit\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t massDebit -p 20191109\n    ```\n### Man Date Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 28"
        }
    },
    "902": {
        "page_content": "```\n### Man Date Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_manDate_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_man_date]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_man_date.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 29"
        }
    },
    "903": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT MAN_DATE\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 30"
        }
    },
    "904": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -p 20191109\n    ```\n### My Bank Extract\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 31"
        }
    },
    "905": {
        "page_content": "```\n### My Bank Extract\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_myBank_YYYYMMDD-YYYYMMDD]\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_my_bank]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_my_bank.sh`\n**User**: `PRODUSER`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 32"
        }
    },
    "906": {
        "page_content": "**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\n**Alert**:\n- DWH_IBank EXTRACT MY_BANK\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 33"
        }
    },
    "907": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t myBank\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t myBank -p 20191109\n    ```\n## Export\n**Export of details tables and part of service_audit columns to mno datawarehouse**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 34"
        }
    },
    "908": {
        "page_content": "## Export\n**Export of details tables and part of service_audit columns to mno datawarehouse**\nThe data are copied temporary to an internal staging table with an impala query and then we use sqoop-export to export the data to Datawarehouse.\nThese jobs are executed by the bank's scheduler. There is an agent installed on the BDA edge node which is called UC4. The UC4 agent executes the below script for every job:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 35"
        }
    },
    "909": {
        "page_content": "**UC4 Agent Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh` on `dr1edge01.mno.gr` **OR** `pr1edge01.mno.gr` (according to which site is the active for the UC4 agent)\n**User**: `PRODUSER`\n**Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\nThe above script executes the below scripts for every job and the below scripts and each of these scripts executes this generic sqoop script:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 36"
        }
    },
    "910": {
        "page_content": "**Generic Sqoop Job Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_to_dwh.sh`\nThe jobs which perform the export of the details to the MSSQL Server are:\n### Transfer Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_transfer] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_transfer_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_TransferDetails_YYYYMMDD-YYYYMMDD]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 37"
        }
    },
    "911": {
        "page_content": "B --> C[Sqoop: PROD_IBank_DWH_EXPORT_TransferDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.TransferDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_transfer.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 38"
        }
    },
    "912": {
        "page_content": "**Alert**:\n- DWH_IBank EXPORT TRANSFER\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 39"
        }
    },
    "913": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t transfer\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t transfer -p 20191109\n    ```\n### Payment Export\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 40"
        }
    },
    "914": {
        "page_content": "```\n### Payment Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_payment] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_payment_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_PaymentDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.PaymentDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_payment.sh`\n**User**: `PRODUSER`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 41"
        }
    },
    "915": {
        "page_content": "**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT TRANSFER\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 42"
        }
    },
    "916": {
        "page_content": "- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t payment\n    ```\n  \n  - For a specified date:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 43"
        }
    },
    "917": {
        "page_content": "```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t payment -p 20191109\n    ```\n### Loan Payment Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_loan_payment] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_loan_payment_stg]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 44"
        }
    },
    "918": {
        "page_content": "B --> C[Sqoop: PROD_IBank_DWH_EXPORT_LoanPaymentDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.LoanPaymentDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_loan_payment.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 45"
        }
    },
    "919": {
        "page_content": "**Alert**:\n- DWH_IBank EXPORT LOAN_PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 46"
        }
    },
    "920": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment -p 20191109\n    ```\n### Cancel Payment Export\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 47"
        }
    },
    "921": {
        "page_content": "```\n### Cancel Payment Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_cancel_payment] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_cancel_payment_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_CancelPaymentDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.CancelPaymentDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_cancel_payment.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 48"
        }
    },
    "922": {
        "page_content": "**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT CANCEL_PAYMENT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 49"
        }
    },
    "923": {
        "page_content": "- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t cancelPayment\n    ```\n  \n  - For a specified date:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 50"
        }
    },
    "924": {
        "page_content": "```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t cancelPayment -p 20191109\n    ```\n### Card Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_card] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_card_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_CardDetails_YYYYMMDD-YYYYMMDD]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 51"
        }
    },
    "925": {
        "page_content": "B --> C[Sqoop: PROD_IBank_DWH_EXPORT_CardDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.CardDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_card.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 52"
        }
    },
    "926": {
        "page_content": "**Alert**:\n- DWH_IBank EXPORT CARD\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 53"
        }
    },
    "927": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t card\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t card -p 20191109\n    ```\n### Stock Export\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 54"
        }
    },
    "928": {
        "page_content": "```\n### Stock Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_stock] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_stock_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_StockDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.StockDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_stock.sh`\n**User**: `PRODUSER`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 55"
        }
    },
    "929": {
        "page_content": "**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT STOCK\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 56"
        }
    },
    "930": {
        "page_content": "- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t stock\n    ```\n  \n  - For a specified date:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 57"
        }
    },
    "931": {
        "page_content": "```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t stock -p 20191109\n    ```\n### Time Deposit Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_time_deposit] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_time_deposit_stg]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 58"
        }
    },
    "932": {
        "page_content": "B --> C[Sqoop: PROD_IBank_DWH_EXPORT_TimeDepositDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.TimeDepositDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_time_deposit.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 59"
        }
    },
    "933": {
        "page_content": "**Alert**:\n- DWH_IBank EXPORT TIME_DEPOSIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 60"
        }
    },
    "934": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t timeDeposit\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t timeDeposit -p 20191109\n    ```\n### Mass Debit Export\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 61"
        }
    },
    "935": {
        "page_content": "```\n### Mass Debit Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_mass_debit] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_mass_debit_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_MassDebitDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.MassDebitDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_mass_debit.sh`\n**User**: `PRODUSER`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 62"
        }
    },
    "936": {
        "page_content": "**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT MASS_DEBIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 63"
        }
    },
    "937": {
        "page_content": "- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t massDebit\n    ```\n  \n  - For a specified date:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 64"
        }
    },
    "938": {
        "page_content": "```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t massDebit -p 20191109\n    ```\n### Man Date Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_man_date] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_man_date_stg]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 65"
        }
    },
    "939": {
        "page_content": "B --> C[Sqoop: PROD_IBank_DWH_EXPORT_ManDateDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.TransferDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_man_date.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 66"
        }
    },
    "940": {
        "page_content": "**Alert**:\n- DWH_IBank EXPORT MAN_DATE\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 67"
        }
    },
    "941": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t manDate\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t manDate -p 20191109\n    ```\n### My Bank Export\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 68"
        }
    },
    "942": {
        "page_content": "```\n### My Bank Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_my_bank] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_my_bank_stg]\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_MyBankDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.MyBankDetails]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_my_bank.sh`\n**User**: `PRODUSER`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 69"
        }
    },
    "943": {
        "page_content": "**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:\n- DWH_IBank EXPORT MY_BANK\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 70"
        }
    },
    "944": {
        "page_content": "- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t myBank\n    ```\n  \n  - For a specified date:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 71"
        }
    },
    "945": {
        "page_content": "```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t myBank -p 20191109\n    ```\n### Service Audit Export\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_service_audit_stg]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 72"
        }
    },
    "946": {
        "page_content": "B --> C[Sqoop: PROD_IBank_DWH_EXPORT_ServiceAuditDetails_YYYYMMDD-YYYYMMDD]\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.ServiceAudit]\n  ```\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_service_audit.sh`\n**User**: `PRODUSER`\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\n**Alert**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 73"
        }
    },
    "947": {
        "page_content": "**Alert**:\n- DWH_IBank EXPORT SERVICE_AUDIT\n**Troubleshooting Steps**:\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \n- The export job will not be executed if the previous day is not a business day\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 74"
        }
    },
    "948": {
        "page_content": "- For the previous day:\n    ``` bash\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t serviceAudit\n    ```\n  \n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t serviceAudit -p 20191109\n    ```\n    \n## Retention Mechanism (Suspended)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 75"
        }
    },
    "949": {
        "page_content": "```\n    \n## Retention Mechanism (Suspended)\nNdef: **This mechanism is suspended. DO NOT Troubleshoot**. As of 22/09/2023 all tables listed bellow have been dropped by mno (**Mail Subject: Incident IM2220978 receipt confirmation. Related Interaction: SD2285018**).\nInformation shown here is for completeness.\n**Execution**: Every day (at **3:30 pm in DR site** by **Cron**)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 76"
        }
    },
    "950": {
        "page_content": "**Execution**: Every day (at **3:30 pm in DR site** by **Cron**) \n**Description**: This script drops partitions from impala tables `prod_trlog_ibank_analytical.dwh_details*` older than 10 days.\n**User**: `PRODUSER`\n**Script Logs**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily.log`\n**Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily_STABLE.sh` on `dr1edge01.mno.gr`\n**Alerts**:\n- Retention DWH_retention {$table}\nWhere $table can be",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 77"
        }
    },
    "951": {
        "page_content": "**Alerts**:\n- Retention DWH_retention {$table}\nWhere $table can be\n- prod_trlog_ibank_analytical.dwh_details_cancel_payment\n- prod_trlog_ibank_analytical.dwh_details_card\n- prod_trlog_ibank_analytical.dwh_details_loan_payment\n- prod_trlog_ibank_analytical.dwh_details_man_date\n- prod_trlog_ibank_analytical.dwh_details_mass_debit\n- prod_trlog_ibank_analytical.dwh_details_my_bank\n- prod_trlog_ibank_analytical.dwh_details_payment\n- prod_trlog_ibank_analytical.dwh_details_stock",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 78"
        }
    },
    "952": {
        "page_content": "- prod_trlog_ibank_analytical.dwh_details_payment\n- prod_trlog_ibank_analytical.dwh_details_stock\n- prod_trlog_ibank_analytical.dwh_details_time_deposit\n- prod_trlog_ibank_analytical.dwh_details_cancel_transfer\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, run manually the following command\n  - To keep the only last 10 days:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 79"
        }
    },
    "953": {
        "page_content": "- To keep the only last 10 days:\n    ``` bash\n        /opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily_STABLE.sh >> /opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily.log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "datawarehouse-ibank.md - Part 80"
        }
    },
    "954": {
        "page_content": "# Internet Banking\n## Stream\n### Wilfly\nUser generated events are sent to Wildfly instances installed at edge servers as HTTP POST requests. The requests originate from the backend servers of Internet Banking. Requests are loadbalanced by NetScaler (Loadbalancer) that is managed by mno's networking department. The events are then sent to a Kafka topic. Usually only one site is active (Primary Site).\n```mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 1"
        }
    },
    "955": {
        "page_content": "```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\n  B -->|Active| C[pr1edge01.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]\n  C -->|Kafka Producer| G[Primary Site's Kafka: prod-trlog-ibank-ingest-stream-mir]\n  D -->|Kafka Producer| G\n  E -.->|Stopped| H[Disaster Site's Kafka: prod-trlog-ibank-ingest-stream-mir]\n  F -.->|Stopped| H\n```\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 2"
        }
    },
    "956": {
        "page_content": "F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodrestib`\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodrestib/server.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 3"
        }
    },
    "957": {
        "page_content": "**Application Logs**: `/var/log/wildfly/prodrestib/server.log`\n**Access Log**: `/var/log/wildfly/prodrestib/access.log`\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\n**Alerts**:\n- [PR][IBANK] REST Endpoints Average Response Time\n- [DR][IBANK] REST Endpoints Average Response Time\n- [PR][IBANK] Ingestion Average Error rate\n- [DR][IBANK] Ingestion Average Error rate",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 4"
        }
    },
    "958": {
        "page_content": "- [PR][IBANK] Ingestion Average Error rate\n- [DR][IBANK] Ingestion Average Error rate\n**Troubleshooting Steps**:\n- Check application logs for error messages.\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\n- If the logs do not indicate a specific problem, check for any problems regarding Kafka performance.\n### Kafka Mirrorring",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 5"
        }
    },
    "959": {
        "page_content": "### Kafka Mirrorring\nAfter being published to Kafka, the traffic is mirrored to the opposite site via Kafka MirrorMaker. In order to be able to have both Sites active at any given time and simplify the failover procedure the each Site produce messages to a topic with the suffix `-mir`. The topics with the `-mir` suffix contain events published to the Site. The `-mir` topics are then copied via Kafka MirrorMakers to the final topics (no `-mir` suffix), that has all the messages from both Sites.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 6"
        }
    },
    "960": {
        "page_content": "#### PR replication\n```mermaid\n  graph TD\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\n  E[dr1edge02.mno.gr] -.->|Stopped| F\n  C -->|Kafka MirrorMakers <br> pr1node01.mno.gr/pr1node04.mno.gr| G[PR Site Kafka: prod-trlog-ibank-ingest-stream]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 7"
        }
    },
    "961": {
        "page_content": "F -->|Kafka MirrorMakers <br> pr1node05.mno.gr/pr1node06.mno.gr| G[PR Site Kafka: prod-trlog-ibank-ingest-stream]\n```\n#### DR replication\n``` mermaid\n  graph TD\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\n  E[dr1edge02.mno.gr] -.->|Stopped| F",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 8"
        }
    },
    "962": {
        "page_content": "E[dr1edge02.mno.gr] -.->|Stopped| F\n  F -->|Kafka MirrorMakers <br> dr1node01.mno.gr/dr1node04.mno.gr| H[DR Site Kafka: prod-trlog-ibank-ingest-stream]\n  C -->|Kafka MirrorMakers <br> dr1node05.mno.gr/dr1node06.mno.gr| H[DR Site Kafka: prod-trlog-ibank-ingest-stream]\n```\n**MirrorMaker User**: `kafka`\n**Configuration**: Cloudera Manager\n**Logs**: Cloudera Manager\n**Instructions for start/stop**: [Here](../procedures/manage_mirrormaker.md)\n**Alerts**:\n- Cloudera Manager alerts regarding Kafka",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 9"
        }
    },
    "963": {
        "page_content": "**Alerts**:\n- Cloudera Manager alerts regarding Kafka\n### Spark Streaming\nAfter being sent to the final Kafka topic, the records are consumed by two separate Spark Streaming topologies\n#### Prod_IBANK_IngestStream\nThis topology takes all records from the final Kafka topics, extracts some values and writes them to a Kudu table. It also inserts some columns that are too big for Kudu to an HBase table. This topology runs independently **on both clusters**.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 10"
        }
    },
    "964": {
        "page_content": "``` mermaid\n  graph TD\n  A[Kafka: prod-trlog-ibank-ingest-stream] --> B[Spark: Prod_IBANK_IngestStream]\n  B --> C[Kudu: prod_trlog_ibank.service_audit_stream]\n  B --> D[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM]\n```\n**User**: `PRODREST`\n**Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 11"
        }
    },
    "965": {
        "page_content": "**Submit Script**: `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- [PR][IBANK] Spark Waiting Batches\n- [DR][IBANK] Spark Waiting Batches\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 12"
        }
    },
    "966": {
        "page_content": "- [PR][IBANK] Spark Waiting Batches\n- [DR][IBANK] Spark Waiting Batches\n**Troubleshooting Steps**:\n- If the alert chart has **no line**, ensure that the application is not running (Cloudera Manager > YARN > Applications) and then submit the topology using the **Submit Script**. After resubmitting the application check the old application logs for the root cause that it failed",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 13"
        }
    },
    "967": {
        "page_content": "- If the alert has exceeded the threshold, investigate HBase/Kudu for delays and check the application logs for any errors. You can also try to kill (use Cloudera Manager or yarn command) and then re-submit the topology.\n#### Prod_IBANK_IngestStream_Visible\n**Visible transaction**: Visible are considered transactions from applications that have show_customer marked as true in the Impala table `prod_trlog_ibank.service_name`.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 14"
        }
    },
    "968": {
        "page_content": "This topology takes **visible** records from the final Kafka topics and writes them to HBase tables. This topology runs independently **on both clusters**.\n``` mermaid\n  graph TD\n  A[Kafka: prod-trlog-ibank-ingest-stream] --> C[Spark: Prod_IBANK_IngestStream_Visible]\n  B[Impala: prod_trlog_ibank.service_name] -->|Queried every 6 hours| C[Spark: Prod_IBANK_IngestStream_Visible]\n  C --> D[HBase: PROD_IBANK:SERVICE_AUDIT]\n  C --> E[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_APP]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 15"
        }
    },
    "969": {
        "page_content": "C --> D[HBase: PROD_IBANK:SERVICE_AUDIT]\n  C --> E[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_APP]\n  C --> F[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_ID]\n  C --> G[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_SNTRNCAT]\n  C --> H[HBase: PROD_IBANK:SERVICE_AUDIT_OBSCURE]\n```\n**User**: `PRODREST`\n**Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 16"
        }
    },
    "970": {
        "page_content": "**Submit Script**: `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_VISIBLE_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- [PR][IBANK Visible] Spark Waiting Batches\n- [DR][IBANK Visible] Spark Waiting Batches\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 17"
        }
    },
    "971": {
        "page_content": "- [DR][IBANK Visible] Spark Waiting Batches\n**Troubleshooting Steps**:\n- If the alert chart has **no line**, ensure that the application is not running (Cloudera Manager > YARN > Applications) and then submit the topology using the **Submit Script**.\n- If the alert has exceeded the threshold, investigate HBase for delays and check the application logs for any errors. You can also try to kill (use Cloudera Manager or yarn command) and then re-submit the topology.\n## Batch\n### Main script",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 18"
        }
    },
    "972": {
        "page_content": "As mentioned before, the information processed by the [Prod_IBANK_IngestStream](#prod_ibank_ingeststream) topology is stored in Kudu/HBase tables. At the end of each day (at **1:00 am in PR site** and **2:00 am in DR site** by **Cron**) the records from HBase/Kudu is enriched with additional information and the transfered to a parquet table. After the data are inserted to the parquet table, sequent jobs are triggered to produce aggregations and data for external systems. This procedure runs **independantly on both sites**.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 19"
        }
    },
    "973": {
        "page_content": "**User**: `PRODREST`\n**Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log`\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- _See below_\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 20"
        }
    },
    "974": {
        "page_content": "**Alerts**:\n- _See below_\n**Troubleshooting Steps**:\nThis procedure runs multiple steps which we will describe below with seperate troubleshooting steps for each step. Use the following steps for all alarms as a general guide:\n- Identify the failed step using the alarm name\n- Identify the root cause for the failed job based on the logs of the submit script/cluster application/query\n- If the problem is with an external system, ask the customer to inform the owners of the external system",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 21"
        }
    },
    "975": {
        "page_content": "- if the problem is temporary or if you cannot determine the root cause of the problem, try to re-run the failed application\n### Sub-steps\nThe following steps run **on both clusters independently**, unless specified otherwise.\n#### MSSQL Sqoop Import (Migration)\nThis step transfers transactions from the legacy MSSQL server, which is managed by mno, to the cluster as part of the daily data migration from the legacy system.\n``` mermaid\n  graph TD",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 22"
        }
    },
    "976": {
        "page_content": "``` mermaid\n  graph TD\n  A[MSSQL] -->|Sqoop Import| B[Impala: prod_trlog_ibank.historical_service_audit_raw_v2]\n  B -->|Impala Insert| C[Impala: prod_trlog_ibank.historical_service_audit_v1]\n```\n**User**: `PRODREST`\n**Script Logs**: ```/opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log```\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 23"
        }
    },
    "977": {
        "page_content": "**Script**: `/opt/ingestion/PRODREST/historical/ibank_migration/sqoop_import_ibank_hist_recursive_prod_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- IBank_Migration Historical JOB\n- IBank_Migration Historical Sqoop_Import\n- IBank_Migration Historical Impala_Insert\n**Troubleshooting Steps**:\n- Use the script/sqoop logs to identify the cause of the failure\n- If the alert is Sqoop_Import, you can safely execute the script again",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 24"
        }
    },
    "978": {
        "page_content": "- If the alert is Sqoop_Import, you can safely execute the script again\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 25"
        }
    },
    "979": {
        "page_content": "/opt/ingestion/PRODREST/historical/ibank_migration/sqoop_import_ibank_hist_recursive_prod_STABLE.sh /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2 FandFUser prod_trlog_ibank.historical_service_audit_raw_v2 prod_trlog_ibank.historical_service_audit_v1 `date +%Y-%m-%d` 1 1 4 jceks://hdfs/user/PRODREST/migration/credentials/mssql_ibank.jceks mssqlprod.password >>  /opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log 2>&1",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 26"
        }
    },
    "980": {
        "page_content": "```\n  - For a specified date:\n    ``` bash\n    # eg. 10-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 27"
        }
    },
    "981": {
        "page_content": "nohup /opt/ingestion/PRODREST/historical/ibank_migration/sqoop_import_ibank_hist_recursive_prod_STABLE.sh /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2 FandFUser prod_trlog_ibank.historical_service_audit_raw_v2 prod_trlog_ibank.historical_service_audit_v1 2019-11-10 1 1 4 jceks://hdfs/user/PRODREST/migration/credentials/mssql_ibank.jceks mssqlprod.password &>>  /opt/icomdev/ibank_hist/prod_migrate/sqoop_20191110_1_1_20191112_0900.log &",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 28"
        }
    },
    "982": {
        "page_content": "```\n- If the alert is Impala_Insert, check the that the records from the MSSQL server and the `prod_trlog_ibank.historical_service_audit_raw_v2` have the same count\n  - MSSQL server\n    ``` bash\n    # Replace the sample date 16/11/2019-17/11/2019\n\tsource /opt/ingestion/PRODREST/historical/ibank_migration/config",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 29"
        }
    },
    "983": {
        "page_content": "sqoop-eval  -Dhadoop.security.credential.provider.path=jceks://hdfs/user/PRODREST/migration/credentials/mssql_ibank.jceks --connect 'jdbc:jtds:sqlserver://999.999.999.999:2544;useNTLMv2=true;databaseName=InternetBankingAudit' --username FandFUser --password-alias mssqlprod.password --query \"select count() from ServiceAudit WITH (nolock, INDEX(IX_ServiceAuditNew_Timestamp)) where Timestamp >= '2019-11-16 00:00:00' and Timestamp < '2019-11-17 00:00:00'\"\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 30"
        }
    },
    "984": {
        "page_content": "```\n  - prod_trlog_ibank.historical_service_audit_raw_v2\n    ``` bash\n    impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.historical_service_audit_raw_v2;\"\n    ```\n- In case the counts are different between the two try again with Hive:\n  - prod_trlog_ibank.historical_service_audit_raw_v2\n    ``` bash\n    # For Primary Site",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 31"
        }
    },
    "985": {
        "page_content": "- prod_trlog_ibank.historical_service_audit_raw_v2\n    ``` bash\n    # For Primary Site\n    beeline -u \"jdbc:hive://pr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAP.mno.GR;ssl=true\" -e \"select  count(*) from prod_trlog_ibank.historical_service_audit_raw_v2;\"\n    # For Disaster Site\n    beeline -u \"jdbc:hive://dr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAD.mno.GR;ssl=true\" -e \"select  count(*) from prod_trlog_ibank.historical_service_audit_raw_v2;\"\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 32"
        }
    },
    "986": {
        "page_content": "```\n- If the counts are the same with Hive:\n  ``` bash\n  # For Primary Site\n  beeline -u \"jdbc:hive://pr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAP.mno.GR;ssl=true\"\n  # For Disaster Site\n  beeline -u \"jdbc:hive://dr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAD.mno.GR;ssl=true\"\n  ```\n  And run the insert:\n  ``` SQL",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 33"
        }
    },
    "987": {
        "page_content": "INSERT INTO prod_trlog_ibank.historical_service_audit_v1 partition (par_dt) SELECT id, to_timestamp(trtimestamp, 'yyyy-MM-dd HH:mm:ss.SSSSSSS'), trim(host), trim(servicename), trim(type), trim(application), cast(substr(executiontime,1,2)as BIGINT)36001000000000 + castsubstr(executiontime,4,2)as BIGINT)601000000000+ cast(substr(executiontime,7,2)as BIGINT)*1000000000 + cast(substr(executiontime,10,7)as BIGINT)*100, clientsession, trim(clientusername), cast(clientcustomer AS bigint), clientipadress, clientdetails, trim(clientrequestpath), cast(clientlongitude AS FLOAT), cast(clientlatitude AS FLOAT), cast(substr(clientexecutiontime,1,2)as BIGINT)36001000000000 + cast(substr(clientexecutiontime,4,2)as BIGINT)601000000000+ cast(substr(clientexecutiontime,7,2)as BIGINT)*1000000000 + cast(substr(clientexecutiontime,10,7)as BIGINT)*100, webhost, responsetextdata, requesttextdata, errordata, replace(to_date(trtimestamp),'-','') AS par_dt FROM prod_trlog_ibank.historical_service_audit_raw_v2;",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 34"
        }
    },
    "988": {
        "page_content": "```\n  And then refresh the table\n  ``` bash\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"SET DDL_SYNC=true; refresh prod_trlog_ibank.historical_service_audit_v1;\"\n  ```\n- Run the rest of the steps\n#### Insert to Service Audit\nThis step transfers transactions migrated from the legacy MSSQL server to the final table.\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.historical_service_audit_v1] -->|Impala Insert| B[Impala: prod_trlog_ibank.service_audit]\n  ```\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 35"
        }
    },
    "989": {
        "page_content": "```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_migrate_hist_to_service_audit.log`\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_migrate_hist_to_service_audit_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- IBank_Migration Historical to SA JOB\n- IBank_Migration Historical to SA Impala_Insert\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 36"
        }
    },
    "990": {
        "page_content": "**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- The script cleans up after failure, so if the problem was temporary run the script again\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODREST/historical/ibank_migrate_hist_to_service_audit_STABLE.sh prod_trlog_ibank.historical_service_audit_v1 prod_trlog_ibank.service_audit `date +%Y%m%d` 1 1 >> /var/log/ingestion/PRODREST/ibank/log/ibank_migrate_hist_to_service_audit.log  2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 37"
        }
    },
    "991": {
        "page_content": "```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\n    /opt/ingestion/PRODREST/historical/ibank_migrate_hist_to_service_audit_STABLE.sh prod_trlog_ibank.historical_service_audit_v1 prod_trlog_ibank.service_audit 20191110 1 1 >> /var/log/ingestion/PRODREST/ibank/log/ibank_migrate_hist_to_service_audit.log  2>&1 &\n    ```\n- Run the rest of the steps\n#### Merge Batch",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 38"
        }
    },
    "992": {
        "page_content": "```\n- Run the rest of the steps\n#### Merge Batch\nThis step transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and deletes the data from the original tables.\n``` mermaid\n  graph TD\n  A[Kudu: prod_trlog_ibank.service_audit_stream] --> B[Spark: PROD_IBank_MergeBatch]\n  C[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM] --> B\n  B --> D[Impala: prod_trlog_ibank.service_audit_old]\n  ```\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 39"
        }
    },
    "993": {
        "page_content": "B --> D[Impala: prod_trlog_ibank.service_audit_old]\n  ```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Script**: `/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 40"
        }
    },
    "994": {
        "page_content": "**Alerts**:\n- IBank_Ingestion MergeBatch JOB\n**Troubleshooting Steps**:\n- Use the script/spark logs to identify the cause of the failure\n- Ensure that no records are present in prod_trlog_ibank.service_audit_old\n  ``` bash\n  # eg. 09-11-2019\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.service_audit_old where par_dt='20191109';\"\n  ```\n- If no records exist and no other process is up, you can ran the script again.\n  - For the previous day:\n    ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 41"
        }
    },
    "995": {
        "page_content": "``` bash\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_IBank_MergeBatch LOCK_IBANK_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_IBank_MergeBatch  /opt/ingestion/PRODREST/ibank/lock/ FULL 1800 \"`date --date='-1 day' '+%Y-%m-%d 00:00:00'`\" \"`date '+%Y-%m-%d 00:00:00'`\"   >> /var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log 2>&1",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 42"
        }
    },
    "996": {
        "page_content": "```\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 43"
        }
    },
    "997": {
        "page_content": "``` bash\n    # eg. 09-11-2019\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_IBank_MergeBatch LOCK_IBANK_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_IBank_MergeBatch  /opt/ingestion/PRODREST/ibank/lock/ FULL 1800 \"2019-11-09 00:00:00\" \"2019-11-10 00:00:00\"   >> /var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log 2>&1 &\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 44"
        }
    },
    "998": {
        "page_content": "```\n- The process runs for well over an hour under normal circumstances or even longer for heavy load. Use of `screen` command advised\n- If the problem is with resources (out-of-memory errors):\n  - You can adjust the values at `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh`. At the moment these values are as high as they can go.\n  - You can run the MergeBatch for parts of the day\n    ``` bash\n    # eg. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 45"
        }
    },
    "999": {
        "page_content": "- You can run the MergeBatch for parts of the day\n    ``` bash\n    # eg. 09-11-2019\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 00:00:00\" \"2019-11-09 12:00:00\"\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 12:00:00\" \"2019-11-09 18:00:00\"",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 46"
        }
    },
    "1000": {
        "page_content": "/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2019-11-09 18:00:00\" \"2019-11-10 00:00:00\"\n    ```\n- Run the rest of the steps\n#### Distinct join to Service Audit",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 47"
        }
    },
    "1001": {
        "page_content": "```\n- Run the rest of the steps\n#### Distinct join to Service Audit\nSome records that are ingested by the [Stream](#stream) can also be present in the MSSQL server. In this step we insert to the final table the transactions that are unique to the [Stream](#stream), excluding the ones that are already present in the final table due to the data migration by MSSQL.\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit_old] -->|Impala Insert| B[Impala: prod_trlog_ibank.service_audit]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 48"
        }
    },
    "1002": {
        "page_content": "```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- IBank_Migration Enrich SA from SA_old JOB\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 49"
        }
    },
    "1003": {
        "page_content": "**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- Ensure that only records coming from prod_trlog_ibank.historical_service_audit_v1 are present in prod_trlog_ibank.service_audit. These records come from Insert to Service Audit [sub-step](#insert-to-service-audit) and their number should match.\n  ``` bash\n  # eg. 09-11-2019\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\"",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 50"
        }
    },
    "1004": {
        "page_content": "impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.historical_service_audit_v1 where par_dt like '20191109';\"\n  ```\n- If these records match and no other process is up, you can run the script again.\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n    ```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 51"
        }
    },
    "1005": {
        "page_content": "```\n  - For a specified date:\n    ``` bash\n    # e.g. 09-11-2019\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n    ```\n- Run the rest of the steps\n#### Report stats to Graphite\nReports statistics about the ingestion process.\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/cron_report_stats.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 52"
        }
    },
    "1006": {
        "page_content": "**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/cron_report_stats.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- This process is not monitored\n- You can safely skip this step\n- Sample execution:\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 53"
        }
    },
    "1007": {
        "page_content": "- This process is not monitored\n- You can safely skip this step\n- Sample execution:\n  ``` bash\n  nohup  /opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh prod_trlog_ibank.service_audit ibank >> /var/log/ingestion/PRODREST/ibank/log/cron_report_stats.log\n  ```\n- Run the rest of the steps\n#### Trigger external flows\nNdef: **ONLY DR SITE**\nCreates a trigger file for external flows. Related to [Datawerehouse](./ibank_dwh.md)\n**User**: `PRODREST`\n**Script Logs**: `-`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 54"
        }
    },
    "1008": {
        "page_content": "**User**: `PRODREST`\n**Script Logs**: `-`\n**Script**: `-`\n**Alerts**:\n- IBank_Migration Create UC4 file Create UC4 file\n**Troubleshooting Steps**:\n- Ensure that you are running steps for the DR site\n- If you are running the steps for the Primary skip this step\n- Execution:\n  ``` bash\n  touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY\n  touch /opt/applications/landing_zone/PRODUSER/triggers/IBANK_SA_`date '+%Y%m%d'`.READY\n  ```\n- Run the rest of the steps",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 55"
        }
    },
    "1009": {
        "page_content": "```\n- Run the rest of the steps\n#### Drop hourly partitions\nNdef: **ONLY DR SITE**\nDrop partitions from the [Hourly Merge Batch](#hourly-merge-batch) tables\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/drop_hourly_partitions.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh`\n**Alerts**:\n- IBank_Migration Drop hourly partitions JOB\n**Troubleshooting Steps**:\n- Ensure that you are running steps for the DR site",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 56"
        }
    },
    "1010": {
        "page_content": "**Troubleshooting Steps**:\n- Ensure that you are running steps for the DR site\n- If you are running the steps for the Primary skip this step\n- Use the script logs to identify the cause of the failure\n- For the previous day:\n  ``` bash\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \"prod_trlog_ibank.service_audit_hourly\" \"`date -d '-1 day' '+%Y%m%d'`\" >> /var/log/ingestion/PRODREST/ibank/log/drop_hourly_partitions.log 2>&1 &\n  ```\n- For a specified date:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 57"
        }
    },
    "1011": {
        "page_content": "```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \"prod_trlog_ibank.service_audit_hourly\" \"20191109\" >> /var/log/ingestion/PRODREST/ibank/log/drop_hourly_partitions.log 2>&1 &\n  ```\n- Run the rest of the steps\n#### Execute aggregations\nNdef: **This flow is supspended. DO NOT EXECUTE**. Information listed here are for completeness.\nThis flow computes aggregations for use with the [Queries](#queries).",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 58"
        }
    },
    "1012": {
        "page_content": "This flow computes aggregations for use with the [Queries](#queries).\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/update_ibank_aggr_tables_STABLE.sh`\n**Alerts**:\n- IBank_Migration Aggregations JOB\n- IBank_Migration Aggregations HBase\n- IBank_Migration Aggregations Kudu\n**Troubleshooting Steps**:\n- **DO NOT RUN THIS STEP**\n- For the previous day:\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 59"
        }
    },
    "1013": {
        "page_content": "**Troubleshooting Steps**:\n- **DO NOT RUN THIS STEP**\n- For the previous day:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/update_ibank_aggr_tables_STABLE.sh prod_trlog_ibank.service_audit prod_trlog_ibank.aggr_service_audit_clun_app prod_trlog_ibank.aggr_service_audit_clun >> /var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log 2>&1\n  ```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 60"
        }
    },
    "1014": {
        "page_content": "```\n- For a specified date:\n  ``` bash\n  # e.g. 09-11-2019\n  /opt/ingestion/PRODREST/common/scripts/update_ibank_aggr_tables_STABLE.sh prod_trlog_ibank.service_audit prod_trlog_ibank.aggr_service_audit_clun_app prod_trlog_ibank.aggr_service_audit_clun 20191109 >> /var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log 2>&1\n  ```\n- Run the rest of the steps\n#### Upsert to HBase (Migration)",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 61"
        }
    },
    "1015": {
        "page_content": "```\n- Run the rest of the steps\n#### Upsert to HBase (Migration)\nThis step enriches the HBase visible tables with the transactions that are unique to MSSQL server.\n``` mermaid\n  graph TD\n  A[Impala: prod_trlog_ibank.service_audit] --> C[Impala Insert/Spark]\n  B[Impala: prod_trlog_ibank.service_audit_old] --> C\n  C --> D[HBase: PROD_IBANK:SERVICE_AUDIT]\n  C --> E[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_APP]\n  C --> F[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_ID]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 62"
        }
    },
    "1016": {
        "page_content": "C --> F[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_ID]\n  C --> G[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_SNTRNCAT]\n  C --> H[HBase: PROD_IBANK:SERVICE_AUDIT_OBSCURE]\n```\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\n**Alerts**:\n- IBank_Migration Enrich hbase tables JOB\n- IBank_Migration Enrich hbase tables Impala_insert",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 63"
        }
    },
    "1017": {
        "page_content": "- IBank_Migration Enrich hbase tables JOB\n- IBank_Migration Enrich hbase tables Impala_insert\n- IBank_Migration Enrich hbase tables Spark\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 64"
        }
    },
    "1018": {
        "page_content": "**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n  Ndef: If job failed and the following error appears :`ERROR: RetriesExhaustedWithDetailsException: Failed <num> actions: CallTimeoutException: <num> times, servers with issues: [dr/pr]1node02.mno.gr`,  execute script again. The error has to do with HBase merging/spliting on a region server, but a detailed reason is unknown.\n- The script uses upsert and can be safely run many times.\n  - For the previous day:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 65"
        }
    },
    "1019": {
        "page_content": "- The script uses upsert and can be safely run many times.\n  - For the previous day:\n    ``` bash\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n    ```\n  - For a specified date:\n    ``` bash\n    # e.g. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 66"
        }
    },
    "1020": {
        "page_content": "```\n  - For a specified date:\n    ``` bash\n    # e.g. 09-11-2019\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20191109  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n    ```\n- Run the rest of the steps\n#### Send reports to business users\nSends email reports to users for duplicate transaction IDS and leftover transactions in `prod_trlog_ibank.service_audit_stream`.\n**User**: `PRODREST`\n**Script Logs**: `-`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 67"
        }
    },
    "1021": {
        "page_content": "**User**: `PRODREST`\n**Script Logs**: `-`\n**Script**: `-`\n**Alerts**:\n- IBank_Migration GUID_Report JOB\n- IBank_Migration GUID_Report Impala\n**Troubleshooting Steps**:\n- Check `/var/log/ingestion/PRODREST/ibank/log/ibank_report_duplicate_identical.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n   ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 68"
        }
    },
    "1022": {
        "page_content": "- You can safely skip this step if not running for the previous day\n- Sample execution:\n   ``` bash\n  /opt/ingestion/PRODREST/common/scripts/ibank_report_duplicate_identical_STABLE.sh  prod_trlog_ibank service_audit_old service_audit_duplicates >> /var/log/ingestion/PRODREST/ibank/log/ibank_report_duplicate_identical.log 2>&1 &\n   ```\n#### Duplicates between Impala and Kudu/HBase",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 69"
        }
    },
    "1023": {
        "page_content": "```\n#### Duplicates between Impala and Kudu/HBase\nDelete data which exist both Impala and Kudu. Sometimes MergeBatch fails because YARN kills executors for exceeding memory limits. AS a result, transactions are inserted to impala and job fails before drop them from kudu and HBase\n**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 70"
        }
    },
    "1024": {
        "page_content": "**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Check `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log` for errors\n- You can safely skip this step if not running for the previous day\n- Sample execution:\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 71"
        }
    },
    "1025": {
        "page_content": "- You can safely skip this step if not running for the previous day\n- Sample execution:\n  ``` bash\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_ibank.service_audit_stream prod_trlog_ibank.service_audit_old ibank >> /var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log 2>&1\n  ```\n- Run the rest of the steps\n#### Update monitoring postgres database",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 72"
        }
    },
    "1026": {
        "page_content": "```\n- Run the rest of the steps\n#### Update monitoring postgres database\nNdef: **IF AND ONLY IF**  all steps performed succesfully and grafana monitoring isn't updated, proceed with the following:\nUpdated the monitoring postgres database to appeared green/success in Grafana.\n- For a specified date:\n```bash\n# e.g 2023-03-30\nssh Exxxx@pr1edge01.mno.gr\nsudo -i -u postgres\npsql -d monitoring\nselect * from prod.monitoring where par_dt = 20230330;",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 73"
        }
    },
    "1027": {
        "page_content": "select * from prod.monitoring where par_dt = 20230330;\nINSERT INTO prod.monitoring (application, job_name,component,status,par_dt,start_time,end_time,description,params,host) VALUES ('IBank_Migration','Enrich SA from SA_old','JOB',0,20230330,'2023-03-31 03:18:30.000','2023-03-31 05:00:42.000','','','pr1edge01.mno.gr') ON CONFLICT (application, job_name,component,par_dt) DO UPDATE SET status=0, start_time='2023-03-31 03:18:30.000', end_time='2023-03-31 05:00:42.000',description='';\n```",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 74"
        }
    },
    "1028": {
        "page_content": "```\n- Check from Grafana that the failed job is now succeded\n### Hourly Merge Batch\nEvery hour a  transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and without deleting the data from the original tables. This table is used by mno to query the latest transactions.\n``` mermaid\n  graph TD\n  A[Kudu: prod_trlog_ibank.service_audit_stream] --> B[Spark: PROD_IBank_MergeBatch_Hourly]\n  C[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM] --> B",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 75"
        }
    },
    "1029": {
        "page_content": "C[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM] --> B\n  B --> D[Impala: prod_trlog_ibank.service_audit_hourly]\n  ```\n**User**: `PRODREST`\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Use the spark logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 76"
        }
    },
    "1030": {
        "page_content": "**Troubleshooting Steps**:\n- Use the spark logs to identify the cause of the failure\n- The data in the Hourly tables is only for the current day. No need to fill missing hours, as the next MergeBatch will insert the data to the final table.\n## Queries",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 77"
        }
    },
    "1031": {
        "page_content": "## Queries\nThe ingested data are queried in order to be displayed by the Internet Banking application (under the Calendar/\u0397\u03bc\u03b5\u03c1\u03bf\u03bb\u03cc\u03b3\u03b9\u03bf application). The application displays to the user only **Visible** transactions. The queries are submitted to the cluster as HTTP requests using the same Wildfly instances as the [Stream](#stream) flow.\n```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\n  B -->|Active| C[pr1edge01.mno.gr]",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 78"
        }
    },
    "1032": {
        "page_content": "B -->|Active| C[pr1edge01.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr]\n  C -->|Impala JDBC / HBase client| G[Primary Site]\n  D -->|Impala JDBC / HBase client| G\n  E -.->|Stopped| H[Disaster Site]\n  F -.->|Stopped| H\n```\n**User**: `PRODREST`\n**Installation Path**: `/opt/wildfly/default/prodrestib`\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 79"
        }
    },
    "1033": {
        "page_content": "**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodrestib/server.log`\n**Access Log**: `/var/log/wildfly/prodrestib/access.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 80"
        }
    },
    "1034": {
        "page_content": "**Access Log**: `/var/log/wildfly/prodrestib/access.log`\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\n**Alerts**:\n- [PR][IBANK] Query Average Response Time\n- [DR][IBANK] Query Average Response Time\n- [PR][IBANK] Query Average Error rate\n- [DR][IBANK] Query Average Error rate\n**Troubleshooting Steps**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 81"
        }
    },
    "1035": {
        "page_content": "- [DR][IBANK] Query Average Error rate\n**Troubleshooting Steps**:\n- Check in Grafana dashboard `MONITORING/Monitoring Alerts` the charts that are in alerting state\n- If the response time is for _Old implementation_ (see below) queries check if the response time is constant, a single query can increase the mean value of the response time. Check the access log for entries with large response time for the specific endpoint. If it is an isolated query the customer is not impacted.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 82"
        }
    },
    "1036": {
        "page_content": "- Check application logs for error messages.\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\n- If the logs do not indicate a specific problem, check for any problems regarding Impala/Kudu/HBase.\n### Old implementation\nThere are two versions for each query. The old implementention queries Impala tables stored in both HDFS and Kudu/HBase. This implementation had performance problems for many concurrent users.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 83"
        }
    },
    "1037": {
        "page_content": "**Endpoints**:\n- auditCount **NOT USED BY mno**\n- auditSearch **NOT USED BY mno**\n- selectById **USED BY mno**\n### New implementation\nThe new implementation uses a subset of the data (only visible transactions) stored in HBase. Queries required to access **non-Visible** transactions have to rely on the old implementation.\n**Endpoints**:\n- auditCountVisible **NOT USED BY mno**\n- auditSearchVisible **USED BY mno**\n- selectByIdVisible **NOT USED BY mno**\n## Retention Mechanism\n### Impala Retention",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 84"
        }
    },
    "1038": {
        "page_content": "- selectByIdVisible **NOT USED BY mno**\n## Retention Mechanism\n### Impala Retention\nEvery day (at **3:00 pm in both sites** by **Cron**) This script drops partitions from impala tables `prod_trlog_ibank.service_audit_old` and `prod_trlog_ibank.historical_service_audit_v1` older than 10 days. It also removes a HDFS directory under `/mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2_vault/` that correspond to 30 days before.\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 85"
        }
    },
    "1039": {
        "page_content": "**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/common/log/retention_mechanism_daily.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/retention_mechanism_daily_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr`\n**Alerts**:\n- Retention prod_trlog_ibank.service_audit_old JOB\n- Retention prod_trlog_ibank.historical_service_audit_v1 JOB",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 86"
        }
    },
    "1040": {
        "page_content": "- Retention prod_trlog_ibank.historical_service_audit_v1 JOB\n- Retention /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2_vault/`date -d \"-30 day\" +%Y-%m-%d`_`date -d \"-29 day\" +%Y-%m-%d` JOB\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, run manually the following commands\n  - For a specified date:\n    ``` bash\n    # eg. 09-11-2019",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 87"
        }
    },
    "1041": {
        "page_content": "- For a specified date:\n    ``` bash\n    # eg. 09-11-2019\n    impala-shell -k -i ${HOSTNAME/01/} --ssl --query \"set SYNC_DDL=true;alter table prod_trlog_ibank.service_audit_old drop partition ( par_dt <= 20191109 ) purge;\"\n    impala-shell -k -i ${HOSTNAME/01/} --ssl --query \"set SYNC_DDL=true;alter table prod_trlog_ibank.historical_service_audit_v1 drop partition ( par_dt <= '20191109' ) purge;\"",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 88"
        }
    },
    "1042": {
        "page_content": "hdfs dfs -rm -R -skipTrash /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2_vault/2019-11-09_2019-11-10\n    ```\n    \n#### Additional Tables\nKudu table's `prod_trlog_ibank.service_audit_stream` partitions are maintained by an Oozie jobs every day. See section [Create next week kudu partitions](#create-next-week-kudu-partitions).",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 89"
        }
    },
    "1043": {
        "page_content": "Parquet table's `prod_trlog_ibank.service_audit_hourly` partitions are dropped after each successful Merge run. See section [Drop hourly partitions](#drop-hourly-partitions).\n#### DEV\nEvery day (at **16:05 on DR site only** by **Cron**). This script drops partitions from impala table `dev_trlog_ibank.service_audit` older than 60 days.\n**User**: `DEVREST`\n**Script Logs**: `/var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 90"
        }
    },
    "1044": {
        "page_content": "**Script Logs**: `/var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log`\n**Script**: `/opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh`\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, run manually the following command\n  ``` bash",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 91"
        }
    },
    "1045": {
        "page_content": "- After the root cause for the failure is resolved, run manually the following command\n  ``` bash  \n  /opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh >> /var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log 2>&1\n  ```\n### HBase retention",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 92"
        }
    },
    "1046": {
        "page_content": "```\n### HBase retention\nEvery day (at **16:00 in both sites** by **Cron**) This script deletes rows from hbase `PROD_IBANK:SERVICE_AUDIT_STREAM` which refere to transactions inserted 7 days ago. Those rows may have been not deleted properly after merge batch job. To do so the topology queries rows from impala table `prod_trlog_ibank.service_audit` inside partition with par_dt refering 7 days ago.\n**User**: `PRODREST`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 93"
        }
    },
    "1047": {
        "page_content": "**User**: `PRODREST`\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/daily_tools_cleanupHBaseSAS.log`\n**Script**: `/opt/ingestion/PRODREST/common/scripts/daily_tools_cleanupHBaseSAS.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr`\n**Alerts**:\n- Retention IbankCleanupHbaseSAS JOB\n**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 94"
        }
    },
    "1048": {
        "page_content": "**Troubleshooting Steps**:\n- Use the script logs to identify the cause of the failure\n- After the root cause for the failure is resolved, check on `/opt/ingestion/PRODREST/no_bkp/duplicate_cid_hbase` if a manual re-run must be done for a specific date\n  - For a specified date eg 2022-06-30:\n    ``` bash\n    /opt/ingestion/PRODREST/common/spark/submit/submitmnoSparkTopology_tools_cleanupHbaseSAS prod_trlog_ibank.service_audit PROD_IBANK:SERVICE_AUDIT_STREAM LIST 20220630\n    ```\n##  Oozie Jobs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 95"
        }
    },
    "1049": {
        "page_content": "```\n##  Oozie Jobs\n###  Lookup tables\nEvery day (at 07:15 by Oozie on DR & PR site ), we transfers 3 tables with reference data from the legacy MSSQL server, which is managed by mno, to the cluster. We keep only latest version to BigData (no partition).\n**User**: `PRODREST`\n**Coordinator**: `Coord_IbankLookupTables_PROD`\n**Workflow**: `ImportLookupTables`\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/lookupTables/import_lookup_tables.sh`\n**Logs**: from HUE\n**Alerts**:",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 96"
        }
    },
    "1050": {
        "page_content": "**Logs**: from HUE\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs\n###  Create next week kudu partitions\nEvery day (at 10:00 by Oozie on DR site and 10:40 by Oozie on PR site ), we delete old Kudu partitions and add new Kudu partitions on `prod_trlog_ibank.service_audit_stream`.\n**User**: `PRODREST`\n**Coordinator**: `Coord_IBankCreateKuduPartitionsPROD`\n**Workflow**: `CreateKuduPartitionsPROD`",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 97"
        }
    },
    "1051": {
        "page_content": "**Coordinator**: `Coord_IBankCreateKuduPartitionsPROD`\n**Workflow**: `CreateKuduPartitionsPROD`\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\n**Logs**: from HUE\n**Alerts**:\n- **Not Monitored**\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 98"
        }
    },
    "1052": {
        "page_content": "- **Not Monitored**\n**Troubleshooting Steps**:\n- Open Hue to find coordinator status and logs\nThere is a case that merge batch has not properly run for a specific date, **even though** the transactions for that specific date are present on impala `service_audit`. For example by moving a service_audit partiton from DR to PR or vice versa, or when merge batch failed **after** deleting transactions from kudu.",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 99"
        }
    },
    "1053": {
        "page_content": "In that case we can manually mark merge batch as complete for that date, and therefore next oozie job created by `Coord_IBankCreateKuduPartitionsPROD` will delete that partition from kudu, by manipulating HBase table `PROD_IBANK:MERGE_BATCH_STATE_INFO`.\nExample for a specific date (10/10/2022):\n - Run HBase shell\n ```\n   hbase shell\n   ```\n - And then inside HBase shell:\n ```\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase', 'true'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 100"
        }
    },
    "1054": {
        "page_content": "put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase_time','0'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu', 'true'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu_time', '0'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running', 'false'",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 101"
        }
    },
    "1055": {
        "page_content": "put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running+time', '0'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala', 'true'\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala_time', '0'\n   ```\n   \n#### DEV",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 102"
        }
    },
    "1056": {
        "page_content": "```\n   \n#### DEV\nEvery day (at **11:10 by Oozie** on **DR site only** ), we delete old Kudu partitions and add new Kudu partitions on `dev_trlog_ibank.service_audit_stream`.\n**User**: `DEVREST`\n**Coordinator**: `Coord_IBankCreateKuduPartitionsDEV`\n**Workflow**: `CreateKuduPartitionsDEV`\n**Local path**: `/opt/ingestion/DEVREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\n**Logs**: from HUE\n**Alerts**:\n- **Not Monitored**",
        "metadata": {
            "category": "applicationFlows",
            "client": "ClientB",
            "name": "ibank.md - Part 103"
        }
    },
    "1057": {
        "page_content": "# Streamsets - Energy Efficiency\n## Access\nStreamsets Login Page: https://999.999.999.999:18636/\nFiles:\nFrom un2 with sdc user:\n```bash\nsftp bigd@999.999.999.999\ncd /ossrc\n```\n## Check for Duplicates\nExecute the following from Impala\n```bash\nselect count(*), par_dt from energy_efficiency.cell where par_dt>'202111201' group by par_dt order by par_dt desc;\n```\n```bash\nselect count(*) from (select distinct * from energy_efficiency.cell where par_dt='20211210') a;\n```\n## Solve Duplicates",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_energy_efficiency_duplicates.md - Part 1"
        }
    },
    "1058": {
        "page_content": "```\n## Solve Duplicates\nExecute the following from Impala\nBackup table:\n```bash\nCREATE TABLE  energy_efficiency.cell LIKE energy_efficiency.cell;\nINSERT INTO TABLE energy_efficiency.cell_bak PARTITION (par_dt) SELECT * FROM energy_efficiency.cell;\n```\nModify table:\n```bash\nINSERT OVERWRITE TABLE energy_efficiency.cell partition (par_dt)\n\tSELECT DISTINCT * FROM energy_efficiency.cell\n\tWHERE par_dt between '20211210' and '20211215';\n```\nDrop Backup table:\n```bash\nDROP TABLE energy_efficiency.cell;",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_energy_efficiency_duplicates.md - Part 2"
        }
    },
    "1059": {
        "page_content": "```\nDrop Backup table:\n```bash\nDROP TABLE energy_efficiency.cell;\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_energy_efficiency_duplicates.md - Part 3"
        }
    },
    "1060": {
        "page_content": "# How to fix openldap replication\nThe broken replication between `kerb1`/`kerb2` could happened in case any of the below cases appeared/happened.\n- Case 1: You changed the `Manager` password of openldap instance\n- Case 2: Replication broken for any other reason (power outage etc) between `kerb1` and `kerb2`\nFor every ldasearch the password of `Manager` is [here](KnowledgeBase/prodsyspasswd.kdbx)\n# For Case 1 follow the below steps:\nLogin into kerb1 node as root\n```bash\nssh kerb1\nsudo -i\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 1"
        }
    },
    "1061": {
        "page_content": "# For Case 1 follow the below steps:\nLogin into kerb1 node as root\n```bash\nssh kerb1\nsudo -i\n```\nBackup:\n```bash\nslapcat -n 0 -l config.ldif\nslapcat -n 2 -l data.ldif\n```\nCreate ldif file replication fix\n```bash\nvi replication_config.ldif\ndn: olcDatabase={0}config,cn=config\nchangetype:modify\nreplace: olcSyncrepl\nolcSyncrepl: rid=001\n  provider=ldaps://kerb1.bigdata.abc.gr/\n  binddn=\"cn=config\"\n  bindmethod=simple\n  credentials=\"new password\"\n  searchbase=\"cn=config\"\n  type=refreshAndPersist",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 2"
        }
    },
    "1062": {
        "page_content": "bindmethod=simple\n  credentials=\"new password\"\n  searchbase=\"cn=config\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nolcSyncrepl: rid=002\n  provider=ldaps://kerb2.bigdata.abc.gr/\n  binddn=\"cn=config\"\n  bindmethod=simple\n  credentials=\"new password\"\n  searchbase=\"cn=config\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nadd: olcMirrorMode\nolcMirrorMode: TRUE\ndn: olcDatabase={2}bdb,cn=config\nchangetype:modify\nreplace: olcSyncrepl\nolcSyncrepl: rid=003",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 3"
        }
    },
    "1063": {
        "page_content": "dn: olcDatabase={2}bdb,cn=config\nchangetype:modify\nreplace: olcSyncrepl\nolcSyncrepl: rid=003\n  provider=ldaps://kerb1.bigdata.abc.gr/\n  binddn=\"cn=Manager,dc=bigdata,dc=abc,dc=gr\"\n  bindmethod=simple\n  credentials=`new password`\n  searchbase=\"dc=bigdata,dc=abc,dc=gr\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nolcSyncrepl: rid=004\n  provider=ldaps://kerb2.bigdata.abc.gr/\n  binddn=\"cn=Manager,dc=bigdata,dc=abc,dc=gr\"\n  bindmethod=simple\n  credentials=`new password`",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 4"
        }
    },
    "1064": {
        "page_content": "binddn=\"cn=Manager,dc=bigdata,dc=abc,dc=gr\"\n  bindmethod=simple\n  credentials=`new password`\n  searchbase=\"dc=bigdata,dc=abc,dc=gr\"\n  type=refreshAndPersist\n  retry=\"5 5 300 +\"\n  timeout=1\nadd: olcMirrorMode\nolcMirrorMode: TRUE\n```\nFix the replication:\n```bash\nldapmodify  -H ldaps://kerb1.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\nldapmodify  -H ldaps://kerb2.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\n```\nChecks:",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 5"
        }
    },
    "1065": {
        "page_content": "ldapmodify  -H ldaps://kerb2.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\n```\nChecks:\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:\nLogin into admin node as root:\nOpen firefox\n```bash\nfirefox\n```\nphpldapadmin link: (https://kerb1.bigdata.abc.gr/phpldapadmin/)\n# Steps to create an ldap user\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\n2. Expand tree `people`\n3. Click `create a new entry here`",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 6"
        }
    },
    "1066": {
        "page_content": "2. Expand tree `people`\n3. Click `create a new entry here`\n4. The type of account will be `User account`\n5. Fill all the empty boxes. \n6. The user will be `no login` and the group/gid `disabled`\n7. Create object\nAfter succesfully creation of `testuser` check if exist on `kerb2` from the dropdown menu or via ldapsearch\n```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=testuser'\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 7"
        }
    },
    "1067": {
        "page_content": "```\nIf user exist then replication fixed. Just delete the `testuser`.\n# Steps to delete an ldap user\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\n2. Expand tree `people`\n3. Check the new user and from the right bar click `delete this entry`\n# For Case 2 follow the below steps:\nIdentify which `kerb` ldap instance has the issue. For example check if they had the same ammount of `users` and `groups` with `ldapsearch` commands from checks",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 8"
        }
    },
    "1068": {
        "page_content": "From the `kerb` ldap instance without corruption :\n```bash\nslapcat -n 0  config.ldif\nslapcat -n 1 data.ldif\nscp *.ldif `ldap_instance_with_corruption`:/tmp\n```\nGo to corrupted `kerb` instance:\nBackup:\n```bash\ncp -rp /etc/openldap/ /tmp/openldap.bak\ncp -rp /var/lib/ldap/ /tmp\n```\nClear:\n```bash\nsystemctl stop slapd\nsystemctl status slapd\ncd /etc/openldap/\nrm \u2013Rf slapd.d\ncd /etc/openldap\nmkdir slapd.d\ncd /var/lib/ldap/\nrm \u2013Rf *\n```\nRestore:\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 9"
        }
    },
    "1069": {
        "page_content": "rm \u2013Rf slapd.d\ncd /etc/openldap\nmkdir slapd.d\ncd /var/lib/ldap/\nrm \u2013Rf *\n```\nRestore:\n```bash\nslapadd -n 0 -F /etc/openldap/slapd.d -l /tmp/config.ldif\nslapadd -n 2 -F /etc/openldap/slapd.d -l /tmp/data.ldif -w\n```\nOwnership:\n```bash\nchown -R ldap:ldap /var/lib/ldap\n```\nStart `slapd` service:\n```bash\nsystemctl start slapd\nsystemctl status slapd\n```\nChecks:\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 10"
        }
    },
    "1070": {
        "page_content": "systemctl status slapd\n```\nChecks:\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:\nThe procedure to create a new user described [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/openldap_change_manager_password.md)\nLogin into admin node as root:\nOpen firefox\n```bash\nfirefox\n```\nphpldapadmin link: (https://kerb1.bigdata.abc.gr/phpldapadmin/)\nAfter succesfully creation of `testuser` check if exist on `kerb2` from the dropdown menu or via ldapsearch\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 11"
        }
    },
    "1071": {
        "page_content": "```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=testuser'\n```\nIf user exist then replication fixed. Just delete the `testuser`.\nThe procedure to delete a new user described [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/openldap_change_manager_password.md)",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "fix_openldap_replication_procedure.md - Part 12"
        }
    },
    "1072": {
        "page_content": "# abc - [One Domain] RCPE integration with GROUPNET\n<b>Description:</b>\n```\nThis document describes the procedure of changing current domain from `central-domain.root.def.gr` to `groupnet` along with their users\nServers:\n999.999.999.999 PVDCAHR01.groupnet.gr\n999.999.999.999 PVDCLAM01.groupnet.gr\nUseful info:\nPROD\n- rcpe1.bigdata.abc.gr, rcpe2.bigdata.abc.gr, \n- https://999.999.999.999:8843/rcpe/#/login\n- https://999.999.999.999:8843/rcpe/#/login\n- https://cne.def.gr:8843/rcpe/#/login\nTEST",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 1"
        }
    },
    "1073": {
        "page_content": "- https://999.999.999.999:8843/rcpe/#/login\n- https://cne.def.gr:8843/rcpe/#/login\nTEST\n- unc2.bigdata.abc.gr\n- https://999.999.999.999:8743/rcpe/\n```\n> Ndef: Following procedure occurs for test. Be sure to apply the same steps for prod \n### Prerequisites\n1. Check if the ssl certificates of the groupnet have already been imported\n```bash\n[root@unc2 ~]# openssl s_client -connect PVDCAHR01.groupnet.gr:636\n[root@unc2 ~]# openssl s_client -connect PVDCLAM01.groupnet.gr:636\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 2"
        }
    },
    "1074": {
        "page_content": "[root@unc2 ~]# openssl s_client -connect PVDCLAM01.groupnet.gr:636\n```\nIf they are not been imported, you should import them using formual at `admin:etc/salt/salt/tls/certificate_authority/import_ca.sls`.\n2. Customer should send an active user that belongs to the new domain for testing \n3. `/etc/hosts` file must be updated to all  BigStreamer servers with the new domain \n4. Perfom an ldap search for the given user:\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 3"
        }
    },
    "1075": {
        "page_content": "4. Perfom an ldap search for the given user:\n```\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -W -b \"dc=groupnet,dc=gr\" -D \"<Bind User sAMAccountName>\" '(sAMAccountName=...)'\n```\n### New Domain Creation\n1. Login to https://999.999.999.999:8743/rcpe/ with the credentilas you have\n2. On the main screen select **User Management** on the left of the page\n3. Select **Domain** from the tabs on the left\n4. Select **Create New** button at the bottom of the view.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 4"
        }
    },
    "1076": {
        "page_content": "4. Select **Create New** button at the bottom of the view.\n5. Enter the name and description of the new domain (DOMAINS_NAME: groupnet.gr, DOMAINS_DESCRIPTION: GROUPNET Domain)\n6. Select **Create** button at the bottom of the view.\n### Create users for the new domain\n> Ndef: This section should be only followed in case the given user does not belong to RCPE. You can check that from **Users** Tab and seach for the username. \n1. Select **Users** from the tabs on the left.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 5"
        }
    },
    "1077": {
        "page_content": "1. Select **Users** from the tabs on the left.\n2. Select **Create New** button at the bottom of the view to create a new user\n3. Enter the username and the required information for the newly user given by the customer ( Domain Attribute included ). \n> Ndef: You should not add a password here\n5. Select **Create** button at the bottom of the view.\n6. Click on **Fetch All** to view existing users including the new one",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 6"
        }
    },
    "1078": {
        "page_content": "6. Click on **Fetch All** to view existing users including the new one\n7. Click on the **magnifying glass** button next to the name of the newly created user in order to assign roles and click on button **USERS_ASSIGN_ROLES** , add SSO-Administrator and click on **Submit**.\n**Time to update sso-configuration**\n1. Login to `test_r_cpe` user\n```bash\nssh unc2\nsu - test_r_cpe #r_cpe for prod\n```\n2. Check trcpe status\n```bash\n[test_r_cpe@unc2 ~]$ trcpe-status #rcpe-status for prod\n```\n3. Stop trcpe",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 7"
        }
    },
    "1079": {
        "page_content": "```bash\n[test_r_cpe@unc2 ~]$ trcpe-status #rcpe-status for prod\n```\n3. Stop trcpe\n```bash\n[test_r_cpe@unc2 ~]$ trcpe-stop #rcpe-stop for prod\n```\n4. Back up sso configuration for central\n```bash\n[test_r_cpe@unc2 ~]$ cp /opt/test_r_cpe/standalone/configuration/ServiceWeaver/sso/sso-security.xml /opt/test_r_cpe/standalone/configuration/ServiceWeaver/sso/sso-security-backup.xml\n#/opt/r_cpe/standalone/configuration/ServiceWeaver/sso/sso-security.xml path for prod \n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 8"
        }
    },
    "1080": {
        "page_content": "#/opt/r_cpe/standalone/configuration/ServiceWeaver/sso/sso-security.xml path for prod \n```\n5. Move newly sso conf file from `/home/users/ilpap/sso-security-groupnet.xml` to the below path:\n`[test_r_cpe@unc2 ~]$ mv /home/users/ilpap/sso-security-groupnet.xml /opt/test_r_cpe/standalone/configuration/ServiceWeaver/sso/sso-security.xml`\n6. Start trcpe and check status\n```bash\ntrcpe-start\ntrcpe-status\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 9"
        }
    },
    "1081": {
        "page_content": "6. Start trcpe and check status\n```bash\ntrcpe-start\ntrcpe-status\n```\n7. Login to https://999.999.999.999:8743/rcpe/ with user and shared credentials. You must be able to see the newly created domain.\n### Move users to the created domain\n1. Back up mysql SSO_USERS table:\n```bash\nmysqldump -u root  -p test_r_cpe SSO_USERS --single-transaction > /tmp/SSO_USERS_BACKUP.sql\n```\n2. Move all users that have domain `central-domain.root.def.gr` to `groupnet.gr`\n```bash\n[root@db01 ~]# mysql -u root -p;",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 10"
        }
    },
    "1082": {
        "page_content": "```bash\n[root@db01 ~]# mysql -u root -p;\nmysql> use test_r_cpe;\nmysql> show tables;\nmysql> select * FROM SSO_DOMAINS LIMIT 5; #newly domain_ID is 5\nmysql> show create table SSO_USERS; #Domain_ID is currently 3\nmysql> UPDATE SSO_USERS SET DOMAIN_ID=5 WHERE DOMAIN_ID=3;\nmysql> select * FROM SSO_USERS where DOMAIN_ID=5;\n```\n### Domain Deletion\n1. Login with a user authorized with SSO access rights on the application\n2. On the main screen select User Management on the left of the page",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 11"
        }
    },
    "1083": {
        "page_content": "2. On the main screen select User Management on the left of the page\n3. Select Domain from the tabs on the left\n4. Select the domain you want to delete by clicking on the left of the record\n5. Select Delete Row(s) button at the bottom of the view.\n6. Verify deletion  ( select Yes, delete on the pop-up view )\n**Congrats!**",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RCPE_Change_Domain_Procedure.md - Part 12"
        }
    },
    "1084": {
        "page_content": "abc\nBigStreamer\nNagios(admin)\nIssue Number: - \nTitle: Nagios Alarms & Errors\nDescription: \nTo fix the following errors appearing in Nagios:\n\"/etc/bashrc: fork: retry: Resource temporarily unavailable\" ,\n\"ssh_exchange_identification: Connection closed by remdef host\" , \n\"Return code of 255 is out of bounds\"  \nKeywords: logs fork bounds Connection closed\nOwner: kpar\nDate: 20210512\nStatus: closed\nActions Taken:  ssh to node admin as root then :\nFor \"fork\" error : \n-------------------",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "nagios-errors.md - Part 1"
        }
    },
    "1085": {
        "page_content": "Actions Taken:  ssh to node admin as root then :\nFor \"fork\" error : \n-------------------\nas root or nagios user: \nvi /home/nagios/.bashrc\n add \nulimit -u 8888\nulimit -n 2222\nFor Connection closed error ( \"ssh_exchange_identification: Connection closed by remdef host\"):\n--------------------------------------------------------------------------------------------------\nas root in file :  \nvi /usr/local/nagios/etc/objects/commands.cfg \nchange :",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "nagios-errors.md - Part 2"
        }
    },
    "1086": {
        "page_content": "as root in file :  \nvi /usr/local/nagios/etc/objects/commands.cfg \nchange :\n$USER1$/check_by_ssh  -H $HOSTADDRESS$ -t 30 -C \"/usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$ -p $ARG3$\"\nto:\n$USER1$/check_by_ssh -E 8 -o StrictHostKeyChecking=no -H $HOSTADDRESS$ -t 30 -C \"/usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$ -p $ARG3$\"\n \nTo stop \"Return code of 255 is out of bounds\" errors \n-------------------------------------------------------\nas root:",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "nagios-errors.md - Part 3"
        }
    },
    "1087": {
        "page_content": "-------------------------------------------------------\nas root:\nIn file /usr/local/nagios/etc/nagios.cfg , \nchange value \"max_concurrent_checks\" from 0 to 50 , and then restart nagios :\n#service nagios restart",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "nagios-errors.md - Part 4"
        }
    },
    "1088": {
        "page_content": "# abc - [One Domain] SpagoBI integration with GROUPNET\n```\nThis document describes the procedure of changing current domain from `central-domain.root.def.gr` to `groupnet` along with their users.\nServer to use: PVDCAHR01.groupnet.gr\nProd Server: un5.bigdata.abc.gr\nURL: https://cne.def.gr/SpagoBI\n999.999.999.999 cne.def.gr cne\n```\n### Prerequisites\n1. Check if the ssl certificates of the groupnet have already been imported\n```bash\n[root@un5 ~]# openssl s_client -connect PVDCAHR01.groupnet.gr:636",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 1"
        }
    },
    "1089": {
        "page_content": "```bash\n[root@un5 ~]# openssl s_client -connect PVDCAHR01.groupnet.gr:636\n```\nIf it is not been imported, you should import them using formula  `admin:etc/salt/salt/tls/certificate_authority/import_ca.sls`.\n2. Customer should send an active user that belongs to the new domain so we can verify that the change is succesfully made. \nVaggos username in our case (enomikos)\n3. Customer should also send a bind user that we will use for groupnet domain configuration.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 2"
        }
    },
    "1090": {
        "page_content": "3. Customer should also send a bind user that we will use for groupnet domain configuration.\n4. `/etc/hosts` file at `un5` must be updated to all  BigStreamer servers with the new domain \n5. Perfom an ldap search for the given bind user. \n```bash\n[root@unrstudio1 ~]# ldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D \"t1-svc-cnebind\" -W -b \"dc=groupnet,dc=gr\" '(sAMAccountName=enomikos)'\n```\n### Backup\n1. Backup spagobi mysql database:\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 3"
        }
    },
    "1091": {
        "page_content": "```\n### Backup\n1. Backup spagobi mysql database:\n```bash\n[root@db01 ~]# mysqldump -u root -p spagobi --single-transaction > /tmp/spagobi.sql\n```\n2. Back up `ldap_authorizations.xml`:\n```bash\n[root@un5 ~]# cp -ap /usr/lib/spagobi/webapps/SpagoBIProject/WEB-INF/conf/webapp/ldap_authorizations.xml /usr/lib/spagobi/webapps/SpagoBIProject/WEB-INF/conf/webapp/ldap_authorizations-central.xml\n```\n3. Back up haproxy:\n```bash\n[root@un1 ~]# cp -ap /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 4"
        }
    },
    "1092": {
        "page_content": "```bash\n[root@un1 ~]# cp -ap /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak\n```\n### Actions \n1. Login to `https://cne.def.gr/SpagoBI` with the credentials you have and create groupnet user for user `enomikos`:\n- User Management\n- Click on **Add**\n- Fill in with the user ID and full name\n- Add roles\n- Save\n2. Verify that the user is successfully created using following commands:\n```bash\n[root@db01 ~]# mysql -u spagobi -p;\nmysql> use spagobi;\nmysql> show tables;",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 5"
        }
    },
    "1093": {
        "page_content": "```bash\n[root@db01 ~]# mysql -u spagobi -p;\nmysql> use spagobi;\nmysql> show tables;\nmysql> select * FROM SBI_USER WHERE USER_ID='enomikos@groupnet';\n```\n3. Stop SpagoBI process:\n```bash\n[root@un5 ~]# docker stop prod-spagobi-7.0.105\n```\n4. Edit the following lines at `un5:/usr/lib/spagobi/webapps/SpagoBIProject/WEB-INF/conf/webapp/ldap_authorizations.xml`:\n```bash\n<!--  SERVER -->\n                <HOST>un1.bigdata.abc.gr</HOST>\n                <PORT>863</PORT>",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 6"
        }
    },
    "1094": {
        "page_content": "<HOST>un1.bigdata.abc.gr</HOST>\n                <PORT>863</PORT>        \n                <ADMIN_USER>replace_with_name_of_admin</ADMIN_USER>\n                <ADMIN_PSW>replace_with_password</ADMIN_PSW> <!-- password in clear text -->\n                <BASE_DN>dc=groupnet,dc=gr</BASE_DN> <!-- base domain, if any -->\n```\n5. Update reverse proxy at `un1` so that Groupnet AD can be reached directly from spagobi app.\nAdd the following at `un1:/etc/haproxy/haproxy.cfg`\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 7"
        }
    },
    "1095": {
        "page_content": "Add the following at `un1:/etc/haproxy/haproxy.cfg`\n```bash\nlisten def-ad-ldaps\n    bind *:863 ssl crt /opt/security/haproxy/node.pem\n    mode tcp\n    balance     source\n    server def_ad1 PVDCAHR01.groupnet.gr:636 ssl check ca-file /etc/ssl/certs/ca-bundle.crt\n```\n6. Test and reload haproxy in order changes to take effect\n```bash\n[root@un1 ~]# haproxy -f /etc/haproxy/haproxy.cfg -c\n[root@un1 ~]# systemctl reload haproxy\n[root@un1 ~]# systemctl status haproxy\n```\n7. Start SpagoBI app:\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 8"
        }
    },
    "1096": {
        "page_content": "[root@un1 ~]# systemctl status haproxy\n```\n7. Start SpagoBI app:\n```bash\n[root@un5 ~]# docker start prod-spagobi-7.0.105\n```\n8. Check if `enomikos` can sign in. If yes, then go to the next step\n9. Move all users that have domain `central-domain.root.def.gr` to `groupnet.gr`\n```bash\n[root@db01 ~]# mysql -u root -p;\nmysql> use spagobi;\nselect * from SBI_USER WHERE USER_ID LIKE '%@central-domain%'; #check existing users that belong to central-domain",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 9"
        }
    },
    "1097": {
        "page_content": "UPDATE SBI_USER SET USER_ID = REPLACE(USER_ID,'@central-domain','@groupnet') WHERE USER_ID LIKE '%@central-domain%';\nselect * from SBI_USER WHERE USER_ID LIKE '%@central-domain%'; #check that no user left to central-domain\n```\n> Ndef: Before moving all users at once to the new domain you can first test just one. For example:\nUPDATE SBI_USER SET USER_ID = REPLACE(USER_ID,'@groupnet.gr','@groupnet') WHERE USER_ID LIKE '%enomikos@groupnet.gr%'",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 10"
        }
    },
    "1098": {
        "page_content": "select * from SBI_USER WHERE USER_ID LIKE '%enomikos@groupnet.gr%'\n**Congrats!**",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "SpagoBI_Change_Domain_Procedure.md - Part 11"
        }
    },
    "1099": {
        "page_content": "# GROUPNET - Change bind users' passwords\n- [GROUPNET - Change bind users' passwords](#groupnet---change-bind-users-passwords)\n  - [RAN.AI Geolocation - t1-svc-cneranaibind](#ranai-geolocation---t1-svc-cneranaibind)\n  - [R-Studio Connect - t1-svc-cnebind](#r-studio-connect---t1-svc-cnebind)\n    - [Re-activate License for R-Studio Connect](#re-activate-license-for-r-studio-connect)",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 1"
        }
    },
    "1100": {
        "page_content": "- [Re-activate License for R-Studio Connect](#re-activate-license-for-r-studio-connect)\nThe purpose of this document is to describe the procedure on how to change the passwords for the bind users used for querying the LDAP of GROUPNET domain.\n## RAN.AI Geolocation - t1-svc-cneranaibind\n1. Inform users that the authentication with GROUPNET accounts may encounter errors during the procedure (approximate 1 hour). No pod will be restarted.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 2"
        }
    },
    "1101": {
        "page_content": "2. Go to [https://cne.def.gr/auth/admin](https://cne.def.gr/auth/admin)\n3. Login with an administrative account\n4. Navigate to User Federation > GROUPNET\n5. Request password update `t1-svc-cneranaibind` via Remedy and obtain the new password\n6. Update `Bind Credential` field and press `Save`\n7. Press `Test authentication`\n## R-Studio Connect - t1-svc-cnebind\n1. Inform users for downtime of approximate 1 hour\n2. Login to `unrstudio1.bigdata.abc.gr` as root via Cyberark",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 3"
        }
    },
    "1102": {
        "page_content": "2. Login to `unrstudio1.bigdata.abc.gr` as root via Cyberark\n3. Request password update `t1-svc-cnebind` via Remedy and obtain the new password\n4. Edit `/etc/rstudio-connect/rstudio-connect.gcfg`\n    ``` bash\n    vi  /etc/rstudio-connect/rstudio-connect.gcfg\n    # Update **BindPassword** with the password obtained in step 3 and save\n    ```\n5. Restart R-Studio Connect\n    ``` bash\n    systemctl restart rstudio-connect\n    ```\n6. Check R-Studio Connect status\n    ``` bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 4"
        }
    },
    "1103": {
        "page_content": "systemctl restart rstudio-connect\n    ```\n6. Check R-Studio Connect status\n    ``` bash\n    systemctl status rstudio-connect\n    ```\n7. Verify LDAP authentication by logging in to [https://unrstudio1.bigdata.abc.gr/connect](https://unrstudio1.bigdata.abc.gr/connect)",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 5"
        }
    },
    "1104": {
        "page_content": "8. Due to the fact that the server is not directly connected to the Internet, R-Studio Connect might display an error about expired license after the reboot. In this case follow the steps listed [below](#re-activate-license-for-r-studio-connect).\n9. Inform users that the application is available.\n### Re-activate License for R-Studio Connect\n1. Login to `unrstudio1.bigdata.abc.gr` as root via Cyberark\n2. Ensure that time is accurate and the time zone is correct for the machine.\n    ```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 6"
        }
    },
    "1105": {
        "page_content": "2. Ensure that time is accurate and the time zone is correct for the machine.\n    ```bash\n    timedatectl\n    ```\n3. Sync date and time to hardware clock of the machine.\n    ``` bash\n    hwclock -w\n    ```\n4. Deactivate license\n    ``` bash\n    export http_proxy=http://un-vip.bigdata.abc.gr:5555\n    export https_proxy=http://un-vip.bigdata.abc.gr:5555\n    /opt/rstudio-connect/bin/license-manager deactivate\n    ```\n5. Activate license\n    ``` bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 7"
        }
    },
    "1106": {
        "page_content": "/opt/rstudio-connect/bin/license-manager deactivate\n    ```\n5. Activate license\n    ``` bash\n    export http_proxy=http://un-vip.bigdata.abc.gr:5555\n    export https_proxy=http://un-vip.bigdata.abc.gr:5555\n    /opt/rstudio-connect/bin/license-manager activate <product-key>\n    # This should display Activation status as Activated \n    ```\n6. In case you  receive the following\n   ``` text",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 8"
        }
    },
    "1107": {
        "page_content": "```\n6. In case you  receive the following\n   ``` text\n   Error activating product key: (13): The activation has expired or the system time has been tampered with. Ensure your time, timezone, and date settings are correct. If you're sure the license is not expired, try performing the following steps, in order: \n   1. Fix the timezone on your system.\n   2. Fix the date on your system.\n   3. Fix the time on your system.\n   4. Perform a system restart (important!)\n   ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 9"
        }
    },
    "1108": {
        "page_content": "3. Fix the time on your system.\n   4. Perform a system restart (important!)\n   ```\n   Fix any time/date issues and **reboot the server**.\n7. Verify license status\n    ``` bash\n    /opt/rstudio-connect/bin/license-manager status\n    /opt/rstudio-connect/bin/license-manager verify\n    ```\n8. Restart R-Studio Connect\n    ``` bash\n    systemctl restart rstudio-connect\n    ```\n9. Check R-Studio Connect status\n    ``` bash\n    systemctl status rstudio-connect\n    ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 10"
        }
    },
    "1109": {
        "page_content": "```\n9. Check R-Studio Connect status\n    ``` bash\n    systemctl status rstudio-connect\n    ```\n10. Verify LDAP authentication by logging in to [https://unrstudio1.bigdata.abc.gr/connect](https://unrstudio1.bigdata.abc.gr/connect)",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "groupnet_change_bind_users_passwords.md - Part 11"
        }
    },
    "1110": {
        "page_content": "# Kubernetes User Environment Setup\n## Tools\nThe main tool that needs to be installed is **kubectl**, instructions for which can be found\n[here](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/). Additionally after\ninstallation, completion can be enabled by executing:\n```bash\nmkdir -p /etc/bash_completion.d\nkubectl completion bash | sudo tee /etc/bash_completion.d/kubectl\n```\nIf possible a nice to have plugin for setting up kubeconfigs can be found at",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_ranai_kubernetes_user.md - Part 1"
        }
    },
    "1111": {
        "page_content": "```\nIf possible a nice to have plugin for setting up kubeconfigs can be found at\n[kubectl-view-serviceaccount-kubeconfig-plugin](https://github.com/superbrothers/kubectl-view-serviceaccount-kubeconfig-plugin/releases). Simply place the binary under `/usr/local/bin/`.\nAdditionally in order to install **helm**, follow the (instructions](https://helm.sh/docs/intro/install/)\nand set up completion by executing hte following:\n```bash\nhelm completion bash | sudo tee /etc/bash_completion.d/helm\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_ranai_kubernetes_user.md - Part 2"
        }
    },
    "1112": {
        "page_content": "```bash\nhelm completion bash | sudo tee /etc/bash_completion.d/helm\n```\n## Service Account\nCreate the following YAML files, that contain the definition for the service account and its\nrole binding:\n- **`service_account.yml`**\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: <Account Name>\n  namespace: <RAN.AI Namespace>\n```\n- **`role_binding.yml`**\n```yaml\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: <Binding Name>\n  namespace: <RAN.AI Namespace>",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_ranai_kubernetes_user.md - Part 3"
        }
    },
    "1113": {
        "page_content": "metadata:\n  name: <Binding Name>\n  namespace: <RAN.AI Namespace>\nsubjects:\n- kind: ServiceAccount\n  name: <Account Name>\n  namespace: <RAN.AI Namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit\n```\nApply both in sequence:\n```bash\nkubectl apply -f service_account.yml\nkubectl apply -f role_binding.yml\n```\n### User Secret\nFor Kubernetes versions over 1.24 when creating a service account it's secret is not automatically created",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_ranai_kubernetes_user.md - Part 4"
        }
    },
    "1114": {
        "page_content": "and mounted, so in that case create the following secret:\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <Account Secret Name>\n  namespace: <RAN.AI Namespace>\n  annotations:\n    kubernetes.io/service-account.name: <Account Name>\ntype: kubernetes.io/service-account-token\n```\nExecute the following to get the new users config. If the plugin is not available the config must be constructed manually:\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_ranai_kubernetes_user.md - Part 5"
        }
    },
    "1115": {
        "page_content": "```\nkubectl view-serviceaccount-kubeconfig -n <RAN.AI Namespace> <Account Name> > <User's Home>/.kube/config\n```\nFor reference the config looks like this:\n```yaml\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: ...\n    server: https://<Server>:<Port>\n  name: <Cluster Name>\ncontexts:\n- context:\n    cluster: <Cluster Name>\n    namespace: <RAN.AI Namespace>\n    user: <Account Name>\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_ranai_kubernetes_user.md - Part 6"
        }
    },
    "1116": {
        "page_content": "name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: <Account Name>\n  user:\n    token: ...\n```\nIn the above segment `certificate-authority-data` and `token` can be obtained by executing:\n```bash\n# Find the Account's Secret Token name\nkubectl get secrets -n <RAN.AI Namespace>\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.ca\\.crt}'",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_ranai_kubernetes_user.md - Part 7"
        }
    },
    "1117": {
        "page_content": "kubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.ca\\.crt}'\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.token}' | base64 --decode\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_ranai_kubernetes_user.md - Part 8"
        }
    },
    "1118": {
        "page_content": "# MoPs Index\nLink to [Dev MoPs Index](https://metis.ghi.com/obss/bigdata/abc/devops/devops-projects/-/blob/master/MoPs_index.md)",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "Dev_MoPs_index.md - Part 1"
        }
    },
    "1119": {
        "page_content": "### Certificate Authority installation\nBelow procedure describe the installation of a certificate authority using SaltStack.\n 1.  Move given certificate under `admin:/etc/salt/salt/tls/internal_certificate/root_certificate/`\n 2.  Rename `.cer` file to `.crt`.\n- If certificate has `.crt` suffix you should first verify that it base64 encoded by opening file and make sure it starts with `-----BEGIN CERTIFICATE----`\n \n ```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "Certificate_authority_installation.md - Part 1"
        }
    },
    "1120": {
        "page_content": "```bash\nmv /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.crt /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.cer\n```\n3. Install certificate by using saltStack formula:\n```bash\n###Test what actions will take affect before actually run the installation formula\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os test=True\n### Install the certificate",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "Certificate_authority_installation.md - Part 2"
        }
    },
    "1121": {
        "page_content": "### Install the certificate\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os\n```\n4. Install jssecacerts by using saltStack formula:\n```bash\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_jssecacerts\n```\n> Ndef: Keep in mind that above command will fail if there is no java installed at the specified node\n**Congratulations!**",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "Certificate_authority_installation.md - Part 3"
        }
    },
    "1122": {
        "page_content": "# Method of Procedure (MoP) - AppEmptyQueryException Alerts Resolution\n## Description\nThis MoP document outlines the resolution steps for addressing AppEmptyQueryException alerts related to missing data from bigcust tables. The alerts pertain to two categories: IF (Interface Metrics) and CPU/MEM (CPU and Memory Metrics).\n## Affected Alerts\n1. IF Alerts:\n   - Occurred for 5-minute intervals starting from `2023-10-12 14:30:00` until `2023-10-12 14:50:00`.\n2. CPU/MEM Alerts:",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md - Part 1"
        }
    },
    "1123": {
        "page_content": "2. CPU/MEM Alerts:\n   - Occurred for 5-minute intervals starting from `2023-10-12 14:30:00` until `2023-10-12 14:45:00` and then for `2023-10-12 14:55:00` & `2023-10-12 15:00:00`.\n## Resolution Steps\n### Resolution for IF Alerts:\n1. Check `bigcust.perf_interfacemetrics_ipvpn_hist` table to confirm the missing data for the mentioned 5-minute intervals.\n2. Review the CSV file loading process into HDFS for errors:\n   - Connect to `ipvpn@un2` and navigate to the log file:\n     ```\n     ssh root@un2",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md - Part 2"
        }
    },
    "1124": {
        "page_content": "- Connect to `ipvpn@un2` and navigate to the log file:\n     ```\n     ssh root@un2\n     su ipvpn\n     tail -f /shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/log/nnm_interface_metrics.cron.20231012.log\n     ```\n   - The last loaded CSV file was `InterfaceMetrics_20231012142500238.csv.gz` at `2023-10-12 14:28:00`. No files were loaded from that point until 14:50, with the message: \"File '/home/custompoller/nnm_interface_metrics/InterfaceMetrics*.csv.gz' not found.\"",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md - Part 3"
        }
    },
    "1125": {
        "page_content": "3. Verify the transfer process on `nnmprd01` for errors:\n   - Connect to `nnmprd01` via passwordless SSH from `un2`:\n     ```\n     ssh custompoller@nnmprd01\n     ```\n   - Review the transfer log:\n     ```\n     tail -f /home/custompoller/export_metrics/log/transfer-new-files-cut.20231012.log\n     ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md - Part 4"
        }
    },
    "1126": {
        "page_content": "tail -f /home/custompoller/export_metrics/log/transfer-new-files-cut.20231012.log\n     ```\n     The last file moved from the source directory to the destination directory on `nnmprd01` was `InterfaceMetrics_20231012142500238.csv.gz` at `14:28:01`. This continues until `14:50`, aligning with the observations in the `bigcust.perf_interfacemetrics_ipvpn_hist` table.\n#### Conclusion for IF Alerts\nNo files were generated by NNM during the intervals when we received the IF alerts.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md - Part 5"
        }
    },
    "1127": {
        "page_content": "No files were generated by NNM during the intervals when we received the IF alerts.\n### Resolution for CPU/MEM Alerts:\n1. Check `bigcust.nnm_ipvpn_componentmetrics_hist` table to confirm the missing data for the mentioned 5-minute intervals.\n2. Examine the SFTP transfer process from `nnprd01` for errors:\n   - Navigate to the log file on `ipvpn@un2`:\n     ```\n     tail -f /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20231012.log\n     ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md - Part 6"
        }
    },
    "1128": {
        "page_content": "```\n     tail -f /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20231012.log\n     ```\n     The last SFTP transfer was for the file `BIG-CUSTOMERS-CPU-MEM-UTIL_20231012142323337.csv.gz` at `14:26`. Subsequently, the logs show the message \"No file found.. sleep and try again..\" until `14:58`, when a CSV file was found.\n#### Conclusion for CPU/MEM Alerts\nNo files were generated by NNM during the intervals when we received the CPU/MEM alerts.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md - Part 7"
        }
    },
    "1129": {
        "page_content": "#check Retention\nlogin @un2 as intra\n1st level:\n$ grep \"Script Status\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.202012.log | tail -n1\n\u03c0.\u03c7. Script Status ==> Scr:203.Retention_Dynamic_Drop_DDL.sh, Dt:2020-12-18 08:13:12, Status:0, Snapshot:1608267602, RunID:1608271202, ExpRows:3327, Secs:790, 00:13:10\nif Status != 0 we have a problem\n---\n2nd level:\nwe get the Snapshot ID from the above (e.g. Snapshot:1608267602)",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "check_anonymization.md - Part 1"
        }
    },
    "1130": {
        "page_content": "---\n2nd level:\nwe get the Snapshot ID from the above (e.g. Snapshot:1608267602)\n$ egrep -i '(error|problem|except|fail)' /shared/abc/cdo/log/Retention/*1608267602*.log\nif it comes out < 10 it doesn't particularly worry us.\nIf it comes out a lot it's not good.\n#Anonymization\n$ grep \"Script Status\" /shared/abc/cdo/log/100.Anonymize_Data_Main.202012.log | tail -n1\nex: Script Status ==> Scr:100.Anonymize_Data_Main.sh, Dt:2020-12-17 21:01:03, Status:, RunID:1608228002, Secs:3661, 01:01:01",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "check_anonymization.md - Part 2"
        }
    },
    "1131": {
        "page_content": "we take RunID from the above (\u03c0.\u03c7. RunID:1608228002)\n$ egrep '(:ERROR|with errors)' /shared/abc/cdo/log/Anonymize/*1608228002*.log | less\n> 0 we have a problem",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "check_anonymization.md - Part 3"
        }
    },
    "1132": {
        "page_content": "# Manage Connectivity with Viavi Kafka\n- [Manage Connectivity with Viavi Kafka](#manage-connectivity-with-viavi-kafka)\n  - [Setup](#setup)\n    - [HAProxy Configuration](#haproxy-configuration)\n  - [Procedure](#procedure)\n    - [Manage HAProxy](#manage-haproxy)\n    - [Manage DNS entries](#manage-dns-entries)\n## Setup",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 1"
        }
    },
    "1133": {
        "page_content": "- [Manage HAProxy](#manage-haproxy)\n    - [Manage DNS entries](#manage-dns-entries)\n## Setup\nIncelligent wants to develop an application with Viavi's Kafka cluster as the datasource.This project aims to replace the current feed that powers RAN.AI Geolocation that is based on SFTP file transfers with a Spark Streaming application. Kafka Cluster is secured using internal/local users and TLS encryption.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 2"
        }
    },
    "1134": {
        "page_content": "abc requested to expose Viavi's Kafka Cluster to BigStreamer's datanodes and enable the development of the application.\nIn order to achieve the connectivity we have added three new IPs (VLAN 300) to the Incelligent node and we have assigned the hostnames from the Kafka Brokers to these internal IPs with DNS entries on the cluster's internal DNS.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 3"
        }
    },
    "1135": {
        "page_content": "The reason we have have used only one node for the connectivity is that the traffic from this flow can possibly saturate all uplinks of the BigStreamer which would impact other flows.\n``` mermaid\ngraph TD\n  subgraph internal [Internal BigStreamer Network - VLAN 300]\n  A[Kafka Client]\n  A-->A1\n  A-->B1\n  A-->C1\n  subgraph proxy [incelligent node]\n    A1[999.999.999.999:9093<br>geolightgr35.geo.abc.gr - Intenral DNS]\n    B1[999.999.999.999:9093<br>geolightgr36.geo.abc.gr - Intenral DNS]",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 4"
        }
    },
    "1136": {
        "page_content": "B1[999.999.999.999:9093<br>geolightgr36.geo.abc.gr - Intenral DNS]\n    C1[999.999.999.999:9093<br>geolightgr37.geo.abc.gr - Intenral DNS]\n  end \n  end\n  subgraph kafka [Viavi's Kafka]\n    A2[999.999.999.999:9093<br>geolightgr35.geo.abc.gr - Actual Broker]\n    B2[999.999.999.999:9093<br>geolightgr36.geo.abc.gr - Actual Broker]\n    C2[999.999.999.999:9093<br>geolightgr37.geo.abc.gr - Actual Broker]\n  end\n  A1-->|HAProxy - Mode TCP|A2\n  B1-->|HAProxy - Mode TCP|B2\n  C1-->|HAProxy - Mode TCP|C2",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 5"
        }
    },
    "1137": {
        "page_content": "end\n  A1-->|HAProxy - Mode TCP|A2\n  B1-->|HAProxy - Mode TCP|B2\n  C1-->|HAProxy - Mode TCP|C2\n```\n### HAProxy Configuration\n``` conf\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 6"
        }
    },
    "1138": {
        "page_content": "#\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         999.999.999.999 local2\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 7"
        }
    },
    "1139": {
        "page_content": "daemon\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 8"
        }
    },
    "1140": {
        "page_content": "option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 999.999.999.999/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 9"
        }
    },
    "1141": {
        "page_content": "timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\nlisten viavi-megafeed-kafka1\n   bind 999.999.999.999:9092\n   mode tcp\n   balance leastconn\n   server viavi-megafeed-kafka1 999.999.999.999:9092\nlisten viavi-megafeed-kafka2\n   bind 999.999.999.999:9092\n   mode tcp\n   balance leastconn\n   server viavi-megafeed-kafka2 999.999.999.999:9092\nlisten viavi-megafeed-kafka3\n   bind 999.999.999.999:9092\n   mode tcp\n   balance leastconn",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 10"
        }
    },
    "1142": {
        "page_content": "listen viavi-megafeed-kafka3\n   bind 999.999.999.999:9092\n   mode tcp\n   balance leastconn\n   server viavi-megafeed-kafka3 999.999.999.999:9092\nlisten viavi-megafeed-kafka1_ssl\n   bind 999.999.999.999:9093\n   mode tcp\n   balance leastconn\n   server viavi-megafeed-kafka1 999.999.999.999:9093\nlisten viavi-megafeed-kafka2_ssl\n   bind 999.999.999.999:9093\n   mode tcp\n   balance leastconn\n   server viavi-megafeed-kafka2 999.999.999.999:9093\nlisten viavi-megafeed-kafka3_ssl",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 11"
        }
    },
    "1143": {
        "page_content": "server viavi-megafeed-kafka2 999.999.999.999:9093\nlisten viavi-megafeed-kafka3_ssl\n   bind 999.999.999.999:9093\n   mode tcp\n   balance leastconn\n   server viavi-megafeed-kafka3 999.999.999.999:9093\n```\n## Procedure\n### Manage HAProxy\nStart - From incelligent node as root\n``` bash\nsystemctl start haproxy\n```\nStop - From incelligent node as root\n``` bash\nsystemctl stop haproxy\n```\nCheck - From incelligent node as root\n``` bash\nsystemctl status haproxy",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 12"
        }
    },
    "1144": {
        "page_content": "systemctl stop haproxy\n```\nCheck - From incelligent node as root\n``` bash\nsystemctl status haproxy\nss -tulnp | grep 9093 # Check listening port\nnc -zv 999.999.999.999 9093 # Check HAProxy IP 1\nnc -zv 999.999.999.999 9093 # Check HAProxy IP 2\nnc -zv 999.999.999.999 9093 # Check HAProxy IP 3\nnc -zv 999.999.999.999 9093 # Check broker 1\nnc -zv 999.999.999.999 9093 # Check broker 2\nnc -zv 999.999.999.999 9093 # Check broker 3\n```\n### Manage DNS entries",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 13"
        }
    },
    "1145": {
        "page_content": "nc -zv 999.999.999.999 9093 # Check broker 3\n```\n### Manage DNS entries\nLogin to [IDM](https://admin.bigdata.abc.gr/ipa/ui/) with an administrative account to manage DNS entries",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_connectivity_with_viavi_kafka.md - Part 14"
        }
    },
    "1146": {
        "page_content": "<b>Description:</b>\n```\nThis document describes the procedure of changing current domain from `central-domain.root.def.gr` to `groupnet` along with their users\nServer:\nPVDCAHR01.groupnet.gr\nUseful info:\nPROD\n- https://999.999.999.999/connect/\n- unrstudio1\n```\n### Prerequisites\n1. Check if the ssl certificates of the groupnet have already been imported\n```bash\n[root@unrstudio1 ~]# openssl s_client -connect PVDCAHR01.groupnet.gr:636\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 1"
        }
    },
    "1147": {
        "page_content": "```bash\n[root@unrstudio1 ~]# openssl s_client -connect PVDCAHR01.groupnet.gr:636\n```\nIf they are not been imported, you should import them using formual at `admin:etc/salt/salt/tls/certificate_authority/import_ca.sls`.\n2. Customer should send an active user that belongs to the new domain for testing \n3. `/etc/hosts` file must be updated to all  BigStreamer servers with the new domain \n4. Perfom an ldap search for the given user:\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 2"
        }
    },
    "1148": {
        "page_content": "4. Perfom an ldap search for the given user:\n```\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -W -b \"dc=groupnet,dc=gr\" -D \"<Bind User sAMAccountName>\" '(sAMAccountName=...)'\n```\n### Backup\n1. Back up `rstudio-connect-central.gcfg`\n```bash\n[root@unrstudio1 ~]# cp -ap /etc/rstudio-connect/rstudio-connect.gcfg /etc/rstudio-connect/rstudio-connect-central.gcfg\n```\n2. Backup database directory `/var/lib/rstudio-connect/db/`\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 3"
        }
    },
    "1149": {
        "page_content": "```\n2. Backup database directory `/var/lib/rstudio-connect/db/`\n```bash\n[root@unrstudio1 ~]# tar -zcvf var_lib_rstudioconnect_db.tar.gz /var/lib/rstudio-connect/db/\n```\n### Update configuration\n1. Stop rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n[root@unrstudio1 ~]# systemctl stop rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n2. Edit `/etc/rstudio-connect/rstudio-connect.gcfg`",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 4"
        }
    },
    "1150": {
        "page_content": "```\n2. Edit `/etc/rstudio-connect/rstudio-connect.gcfg`\nYou can find new configuration at: `[root@unrstudio1 ~]# /etc/rstudio-connect/rstudio-connect-groupnet.gcfg`\nValues that must be changed:\n- ServerAddress\n- UserSearchBaseDN\n- GroupSearchBaseDN\n- PermittedLoginGroup #This value must be set according to the ouput of previous ldap search\n- BindDN\n- BindPassword\n- PublisherRoleMapping #This value must be set according to the ouput of previous ldap search",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 5"
        }
    },
    "1151": {
        "page_content": "- PublisherRoleMapping #This value must be set according to the ouput of previous ldap search\n- ViewerRoleMapping #This value must be set according to the ouput of previous ldap search\n- AdministratorRoleMapping #This value must be set according to the ouput of previous ldap search\n3. Start rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n[root@unrstudio1 ~]# systemctl start rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 6"
        }
    },
    "1152": {
        "page_content": "[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n4. Login to https://999.999.999.999/connect/ with the active user.\n### Rstudio Lisence\nRStudio Connect has a limit for the number of active users it can serve. Currently, the license we have, can serve only 40 active users.\nWhat can you do though in case you want to add another user but there are not free licenses? \n**Only after getting customer's confirmation you can delete another user that it is not used**\n### Delete user",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 7"
        }
    },
    "1153": {
        "page_content": "### Delete user\n1. In order to use `/opt/rstudio-connect/bin/usermanager list --users` command you must first stop RStudio connect\n```bash\n[root@unrstudio1 ~]# systemctl stop rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n2. List existing users\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager list --users\n```\n3. Let's assume that we want to delete `dsimantir` account. Let's find his GUID.\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 8"
        }
    },
    "1154": {
        "page_content": "```\n3. Let's assume that we want to delete `dsimantir` account. Let's find his GUID.\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager list --users | grep -iv dsimantir\n```\nOutput must be something like below:\n| GUID  |  ID | Username   |  First |  Last  |  Email   |   Role |  DN  | UniqueID  |\n| ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ |",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 9"
        }
    },
    "1155": {
        "page_content": "| e633e5b9-cbc3-4fb3-8c3b-19ba4aa617b7  | 16  |  dsimantir  |   |   | dsimantir@uatdef.gr  | publisher   | CN=dsimantir,OU=def_users,DC=uatdef,DC=gr  |  EQGFgRGDt0KZ9sAipdlzhw== |\n4. Delete user\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager delete --users --user-guid e633e5b9-cbc3-4fb3-8c3b-19ba4aa617b7\n```\n5. Verify that user is deleted by re-running step 3 and make sure that there is no ouptput.\n6. Start rstudio-connect\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 10"
        }
    },
    "1156": {
        "page_content": "6. Start rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl start rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n7. You can also verify that the user is deleted by login to https://999.999.999.999/connect/ with the active user account > People \n### Transfer projects/context from one user to another in case of duplicate users",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 11"
        }
    },
    "1157": {
        "page_content": "### Transfer projects/context from one user to another in case of duplicate users\nIn our case, when we changed `central-domain` to `groupnet` we noticed that when users logged in to the `groupnet` domain they were not able to see their projects.\nThat issue occurred due to the fact that name, email and other attributes where different to `central` and `groupnet`. For example:\n- \u03a7\u03a1\u03a5\u03a3\u039f\u03a3\u03a4\u039f\u039c\u039f\u03a3 Charisis, ccharisis@def.gr -> central domain\n- \u03a7\u03c1\u03c5\u03c3\u03bf\u03c3\u03c4\u03bf\u03bc\u03bf\u03c2 \u03a7\u03b1\u03c1\u03b9\u03c3\u03b7\u03c2, CCHARISIS@abc.GR -> groupnet domain",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 12"
        }
    },
    "1158": {
        "page_content": "- \u03a7\u03c1\u03c5\u03c3\u03bf\u03c3\u03c4\u03bf\u03bc\u03bf\u03c2 \u03a7\u03b1\u03c1\u03b9\u03c3\u03b7\u03c2, CCHARISIS@abc.GR -> groupnet domain\n> Ndef: Login to https://999.999.999.999/connect/ with the active user account > People > Search for the specific user and check the contents of the duplicate user\nAs a result, the user was considered as different account and a different registration was created.\nSo, how can merge those two accounts? \n1. Stop rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl stop rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 13"
        }
    },
    "1159": {
        "page_content": "[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n2. Find id of above duplicate users:\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager list --users | grep -iv ccharisis\n```\nLet's assume that the id of ccharisis in central domain is 7 and the id of ccharisis in groupnet is 145\n3. Transfer account from `central-domain` to `groupnet` using following command:\n```bash\n[root@unrstudio1 ~]# /opt/rstudio-connect/bin/usermanager transfer -source-user-id 7 -target-user-id 145\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 14"
        }
    },
    "1160": {
        "page_content": "```\n4. Start rstudio-connect\n```bash\n[root@unrstudio1 ~]# systemctl start rstudio-connect\n[root@unrstudio1 ~]# systemctl status rstudio-connect\n```\n5.  Login to https://999.999.999.999/connect/ with the active user account > People > Search for the specific user and check the contents of the duplicate user have been transferred\n6. Delete user that belongs to `central-domain` as described in previous section",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "RStudio_Change_Domain_Procedure.md - Part 15"
        }
    },
    "1161": {
        "page_content": "# How to create a keytab in NYMA\nLogin into kerb1 node as root\n```bash\nssh kerb1\nsudo -i\n```\nUse command-line interface to the Kerberos administration system\n```bash\nkadmin.local\n```\nCheck if there is a principal for the corresponding username\n```bash\nlistprincs <username>@CNE.abc.GR\n```\nCreate a principal if there is not one\n```bash\naddprinc <username>CNE.abc.GR\n```\nCreate the keytab\n```bash\nktadd -k /tmp/<username>.keytab -norandkey <username>@CNE.abc.GR\n```\nCopy the keytab file to un2 node",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_keytab.md - Part 1"
        }
    },
    "1162": {
        "page_content": "```\nCopy the keytab file to un2 node\n```bash\nscp -p /tmp/<username>.keytab un2:/tmp\n```\nLogin into un2, place keytab file under /home/users/skokkoris/ and change ownership into skokkoris\n```bash\nssh un2\nsudo -i\ncp -p /tmp/<username>.keytab /home/users/skokkoris/\nchown skokkoris. /home/users/skokkoris/<username>.keytab\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "create_keytab.md - Part 2"
        }
    },
    "1163": {
        "page_content": "# abc - BigStreamer - How to open a ticket to DELL\n<b>Description:</b>\n```\nBelow is a step-by-step description of the process from opening a ticket to collecting TSR logs from IDRAC.\n```\n<b>Actions Taken:</b>\n1. ssh with your personal account on the issue node.\n2. sudo -i;ipmitool lan print | grep -i 'IP Address' # To find the Managment IP. Otherwise e.g grep nodew /etc/hosts \n```\nIf the ipmitool package did not exist just install it. yum install ipmitool;\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "how_to_create_dell_ticket.md - Part 1"
        }
    },
    "1164": {
        "page_content": "```\nIf the ipmitool package did not exist just install it. yum install ipmitool;\n```\n3. connect via vnc. Open firefox and type the `IP Address` from step 2\n4. From `Server-->Overview-->Server Information` copy the `Service Tag number`\n5. Call Dell support `2108129800`. They need the `Service Tag number` from step 4",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "how_to_create_dell_ticket.md - Part 2"
        }
    },
    "1165": {
        "page_content": "5. Call Dell support `2108129800`. They need the `Service Tag number` from step 4\n6. An engineer will create a case and sent you all the necessary steps. If not the link to collect the TSR logs is `https://www.dell.com/support/kbdoc/el-gr/000126803/export-a-supportassist-collection-via-idrac7-and-idrac8`\n7. Inform `abc` before any action on the IDRAC.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "how_to_create_dell_ticket.md - Part 3"
        }
    },
    "1166": {
        "page_content": "7. Inform `abc` before any action on the IDRAC.\n8. Download localy the TSR gz file. ssh on the node with vnc (e.g un4). The downloaded files stored under `/home/cloudera/Downloads/` and the format is `TSRdate_service_tag.zip`\n9. Send the zip file/files to DELL and wait for their response.\nDone!",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "how_to_create_dell_ticket.md - Part 4"
        }
    },
    "1167": {
        "page_content": "### Execute Cube Indicators via Terminal\n1. Connect to `un1.bigdata.abc.gr` and change user to `intra` using sudo.\n2. Change directory \n```\ncd projects/cube_ind\n```\n3. Remove old pyspark script\n```\nrm Indicators_Spark_Job.py\n```\n4. Get new one from HDFS\n```\nkinit -kt /home/intra/intra.keytab intra\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\n```\n6. Edit submit script to change execution date. Execution date equals the missing date + 2. E.g data for 20210101 will be loaded on 20210103.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "execute_indicators_terminal.md - Part 1"
        }
    },
    "1168": {
        "page_content": "```\nvim run_cube.sh\n...verhead=4096 Indicators_Spark_Job.py <date> brond.dsl_s...\n```\n7. Run the submit script \n```\n./run_cube.sh\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "execute_indicators_terminal.md - Part 2"
        }
    },
    "1169": {
        "page_content": "# Streamsets - Java Heap Space\nStreamsets Java Heap Memory was increased due to [this](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/102#ndef_95081) issue.\nActions Taken:\n1. Configure Java Options from CLoudera Manager\n   ```bash\n   cluster -> Streamsets -> Configuration -> Java options: `-Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow`\n   ```\n2. Remove old configuration\n   ```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 1"
        }
    },
    "1170": {
        "page_content": "```\n2. Remove old configuration\n   ```bash\n   cluster-> Streamsets -> Configuration -> Data Collector Advanced Configuration Snippet (Safety Valve) for sdc-env.sh\n   ```\n   ```bash\n   #Remove the following line, if exists\n   export SDC_JAVA_OPTS=\"-Xmx16384m -Xms16384m -server -XX:-OmitStackTraceInFastThrow ${SDC_JAVA_OPTS}\"\n   ```\n3. Restart Streamsets\n   ```bash\n   cluster -> Streamsets -> Restart\n   ```\n4. Check Streamsets Process Options\n   ```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 2"
        }
    },
    "1171": {
        "page_content": "```bash\n   cluster -> Streamsets -> Restart\n   ```\n4. Check Streamsets Process Options\n   ```bash\n   [root@un2 ~]# ps -ef | grep -i streamsets | grep -i xmx",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 3"
        }
    },
    "1172": {
        "page_content": "sdc      24898 24873 45 12:45 ?        00:40:11 /usr/java/default/bin/java -classpath /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs/main/streamsets-datacollector-bootstrap-3.21.0.jar:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/root-lib/* -Djava.security.manager -Djava.security.policy=file:///var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR/sdc-security.policy -Xmx1024m -Xms1024m -server -XX:-OmitStackTraceInFastThrow -Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow -Dsdc.dist.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0 -Dsdc.resources.dir=/var/lib/sdc/resources -Dsdc.hostname=un2.bigdata.abc.gr -Dsdc.conf.dir=/var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR -Dsdc.data.dir=/shared/sdc/data -Dsdc.log.dir=/shared/sdc/log/ -javaagent:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs/main/streamsets-datacollector-bootstrap-3.21.0.jar -Dsdc.libraries.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs -Dsdc.librariesExtras.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs-extras -Dsdc.rootLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/root-lib -Dsdc.bootstrapLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs -Dsdc.apiLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/api-lib -Dsdc.asterClientLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/aster-client-lib -Dsdc.containerLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/container-lib -Dsdc.libsCommon.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libs-common-lib -Dsdc.userLibs.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/user-libs -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -Djdk.nio.maxCachedBufferSize=262144 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/shared/sdc/log//gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/shared/sdc/log//sdc_heapdump_1675334705.hprof -XX:ErrorFile=/shared/sdc/log//hs_err_1675334705.log com.streamsets.pipeline.BootstrapMain -mainClass com.streamsets.datacollector.main.DataCollectorMain -apiClasspath /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/api-lib/*.jar -containerClasspath /var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/container-lib/*.jar -streamsetsLibrariesDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs -userLibrariesDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/user-libs -configDir /var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR -libsCommonLibDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libs-common-lib -streamsetsLibrariesExtraDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs-extras",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 4"
        }
    },
    "1173": {
        "page_content": "```\n   > Tip: When defining Java configuration options, avoid defining duplicate options. If you do define duplicates, the last option passed to the JVM usually takes precedence.\n5. Check Max Java Heap Space for Streamsets via jconsole or jmap\n   **jconsole**",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 5"
        }
    },
    "1174": {
        "page_content": "5. Check Max Java Heap Space for Streamsets via jconsole or jmap\n   **jconsole**\n   > Needs additional Java Options: -Dcom.sun.management.jmxremdef -Dcom.sun.management.jmxremdef.port=3333 -Dcom.sun.management.jmxremdef.local.only=false -Dcom.sun.management.jmxremdef.authenticate=false -Dcom.sun.management.jmxremdef.ssl=false\"\n   ```bash\n   [root@un2 ~]# /usr/java/latest/bin/jconsole\n   ```\n   Select `com.streamsets.pipeline.BootstrapMain`,  `Connect` and check the metrics.\n   **jmap**",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 6"
        }
    },
    "1175": {
        "page_content": "Select `com.streamsets.pipeline.BootstrapMain`,  `Connect` and check the metrics.\n   **jmap**\n   ```bash\n   jmap -heap <pid>\n   #output example\n   [root@un2 ~]# jmap -heap 24898\n   Attaching to process ID 24898, please wait...\n   Debugger attached successfully.\n   Server compiler detected.\n   JVM version is 25.181-b13\n   using parallel threads in the new generation.\n   using thread-local object allocation.\n   Concurrent Mark-Sweep GC\n   Heap Configuration:\n      MinHeapFreeRatio         = 40",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 7"
        }
    },
    "1176": {
        "page_content": "Concurrent Mark-Sweep GC\n   Heap Configuration:\n      MinHeapFreeRatio         = 40\n      MaxHeapFreeRatio         = 70\n      MaxHeapSize              = 34359738368 (32768.0MB)\n      NewSize                  = 2442723328 (2329.5625MB)\n      MaxNewSize               = 2442723328 (2329.5625MB)\n      OldSize                  = 31917015040 (30438.4375MB)\n      NewRatio                 = 2\n      SurvivorRatio            = 8\n      MetaspaceSize            = 21807104 (20.796875MB)",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 8"
        }
    },
    "1177": {
        "page_content": "SurvivorRatio            = 8\n      MetaspaceSize            = 21807104 (20.796875MB)\n      CompressedClassSpaceSize = 1073741824 (1024.0MB)\n      MaxMetaspaceSize         = 17592186044415 MB\n      G1HeapRegionSize         = 0 (0.0MB)\n   Heap Usage:\n   New Generation (Eden + 1 Survivor Space):\n      capacity = 2198470656 (2096.625MB)\n      used     = 1493838840 (1424.6357345581055MB)\n      free     = 704631816 (671.9892654418945MB)\n      67.94900063473942% used\n   Eden Space:",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 9"
        }
    },
    "1178": {
        "page_content": "free     = 704631816 (671.9892654418945MB)\n      67.94900063473942% used\n   Eden Space:\n      capacity = 1954217984 (1863.6875MB)\n      used     = 1433160568 (1366.768424987793MB)\n      free     = 521057416 (496.91907501220703MB)\n      73.33678124620104% used\n   From Space:\n      capacity = 244252672 (232.9375MB)\n      used     = 60678272 (57.8673095703125MB)\n      free     = 183574400 (175.0701904296875MB)\n      24.84241891937215% used\n   To Space:\n      capacity = 244252672 (232.9375MB)",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 10"
        }
    },
    "1179": {
        "page_content": "24.84241891937215% used\n   To Space:\n      capacity = 244252672 (232.9375MB)\n      used     = 0 (0.0MB)\n      free     = 244252672 (232.9375MB)\n      0.0% used\n   concurrent mark-sweep generation:\n      capacity = 31917015040 (30438.4375MB)\n      used     = 12194092928 (11629.193237304688MB)\n      free     = 19722922112 (18809.244262695312MB)\n      38.20561826573617% used\n   57229 interned Strings occupying 8110512 bytes.\n   ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 11"
        }
    },
    "1180": {
        "page_content": "38.20561826573617% used\n   57229 interned Strings occupying 8110512 bytes.\n   ```\nReference: https://docs.streamsets.com/platform-datacollector/latest/datacollector/UserGuide/Configuration/DCEnvironmentConfig.html",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "configure_streamsets_java_heap_space.md - Part 12"
        }
    },
    "1181": {
        "page_content": "# abc - Permanent Anonymization & Retention UI issue\n<b>Description:</b>\n```\nRegarding the problem with the Permanent Anonymization & Retention UI (https://cne.def.gr:8643/customapps)\nLet me remind you that access to this particular UI is not possible via VPN.\nThe reason is that it is trying to load a library from the internet (cdn.jsdelivr.net).\nA new war has been added which should replace the old one in wildfly.\n```\n<b>Actions Taken:</b>\n1. ssh with you personal account @unc2",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "Change_war_for_Anonymization&Retention_UI.md - Part 1"
        }
    },
    "1182": {
        "page_content": "```\n<b>Actions Taken:</b>\n1. ssh with you personal account @unc2\n2. sudo -i; less /etc/haproxy/haproxy.cfg and search for backend `tru-backend`.\n3. Backup the old war file `ssh @unekl1; cp -rp /opt/trustcenter/wf_cdef_trc/standalone/deployments/wftrust-landing-web.war /opt/trustcenter/wf_cdef_trc/standalone/wftrust-landing-web.war.bkp`\n4. chown trustuser:trustcenter `<new_war_file>`; chmod 644 `<new_war_file>`\n5. mv `<new_war_file>` /opt/trustcenter/wf_cdef_trc/standalone/deployments/",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "Change_war_for_Anonymization&Retention_UI.md - Part 2"
        }
    },
    "1183": {
        "page_content": "5. mv `<new_war_file>` /opt/trustcenter/wf_cdef_trc/standalone/deployments/\n6. Restart of wildfly is not `necessary`. Automaticcaly a new `wftrust-landing-web.war.deployed` will be created\n7. su - trustuser; bash ; trust-status `to check that wildfly is running`\n8. Make the same changes `@unekl2`\n9. Clear your cache and try again `https://cne.def.gr:8643/customapps`\n<b>Affected Systems:</b>\nabc Bigstreamer",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "Change_war_for_Anonymization&Retention_UI.md - Part 3"
        }
    },
    "1184": {
        "page_content": "# Manage IDM Replication\n[TOC]\n## Setup\nIDM (FreeIPA) has been install on two nodes for High Availability. Replication between the two nodes is performed on the LDAP service using GSSAPI authentication (Kerberos) using `ldap/_HOST` Service Principal Names (SPNs). It is a \"push\" replication, so each change is propagated to the other instance from the instance that it was performed.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 1"
        }
    },
    "1185": {
        "page_content": "Each KDC uses the LDAP on the same host as backend, so since both LDAP Servers are replicated between the two instances (active-active) KDCs are up to date with between the two hosts.\n```mermaid\n  graph LR\n  A[idm1.bigdata.abc.gr<br>SPN: ldap/idm1.bigdata.abc.gr]\n  B[idm2.bigdata.abc.gr<br>SPN: ldap/idm2.bigdata.abc.gr]\n  A-->B\n  B-->A\n```\n## Procedure\n### Check replication\n``` bash\n# Assuming you are on idm1\nkinit <admin user>\nipa-replica-manage list -v # List replication targets of idm1",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 2"
        }
    },
    "1186": {
        "page_content": "kinit <admin user>\nipa-replica-manage list -v # List replication targets of idm1\nipa-replica-manage list -v idm2.bigdata.abc.gr # This will connect to idm2 and show if it managed to push it's changes to the local instance (idm1)\n```\n```log\nidm1.bigdata.abc.gr: replica\n  last update status: Error (0) Replica acquired successfully: Incremental update succeeded\n  last update ended: 2023-12-21 12:41:17+00:00\n```\n### Force replication\n``` bash\n# Assuming you are on idm1\nkinit <admin user>",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 3"
        }
    },
    "1187": {
        "page_content": "```\n### Force replication\n``` bash\n# Assuming you are on idm1\nkinit <admin user>\nipa-replica-manage force-sync  --from idm2.bigdata.abc.gr # This will connect to idm2 and schedule an immediate sync (push) to the local instance (idm1)\n```\n```log\nipa: INFO: Setting agreement cn=meToidm1.bigdata.abc.gr,cn=replica,cn=dc\\=bigdata\\,dc\\=abc\\,dc\\=gr,cn=mapping tree,cn=config schedule to 2358-2359 0 to force synch",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 4"
        }
    },
    "1188": {
        "page_content": "ipa: INFO: Deleting schedule 2358-2359 0 from agreement cn=meToidm1.bigdata.abc.gr,cn=replica,cn=dc\\=bigdata\\,dc\\=abc\\,dc\\=gr,cn=mapping tree,cn=config\nipa: INFO: Replication Update in progress: FALSE: status: Error (0) Replica acquired successfully: Incremental update succeeded: start: 0: end: 0\n```\n## Troubleshooting\n:warning: All problems in this section have been solved and all actions mentioned below have been reverted as part of obss/oss/sysadmin-group/abc/BigStreamer/bigstreamer#53",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 5"
        }
    },
    "1189": {
        "page_content": "### A brief history of preauthentication\nPreauthentication is authentication of the client to the KDC when requesting a Kerberos ticket. This means that instead of issuing an `AS-REQ` the client issues a `TGS-REQ` which is logged in KDCs' logs.\nWe will not include specific preauthentication protocols in this presentation. Feel free to research them.\n### A brief history of errors",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 6"
        }
    },
    "1190": {
        "page_content": "### A brief history of errors\n_At the time of writing we cannot replicate the issue to get accurate log entries, but we can give you is accurate commands for the investigation_ :smile:\nWhen we failed over the Cloudera Services from `CNE.abc.GR` to `BIGDATA.abc.GR` we faced a problem with users coming from `CNE.abc.GR` not being able to login to Cloudera Services.\nTo resolve the issue we issued the following command, that disables preauthentication for SPNs:\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 7"
        }
    },
    "1191": {
        "page_content": "```bash\n# Assuming you are on idm1 and have valid admin Kerberos ticket\nipa config-mod --ipaconfigstring=\"KDC:Disable Default Preauth for SPNs\"\n```\nThis resolved our issue, but created two new problems:",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 8"
        }
    },
    "1192": {
        "page_content": "```\nThis resolved our issue, but created two new problems:\n1. SPNs do not require preauthentication, but `krbtgt/BIGDATA.abc.GR` requires preauthentication. This means that if you `kinit` with an SPN `kinit -R` will refuse to run with the dreaded `NO_PREAUTH`. This is problematic for the Hue Kerberos Renewer. As a workaround we are renewing the Kerberos ticket cache of Hue via `cron`.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 9"
        }
    },
    "1193": {
        "page_content": "2. Replication from `idm2.bigdata.abc.gr` to `idm1.bigdata.abc.gr`. The error is the error is also `NO_PREAUTH`. This means that `ldap/idm2.bigdata.abc.gr` was rejected by `ldap/idm1.bigdata.abc.gr` due to the lack of preauthentication. Let's inspect the service principals:\n    ```bash\n    ipa service-find ldap/idm1.bigdata.abc.gr --all --raw\n    ```\n    ```log\n    -----------------\n    1 service matched\n    -----------------",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 10"
        }
    },
    "1194": {
        "page_content": "```\n    ```log\n    -----------------\n    1 service matched\n    -----------------\n      dn: krbprincipalname=ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      krbcanonicalname: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\n      krbprincipalname: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\n      ...\n      managedby: fqdn=idm1.bigdata.abc.gr,cn=computers,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      ipaKrbPrincipalAlias: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 11"
        }
    },
    "1195": {
        "page_content": "ipaKrbPrincipalAlias: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\n      ...\n      krbPwdPolicyReference: cn=Default Service Password Policy,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      krbTicketFlags: 128\n      memberof: cn=replication managers,cn=sysaccounts,cn=etc,dc=bigdata,dc=abc,dc=gr\n      objectClass: ipaobject\n      objectClass: top\n      objectClass: ipaservice\n      objectClass: pkiuser\n      objectClass: krbprincipal\n      objectClass: krbprincipalaux",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 12"
        }
    },
    "1196": {
        "page_content": "objectClass: pkiuser\n      objectClass: krbprincipal\n      objectClass: krbprincipalaux\n      objectClass: krbTicketPolicyAux\n      objectClass: ipakrbprincipal\n    ----------------------------\n    Number of entries returned 1\n    ----------------------------\n    ```\n    ```bash\n    ipa service-find ldap/idm2.bigdata.abc.gr --all --raw\n    ```\n    ```log\n    -----------------\n    1 service matched\n    -----------------",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 13"
        }
    },
    "1197": {
        "page_content": "```\n    ```log\n    -----------------\n    1 service matched\n    -----------------\n      dn: krbprincipalname=ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      krbcanonicalname: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\n      krbprincipalname: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\n      ...\n      managedby: fqdn=idm2.bigdata.abc.gr,cn=computers,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      ipaKrbPrincipalAlias: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 14"
        }
    },
    "1198": {
        "page_content": "ipaKrbPrincipalAlias: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\n      ...\n      krbPwdPolicyReference: cn=Default Service Password Policy,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\n      krbTicketFlags: 0\n      memberof: cn=replication managers,cn=sysaccounts,cn=etc,dc=bigdata,dc=abc,dc=gr\n      objectClass: ipaobject\n      objectClass: top\n      objectClass: ipaservice\n      objectClass: pkiuser\n      objectClass: krbprincipal\n      objectClass: krbprincipalaux",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 15"
        }
    },
    "1199": {
        "page_content": "objectClass: pkiuser\n      objectClass: krbprincipal\n      objectClass: krbprincipalaux\n      objectClass: krbTicketPolicyAux\n      objectClass: ipakrbprincipal\n    ----------------------------\n    Number of entries returned 1\n    ----------------------------\n    ```\n    `krbTicketFlags` seems to be different. Checking the same principals from `kadmin` interface:\n    ```bash\n    kadmin.local -q \"get_principal ldap/idm1.bigdata.abc.gr\"\n    ```\n    ```log",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 16"
        }
    },
    "1200": {
        "page_content": "```bash\n    kadmin.local -q \"get_principal ldap/idm1.bigdata.abc.gr\"\n    ```\n    ```log\n    Principal: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\n    Expiration date: [never]\n    ...\n    Attributes: REQUIRES_PRE_AUTH\n    Policy: [none]\n    ```\n    ```bash\n    kadmin.local -q \"get_principal ldap/idm2.bigdata.abc.gr\"\n    ```\n    ```log\n    Principal: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\n    Expiration date: [never]\n    ...\n    Attributes:\n    Policy: [none]\n    ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 17"
        }
    },
    "1201": {
        "page_content": "Expiration date: [never]\n    ...\n    Attributes:\n    Policy: [none]\n    ```\n    Seems that preauthentication is enforced via `krbTicketFlags`. We can remove it by issuing the following command:\n    ```bash\n    kadmin.local -q \"modify_principal -requires_preauth ldap/idm1.bigdata.abc.gr\" # This is also replicated to idm2, since pushing from idm1 to idm2 worked\n    ```\n    Now replication works.",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 18"
        }
    },
    "1202": {
        "page_content": "```\n    Now replication works.\nWhen `CNE.abc.GR` is removed we will re-enable preauthentication with the following command to resolve the issue with Hue:\n```bash\nkadmin.local -q \"modify_principal +requires_preauth ldap/idm1.bigdata.abc.gr\"\nipa config-mod --ipaconfigstring=\"\"\nipactl restart\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "manage_idm_replication.md - Part 19"
        }
    },
    "1203": {
        "page_content": "### Cube Indicators Pipeline\n* Coordinator `1011_Fixed_brond_customers`  populates table `brond.fixed_brond_customers_daily`\n* Coordinator `Coord_post_BROND_FIXED_CUSTOMERS` populates tables `brond.fixed_brond_customers_daily_unq` & `brond.fixed_customers_brond_latest`\n* Coordinator `Coord_Cube_Spark_Indicators` for `par_date=date -2 days` populates table `brond.cube_indicators`  and its dependencies are:",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "cube_indicators_pipeline.md - Part 1"
        }
    },
    "1204": {
        "page_content": "* `brond.fixed_radio_matches_unq_inp` populated by script `un2:/shared/abc/brond/bin/101_fixed_radius.sh`\n  * `brond.fixed_brond_customers_daily_unq`\n  * `radius.radacct_hist` populated by script `un2:/shared/abc/radius/DataParser/scripts/radius.pl`\n  * `brond.brond_retrains_hist` populated by script `un2:/shared/abc/brond/DataParser/scripts/brond_retrains.pl`\n  * `brond.dsl_stats_week_xdsl_hist` populated by Coordinator `coord_brond_load_dsl_daily_stats`",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "cube_indicators_pipeline.md - Part 2"
        }
    },
    "1205": {
        "page_content": "# How to change Openldap Manager password\nFor every ldasearch the password of `Manager` is [here](KnowledgeBase/prodsyspasswd.kdbx)\n1. Login into kerb1 node as root:\n```bash\nssh kerb1\nsudo -i\n```\n2. Use command-line in order to create a  slapd password\n```bash\nslappasswd -h {SSHA}\n```\n3. Store the output which will be start like `{SSHA}xxxxxxx` \n4. Create ldif files change password\na.\n```bash\nvi changepwconfig.ldif\ndn: olcDatabase={0}config,cn=config\nchangetype: modify\nreplace: olcRootPW",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "openldap_change_manager_password.md - Part 1"
        }
    },
    "1206": {
        "page_content": "vi changepwconfig.ldif\ndn: olcDatabase={0}config,cn=config\nchangetype: modify\nreplace: olcRootPW\nolcRootPW: paste the output from step `3`\n```\nb.\n```bash\nvi changepwmanager.ldif\ndn: olcDatabase={2}bdb,cn=config\nchangetype: modify\nreplace: olcRootPW\nolcRootPW: paste the output from step `3`\n```\n5. Backup `config` and `data` of openldap:\n```bash\nslapcat -n 0 -l config.ldif\nslapcat -n 2 -l data.ldif\n```\n6. Modify Manager password:\n```bash\nldapmodify -H ldapi:// -Y EXTERNAL -f changepwmanager.ldif",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "openldap_change_manager_password.md - Part 2"
        }
    },
    "1207": {
        "page_content": "```\n6. Modify Manager password:\n```bash\nldapmodify -H ldapi:// -Y EXTERNAL -f changepwmanager.ldif\nldapmodify -H ldapi:// -Y EXTERNAL -f changepwconfig.ldif\n```\n7. Checks \na. Via command line\nFor `kerb1`. Where `uid` add a ldap user. e.g your ldap username:\n```bash\nldapsearch -H ldaps://kerb1.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=xxxx'\n```\nFor `kerb2`. Where `uid` add a ldap user. e.g your ldap username::\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "openldap_change_manager_password.md - Part 3"
        }
    },
    "1208": {
        "page_content": "```\nFor `kerb2`. Where `uid` add a ldap user. e.g your ldap username::\n```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=xxxx'\n```\nb. Via `UI`.\nLogin into `admin` node as `root`:\nOpen firefox\n```bash\nfirefox\n```\nphpldapadmin link: https://kerb1.bigdata.abc.gr/phpldapadmin/\nTry to connect with the new `Manager` password",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "openldap_change_manager_password.md - Part 4"
        }
    },
    "1209": {
        "page_content": "# Scope\nOnce a year the kubernetes certificates are expiring. In order to prevent that, we have a procedure below in which we are describing the steps that will help you to renew them.\n## Setup\n**Masters**: `kubemaster1`, `kubemaster2`, `kubemaster3`\n**Important ndef:** This procedure requires downtime.\n## Procedure\nLogin to kubemaster1.bigdata.abc.gr with your personal account and become root:\n- Check the certificates expiration date:\n \n    ```bash\n    sudo su -",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ranai_kubernetes_renew_certificates.md - Part 1"
        }
    },
    "1210": {
        "page_content": "- Check the certificates expiration date:\n \n    ```bash\n    sudo su -\n    \n    kubeadm certs check-expiration\n    ```\n- Keep a backup of kubernetes configuration to tmp \n    ```bash\n    cp -ar /etc/kubernetes /tmp/\n    ```\n- Keep a backup of incelligent service account\n```bash\ncp -ar /home/users/incelligent/.kube/config /tmp/bckup_renew_certificates/\n```\n- Renew the certificates\n    ```bash\n    kubeadm  certs renew all\n    kubeadm certs check-expiration\n    ```\n- Run the following\n    ```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ranai_kubernetes_renew_certificates.md - Part 2"
        }
    },
    "1211": {
        "page_content": "kubeadm certs check-expiration\n    ```\n- Run the following\n    ```bash\n    cp -p /root/.kube/config /root/.kube/config_old\n    cp /etc/kubernetes/admin.conf  /root/.kube/config\n    ```\n- From the newly create`/etc/admin/conf` make sure to copy the `client-certificate-data` and `client-key-data` data content. After that, ` vi /home/users/incellignet/.kube/config` and replace the values you copied earlier in order to add the new certificates.\n- Check again the certificates expiration date",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ranai_kubernetes_renew_certificates.md - Part 3"
        }
    },
    "1212": {
        "page_content": "- Check again the certificates expiration date\n    ```bash\n    kubeadm certs check-expiration\n   ```\n- Check the kubectl functionality\n    ```bash\n    kubectl get pods\n    ```\n- When the certificates of controller, apiserver and scheduler are renewed you must also stop containers of those three:\n```bash\nctrctl ps\nCONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ranai_kubernetes_renew_certificates.md - Part 4"
        }
    },
    "1213": {
        "page_content": "1350c48cbfb5        b3c57ca578fb           \"kube-controller-man\u2026\"   11 minutes ago      Up 11 minutes                           k8s_kube-controller-manager_kube-controller-manager-cti-cx1_kube-system_9eb854fb973ddd6df55fb792a2fbf743_9\n1bd22e95ef01        5a84bb672db8           \"kube-scheduler --au\u2026\"   11 minutes ago      Up 11 minutes                           k8s_kube-scheduler_kube-scheduler-cti-cx1_kube-system_649aa160f1bd0840b2bb0f70b6493f99_9",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ranai_kubernetes_renew_certificates.md - Part 5"
        }
    },
    "1214": {
        "page_content": "cf43799ae77d0       b6e18ffb844e6          \"kube-apiserver --au\u2026\"   11 minutes ago      Up 11 minutes                 \n```\nStop containers IDs:\n```bash\nctrctl stop 1350c48cbfb5 1bd22e95ef01 cf3cb7655b99d\n```\n> Ndef_1: Keep in mind that when the containers will be stopped, there will be downtime\n> Ndef_2: Keep in mind that those pods are static so you don't need to kill them. They will be automatically restarted\n- Also delete core-dns pod:\n```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ranai_kubernetes_renew_certificates.md - Part 6"
        }
    },
    "1215": {
        "page_content": "- Also delete core-dns pod:\n```bash\nkubectl get pod -n kube-system -l k8s-app=kube-dns\nNAME                      READY   STATUS    RESTARTS      AGE\ncoredns-64897985d-7dzkl   1/1     Running   3 (84d ago)   644d\ncoredns-64897985d-rw5kc   1/1     Running   0             83d\n```\n```bash\nkubectl delete pod coredns-64897985d-7dzkl  coredns-64897985d-rw5kc  -n kube-system\n```\n- Repeat the same procedure for `kubemaster2` and `kubemaster3`",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ranai_kubernetes_renew_certificates.md - Part 7"
        }
    },
    "1216": {
        "page_content": "```\n- Repeat the same procedure for `kubemaster2` and `kubemaster3`\n> Ndef: incelligent user exists only at kubemaster1, so you dont have to to do actions for this users at kubemaster2 and kubemaster3",
        "metadata": {
            "category": "procedures",
            "client": "ClientA",
            "name": "ranai_kubernetes_renew_certificates.md - Part 8"
        }
    },
    "1217": {
        "page_content": "How to enable acls in spark and yarn in two steps, in order to give access to spark logs for some specific groups\n1. Yarn configuration\na. Go to yarn --> configuration then search for \"acl\"\nThe field we need to modify is \"ACL For Viewing A Job\"\nAnd we've added extra groups in order to view map-reduce jobs.\nexample:  `hue WBDADMIN,WBDOPDEV,WBDOPPRO,WBDOPQA`\nYou must be very careful with the syntax, click the question mark \nb. Also we need to Enable Job ACL JobHistory Server Default Group",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "Yarn_and_Spark_ACLs_Enablement.md - Part 1"
        }
    },
    "1218": {
        "page_content": "b. Also we need to Enable Job ACL JobHistory Server Default Group\n2. Spark configuration\nGo to spark --> configuration then search for \"Spark Client Advanced Configuration Snippet\"\nThen enable spark acl by adding the following line:\n`spark.acls.enable=true`\n& enable the acls for admin groups\n`spark.admins.acls.groups=WBDADMIN`\nAlso add the following in order to give permissions to spark history server into a group\n`spark.history.ui.admin.acls.groups=WBDADMIN`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "Yarn_and_Spark_ACLs_Enablement.md - Part 2"
        }
    },
    "1219": {
        "page_content": "`spark.history.ui.admin.acls.groups=WBDADMIN`\nLastly, add the following which is the groups\n`spark.ui.view.acls.groups=WBDOPDEV,WBDOPPRO,WBDOPQA`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "Yarn_and_Spark_ACLs_Enablement.md - Part 3"
        }
    },
    "1220": {
        "page_content": "# OS Upgrade\n[[_TOC_]]\nAll procedures pertain to PR and DR edge nodes:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Updating within the same OS version\nOS packages are sourced from the mno Nexus Repository, which in itself is a yum proxy\nto the official oracle repositories for Oracle Linux 7.9. As such updating them requires only putting\nan edge node on standby and updating through **YUM**:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "os_upgrade.md - Part 1"
        }
    },
    "1221": {
        "page_content": "an edge node on standby and updating through **YUM**:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i\nFollow the procedures described in the **Switchover of Cluster Resources** chapter\nof the **Security Vulnerabilities** MOP [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx).\n    # yum clean all\n    # yum check-update\nAfter reviewing the packages that will be updated continue with the update and after it is",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "os_upgrade.md - Part 2"
        }
    },
    "1222": {
        "page_content": "After reviewing the packages that will be updated continue with the update and after it is\ncomplete unstandby the node:\n    # yum update\n    # systemctl reboot\n    # cat /etc/oracle-release\n## Rollback\nLogin to each edge node nad downgrade using **YUM**:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i\n    # yum clean all\n    # yum downgrade\n    # reboot\n    # cat /etc/oracle-release\n## Nexus Repositories\nMake sure that OS packages are sourced from the already setup Nexus repository.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "os_upgrade.md - Part 3"
        }
    },
    "1223": {
        "page_content": "Make sure that OS packages are sourced from the already setup Nexus repository.\nLogin to each edge node and edit/create the following repositories accordingly:\n    $ ssh Exxxxx@XXXedgeXX\n    $ sudo \u2013i\n    # cd /etc/yum.repos.d\n    # vi el7_uek_latest.repo\n    [el7_uek_latest]\n    name = el7_uek_latest\n    baseurl = http://999.999.999.999:8081/repository/el7_uek_latest/\n    enabled = 1\n    gpgcheck = 0\n    exclude=postgresql*\n    # vi uek_release_4_packages.repo\n    [uek_release_4_packages]",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "os_upgrade.md - Part 4"
        }
    },
    "1224": {
        "page_content": "exclude=postgresql*\n    # vi uek_release_4_packages.repo\n    [uek_release_4_packages]\n    name = uek_release_4_packages\n    baseurl = http://999.999.999.999:8081/repository/uek_release_4_packages/\n    enabled = 1\n    gpgcheck = 0\n    exclude=postgresql*\n    # vi ol7_9_latest.repo\n    [ol7_9_latest]\n    name = ol7_9_latest\n    baseurl = http://999.999.999.999:8081/repository/latest_packages/\n    enabled = 1\n    gpgcheck = 0\n    exclude=postgresql*\n    # vi ol7_9_epel.repo\n    [ol7_9_epel]",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "os_upgrade.md - Part 5"
        }
    },
    "1225": {
        "page_content": "enabled = 1\n    gpgcheck = 0\n    exclude=postgresql*\n    # vi ol7_9_epel.repo\n    [ol7_9_epel]\n    name = ol7_9_epel\n    baseurl = http://999.999.999.999:8081/repository/latest_epel_packages/\n    enabled = 1\n    gpgcheck = 0\n    exclude=postgresql*",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "os_upgrade.md - Part 6"
        }
    },
    "1226": {
        "page_content": "# Fix Mysql Replication\n## Scope\nSometimes there are invalid MySQL queries which cause the replication to not work anymore. In this short guide, it explained how you can repair the replication on the MySQL slave. This guide is for MySQL.\n### Glossary\n- MYSQL replication: It is a process that enables data from one MySQL database server (the master) to copied automatically to one or more MySQL database servers (the slaves)\n## Setup\n### Mysql Instances\n#### PR",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "sync_mysql.md - Part 1"
        }
    },
    "1227": {
        "page_content": "## Setup\n### Mysql Instances\n#### PR\nMysql supported by Oracle and if any other issue occured a critical ticket should created on Oracle Support. **This instance is not supported by jkl Telecom S.A.**\n**User**: `mysql`\n**Port**: `3306`\n**Password**: [prodsyspasswd.kbdx](/KnowledgeBase/prodsyspasswd.kdbx)\n**Master Mysql Host**: `pr1node03.mno.gr`\n**Slave Mysql Host**: `pr1node02.ngr.gr`\n**Mysql Configuration**: `/etc/my.cnf`\n**Mysql Data Path**: `/var/lib/mysql/`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "sync_mysql.md - Part 2"
        }
    },
    "1228": {
        "page_content": "**Mysql Configuration**: `/etc/my.cnf`\n**Mysql Data Path**: `/var/lib/mysql/`\n**Mysql General Log File**: `/var/log/mysqld.log`\n**Mysql Error Log File**: `/var/log/mysqld_error.log`\n#### DR\n**User**: `mysql`\n**Port**: `3306`\n**Password**: [prodsyspasswd.kbdx](/KnowledgeBase/prodsyspasswd.kdbx)\n**Master Mysql Host**: `dr1node03.mno.gr`\n**Slave Mysql Host**: `dr1node02.mno.gr`\n**Mysql Configuration**: `/etc/my.cnf`\n**Mysql Data Path**: `/var/lib/mysql/`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "sync_mysql.md - Part 3"
        }
    },
    "1229": {
        "page_content": "**Mysql Configuration**: `/etc/my.cnf`\n**Mysql Data Path**: `/var/lib/mysql/`\n**Mysql General Log File**: `/var/log/mysqld.log`\n**Mysql Error Log File**: `/var/log/mysqld_error.log`\n## Procedure\n### Identify the problem\n1. From **Slave Mysql Host** as `root`:\n      ```bash\n      mysql -u root -p\n      SHOW SLAVE STATUS\\G;\n      ```\n2. If one of `Slave_IO_Running` or `Slave_SQL_Running` is set to `No`, then the replication is broken\n### Repair MySQL Replication",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "sync_mysql.md - Part 4"
        }
    },
    "1230": {
        "page_content": "### Repair MySQL Replication\n1. From **Slave Mysql Host** as `root`:\n      ```bash\n      mysql -u root -p\n\tSTOP SLAVE;\n      ```\n    - Just to go sure, we stop the slave:\n\t\n      ``` bash\n      SHOW SLAVE STATUS\\G\n      ```\n    - Now both `Slave_IO_Running` & `Slave_SQL_Running` is set to `No`.\n\t\n2. Restore from latest mysqldump backup:\n    \n\t- From **Slave Mysql Host** as `root`:\n      ```bash\n      cd /backup\n      ls -ltr",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "sync_mysql.md - Part 5"
        }
    },
    "1231": {
        "page_content": "```bash\n      cd /backup\n      ls -ltr\n      tar -ztvf /backup/DRBDA_year-month-day.tar.gz | grep -i mysql_backup # List contents of the tar.gz file.Under backup folder stored tar.gz files from daily backup procedure,for both sites, with the format CLUSTER_year-month-day.tar.gz (e.g DRBDA_2022-03-21.tar.gz). This files contains several gz files combined in a tar.gz. Now we need to find the exact name of the gz backup file for mysql backup to proceed at next step.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "sync_mysql.md - Part 6"
        }
    },
    "1232": {
        "page_content": "tar -zxvf /backup/DRBDA_year-month-day.tar.gz mysql_backup_yearmonthday.sql.gz # Untar from the tar.gz file the exact gz backup file for mysql backup that found from previous step. The exaxt name would be placed on mysql_backup_yearmonthday.sql.gz possition\n      gunzip mysql_backup_yearmonthday.sql.gz # Decompress the file that untared from previous step\n      mysql -uroot -p < mysql_backup_yearmonthday.sql\n      ```\n3. After succesfully restoration on **Slave Mysql Host** start slave:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "sync_mysql.md - Part 7"
        }
    },
    "1233": {
        "page_content": "```\n3. After succesfully restoration on **Slave Mysql Host** start slave:\n      ``` bash\n      mysql -u root -p\n      SHOW SLAVE STATUS\\G\n      ```\n      - No error should exist on `Last_Error`\n      - If no error appeared then `START SLAVE`\n      ```bash\n      START SLAVE;\n      ```\n\t  \n4. Check if replication is working again\n \n      ``` bash\n      SHOW SLAVE STATUS\\G\n      ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "sync_mysql.md - Part 8"
        }
    },
    "1234": {
        "page_content": "4. Check if replication is working again\n \n      ``` bash\n      SHOW SLAVE STATUS\\G\n      ```\n\t- Both Slave_IO_Running and Slave_SQL_Running are set to `Yes` now. And the replication is running without any error.\n\t- `Seconds_Behind_Master` should be 0 after some minutes",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "sync_mysql.md - Part 9"
        }
    },
    "1235": {
        "page_content": "# InfiniBand Replacement\nReplacing an IB card require powering off the host. Before doing so some checks must be performed and the host\nmust be decommissioned. After replacing, some configurations must be performed in the interfaces and the roles\nrecommissioned.\n## Decommission\nTo decommission the node from Cloudera Manger, select the specific host and:\n1. Enter maintenance mode\n2. Select decommission roles\n3. If a datanode role is present on this host, take it offline for at least 4 hours",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ib_card_replacement.md - Part 1"
        }
    },
    "1236": {
        "page_content": "3. If a datanode role is present on this host, take it offline for at least 4 hours\n## Check for non-default IB partitions\nMost probably only the default IB partitions are present. To check this perform the following steps provided\nby Oracle:\n```\nNdef:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ib_card_replacement.md - Part 2"
        }
    },
    "1237": {
        "page_content": "by Oracle:\n```\nNdef:\nIf a system uses custom non-default InfiniBand partitions [e.g., Exalogic (virtual/physical/hybrid), Exadata (virtual/physical), SuperCluster, BDA] then the HCA Port GUIDs might need to be updated in the InfiniBand partition(s) after replacing an HCA.\nDetermine the switch running as Primary. From it, check for any custom, non-default IP partitions.\n[root@bda01node05 ~]# sminfo",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ib_card_replacement.md - Part 3"
        }
    },
    "1238": {
        "page_content": "[root@bda01node05 ~]# sminfo\nsminfo: sm lid 15 sm guid 0x10e0406d5aa0a0, activity count 26263191 priority 14 state 3 SMINFO_MASTER\n[root@bda01node05 ~]# ibnetdiscover | grep 10e0406d5aa0a0\nswitchguid=0x10e0406d5aa0a0(10e0406d5aa0a0)\nSwitch 36 \"S-0010e0406d5aa0a0\" # \"SUN DCS 36P QDR bdax01sw-ib1 xxx.xxx.171.24\" enhanced port 0 lid 15 lmc 0\n[root@bda01node05 ~]# ssh root@xxx.xxx.171.24\n[root@bda01sw-ib1 ~]# smpartition list active\n# Sun DCS IB partition config file",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ib_card_replacement.md - Part 4"
        }
    },
    "1239": {
        "page_content": "[root@bda01sw-ib1 ~]# smpartition list active\n# Sun DCS IB partition config file\n# This file is generated, do not edit\n#! version_number : 0\nDefault=0x7fff, ipoib : ALL_CAS=full, ALL_SWITCHES=full, SELF=full;\nSUN_DCS=0x0001, ipoib : ALL_SWITCHES=full;\nIf there are IB partitions other than default partitions, then refer to MOS ndef 1985159.1 for additional steps that will need to be taken before the old HCA is removed.\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ib_card_replacement.md - Part 5"
        }
    },
    "1240": {
        "page_content": "```\n- [MOS ndef 1985159.1](https://support.oracle.com/epmos/faces/DocumentDisplay?parent=SrDetailText&sourceId=3-37179888534&id=1985159.1)\nIf `smpartition list active` shows output similar to the above, no actions are needed. If not the attached procedure must followed in order to replace the UUIDs.\n## Fix Interface\nOracle runs an automated configuration scripts that sets arp checking of the gateway in certain interfaces. If",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ib_card_replacement.md - Part 6"
        }
    },
    "1241": {
        "page_content": "the interfaces are non-routable, as is the case for bondeth1 and bondeth2, these options must be removed.\n1. Check for any interfaces that should not be in the DOWN state with `ip a`\n2. ssh into another known with known good configs\n3. compare the interfaces in question with the interfaces in the changed hosts and remove\n   any option not present in the known-good node. Generally these would be options referring to ARP.\n4. Bring the interfaces down with `ifdown <IFCACE_NAME>`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ib_card_replacement.md - Part 7"
        }
    },
    "1242": {
        "page_content": "4. Bring the interfaces down with `ifdown <IFCACE_NAME>`\n5. Bring it back up with ``ifup <IFCACE_NAME>``\n6. Check if the interfaces are in the UP state with `ip a`\n7. Check that ARP entries are complete with `arp`\n## Recommission\nRecommission the node through cloudera manager. Recommissioning and starting roles in the same step might\nfail so it is best to recommission first without starting roles.\n1. Recommission without starting roles\n2. Start roles",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ib_card_replacement.md - Part 8"
        }
    },
    "1243": {
        "page_content": "1. Recommission without starting roles\n2. Start roles\nAfter everything is back online the kudu tablet on the host might not have taken on any tablets. This\nis normal as all tablets have been replicated to the other hosts. With time it will take on tablets as\nwell.\nThis can be verified using `ksck` as the kudu user.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ib_card_replacement.md - Part 9"
        }
    },
    "1244": {
        "page_content": "# Manage Wildfly\n## Scope",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 1"
        }
    },
    "1245": {
        "page_content": "Integration between the Big Data clusters and the backend servers of mno is done over REST APIs. The applications that handle the HTTP calls are installed on the edge servers of both sites. At normal operation only one site is active. These applications are deployed on top of Wildfly instances. There are four sets of Wildfly installations one for the `ibank` flow and one for the `online` flow and two others for applications developed by mno. All application servers are managed by `supervisord` owned by `root` user.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 2"
        }
    },
    "1246": {
        "page_content": "**DEV/QA Information**:\nThe information below are written for the Production enviroment. There is a development/QA environment that runs only on DR site. In case of a problem the paths are exectly the same if you substitute the `prod` with the `dev` preffix. For the networking part of the DEV/QA environment use the [Network Information](#network-information) chapter\n### Glossary\n- NetScaler: Loadbalancer managed by mno. It handles SSL offloading\n- VIP: Virtual IP of the Loadbalancer",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 3"
        }
    },
    "1247": {
        "page_content": "- VIP: Virtual IP of the Loadbalancer\n- SNIP: IP of the Loadbalancer that initiates the connection to Wildfly instances\n- Health check: Endpoint that the Loadbalancer uses to determine if a specific Wildfly instance is active. It expects a `HTTP 200/OK` response\n## Setup\n### Internet Banking Wildfly Instances\n#### prodrestib\nHandles ingestion and queries for the Internet Banking (`ibank`) flow.\n**User**: `PRODREST`\n**Port**: `8080`\n**Health Check Endpoint**: `/trlogibank/app`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 4"
        }
    },
    "1248": {
        "page_content": "**User**: `PRODREST`\n**Port**: `8080`\n**Health Check Endpoint**: `/trlogibank/app`\n**Supervisor Configuration**: `/etc/supervisor.d/wildfly-prodrestib.ini`\n**Installation Path**: `/opt/wildfly/default/prodrestib`\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/standalone.xml`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 5"
        }
    },
    "1249": {
        "page_content": "**Application Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodrestib/server.log`\n**Access Log**: `/var/log/wildfly/prodrestib/access.log`\n#### prodrestibmetrics\nHosts applications developed by mno and accessed by the Internet Banking backend servers. **This instance is not supported by jkl Telecom S.A.**\n**User**: `PRODREST`\n**Port**: `8081`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 6"
        }
    },
    "1250": {
        "page_content": "**User**: `PRODREST`\n**Port**: `8081`\n**Health Check Endpoint**: `/ibankmetrics/app`\n**Supervisor Configuration**: `/etc/supervisor.d/wildfly-prodrestibmetrics.ini`\n**Installation Path**: `/opt/wildfly/default/prodrestibmetrics`\n**Deployments Path**: `/opt/wildfly/default/prodrestibmetrics/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestibmetrics/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `Managed by mno`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 7"
        }
    },
    "1251": {
        "page_content": "**Application Configuration Path**: `Managed by mno`\n**Application Logs**: `/var/log/wildfly/prodrestibmetrics/server.log`\n**Access Log**: `/var/log/wildfly/prodrestibmetrics/access.log`\n### Internet Banking Loadbalancer farms\nThere are two active Loadbalancers for Internet Banking. The original setup routes all traffic to `prodrestib`, while the later one routes conditionaly traffic between `prodrestib` and `prodrestibmetrics`\n#### Original setup for Internet Banking\n```mermaid\n  graph TD",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 8"
        }
    },
    "1252": {
        "page_content": "#### Original setup for Internet Banking\n```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|prodrestibank.mno.gr:443 <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta12.mno.gr]",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 9"
        }
    },
    "1253": {
        "page_content": "B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta21.mno.gr]\n```\n#### Setup with routing for Internet Banking\nIf the request from `ibank` starts with `/trlogibank`:\n```mermaid\n  graph TD",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 10"
        }
    },
    "1254": {
        "page_content": "If the request from `ibank` starts with `/trlogibank`:\n```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|prodrestibank.mno.gr:443 <br> Request that starts with /trlogibank <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta12.mno.gr]",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 11"
        }
    },
    "1255": {
        "page_content": "B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodresta21.mno.gr]\n```\nIf the request from `ibank` does not start with `/trlogibank`:\n```mermaid\n  graph TD",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 12"
        }
    },
    "1256": {
        "page_content": "```\nIf the request from `ibank` does not start with `/trlogibank`:\n```mermaid\n  graph TD\n  A[Internet Banking Backend Servers] -->|prodrestibank.mno.gr:443 <br> Request that does not start with /trlogibank <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodresta11.mno.gr]",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 13"
        }
    },
    "1257": {
        "page_content": "B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodresta12.mno.gr]\n  B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodresta21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodresta21.mno.gr]\n```\n### Online Wildfly Instances\n#### prodreston\nHandles ingestion and queries for the Online (`online`) flow.\n**User**: `PRODREST`\n**Port**: `8080`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 14"
        }
    },
    "1258": {
        "page_content": "Handles ingestion and queries for the Online (`online`) flow.\n**User**: `PRODREST`\n**Port**: `8080`\n**Health Check Endpoint**: `/trlogonline/app`\n**Supervisor Configuration**: `/etc/supervisor.d/wildfly-prodreston.ini`\n**Installation Path**: `/opt/wildfly/default/prodreston`\n**Deployments Path**: `/opt/wildfly/default/prodreston/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/standalone.xml`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 15"
        }
    },
    "1259": {
        "page_content": "**Application Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml`\n**Application Logs**: `/var/log/wildfly/prodreston/server.log`\n**Access Log**: `/var/log/wildfly/prodreston/access.log`\n#### prodrestintapps\nHosts applications developed by mno and accessed by the Online backend servers. **This instance is not supported by jkl Telecom S.A.**\n**User**: `PRODREST`\n**Port**: `8081`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 16"
        }
    },
    "1260": {
        "page_content": "**User**: `PRODREST`\n**Port**: `8081`\n**Health Check Endpoint**: `/intapps/app`\n**Supervisor Configuration**: `/etc/supervisor.d/wildfly-prodrestintapps.ini`\n**Installation Path**: `/opt/wildfly/default/prodrestintapps`\n**Deployments Path**: `/opt/wildfly/default/prodrestintapps/standalone/deployments`\n**General Configuration Path**: `/opt/wildfly/default/prodrestintapps/standalone/configuration/standalone.xml`\n**Application Configuration Path**: `Managed by mno`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 17"
        }
    },
    "1261": {
        "page_content": "**Application Configuration Path**: `Managed by mno`\n**Application Logs**: `/var/log/wildfly/prodrestintapps/server.log`\n**Access Log**: `/var/log/wildfly/prodrestintapps/access.log`\n### Online Loadbalancer farms\nThere are two active Loadbalancers for Online. The original setup routes all traffic to `prodreston`, while the later one routes conditionaly traffic between `prodreston` and `prodrestintapps`\n#### Original setup for Online\n```mermaid\n  graph TD",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 18"
        }
    },
    "1262": {
        "page_content": "#### Original setup for Online\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|prodrestonline.mno.gr:443 <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb12.mno.gr]",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 19"
        }
    },
    "1263": {
        "page_content": "B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb21.mno.gr]\n```\n#### Setup with routing for Online\nIf the request from `online` starts with `/trlogonline`:\n```mermaid\n  graph TD",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 20"
        }
    },
    "1264": {
        "page_content": "If the request from `online` starts with `/trlogonline`:\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|prodrestonline.mno.gr:443 <br> Request that starts with /trlogonline <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb12.mno.gr]",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 21"
        }
    },
    "1265": {
        "page_content": "B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8080 <br> Hostname: prodrestb21.mno.gr]\n```\nIf the request from `online` does not start with `/trlogonline`:\n```mermaid\n  graph TD",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 22"
        }
    },
    "1266": {
        "page_content": "```\nIf the request from `online` does not start with `/trlogonline`:\n```mermaid\n  graph TD\n  A[Online Backend Servers] -->|prodrestonline.mno.gr:443 <br> Request that does not start with /trlogonline <br> 999.999.999.999 | B[Load Balancer - Netscaler <br> SNIP: 999.999.999.999]\n  B -->|Active| C[pr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodrestb11.mno.gr]\n  B -->|Active| D[pr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodrestb12.mno.gr]",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 23"
        }
    },
    "1267": {
        "page_content": "B -.->|Stopped| E[dr1edge01.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodrestb21.mno.gr]\n  B -.->|Stopped| F[dr1edge02.mno.gr <br> IP: 999.999.999.999 <br> Port:8081 <br> Hostname: prodrestb21.mno.gr]\n```\n### Consolidated Network Information\n#### Production\n|   #   |     Hostname     | Prod-Rest-1 Hostname | Prod-Rest-1 IP | Prod-Rest-2 Hostname | Prod-Rest-2  IP |\n| :---: | :--------------: | :------------------: | :------------: | :------------------: | :-------------: |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 24"
        }
    },
    "1268": {
        "page_content": "|  001  | pr1edge01.mno.gr |  prodresta11.mno.gr  |  999.999.999.999  |  prodrestb11.mno.gr  |  999.999.999.999   |\n|  002  | pr1edge02.mno.gr |  prodresta12.mno.gr  |  999.999.999.999  |  prodrestb12.mno.gr  |  999.999.999.999   |\n|  003  | dr1edge01.mno.gr |  prodresta21.mno.gr  |  999.999.999.999  |  prodrestb21.mno.gr  |  999.999.999.999   |\n|  004  | dr1edge02.mno.gr |  prodresta22.mno.gr  |  999.999.999.999  |  prodrestb22.mno.gr  |  999.999.999.999   |\n#### Development",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 25"
        }
    },
    "1269": {
        "page_content": "#### Development\n|   #   |     Hostname     | Dev-Rest-1 Hostname | Dev-Rest-1 IP | Dev-Rest-2 Hostname | Dev-Rest-2 IP |\n| :---: | :--------------: | :-----------------: | :-----------: | :-----------------: | :-----------: |\n|  001  | pr1edge01.mno.gr |  devresta11.mno.gr  | 999.999.999.999  |  devrestb11.mno.gr  | 999.999.999.999  |\n|  002  | pr1edge02.mno.gr |  devresta12.mno.gr  | 999.999.999.999  |  devrestb12.mno.gr  | 999.999.999.999  |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 26"
        }
    },
    "1270": {
        "page_content": "|  003  | dr1edge01.mno.gr |  devresta21.mno.gr  | 999.999.999.999  |  devrestb21.mno.gr  | 999.999.999.999  |\n|  004  | dr1edge02.mno.gr |  devresta22.mno.gr  | 999.999.999.999  |  devrestb22.mno.gr  | 999.999.999.999  |\n#### QA\n|   #   |     Hostname     | QA-Rest-1 Hostname | QA-Rest-1 IP  | QA-Rest-2 Hostname | QA-Rest-2 IP  |\n| :---: | :--------------: | :----------------: | :-----------: | :----------------: | :-----------: |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 27"
        }
    },
    "1271": {
        "page_content": "|  001  | pr1edge01.mno.gr |  qaresta11.mno.gr  | 999.999.999.999 |  qarestb11.mno.gr  | 999.999.999.999 |\n|  002  | pr1edge02.mno.gr |  qaresta12.mno.gr  | 999.999.999.999 |  qarestb12.mno.gr  | 999.999.999.999 |\n|  003  | dr1edge01.mno.gr |  qaresta21.mno.gr  | 999.999.999.999 |  qarestb21.mno.gr  | 999.999.999.999 |\n|  004  | dr1edge02.mno.gr |  qaresta22.mno.gr  | 999.999.999.999 |  qarestb22.mno.gr  | 999.999.999.999 |\n#### Virtual IPs",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 28"
        }
    },
    "1272": {
        "page_content": "#### Virtual IPs\n|   #   |       Hostname        |               IP               |                                      Servers                                       |                                              Comment                                              |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 29"
        }
    },
    "1273": {
        "page_content": "| :---: | :-------------------: | :----------------------------: | :--------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 30"
        }
    },
    "1274": {
        "page_content": "|  001  | prodrestibank.mno.gr  | 999.999.999.999 <br> 999.999.999.999 |   mno's Loadbalancer (Netscaler) <br> Source IP for the cluster:  999.999.999.999    |                        Used for the Production servers of Internet Banking                        |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 31"
        }
    },
    "1275": {
        "page_content": "|  002  | prodrestonline.mno.gr | 999.999.999.999  <br> 999.999.999.999 |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |                             Used for the Production servers of Online                             |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 32"
        }
    },
    "1276": {
        "page_content": "|  003  |  devrestibank.mno.gr  |          999.999.999.999           |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |   Used for the QA servers of Internet Banking  <br> Accessible to all developers' workstations    |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 33"
        }
    },
    "1277": {
        "page_content": "|  004  | devrestonline.mno.gr  |          999.999.999.999           |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |        Used for the QA servers of Online  <br> Accessible to all developers' workstations         |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 34"
        }
    },
    "1278": {
        "page_content": "|  005  |  qarestibank.mno.gr   |          999.999.999.999           |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |                                            **UNUSED**                                             |",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 35"
        }
    },
    "1279": {
        "page_content": "|  006  |  qarestonline.mno.gr  |          999.999.999.999           |   mno's Loadbalancer (Netscaler)  <br> Source IP for the cluster:  999.999.999.999    |                                            **UNUSED**                                             |\n## Procedure\n### Stop a Wildfly instance - prodrestib\n1. Shutdown the Health Check endpoint:\n    - If you are in a call with mno, ask for a Network administrator to join the call",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 36"
        }
    },
    "1280": {
        "page_content": "- If you are in a call with mno, ask for a Network administrator to join the call\n    - Ask them to disable **gracefully** the server (mention the two Loadbalancer IPs and the IP you want them to disable)\n    - If there is no one from mno's networking team and **only after communicating with mno**, use the following command:\n  \n      From the server as `PRODREST`:\n      ``` bash\n      curl -XPUT https://<hostname>:8080/trlogibank/app/app-disable\n      ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 37"
        }
    },
    "1281": {
        "page_content": "``` bash\n      curl -XPUT https://<hostname>:8080/trlogibank/app/app-disable\n      ```\n    - Check access logs to ensure no traffic is sent to the Wildfly\n2. Stop the Wildfly instance\n    From the server as `root`:\n    ``` bash\n    supervisorctl stop wildfly-prodrestib\n    ```\n3. Ensure that Wildfly is down\n    From the server as `root`:\n    ``` bash\n    ps -ef | grep 'prodrestib/'\n    supervisorctl status wildfly-prodrestib\n    tail -f /var/log/wildfly/prodrestib/server.log",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 38"
        }
    },
    "1282": {
        "page_content": "supervisorctl status wildfly-prodrestib\n    tail -f /var/log/wildfly/prodrestib/server.log\n    tail -f /var/log/wildfly/prodrestib/access.log\n    ```\n### Stop a Wildfly instance - prodreston\n1. Shutdown the Health Check endpoint:\n    - If you are in a call with mno, ask for a Network administrator to join the call\n    - Ask them to disable **gracefully** the server (mention the two Loadbalancer IPs and the IP you want them to disable)",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 39"
        }
    },
    "1283": {
        "page_content": "- If there is no one from mno's networking team and **only after communicating with mno**, use the following command:\n  \n      From the server as `PRODREST`:\n      ``` bash\n      curl -XPUT https://<hostname>:8080/trlogonline/app/app-disable\n      ```\n    - Check access logs to ensure no traffic is sent to the Wildfly\n2. Stop the Wildfly instance\n    From the server as `root`:\n    ``` bash\n    supervisorctl stop wildfly-prodreston\n    ```\n3. Ensure that Wildfly is down",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 40"
        }
    },
    "1284": {
        "page_content": "``` bash\n    supervisorctl stop wildfly-prodreston\n    ```\n3. Ensure that Wildfly is down\n    From the server as `root`:\n    ``` bash\n    ps -ef | grep 'prodrestib/'\n    supervisorctl status wildfly-prodreston\n    tail -f /var/log/wildfly/prodreston/server.log\n    tail -f /var/log/wildfly/prodreston/access.log\n    ```\n### Start a Wildfly instance - prodrestib\n1. Check configuration:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 41"
        }
    },
    "1285": {
        "page_content": "```\n### Start a Wildfly instance - prodrestib\n1. Check configuration:\n    - If the server is in the DR site, check `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml` against PR site servers. The two sites are identical so the only change should be in hostnames.\n2. Start the Wildfly instance\n    From the server as `root`:\n    ``` bash\n    supervisorctl start wildfly-prodrestib\n    ```\n3. Ensure that Wildfly is up and has traffic",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 42"
        }
    },
    "1286": {
        "page_content": "supervisorctl start wildfly-prodrestib\n    ```\n3. Ensure that Wildfly is up and has traffic\n    From the server as `root`:\n    ``` bash\n    ps -ef | grep 'prodrestib/'\n    supervisorctl status wildfly-prodrestib\n    tail -f /var/log/wildfly/prodrestib/server.log\n    tail -f /var/log/wildfly/prodrestib/access.log\n    ```\n### Start a Wildfly instance - prodreston\n1. Check configuration:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 43"
        }
    },
    "1287": {
        "page_content": "```\n### Start a Wildfly instance - prodreston\n1. Check configuration:\n    - If the server is in the DR site, check `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml` against PR site servers. The two sites are identical so the only change should be in hostnames.\n2. Start the Wildfly instance\n    From the server as `root`:\n    ``` bash\n    supervisorctl start wildfly-prodreston\n    ```\n3. Ensure that Wildfly is up and has traffic",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 44"
        }
    },
    "1288": {
        "page_content": "supervisorctl start wildfly-prodreston\n    ```\n3. Ensure that Wildfly is up and has traffic\n    From the server as `root`:\n    ``` bash\n    ps -ef | grep 'prodreston/'\n    supervisorctl status wildfly-prodreston\n    tail -f /var/log/wildfly/prodreston/server.log\n    tail -f /var/log/wildfly/prodreston/access.log\n    ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_wildfly.md - Part 45"
        }
    },
    "1289": {
        "page_content": "# Benchmarking HBASE on Lab with YCSB Tool\n## Introduction",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 1"
        }
    },
    "1290": {
        "page_content": "## Introduction\nOur purpose was to run performance tests on a created Hbase table on Lab environment and document the results which will be used as a point of reference to evaluate the efficacy of quotas that will be applied. After running tests with no quotas, we run the same tests after setting firstly read throtttle quotas and secondly write quotas. We implemented the following procedure on jakarta node (999.999.999.999). Kinit with hbase keytab was a prerequisite.\n## Hbase table creation",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 2"
        }
    },
    "1291": {
        "page_content": "## Hbase table creation\n- Created an hbase table using pre-splitting strategy to ensure write operations target region servers uniformly\n  \n    ```bash\n    hbase shell\n    n_splits = 300 # HBase recommends (10 * number of regionservers, in our case 3 regionservers)\n    create 'usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}}\n    ```\n## Installing YCSB Tool and system configuration",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 3"
        }
    },
    "1292": {
        "page_content": "```\n## Installing YCSB Tool and system configuration\n- Get latest release of ycsb-0.17.0.tar.gz from https://github.com/brianfrankcooper/YCSB/releases/tag/0.17.0 on jakarta\n- Unzip file\n    ```bash\n    tar xfvz ycsb-0.17.0.tar.gz\n    ```\n- Specify a HBase config directory containing  hbase-site.xml\n    ```bash\n    mkdir -p  /HBASE-HOME-DIR/conf\n    cd /HBASE-HOME-DIR/conf\n    cp /etc/hbase/conf/hbase-site.xml .\n    ```\n- Get to YCSB directory\n    ```bash\n    cd ycsb-0.17.0\n    ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 4"
        }
    },
    "1293": {
        "page_content": "```\n- Get to YCSB directory\n    ```bash\n    cd ycsb-0.17.0\n    ```\n## Performance tests on Hbase with YCSB before setting quotas\n- Load the data\n    ```bash\n    bin/ycsb load hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table=usertable -p columnfamily=family\n    ```\n- Execute the workload requesting a time series with 1 sec granularity and directing output to a datafile\n    ```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 5"
        }
    },
    "1294": {
        "page_content": "```bash\n    bin/ycsb run hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table=usertable -p columnfamily=family -p measurementtype=timeseries -p timeseries.granularity=1000 -s > workloada.dat\n    ```\n- Delete and recreate table to repeat the aforementioned steps with workload{b-f} changing workload and datafiles respectively\n  ```bash\n  hbase shell\n  disable 'usertable'\n  drop 'usertable'\n  exists 'usertable'\n  n_splits = 300",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 6"
        }
    },
    "1295": {
        "page_content": "hbase shell\n  disable 'usertable'\n  drop 'usertable'\n  exists 'usertable'\n  n_splits = 300\n  create 'usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n  ```\n## Create namespace and set throttle quotas\n- Create namespace\n    ```bash\n    hbase shell\n    create_namespace 'quotas_test'\n    list_namespace\n    ```\n- Create table in the namespace\n    ```bash\n    hbase shell",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 7"
        }
    },
    "1296": {
        "page_content": "list_namespace\n    ```\n- Create table in the namespace\n    ```bash\n    hbase shell\n    n_splits = 300 # HBase recommends (10 * number of regionservers, in our case 3 regionservers)\n    create 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}}\n    ```\n- Set throttle quotas of type 'read'\n    ```bash\n    hbase shell\n    set_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => 'quotas_test', LIMIT => '20req/sec'\n    list_quotas",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 8"
        }
    },
    "1297": {
        "page_content": "list_quotas\n    ```\n## Performance tests on Hbase with YCSB after setting quotas of type 'read'\n- Load the data\n    ```bash\n    bin/ycsb load hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table='quotas_test:usertable' -p columnfamily=family\n    ```\n- Execute the workload requesting a time series with 1 sec granularity and directing output to a datafile\n    ```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 9"
        }
    },
    "1298": {
        "page_content": "```bash\n    bin/ycsb run hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table='quotas_test:usertable' -p columnfamily=family -p measurementtype=timeseries -p timeseries.granularity=1000 -s > workloada_read_quotas.dat\n    ```\n- Delete and recreate table to repeat the aforementioned steps with workload{b-f} changing workload and datafiles respectively\n  ```bash\n  hbase shell\n  disable 'quotas_test:usertable'\n  drop 'quotas_test:usertable'\n  exists 'quotas_test:usertable'",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 10"
        }
    },
    "1299": {
        "page_content": "disable 'quotas_test:usertable'\n  drop 'quotas_test:usertable'\n  exists 'quotas_test:usertable'\n  n_splits = 300\n  create 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n  ```\n## Remove read quotas and set write quotas\n- Remove read quotas\n    ```bash\n    hbase shell\n    set_quota TYPE => THROTTLE, NAMESPACE => 'quotas_test', LIMIT => NONE\n    list_quotas\n    ```\n- Set write quotas\n    ```bash\n    hbase shell",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 11"
        }
    },
    "1300": {
        "page_content": "list_quotas\n    ```\n- Set write quotas\n    ```bash\n    hbase shell\n    set_quota TYPE => THROTTLE, THROTTLE_TYPE => WRITE, NAMESPACE => 'quotas_test', LIMIT => '20req/sec'\n    list_quotas\n    ```\n-  Delete and recreate table to repeat to run tests with write quotas\n    ```bash\n    hbase shell\n    disable 'quotas_test:usertable'\n    drop 'quotas_test:usertable'\n    exists 'quotas_test:usertable'\n    n_splits = 300",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 12"
        }
    },
    "1301": {
        "page_content": "drop 'quotas_test:usertable'\n    exists 'quotas_test:usertable'\n    n_splits = 300\n    create 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n    ```\n## Performance tests on Hbase with YCSB after setting quotas of type 'write'\n- Load the data\n    ```bash\n    bin/ycsb load hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table='quotas_test:usertable' -p columnfamily=family\n    ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 13"
        }
    },
    "1302": {
        "page_content": "```\n- Execute the workload requesting a time series with 1 sec granularity and directing output to a datafile\n    ```bash\n    bin/ycsb run hbase20 -P workloads/workloada -cp /HBASE-HOME-DIR/conf -p table='quotas_test:usertable' -p columnfamily=family -p measurementtype=timeseries -p timeseries.granularity=1000 -s > workloada_write_quotas.dat\n    ```\n- Delete and recreate table to repeat the aforementioned steps with workload{b-f} changing workload and datafiles respectively\n  ```bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 14"
        }
    },
    "1303": {
        "page_content": "```bash\n  hbase shell\n  disable 'quotas_test:usertable'\n  drop 'quotas_test:usertable'\n  exists 'quotas_test:usertable'\n  n_splits = 300\n  create 'quotas_test:usertable', 'family', {SPLITS => (1..n_splits).map {|i| \"user#{1000+i*(9999-1000)/n_splits}\"}\n  ```\n**References:**\nhttps://github.com/brianfrankcooper/YCSB#ycsb\nhttps://github.com/brianfrankcooper/YCSB/tree/master/hbase2\nhttps://github.com/brianfrankcooper/YCSB/wiki/Running-a-Workload",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 15"
        }
    },
    "1304": {
        "page_content": "https://github.com/brianfrankcooper/YCSB/wiki/Running-a-Workload\nhttps://github.com/brianfrankcooper/YCSB/wiki/Core-Workloads\nhttps://blog.cloudera.com/hbase-performance-testing-using-ycsb/",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "hbase_benchmarking.md - Part 16"
        }
    },
    "1305": {
        "page_content": "# Below procedure describes how to decrypt an encrypted disk\n###### Back up \nBackup data of encrypted disk\n>Ndef_1: In our case we don't have data in this partition. So, we don't have to backup\n```bash\ntar zcvf /backup/data_1-$(date +%Y-%m-%d).tar.gz /data/1\n```\n###### Decrypt encrypted disk\n1. Make sure that Kafka and Kudu services are down\n>Ndef_2: You should stop kafka and kudu in case we have data at `/data/1` partition. In our case we don't have data so we skip this step",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "navencrypt_disk_decryption.md - Part 1"
        }
    },
    "1306": {
        "page_content": "- From Cloudera Manager > Kafka > Stop\n- From Cloudera Manager > Kudu > Stop\n2. Check that KTS is up and running\nFrom Cloudera Manager with admin account:\n- Go to Keytrustee > Key Trustee Server  \n3. Remove /data/1 mountpoint that is no longer in use\n```bash\nnavencrypt-prepare --undo-force /data/1\n```\n4. Check ztab:\n```bash\ncat /etc/navencrypt/ztab | grep /data/1\n```\nThe output should be commented \n5. List the mountpoints\n```bash\nmount -l\n```\n6. Uncomment the decrypted mount points on fstab",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "navencrypt_disk_decryption.md - Part 2"
        }
    },
    "1307": {
        "page_content": "5. List the mountpoints\n```bash\nmount -l\n```\n6. Uncomment the decrypted mount points on fstab\nUncomment line for `/dev/mapper/ol_pr1edge01-data_1 /data/1 xfs defauts 0 0` at `/etc/fstab`\n7. Check if disk is mounted with below command\n```bash\nmount -a\n```\n8. Move data from backup directory back to decrypted disk\n>Ndef_3: Occurs only if step 1 is performed\n```bash\ntar -xvf /backup/data_1.tar.gz -C /data/1\n```\n9. Start kudu and kafka\n>Ndef_4: Occurs only if step 1 is performed",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "navencrypt_disk_decryption.md - Part 3"
        }
    },
    "1308": {
        "page_content": "```\n9. Start kudu and kafka\n>Ndef_4: Occurs only if step 1 is performed \n- From Cloudera Manager > Kafka > Start\n- From Cloudera Manager > Kudu > Start",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "navencrypt_disk_decryption.md - Part 4"
        }
    },
    "1309": {
        "page_content": "# Java Upgrade\n[[_TOC_]]\nThis document outlines the upgrade process of minor java version for Oracle Java 1.8\non mno's edge nodes. All procedures pertain to PR and DR edge nodes, except the RPM repository\ncreation which is performed on pr1node1:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Repository Creation\nThis step only needs to be performed once as all subsequent RPM's will be placed inside this\nrepository. SSH into **p1node01** and as root create the repository directories:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "java_upgrade.md - Part 1"
        }
    },
    "1310": {
        "page_content": "repository. SSH into **p1node01** and as root create the repository directories:\n    $ ssh Exxxx@pr1node01\n    $ sudo -i\n    # mkdir -p /var/www/html/oracle_java/Packages\nDownload the desired RPMs from [Oracle Java SE Archive Downloads](https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html), place them inside\n`/var/www/html/oracle_java/Packages` and create the repository:\n    # cd /var/www/html/oracle_java\n    # createrepo .",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "java_upgrade.md - Part 2"
        }
    },
    "1311": {
        "page_content": "# cd /var/www/html/oracle_java\n    # createrepo .\nSSH into one of the edge nodes, create the corresponding yum repo file and **scp** it into\nall other edge nodes:\n    $ ssh Exxx@pr1edge01\n    $ sudo -i\n    # vi /etc/yum.repos.d/oracle_java.repo\n    [oracle_java]\n    name = oracle_java\n    baseurl =  http://p1node01.mno.gr/oracle_java\n    enabled = 1\n    gpgcheck = 0\n    # scp /etc/yum.repos.d/oracle_java.repo XXXedgeXX:/etc/yum.repos.d/\nFinally on each edge node install the above packages:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "java_upgrade.md - Part 3"
        }
    },
    "1312": {
        "page_content": "Finally on each edge node install the above packages:\n    # yum clean all\n    # yum install jdk-1.8\n## Repository Update\nDownload the desired version of Oracle Java 8 from\n[Oracle Java SE Archive Downloads](https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html) or\n[Oracle Java SE Downloads](https://www.oracle.com/java/technologies/downloads/) and\nplace the RPMs inside `/var/www/html/oracle_java/Packages` of **pr1node01**. Login to",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "java_upgrade.md - Part 4"
        }
    },
    "1313": {
        "page_content": "place the RPMs inside `/var/www/html/oracle_java/Packages` of **pr1node01**. Login to\n**pr1node01** and update the repository with the new packages:\n    $ ssh Exxxx@pr1node01\n    $ sudo -i\n    # cd /var/www/html/oracle_java\n    # createrepo --update .\n## Edge Host Update\n### Preparation\nBefore upgrading the edge nodes, their resources must be moved to other nodes and a backup\nof the old java be made. Login to each edge node:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "java_upgrade.md - Part 5"
        }
    },
    "1314": {
        "page_content": "of the old java be made. Login to each edge node:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i\n    # cp -rap /usr/java/jdk1.8.0_<old version>-amd64/  /usr/java/jdk1.8.0_<old version>-amd64.bak/\nAnd follow the procedures described in the **Switchover of Cluster Resources** chapter\nof the **Security Vulnerabilities** MOP [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx).\n### Execution",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "java_upgrade.md - Part 6"
        }
    },
    "1315": {
        "page_content": "### Execution\nInside each edge node, update the java package using **YUM**:\n    # yum clean all\n    # yum update java-1.8\nCopy the old certificates into the new installation directory and run the update alternatives\ntool where you input the new version when prompted:\n    # cp -ap /usr/java/jdk1.8.0_<old version>-amd64.bak/jre/lib/security/jssecacerts \\\n        /usr/java/jdk1.8.0_<new version>-amd64/jre/lib/security/\n    # update alternatives --config java * javac\n    # java -version",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "java_upgrade.md - Part 7"
        }
    },
    "1316": {
        "page_content": "# update alternatives --config java * javac\n    # java -version\nIf everything is OK unstandby the node and check each wildfly instance's access and\nserver logs for the following:\n- `/var/log/wildfly/*/server.log`: There are no undeployed WARs\n- `/var/log/wildfly/*/access.log`: Everything is reporting HTTP 200 status\nDetailed wildfly information and management instructions can be found",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "java_upgrade.md - Part 8"
        }
    },
    "1317": {
        "page_content": "Detailed wildfly information and management instructions can be found\n[here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/manage_wildfly.md).\n## Rollback\nLogin to each edge node and downgrade using the update-alternatives and inputting the\nprevious version:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i\n    # update alternatives --config java * javac\n    # java -version",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "java_upgrade.md - Part 9"
        }
    },
    "1318": {
        "page_content": "# Failover\n## Scope\nIn case that the active site faces multiple issues that cannot be resolved in a small amount of time, we need to failover applications and procedures to the standby one. \n## Setup\nTwo symmetrical clusters have been setup named production (PR) and disaster (DR). Streaming and batch procedures are running in both sites. External traffic and UC4 flows however are only active in one of them. \n## Procedure\n### Stop streaming procedures",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 1"
        }
    },
    "1319": {
        "page_content": "## Procedure\n### Stop streaming procedures\n1. Stop production IBank, Online Spark topologies:\n    - Login with your personal account at `dr1edge01` or `pr1edge01`, based on the site that will be stopped.\n    - Switch user to `PRODREST`.\n    - Comment lines in crontab that run `/opt/ingestion/PRODREST/common/scripts/restart_topology_STABLE.sh` and `/opt/ingestion/PRODREST/common/scripts/restart_visible_topology.sh`.\n    - Create `SHUTDOWN` markers for the Spark topologies.\n      \n      ``` bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 2"
        }
    },
    "1320": {
        "page_content": "- Create `SHUTDOWN` markers for the Spark topologies.\n      \n      ``` bash\n      [PRODREST@Xr1edge01]# touch SHUTDOWN\n      [PRODREST@Xr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\n      [PRODREST@Xr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown_marker/\n      [PRODREST@Xr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\n      ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 3"
        }
    },
    "1321": {
        "page_content": "```\n    - Wait for 5 minutes and check that the above applications are no longer running.\n      ``` bash\n      [PRODREST@Xr1edge01]# yarn application \u2013list | grep PRODUSER\n      ```\n1. Stop development IBank, Online Spark topologies:\n    - Login with your personal account at `dr1edge01`. **This is done only on DR site**\n    - Switch user to `DEVREST`.\n    - Comment line in crontab that run `/opt/ingestion/DEVREST/common/scripts/restart_topology.sh`.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 4"
        }
    },
    "1322": {
        "page_content": "- Comment line in crontab that run `/opt/ingestion/DEVREST/common/scripts/restart_topology.sh`.\n    - Create `SHUTDOWN` markers for the Spark topologies.\n      \n      ``` bash\n      [DEVREST@dr1edge01]# touch SHUTDOWN\n      [DEVREST@dr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/DEVREST/service/DEV_IBank_Ingest/topology_shutdown_marker/\n      [DEVREST@dr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/DEVREST/service/DEV_Online_Ingest/topology_shutdown_marker/\n      ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 5"
        }
    },
    "1323": {
        "page_content": "```\n    - Wait for 5 minutes and check that the above applications are no longer running.\n      ``` bash\n      [DEVREST@dr1edge01]# yarn application \u2013list | grep DEVREST\n      ```\n### Stop batch procedures\n1. Disable daily and hourly IBank production batch jobs:\n    - Login with your personal account at `dr1edge01` or `pr1edge01`, based on the site that will be stopped.\n    - Switch user to `PRODREST`.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 6"
        }
    },
    "1324": {
        "page_content": "- Switch user to `PRODREST`.\n    - Comment lines in crontab that run `/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh` and `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh`.\n    - Check that batch jobs are not already running.\n      ``` bash\n      [PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh'",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 7"
        }
    },
    "1325": {
        "page_content": "[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh'\n      ```\n    - If they are already running wait for them to stop.\n  \n2. Disable daily and hourly Online production batch jobs:\n    - Login with your personal account at `dr1edge01` or `pr1edge01`, based on the site that will be stopped.\n    - Switch user to `PRODREST`.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 8"
        }
    },
    "1326": {
        "page_content": "- Switch user to `PRODREST`.\n    - Comment lines in crontab that run `/opt/ingestion/PRODREST/common/scripts/online_daily_batch_jobs_STABLE.sh` and `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh`.\n    - Check that batch job is not already running.\n      ``` bash\n      [PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/common/scripts/online_daily_batch_jobs_STABLE.sh'",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 9"
        }
    },
    "1327": {
        "page_content": "[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh'\n      ```\n    - If they are already running wait for them to stop.\n3. Disable daily IBank, Online development batch jobs:\n    - Login with your personal account at `dr1edge01`. **This is done only on DR site**\n    - Switch user to `DEVREST`.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 10"
        }
    },
    "1328": {
        "page_content": "- Switch user to `DEVREST`.\n    - Comment lines that run `/opt/ingestion/DEVREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs.sh` in crontab.\n    - Check that batch jobs are not already running.\n      ``` bash\n      [DEVREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/DEVREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs.sh'\n      ```\n    - If they are already running wait for them to stop.\n### Migrate traffic between DR/PR",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 11"
        }
    },
    "1329": {
        "page_content": "```\n    - If they are already running wait for them to stop.\n### Migrate traffic between DR/PR\n1. Start `prodrestib` Wildfly instances at both edge nodes of the other site using this [procedure](manage_wildfly.md#start-a-wildfly-instance-prodrestib).\n2. Start `prodreston` Wildfly instances at both edge nodes of the other site using this [procedure](manage_wildfly.md#start-a-wildfly-instance-prodreston).\n3. Ask for a mno Network administrator to make a call.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 12"
        }
    },
    "1330": {
        "page_content": "3. Ask for a mno Network administrator to make a call.\n   \n4. Ask them to enable the new servers (mention the Loadbalancer IPs and the IP you want them to enable as explained [here](manage_wildfly.md#consolidated-network-information)).\n   \n5. Check logs for both Wildfly instances at both servers to ensure everything works.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 13"
        }
    },
    "1331": {
        "page_content": "5. Check logs for both Wildfly instances at both servers to ensure everything works.\n   \n6. When you are certain everything is OK, ask the mno Network administrators to disable the prexisting servers (mention the Loadbalancer IPs and the IP you want them to disable).\n   \n7. From the access logs of the prexisting Wildfly instances check that no traffic is received.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 14"
        }
    },
    "1332": {
        "page_content": "7. From the access logs of the prexisting Wildfly instances check that no traffic is received. \n   \n8. Stop these Wildfly instances as described in the procedures [here](manage_wildfly.md#stop-a-wildfly-instance-prodrestib) and [here](manage_wildfly.md#stop-a-wildfly-instance-prodreston).\n### Migrate UC4 flows between PR/DR\n1. Login to the edge servers of the active and passive site using your personal account and become `root`.\n2. Stop UC4 agent at the edge nodes of the active site.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 15"
        }
    },
    "1333": {
        "page_content": "2. Stop UC4 agent at the edge nodes of the active site.\n   \n  ``` bash\n  systemctl stop uc4agent\n  ```\n3. Start service for UC4 agent at the edge servers of the passive site.\n  ``` bash\n  systemctl start uc4agent\n  ```\n4. Add entries for last successful execution of IBank DataWarehouse at the edge servers of the passive site.\n  ``` bash\n  sudo -u PRODUSER /opt/ingestion/PRODUSER/datawarehouse-ibank/insert_rows_dwh_monitoring.sh <date> \n  # Previous day date (YYYYMMdd), unless Sunday or Monday",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 16"
        }
    },
    "1334": {
        "page_content": "# Previous day date (YYYYMMdd), unless Sunday or Monday\n  # If Sunday or Monday enter the date of last Friday\n  ```\n5. Migrate the creation of trigger files for external jobs\n  - On the active site:\n    ``` bash\n    vi /opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh\n    # Comment the followin lines along with the assosiated checks\n    # touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 17"
        }
    },
    "1335": {
        "page_content": "# touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY\n    # touch /opt/applications/landing_zone/PRODUSER/triggers/IBANK_SA_`date '+%Y%m%d'`.READY\n    ```  \n  - On the passive site:\n    ``` bash\n    vi /opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh\n    # Uncomment the followin lines along with the assosiated checks\n    # touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 18"
        }
    },
    "1336": {
        "page_content": "# touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY\n    # touch /opt/applications/landing_zone/PRODUSER/triggers/IBANK_SA_`date '+%Y%m%d'`.READY\n    ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "failover.md - Part 19"
        }
    },
    "1337": {
        "page_content": "# Certificate Renewal Procedure\nBack up every certificate before doing any action\n### Backup Procedure\n- From node1 as root:\n``` \ndcli -C \"cp -r /opt/cloudera/security /opt/cloudera/security.BAK_`date +%d%b%Y%H%M%S`\" \n```\n- From edge nodes as root:\n```\ncp -r /opt/haproxy/security /opt/haproxy/security.BAK_`date +%d%b%Y%H%M%S`\n```\n## Node and VIP Certificates check\n### Check unsigned certificates",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 1"
        }
    },
    "1338": {
        "page_content": "```\n## Node and VIP Certificates check\n### Check unsigned certificates\n- In this step we checked the presigned certificates in `backup/new_certs/cert_requests` if they have correct subject with the following command\n`openssl req -in new_certs/cert_requests/dr1edge.mno.gr-ert-file -noout -text`\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/certreq.JPG)\nand also we check the modulus if it is the same. Basically we check the output of the following commands:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 2"
        }
    },
    "1339": {
        "page_content": "`openssl rsa -in /opt/cloudera/security/x509/node.key -noout -text modulus`\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/modulus1.JPG)\n`openssl req -in new_certs/cert_requests/dr1edge.mno.gr-ert-file -noout -text -modulus`\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/modulus2.JPG)\nThen we provide to the customer the certificates located in `backup/new_certs/cert_requests` in order to be signed\n### Check signed certificates from mno",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 3"
        }
    },
    "1340": {
        "page_content": "### Check signed certificates from mno\nIn the following folder are located the signed certificates\nBackup NFS Folder: `/backup/new_certs/certificates`\nCheck the certificates in the above mentioned folder for issuer , subject ,TLS Web, date\nThe `'ln -1'` feature prints all files in the for loop per line\n- Check the issuer\n`for i in 'ln -1'; do echo $i; openssl x509 -noout -text -in $i | grep -i issuer ; done` \nIn the above command we wait a return such as this:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 4"
        }
    },
    "1341": {
        "page_content": "In the above command we wait a return such as this: \n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/issuer.JPG)\n- Check the subject\n`for i in 'ln -1'; do echo $i; openssl x509 -noout -text -in $i | grep -i subject ; done`\nIn the above command we wait a return such as this: \n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/subject.JPG)\n- Check the TLS Web",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 5"
        }
    },
    "1342": {
        "page_content": "- Check the TLS Web\n`for i in 'ln -1'; do echo $i; openssl x509 -noout -text -in $i | grep -i 'TLS Web' ; done` \nIn the above command we wait a return such as this: \n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/tls.JPG)\n- Check the dates\n`openssl x509 -noout -text -in 'cert_file' - dates`\nIn the above command we wait a return such as this: \n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/dates.JPG)\n - Or with a for loop for all the files",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 6"
        }
    },
    "1343": {
        "page_content": "- Or with a for loop for all the files\n`for i in 'ln -1'; do openssl x509 -noout -text -in $i | grep -i 'ndef after'; done`\nIn the above command we wait a return such as this: \n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/notafter.JPG)\n### Haproxy certificates check and replacement\nBackup NFS Folder: `/backup/haproxy_certs`\n`ssh root@pr1edge01`\nIn order to set the new haproxy certificates we need to have 9 certificates",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 7"
        }
    },
    "1344": {
        "page_content": "`ssh root@pr1edge01`\nIn order to set the new haproxy certificates we need to have 9 certificates\nCheck the haproxy security folder : `/opt/haproxy/security/x509/`\n```\ndevsqla_mno_gr.haproxy.pem\npr1edge_mno_gr.haproxy.pem\ndr1edge_mno_gr.haproxy.pem\nqasqla_mno_gr.haproxy.pem\nprodsqla_mno_gr.haproxy.pem\n```\nand the node certifate for PR and DR in the following format \n`node.haproxy.pem`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 8"
        }
    },
    "1345": {
        "page_content": "```\nand the node certifate for PR and DR in the following format \n`node.haproxy.pem`\n- Now in the NFS mentioned files we need to replace the second certificate with the one that is located in the signed cert files that the customer has send to us\n`vi /backup/haproxy_certs/devsqla_mno_gr.haproxy.pem` and replace the \n```\n--- BEGIN CERTIFICATE --- \n... \n--- END CERTIFICATE ---\n```\n with the one located in `/backup/new_certs/certificates/devsqla_mno_gr-cert-file.cer`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 9"
        }
    },
    "1346": {
        "page_content": "```\n with the one located in `/backup/new_certs/certificates/devsqla_mno_gr-cert-file.cer`\n- Moreover, as root replace the CERTIFICATE to the\n`vi /backup/haproxy_certs/devsqla_mno_gr.haproxy.pem`\nwith the certificate from \n\u00a0\u00a0\u00a0 `cat /backup/new_certs/certificates/devsql_mno_gr-cert-file.cer` \nand copy the section\n```\n\u00a0\u00a0\u00a0 ---BEGIN CERTIFICATE---\n\u00a0\u00a0\u00a0 .....\n\u00a0\u00a0\u00a0 ---END CERTIFICATE---\n```\n\u00a0\u00a0\u00a0 and replace it with the certificate on the pem file `/backup/haproxy_certs/devsqla_mno_gr.haproxy.pem`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 10"
        }
    },
    "1347": {
        "page_content": "For all the other `pem` files we need to do the same procedure accordingly. **EVERY PEM HAS A UNIQUE CER FILE**\nWe need to specify in more detail the above steps especially for the 4 edge nodes we have on PR & DR sites.\n- Firstly, under `/opt/haproxy/security/x509/` folder there is the `node.haproxy.pem` certificate as mentioned before. We must copy this file under the NFS mentioned folder `/backup/haproxy_certs/`. \nFor example:\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 11"
        }
    },
    "1348": {
        "page_content": "For example:\n```\ncp /opt/haproxy/security/x509/node.haproxy.pem /backup/haproxy_certs/Xredge0X.node.haproxy.pem\n```\n- Then, from `/backup/haproxy_certs/Xredge0X.node.haproxy.pem` we must keep only the private key certificate section and replace the rest of the certificates with the ones that are located in the signed cert file `/backup/new_certs/certificates/Xr1edge0X_mno_gr-cert-file.cer` that the customer has send to us.\n- We must follow the same procedure for all edge nodes certificates.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 12"
        }
    },
    "1349": {
        "page_content": "- We must follow the same procedure for all edge nodes certificates.\n#### Checks\n- Check the issuer on previous certificates located in `/etc/pki/ca-trust/source/anchors/`\n```\nca1.crt\nca.crt\nca3.crt\n```\n- Check the issuer in the above mentioned crt\n`for i in ca1.crt ca3.crt ca.crt; do echo $i; openssl x509 -noout -text -in $i | grep -i issuer; done`\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/pki.JPG)",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 13"
        }
    },
    "1350": {
        "page_content": "![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/pki.JPG)\nFrom the above image we excepted to see the issuer as `mnoInternalRoot` which is correct in `ca3.crt`.\n- In order to check if the certificates has been signed from the same issuer. Run the following commands and check the output. It should be the same . If not, the certificate is wrong\n```\nopenssl x509 -noout -modulus -in 'cert_file'\nopenssl rsa -noout -modulus -in 'cert_file'\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 14"
        }
    },
    "1351": {
        "page_content": "```\nopenssl x509 -noout -modulus -in 'cert_file'\nopenssl rsa -noout -modulus -in 'cert_file'\n```\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/x509.JPG)\n### Actions Before Distributing the certificates\nmno is obliged to move the traffic from PR site to DR site.\nStop the flows, as user PRODREST:\n```\n[PRODREST@Xr1edge01]# touch SHUTDOWN\n[PRODREST@Xr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 15"
        }
    },
    "1352": {
        "page_content": "[PRODREST@Xr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown_marker/\n[PRODREST@Xr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\n```\nCheck that flows stopped.\n```\n[PRODREST@Xr1edge01]# yarn application \u2013list | grep -i PROD_\n```\nWhen executing the same procedure on the DR site, we should **additionally** stop the following flows as user DEVREST:\n```\n[DEVREST@dr1edge01]# touch SHUTDOWN",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 16"
        }
    },
    "1353": {
        "page_content": "```\n[DEVREST@dr1edge01]# touch SHUTDOWN\n[DEVREST@dr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/DEVREST/service/DEV_IBank_Ingest/topology_shutdown_marker/\n[DEVREST@dr1edge01]# hdfs dfs \u2013put SHUTDOWN /user/DEVREST/service/DEV_Online_Ingest/topology_shutdown_marker/\n```\nCheck that flows stopped.\n```\n[DEVREST@dr1edge01]# yarn application \u2013list | grep DEVREST\n```\n## Distribute the certificates\n### Generate the keystore password (It's not the same for both sites)",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 17"
        }
    },
    "1354": {
        "page_content": "## Distribute the certificates\n### Generate the keystore password (It's not the same for both sites)\n`bdacli getinfo cluster_https_keystore_password`\nFrom node01:\n#### Node certificates\nFor internal nodes:\n```\ndcli -C cp /backup/new_certs/certificates/\\$HOSTNAME-cert-file.cer /opt/cloudera/security/x509/node.cert\n```\nFor edge nodes:\n```\ncp /backup/new_certs/cert_2024/$HOSTNAME-cert-file.cer /opt/cloudera/security/x509/node.cert\n```\n#### JKS certificates\nFor internal nodes:\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 18"
        }
    },
    "1355": {
        "page_content": "```\n#### JKS certificates\nFor internal nodes:\n```\ndcli -C keytool -import -file /opt/cloudera/security/x509/node.cert -alias \\$HOSTNAME -keystore /opt/cloudera/security/jks/node.jks -storepass KEYSTORE_PASS_FROM_ABOVE -keypass KEYSTORE_PASS_FROM_ABOVE -noprompt\n```\nFor edge nodes:\n```\nkeytool -import -file /opt/cloudera/security/x509/node.cert -alias $HOSTNAME -keystore /opt/cloudera/security/jks/node.jks -storepass KEYSTORE_PASS_FROM_ABOVE -keypass KEYSTORE_PASS_FROM_ABOVE -noprompt\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 19"
        }
    },
    "1356": {
        "page_content": "```\n#### Check new certificates\nFor internal nodes:\n \n```\ndcli -C \"keytool -list -v -keystore /opt/cloudera/security/jks/node.jks -alias \\$HOSTNAME\"\n```\nFor edge nodes:\n \n```\nkeytool -list -v -keystore /opt/cloudera/security/jks/node.jks -alias $HOSTNAME\n```\n#### Haproxy certificates\nCopy the files from `/backup/haproxy_certs/` to `/opt/haproxy/security/x509/` and replace the existing ones.\n**Special caution**:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 20"
        }
    },
    "1357": {
        "page_content": "**Special caution**:\nMust copy `Xr1edge0X_mno_gr.node.haproxy.pem` that we created in the previous steps to the `node.haproxy.pem` certificate existing on the edge nodes\n```\ncp /backup/haproxy_certs/Xr1edge0X_mno_gr.node.haproxy.pem /opt/haproxy/security/x509/node.haproxy.pem\n```\n**Do not copy root.inter.pem**\nAfter copying the certificates, restart the haproxy service on both edge nodes\n```\nsystemctl reload haproxy \nsystemctl status haproxy\npcs resource cleanup haproxy-clone`\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 21"
        }
    },
    "1358": {
        "page_content": "```\nsystemctl reload haproxy \nsystemctl status haproxy\npcs resource cleanup haproxy-clone`\n```\nIn case of an error that the node haproxy certificate is not valid after the restart, do the following :\n```\ncd /opt/cloudera/security/x509\ncat node.hue.key node.cert > /opt/haproxy/security/x509/node.haproxy.pem\n```\n### Actions After Distributing the certificates",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 22"
        }
    },
    "1359": {
        "page_content": "```\n### Actions After Distributing the certificates\nWhen the new certificates replace the old ones, the services of the cluster will become healthy. Restart is required. [Official Guide](https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cm_mc_start_stop_service.html)\nAll services except zookeeper need restart. The service `Bigdatamanager` should always remain stopped",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 23"
        }
    },
    "1360": {
        "page_content": "We prefer to start with Kudu because it takes longer to synchronize. Also, for KMS service perform restart 1 by 1. For KMS server perform restart on both passive services and then on both active.  \nLastlty, after Kudu syncs start the flows.\nWhen the cluster be stopped then:\n \nFor edge nodes:\n \n```\nsystemctl status cloudera-scm-agent\nsystemctl restart cloudera-scm-agent \n```\nFor internal nodes:\n \n```\ndcli -C \"systemctl status cloudera-scm-agent | grep -i Active\"",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 24"
        }
    },
    "1361": {
        "page_content": "```\nFor internal nodes:\n \n```\ndcli -C \"systemctl status cloudera-scm-agent | grep -i Active\" \ndcli -C \"systemctl restart cloudera-scm-agent\" \ndcli -C \"systemctl status cloudera-scm-agent | grep -i Active\"\n```\n \n```\ndcli -c Xr1node03 \"systemctl restart cloudera-scm-server\" \ndcli -c Xr1node03 \"systemctl status cloudera-scm-server\"\n```\n### Kudu Checks\nOpen UIs from masters and tablets from internal firefox from edge nodes. Firefox launced with your personal Exxx account.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 25"
        }
    },
    "1362": {
        "page_content": "Logs from kudu logs on every node:\n`tail -f /var/log/kudu/kudu-tserver.INFO` checks that the number will be equal to this number => `ls /u12/kudu/tablet/data/data/ | grep metadata | wc -l`, when the synchronization is successfully completed.\n#### Start flows\nStart ibank from edge Node as PRODREST\n \n```\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\n```\nStart ibank visible from edge Node as PRODREST\n \n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 26"
        }
    },
    "1363": {
        "page_content": "```\nStart ibank visible from edge Node as PRODREST\n \n```\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_VISIBLE_STABLE.sh\n```\n \nStart online from edge Node as PRODREST\n \n```\n/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\n```\nSimilarly from a DR edge node as DEVREST:\nStart ibank\n```\n/opt/ingestion/DEVREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno.sh\n```\nStart online\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 27"
        }
    },
    "1364": {
        "page_content": "```\nStart online\n```\n/opt/ingestion/DEVREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno.sh\n```\n### Applications checks\nWhen all Kudu Tablets are synchronized and **all flows are on RUNNING status** then:\n \nas user PRODREST from an edge node:\n```\nimpala-shell xr1edge.mno.gr -k -ssl\n```\nExecute the following query:\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 28"
        }
    },
    "1365": {
        "page_content": "```\nimpala-shell xr1edge.mno.gr -k -ssl\n```\nExecute the following query:\n```\nselect max(timestamp) as time, 'ibank' as application from prod_trlog_ibank.service_audit_stream union select max(timestamp) as time, 'online' as application from prod_trlog_online.service_audit_stream;\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "certificate_renewal_procedure.md - Part 29"
        }
    },
    "1366": {
        "page_content": "# Manage HBase Quotas\n## Enable HBase quotas\n1. Go to ```Cloudera Manager => HBase => Configuration => HBase Service Advanced configuration Snippet (Safety Valve) for hbase-site.xml```\n2. Add the following configuration:\n   ```\n   Name: hbase.quota.enabled\n   Value: true\n   Description: enable hbase quotas\n   ```\n3. Restart HBase service\n## Add HBase quotas to a namespace\n1. ssh to an edge node\n2. kinit as hbase\n   ```bash\n   cd /var/run/cloudera-scm-agent/processes\n   ls \u2013ltr HBASE",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_hbase_quotas.md - Part 1"
        }
    },
    "1367": {
        "page_content": "2. kinit as hbase\n   ```bash\n   cd /var/run/cloudera-scm-agent/processes\n   ls \u2013ltr HBASE\n   cd <latest hbase process folder>\n   kinit -kt hbase.keytab `hostname`\n   ```\n3. Get list of namespaces\n   ```bash\n   hbase shell\n   list_namespace\n   ```\n4. Set throttle READ quotas \n   ```bash\n   hbase shell\n   set_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => \u2018namespace', LIMIT => 'Xreq/sec'\n   ```\n5. Set throttle WRITE quotas\n   ```bash\n   hbase shell",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_hbase_quotas.md - Part 2"
        }
    },
    "1368": {
        "page_content": "```\n5. Set throttle WRITE quotas\n   ```bash\n   hbase shell\n   set_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => \u2018namespace', LIMIT => 'Xreq/sec'\n   ```\n6. Show all quotas\n   ```bash\n   hbase shell\n   list_quotas\n   ```\n## Remove HBase quotas from a namespace\n1. ssh to an edge node\n2. kinit as hbase\n   ```bash\n   cd /var/run/cloudera-scm-agent/processes\n   ls \u2013ltr HBASE\n   cd <latest hbase process folder>\n   kinit -kt hbase.keytab `hostname`\n   ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_hbase_quotas.md - Part 3"
        }
    },
    "1369": {
        "page_content": "ls \u2013ltr HBASE\n   cd <latest hbase process folder>\n   kinit -kt hbase.keytab `hostname`\n   ```\n3. Get list of namespaces and list of quotas already set\n   ```bash\n   hbase shell\n   list_namespace\n   list_quotas\n   ```\n4. Delete throttle quotas\n   ```bash\n   hbase shell\n   set_quota TYPE => THROTTLE, NAMESPACE => \u2018namespace', LIMIT => NONE\n   ```\n5. Verify that quotas have been removed\n   ```bash\n   hbase shell\n   list_quotas\n   ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_hbase_quotas.md - Part 4"
        }
    },
    "1370": {
        "page_content": "# Postgres Upgrade\n[[_TOC_]]\nAll procedures pertain to PR and DR edge nodes, except the RPM repository creation\nwhich is performed on pr1node1:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Preparation\nBefore continuing with the changes it is best to put the edge node you are\nworking on in standby mode, so as to not disrupt services:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i\nAnd follow the procedures described in the **Switchover of Cluster Resources** chapter",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 1"
        }
    },
    "1371": {
        "page_content": "$ sudo -i\nAnd follow the procedures described in the **Switchover of Cluster Resources** chapter\nof the **Security Vulnerabilities** MOP [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx).\nStop the running postgres service:\n    # sudo -iu postgres\n    $ systemctl stop postgresql-9.5.service\n    $ systemctl disable postgresql-9-5.service\n    $ systemctl status postgresql-9.5.service",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 2"
        }
    },
    "1372": {
        "page_content": "$ systemctl disable postgresql-9-5.service\n    $ systemctl status postgresql-9.5.service\nBackup data on each edge server:\n- edge01: `$ pg_dumpall > edge01_postgres_backup`\n- edge02: `$ pg_dumpall > edge02_postgres_backup`\nBackup **pg_hba.conf** and **postgresql.conf**:\n    # cp -ap /var/lib/psql/9.5/data/pg_hba.conf /var/lib/psql/9.5/data/pg_hba.conf.bak\n    # cp -ap /var/lib/psql/9.5/data/postgresql.conf /var/lib/psql/9.5/data/postgresql.conf.bak\n### Repositories Creation",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 3"
        }
    },
    "1373": {
        "page_content": "### Repositories Creation\nDownload rpms for Postgres 14 from the Postgres site\nhttps://download.postgresql.org/pub/repos/yum/14/redhat/rhel-7.9-x86_64/.....\nand prepare the new postgres repository on pr1node01:\n    $ ssh Exxxx@pr1node01\n    $ sudo -i\n    # mkdir -p /var/www/postgres14/Packages/\nMove all the rpm files of Postgres14 under `/var/www/html/postgres14/Packages` and\ncreate the **YUM** repository:\n    # cd /var/www/postgres14/\n    # createrepo .\nor if the repository existed:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 4"
        }
    },
    "1374": {
        "page_content": "# cd /var/www/postgres14/\n    # createrepo .\nor if the repository existed:\n    # createrepo --update .\nCreate the repository file on one of the edge nodes and copy it to all others:\n    $ ssh Exxx@pr1edge01\n    $ sudo -i\n    # vi /etc/yum.repos.d/postgres14.repo\n    [postgres14]\n    name = Postgres14\n    baseurl =  http://pr1node01.mno.gr/postgres14/\n    enabled = 1\n    gpgcheck = 0\n    # scp /etc/yum.repos.d/postgres14.repo XXXedgeXX:/etc/yum.repos.d/",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 5"
        }
    },
    "1375": {
        "page_content": "gpgcheck = 0\n    # scp /etc/yum.repos.d/postgres14.repo XXXedgeXX:/etc/yum.repos.d/\nOn each edge node disable the old postgres repositorry by setting `enabled = 0` inside\nits repo file under `/etc/yum.repos.d/`.\n## Execution\nPerform the update using **YUM**, while enabling the repository for the new Postgres\nand disabling the previous repository if exists on each edge node:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i\n    # yum clean all",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 6"
        }
    },
    "1376": {
        "page_content": "$ ssh Exxxx@XXXedgeXX\n    $ sudo -i\n    # yum clean all\n    # yum install --disablerepo=* --enablerepo=postgres14 postgresql14 postgresql14-server postgresql14-contrib postgresql14-libs\nChange the data directory and setup the newly updated PostgreSQL:\n    # vi usr/lib/systemd/system/postgresql-14.service\n    Environment=PGDATA=/var/lib/pgsql/9.14/data\n    # /usr/pgsql-14/bin/postgresql-14-setup initdb\n    # systemctl enable --now postgresql-14",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 7"
        }
    },
    "1377": {
        "page_content": "# /usr/pgsql-14/bin/postgresql-14-setup initdb\n    # systemctl enable --now postgresql-14\nLogin to each edge node and restore data from backup:\n    $ ssh Exxx@XXXedgeXX:\n    $ sudo -iu postgres\n    $ psql -f edgeXX_postgres_backup postgres\n    $ systemctl restart postgresql-14.service\n    $ systemctl status postgresql-14.service\nCheck **pg_hba.conf** and **postgresql.conf** for differencies between versions:\n    $ sdiff /var/lib/pgsql/9.14/data/pg_hba.conf /var/lib/psql/9.5/data/pg_hba.conf",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 8"
        }
    },
    "1378": {
        "page_content": "$ sdiff /var/lib/pgsql/9.14/data/pg_hba.conf /var/lib/psql/9.5/data/pg_hba.conf\n    $ sdiff /var/lib/pgsql/9.14/data/postgresql.conf /var/lib/psql/9.5/data/postgresql.conf\nIf everything is ok, unstandby the node\n## Rollback\nLogin to each edge node, stop the postgres service and downgrade using **YUM**:\n    $ ssh Exxx@XXXedgeXX:\n    $ sudo -iu postgres\n    $ systemctl disable --now postgresql-14.service\n    $ systemctl status postgresql-14.service\n    $ sudo -i\n    # yum clean all",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 9"
        }
    },
    "1379": {
        "page_content": "$ systemctl status postgresql-14.service\n    $ sudo -i\n    # yum clean all\n    # yum downgrade --disablerepo=* --enablerepo=postgres9 postgresql\n    # systemctl enable --now postgresql-9-5.service",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "postgres_upgrade.md - Part 10"
        }
    },
    "1380": {
        "page_content": "# SSL Configuration Changes\n[[_TOC_]]\nAll procedures pertain to PR and DR edge nodes:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Preparation\nBefore continuing with the changes it is best to put the edge node you are\nworking on in standby mode, so as to not disrupt services:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i\nAnd follow the procedures described in the **Switchover of Cluster Resources** chapter",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ssl_configuration_changes.md - Part 1"
        }
    },
    "1381": {
        "page_content": "$ sudo -i\nAnd follow the procedures described in the **Switchover of Cluster Resources** chapter\nof the **Security Vulnerabilities** MOP [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx).\n## httpd\nBackup the old httpd configs:\n    # cp \u2013ap /etc/httpd/conf.d/ssl.conf  \"/etc/httpd/conf.d/ssl.conf.bak.$(date +%Y%m%d)\"",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ssl_configuration_changes.md - Part 2"
        }
    },
    "1382": {
        "page_content": "# cp \u2013ap /etc/httpd/conf.d/ssl.conf  \"/etc/httpd/conf.d/ssl.conf.bak.$(date +%Y%m%d)\"\n    # cp \u2013ap /etc/httpd/conf/httpd.conf  \"/etc/httpd/conf/httpd.conf.bak.$(date +%Y%m%d)\"\n    # cp -ap /etc/httpd/conf.d/graphite-web.conf \"/etc/httpd/conf.d/graphite-web.conf.bak.$(date +%Y%m%d)\"\nAdd the following line in `/etc/httpd/conf/httpd.conf`:\n    TraceEnable Off\nAdd the following line in `/etc/httpd/conf/httpd.conf`, `/etc/httpd/conf.d/ssl.conf`\nand `/etc/httpd/conf.d/graphite-web.conf`:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ssl_configuration_changes.md - Part 3"
        }
    },
    "1383": {
        "page_content": "and `/etc/httpd/conf.d/graphite-web.conf`:\n    SSLProtocol +TLSv1.2\nEdit `/etc/httpd/conf/httpd.conf`, `/etc/httpd/conf/ssl.conf` and\n`/etc/httpd/conf/graphite-web.conf` and remove the following lines:\n    SSLHonorCipherOrder Off\n    SSLCipherSuite ECDH+AESGCM:ECDH+CHACHA20:ECDH+AES256:ECDH+AES128:!aNULL:!SHA1:!AESCCM:!MD5:!3DES:!DES:!IDEA\nRestart the **httpd** service:\n    # systemctl restart httpd\n## nginx\nBackup the old **nginx.conf**:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ssl_configuration_changes.md - Part 4"
        }
    },
    "1384": {
        "page_content": "Restart the **httpd** service:\n    # systemctl restart httpd\n## nginx\nBackup the old **nginx.conf**:\n    # cp -ap /etc/nginx/nginx.conf \"/etc/nginx/nginx.conf.bak.$(date +%Y%m%d)\"\nAdd the following line in `/etc/nginx/nginx.conf`:\n    ssl_protocols TLSv1.2;\nDisable and restart the **nginx** service:\n    # systemctl disable --now nginx\n    # systemctl start nginx\n## haproxy\nBackup the old **haproxy.cfg**:\n    # cp -ap /etc/haproxy/haproxy.cfg \"/etc/haproxy/haproxy.cfg.bak.$(date +%Y%m%d)\"",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ssl_configuration_changes.md - Part 5"
        }
    },
    "1385": {
        "page_content": "# cp -ap /etc/haproxy/haproxy.cfg \"/etc/haproxy/haproxy.cfg.bak.$(date +%Y%m%d)\"\nAdd options for 8889 and 25002 port and repeat for **hue_vip**:\n    bind 999.999.999.999:25002 ssl crt no-sslv3 /opt/haproxy/security/x509/node.haproxy.pem\nRestart the **haproxy** service:\n    # systemctl restart haproxy\n## sshd\nBackup the old **sshd_config**:\n    # cp -ap /etc/ssh/sshd_config \"/etc/ssh/sshd_config.bak.$(date +%Y%m%d)\"\nEdit the sshd config `/etc/ssh/sshd_config` and add the following:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ssl_configuration_changes.md - Part 6"
        }
    },
    "1386": {
        "page_content": "Edit the sshd config `/etc/ssh/sshd_config` and add the following:\n    Ciphers aes256-ctr,aes192-ctr,aes128-ctr # 5.2.11\n    KexAlgorithms ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group14-sha1,diffie-hellman-group-exchange-sha256\nRestart the **sshd** service:\n    # systemctl restart sshd",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "ssl_configuration_changes.md - Part 7"
        }
    },
    "1387": {
        "page_content": "# Grafana Upgrade\n[[_TOC_]]\nAll procedures pertain to PR and DR edge nodes, except the RPM repository creation\nwhich is performed on pr1node1:\n- pr1edge01\n- pr1edge02\n- dr1edge01\n- dr1edge02\n## Preparation\nBefore continuing with the changes it is best to inform the monitoring team\nthat there will be an outage on the monitoring service.\nLogin to each edge node and get a root shell:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i\n### Backup",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "grafana_upgrade.md - Part 1"
        }
    },
    "1388": {
        "page_content": "Login to each edge node and get a root shell:\n    $ ssh Exxxx@XXXedgeXX\n    $ sudo -i\n### Backup\nBackup Installed plugins before you upgrade them in case you want to rollback the\nGrafana version and want to get the exact same versions you were running before the\nupgrade or in case you add new configuration options after upgrade and then rollback.\n- pr1edge01:\n  - Plugins: `# tar -zcvf grafana_plugins_edge01.tar.gz /var/lib/grafana/plugins`",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "grafana_upgrade.md - Part 2"
        }
    },
    "1389": {
        "page_content": "- pr1edge01:\n  - Plugins: `# tar -zcvf grafana_plugins_edge01.tar.gz /var/lib/grafana/plugins`\n  - INI file: `# tar -zcvf grafana_ini_edge01.tar.gz /etc/grafana/grafana.ini`\nBackup Grafana Datasources and Dashboards\nWith an admin account login to Grafana and go to Configuration > API keys. Create a new key by clicking on \u201cAdd API key\u201d with role admin. Copy the authorization token for the key.\nLogin to an edge node and use the API to back up the datasources and dashboards:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "grafana_upgrade.md - Part 3"
        }
    },
    "1390": {
        "page_content": "Login to an edge node and use the API to back up the datasources and dashboards:\n    # curl -H \"Authorization: Bearer <insert token>\"  https://<grafana host>:3000/api/datasources > grafana_datasources.json\n    # curl -H \"Authorization: Bearer <insert token>\"  https://<grafana host>:3000/api/search | grep -o -E '\"uid\":\"[a-zA-Z0-9_-]+\"' | sed 's/\"uid\":\"//g' | sed 's/\"//g' > grafana_dashboards_uids",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "grafana_upgrade.md - Part 4"
        }
    },
    "1391": {
        "page_content": "# for uid in `cat grafana_dashboards_uids`; do curl -H \"Authorization: Bearer <insert token>\"  https://<grafana host>:3000/api/dashboards/uid/${uid}; done > /tmp/grafana_dashboard_${uid}.json\n### Repositories\nDownload Grafana RPMs from [Grafana Downloads](https://grafana.com/grafana/download?edition=oss)\nand prepare their repositories on pr1node01, from where PR and DR edge nodes will download them:\n    $ ssh Exxxx@pr1node01\n    $ sudo -i\n    # mkdir -p /var/www/grafana8/Packages/",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "grafana_upgrade.md - Part 5"
        }
    },
    "1392": {
        "page_content": "$ ssh Exxxx@pr1node01\n    $ sudo -i\n    # mkdir -p /var/www/grafana8/Packages/\nMove all the downloaded RPMs under `/var/www/html/grafana8/Packages` and create the\nrepository:\n    # cd /var/www/grafana8\n    # createrepo .\nIf the repository already exists, issue:\n    # createrepo --update .\nLogin to an edge node, create the repository file and copy it to all other\nedge nodes appropriately:\n    $ ssh Exxx@XXXedgeXX\n    $ sudo -i\n    # vi /etc/yum.repos.d/grafana8.repo\n    [grafana8]",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "grafana_upgrade.md - Part 6"
        }
    },
    "1393": {
        "page_content": "$ ssh Exxx@XXXedgeXX\n    $ sudo -i\n    # vi /etc/yum.repos.d/grafana8.repo\n    [grafana8]\n    name = Grafana8\n    baseurl =  http://pr1node01.mno.gr/grafana8/\n    enabled = 1\n    gpgcheck = 0\n    # scp /etc/yum.repos.d/grafana8.repo repo XXXedgeXX:/etc/yum.repos.d/\n## Execution\nLogin to each edge node, stop the **grafana-server** and update it using\nusing **YUM**:\n    $ ssh Exxx@XXXedgeXX\n    $ sudo -i\n    # systemctl stop grafana-server\n    # systemctl status grafana-server",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "grafana_upgrade.md - Part 7"
        }
    },
    "1394": {
        "page_content": "$ sudo -i\n    # systemctl stop grafana-server\n    # systemctl status grafana-server\n    # yum clean all\n    # yum update grafana\n    # systemctl start grafana-server\n    # systemctl  status grafana-server\nCheck Grafana UI, Dashboards and compare new and old configs with **diff**\nfor any discrepancies:\n    # sdiff /etc/grafana/grafana.ini <path/to/old/grafana.ini>`\n## Rollback\nLogin to each edge node to stop the grafana service and downgrade the package using **YUM**:\n    $ ssh Exxx@XXXedgeXX",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "grafana_upgrade.md - Part 8"
        }
    },
    "1395": {
        "page_content": "$ ssh Exxx@XXXedgeXX\n    $ sudo -i\n    # systemctl stop grafana-server\n    # systemctl status grafana-server\n    # yum clean all\n    # yum downgrade grafana\nRestore plugins and INI files from the backups previously created at `grafana_plugins_edge0X.tar.gz`\nand `grafana_ini_edge0X.tar.gz`, start the grafana service and check dashboards:\n    # systemctl start grafana-server\n    # systemctl status grafana-server",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "grafana_upgrade.md - Part 9"
        }
    },
    "1396": {
        "page_content": "# Manage Kafka MirrorMaker\n## Scope\nKafka MirrorMaker has been configured with auto.offsets.reset=false. This means that the MirrorMaker consumers never commit the offsets.\nIf the MirrorMakers' consumer group goes inactive(both MirrorMakers are offline at the same time), their consumer group will reset to the earliest offset available.\nThis means that the last week will be mirrored to the final topic and will be replayed by the Spark streaming topology.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 1"
        }
    },
    "1397": {
        "page_content": "This document describes how to commit the offsets for the MirrorMaker consumer groups, in order to avoid this issue.\n## Setup\n1. MirrorMakers on nodes pr1node01 and pr1node04:\n    - Replicate the traffic from the **Primary Site Mirror Topics** to the **Primary Site Final Topics**.\n    - Replicate Production Topics for both Internet Banking and Online Applications.\n    - Use **mir-trlog-ingest-stream-con-002** consumer group.\n    - Offsets are committed to the **Primary Site Kafka cluster**.",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 2"
        }
    },
    "1398": {
        "page_content": "- Offsets are committed to the **Primary Site Kafka cluster**.\n2. MirrorMakers on nodes pr1node05 and pr1node06:\n  \n    - Replicate the traffic from the **Disaster Site Mirror Topics** to the **Primary Site Final Topics**.\n    - Replicate Production Topics for both Internet Banking and Online Applications.\n    - Use **mir-trlog-ingest-stream-con-001** consumer group.\n    - Offsets are committed to the **Disaster Site Kafka cluster**.\n3. MirrorMakers on nodes dr1node01 and dr1node04:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 3"
        }
    },
    "1399": {
        "page_content": "3. MirrorMakers on nodes dr1node01 and dr1node04:\n    - Replicate the traffic from the **Disaster Site Mirror Topics** to the **Disaster Site Final Topics**.\n    - Replicate Production Topics for both Internet Banking and Online Applications.\n    - Use **mir-trlog-ingest-stream-con-002** consumer group.\n    - Offsets are committed to the **Disaster Site Kafka cluster**.\n4. MirrorMakers on nodes dr1node05 and dr1node06:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 4"
        }
    },
    "1400": {
        "page_content": "4. MirrorMakers on nodes dr1node05 and dr1node06:\n  \n    - Replicate the traffic from the **Primary Site Mirror Topics** to the **Disaster Site Final Topics**.\n    - Replicate Production Topics for both Internet Banking and Online Applications.\n    - Use **mir-trlog-ingest-stream-con-001** consumer group.\n    - Offsets are committed to the **Primary Site Kafka cluster**.\n## Procedure\n### Stop Kafka MirrorMakers for PR site\n1. Stop Primary Site MirrorMakers:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 5"
        }
    },
    "1401": {
        "page_content": "## Procedure\n### Stop Kafka MirrorMakers for PR site\n1. Stop Primary Site MirrorMakers:\n    From the Primary Site Cloudera Manager with a user that has administrator privileges:\n    - PRBDA > Kafka > Instances\n    - Select the MirrorMakers running on nodes pr1node01,pr1node04,pr1node05 and pr1node06\n    - Stop\n2. Stop Disaster Site MirrorMakers:\n    From the Disaster Site Cloudera Manager with a user that has administrator privileges:\n    - DRBDA > Kafka > Instances",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 6"
        }
    },
    "1402": {
        "page_content": "- DRBDA > Kafka > Instances\n    - Select the MirrorMakers running on nodes dr1node05 and dr1node06\n    - Stop\n### Stop Kafka MirrorMakers for DR site\n1. Stop Primary Site MirrorMakers:\n    From the Primary Site Cloudera Manager with a user that has administrator privileges:\n    - DRBDA > Kafka > Instances\n    - Select the MirrorMakers running on nodes dr1node01,dr1node04,dr1node05 and dr1node06\n    - Stop\n2. Stop Disaster Site MirrorMakers:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 7"
        }
    },
    "1403": {
        "page_content": "- Stop\n2. Stop Disaster Site MirrorMakers:\n    From the Disaster Site Cloudera Manager with a user that has administrator privileges:\n    - PRBDA > Kafka > Instances\n    - Select the MirrorMakers running on nodes pr1node05 and pr1node06\n    - Stop\n### Commit Consumer Groups Offsets for PR site\nThe following steps can be performed at any node of the Primary Site cluster. The consumer groups need to be **inactive** for these action to be performed.\n1. Create a file named group.properties:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 8"
        }
    },
    "1404": {
        "page_content": "1. Create a file named group.properties:\n    ``` conf\n    security.protocol=SASL_SSL\n    sasl.kerberos.service.name=kafka\n    ```\n2. Create a file named jaas.conf:\n    ``` conf\n    Client {\n      com.sun.security.auth.module.Krb5LoginModule required\n      useKeyTab=false\n      useTicketCache=true\n      doNotPrompt=true\n      principal=\"kafka@BDAP.mno.GR\";\n    };\n    KafkaClient {\n      com.sun.security.auth.module.Krb5LoginModule required\n      useKeyTab=false\n      useTicketCache=true",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 9"
        }
    },
    "1405": {
        "page_content": "useKeyTab=false\n      useTicketCache=true\n      doNotPrompt=true\n      principal=\"kafka@BDAP.mno.GR\"\n      service=\"kafka\";\n    };\n    ```\n3. Log in to kerberos as the **_kafka@BDAP.mno.GR_** principal and configure security:\n    ``` bash\n    kinit kafka@BDAP.mno.GR\n    export KAFKA_JVM_PERFORMANCE_OPTS=\"-Djava.security.auth.login.config=./jaas.conf\"\n    ```\n4. Commit the offsets for all relevant consumer groups:\n    ``` bash",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 10"
        }
    },
    "1406": {
        "page_content": "```\n4. Commit the offsets for all relevant consumer groups:\n    ``` bash\n    export DATETIME=1970-01-01T00:00:00.000Z #UTC time. See ndefs\n    kafka-consumer-groups --bootstrap-server pr1node01.mno.gr:9093,pr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --reset-offsets --to-datetime $DATETIME --execute",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 11"
        }
    },
    "1407": {
        "page_content": "kafka-consumer-groups --bootstrap-server pr1node01.mno.gr:9093,pr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\n    kafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\n    ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 12"
        }
    },
    "1408": {
        "page_content": "```\n### Commit Consumer Groups Offsets for DR site\nThe following steps can be performed at any node of the Disaster Site cluster. The consumer groups need to be **inactive** for these action to be performed.\n1. Create a file named group.properties:\n    ``` conf\n    security.protocol=SASL_SSL\n    sasl.kerberos.service.name=kafka\n    ```\n2. Create a file named jaas.conf:\n    ``` conf\n    Client {\n      com.sun.security.auth.module.Krb5LoginModule required\n      useKeyTab=false",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 13"
        }
    },
    "1409": {
        "page_content": "Client {\n      com.sun.security.auth.module.Krb5LoginModule required\n      useKeyTab=false\n      useTicketCache=true\n      doNotPrompt=true\n      principal=\"kafka@BDAD.mno.GR\";\n    };\n    KafkaClient {\n      com.sun.security.auth.module.Krb5LoginModule required\n      useKeyTab=false\n      useTicketCache=true\n      doNotPrompt=true\n      principal=\"kafka@BDAD.mno.GR\"\n      service=\"kafka\";\n    };\n    ```\n3. Log in to kerberos as the **_kafka@BDAD.mno.GR_** principal and configure security:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 14"
        }
    },
    "1410": {
        "page_content": "```\n3. Log in to kerberos as the **_kafka@BDAD.mno.GR_** principal and configure security:\n    ``` bash\n    kinit kafka@BDAD.mno.GR\n    export KAFKA_JVM_PERFORMANCE_OPTS=\"-Djava.security.auth.login.config=./jaas.conf\"\n    ```\n4. Commit the offsets for all relevant consumer groups:\n    ``` bash\n    export DATETIME=1970-01-01T00:00:00.000Z #UTC time. See ndefs",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 15"
        }
    },
    "1411": {
        "page_content": "``` bash\n    export DATETIME=1970-01-01T00:00:00.000Z #UTC time. See ndefs\n    kafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --reset-offsets --to-datetime $DATETIME --execute",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 16"
        }
    },
    "1412": {
        "page_content": "kafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\n    kafka-consumer-groups --bootstrap-server pr1node01.mno.gr:9093,pr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\n    ```\n### Start Kafka MirrorMakers for PR site",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 17"
        }
    },
    "1413": {
        "page_content": "```\n### Start Kafka MirrorMakers for PR site\n1. Start Primary Site MirrorMakers:\n    From the Primary Site Cloudera Manager with a user that has administrator privileges:\n    - PRBDA > Kafka > Instances\n    - Select the MirrorMakers running on nodes pr1node01,pr1node04,pr1node05 and pr1node06\n    - Start\n    All messages should be consumed in about one to two minutes.\n2. Start Disaster Site MirrorMakers:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 18"
        }
    },
    "1414": {
        "page_content": "2. Start Disaster Site MirrorMakers:\n    From the Disaster Site Cloudera Manager with a user that has administrator privileges:\n    - DRBDA > Kafka > Instances\n    - Select the MirrorMakers running on nodes dr1node05 and dr1node06\n    - Start\n    Wait for traffic on all topics to get back to normal values before any changes.\n### Start Kafka MirrorMakers for DR site\n1. Start Primary Site MirrorMakers:\n    From the Disaster Site Cloudera Manager with a user that has administrator privileges:",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 19"
        }
    },
    "1415": {
        "page_content": "From the Disaster Site Cloudera Manager with a user that has administrator privileges:\n    - DRBDA > Kafka > Instances\n    - Select the MirrorMakers running on nodes dr1node01,dr1node04,dr1node05 and dr1node06\n    - Start\n    All messages should be consumed in about one to two minutes.\n2. Start Disaster Site MirrorMakers:\n    From the Primary Site Cloudera Manager with a user that has administrator privileges:\n    - PRBDA > Kafka > Instances",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 20"
        }
    },
    "1416": {
        "page_content": "- PRBDA > Kafka > Instances\n    - Select the MirrorMakers running on nodes pr1node05 and pr1node06\n    - Start\n    Wait for traffic on all topics to get back to normal values before any changes.\n## Ndefs\n- The result from the following queries can be useful during startup:\n    ``` sql\n    SELECT min(`timestamp`),max(`timestamp`) FROM prod_trlog_ibank.service_audit_stream\n    SELECT min(`timestamp`),max(`timestamp`) FROM prod_trlog_online.service_audit_stream\n    ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 21"
        }
    },
    "1417": {
        "page_content": "SELECT min(`timestamp`),max(`timestamp`) FROM prod_trlog_online.service_audit_stream\n    ```\n- Consider committing offsets at a time 5 minutes prior to max timestamp\n- Time should be in UTC e.g. 2019-09-02T12:30:00.000Z = 2019-09-02T15:30:00.000 EEST\n- The option _--all-topics_ commits the offsets **only** for the topics this consumer group has ever subscribed and not all the topics of the Kafka cluster\n- These commands are only for consumers that use the new API (version 0.10 and later)",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 22"
        }
    },
    "1418": {
        "page_content": "- These commands are only for consumers that use the new API (version 0.10 and later)\n- The following commands can be useful:\n    ``` bash\n    export DATETIME=1970-01-01T00:00:00.000Z\n    kafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --describe # List offsets",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 23"
        }
    },
    "1419": {
        "page_content": "kafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --reset-offsets --to-datetime $DATETIME # Dry run\n    ```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "manage_mirrormaker.md - Part 24"
        }
    },
    "1420": {
        "page_content": "# Below procedure describes how to add a new repostiory on Nexus. \n1. Login with your personal account to an edge node and open firefox\n```bash\nssh -X xedge0x\nfirefox\n```\n2. When firefox vwxow pops up login to `https://999.999.999.999:8081/` with Nexus creds.\n[Click me for the credentials](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/passwords.kdbx)",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "nexus_new_repo_procedure.md - Part 1"
        }
    },
    "1421": {
        "page_content": "3. Click on the gear icon and then **repositories** button and **Create repository** and select **yum (proxy)**.\nAdd below values:\n- **Name**: name_of_repo\n- **Remdef storage**: remdef_storage_url \n- **Maximum Component age**: 20\n- **Minimum Component age**: 20\n- **Clean up policies**: daily_proxy_clean\nLeave the rest of the settings as default\n4. Click on **Create repository**\n5. Login with your personal account at node and add the following repos:\n```bash\nvi /etc/yum.repos.d/name_of_repo.repo",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "nexus_new_repo_procedure.md - Part 2"
        }
    },
    "1422": {
        "page_content": "```bash\nvi /etc/yum.repos.d/name_of_repo.repo\n[name_of_repos]\nname = name_of_repo\nbaseurl = http://999.999.999.999:8081/repository/name_of_repo.repo\nenabled = 1\ngpgcheck = 0\n```\n6. Check and add new repo\n```bash\nssh to_node\nyum clean all\nyum check-update > /tmp/test-repo.txt\nyum repolist\n```",
        "metadata": {
            "category": "procedures",
            "client": "ClientB",
            "name": "nexus_new_repo_procedure.md - Part 3"
        }
    }
}