{
    "qa_pairs": [
        {
            "question": "How can I prevent root SSH login on all servers?",
            "answer": "To disable root SSH login on all servers, modify the SSH configuration. First, check the current configuration:\n```bash\nsalt '*' cmd.run 'grep \"^PermitRootLogin\" /etc/ssh/sshd_config'\n```\nThen, modify the configuration to disallow root login:\n```bash\nsed -i -e 's/^PermitRootLogin.*/PermitRootLogin no/' /etc/ssh/sshd_config\n```\nApply the changes using:\n```bash\nsalt '*' cmd.script salt://disable_root_login.sh\nsalt '*' cmd.run 'service sshd reload'\n```",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the correct way to restart the mn3 node if it becomes unresponsive?",
            "answer": "To restart the mn3 node, access its iDRAC interface at `https://10.255.242.85/`, navigate to `Server -> Power Cycle System (cold boot)`, and confirm the reboot.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can I check if the mn3 node is online and responsive?",
            "answer": "You can check if the mn3 node is online by running:\n\n```bash\nping mn3\n```\nIf there is no response, attempt to SSH into mn3 from the admin server and check the Cloudera Manager interface for its status.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the process for setting up a new MySQL replication?",
            "answer": "To set up MySQL replication:\n1. Configure MySQL master:\n   ```bash\n   mysql -u root -p\n   CHANGE MASTER TO MASTER_HOST='master_host', MASTER_USER='replication_user', MASTER_PASSWORD='password';\n   START SLAVE;\n   ```\n2. Check slave status:\n   ```bash\n   SHOW SLAVE STATUS\\G;\n   ```\n3. If replication fails, restart it:\n   ```bash\n   STOP SLAVE;\n   RESET SLAVE;\n   START SLAVE;\n   ```",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How does the Initialization/Synchronization flow in TeMIP work?",
            "answer": "The Initialization/Synchronization Flow in TeMIP is managed by the Oozie Coordinator `TeMIP_Synchronization_CO`. It establishes a connection between the Wildfly Server running the TeMIP application and the TeMIP Server. This coordinator must be run manually after every restart or deployment of the Wildfly Server.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What are the steps to restart the Cloudera CDSW system?",
            "answer": "To restart Cloudera CDSW:\n1. Navigate to Cloudera Manager -> CDSW -> Restart.\n2. Monitor the restart process:\n   ```bash\n   cdsw status\n   ```\n3. If errors persist, investigate logs:\n   ```bash\n   less /var/log/cdsw/cdsw_health.log\n   ```\n(Source: X20230130-IM2073052.md)",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the purpose of the `ztab` file in the navencrypt disk decryption process?",
            "answer": "The `ztab` file lists encrypted mount points. When decrypting a disk, check the file using:\n```bash\ncat /etc/navencrypt/ztab | grep /data/1\n```\nIf the entry exists, it should be commented out before proceeding.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How does the Wildfly service handle incoming user-generated events in Internet Banking?",
            "answer": "User-generated events are sent to Wildfly instances installed at edge servers as HTTP POST requests. These requests originate from the backend servers of Internet Banking and are load-balanced by a NetScaler managed by mno's networking department. The events are then forwarded to a Kafka topic, usually with only one active site (Primary Site).",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can a Kerberos keytab file be created for user authentication?",
            "answer": "Log in to the `kerb1` node, use `kadmin.local`, check if the principal exists, and create it if necessary using `addprinc <username>@CNE.abc.GR`. Generate the keytab file using `ktadd -k /tmp/<username>.keytab -norandkey <username>@CNE.abc.GR` and move it to the appropriate directory.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the purpose of the Alert Mail Flow in TeMIP?",
            "answer": "The Alert Mail Flow in TeMIP is an Oozie Coordinator (`TeMIP_Alert_Mail_CO`) that runs every hour to check whether alarms are being received from the TeMIP Server. If no alarms are received in the last hour, it sends an email notification to engineers.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can we resolve a 'Batch Job Failed' error in Grafana for the DWH_IBank application?",
            "answer": "The failure is due to duplicate keys in the srcib.MandateDetails table. The resolution involves running the extract script with the `-f` flag to truncate the table before inserting new records:\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n```\nAfter extraction, the export script must also be executed with the `-f` flag:\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n```\nThis ensures that the extracted records are properly inserted and exported.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the correct way to restart Cloudera Data Science Workbench (CDSW) services when jobs fail due to Spark port forwarder issues?",
            "answer": "First, check logs for SparkPortForwarder using `kubectl logs spark-port-forwarder-<pod-name> -n <namespace>`. If errors like 'connection refused' appear, try deleting the failing pod using `kubectl delete pod <pod-name> -n <namespace>`. If it remains in a 'Terminating' state, restart the Docker Daemon Worker from Cloudera Manager (`Cloudera Manager -> CDSW -> Docker Daemon Worker -> Restart`) and restart the CDSW application (`Cloudera Manager -> CDSW -> Application Role -> Restart`). Verify that SparkPortForwarder is operational by checking logs with `kubectl logs <spark-forwarder-pod> -n <namespace>` and confirming successful port forwarding entries.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How do I check if the Kafka and Kudu services are running before decrypting a disk?",
            "answer": "To check if Kafka and Kudu are running, navigate to Cloudera Manager and:\n1. Go to `Kafka > Status`.\n2. Go to `Kudu > Status`.\nEnsure they are stopped before proceeding with disk decryption.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How do you renew Kubernetes certificates for RAN.AI?",
            "answer": "Login to `kubemaster1` and check expiration with `kubeadm certs check-expiration`. Backup Kubernetes configuration, renew certs using `kubeadm certs renew all`, and restart kube-apiserver, kube-controller-manager, and kube-scheduler to apply changes.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can I check for missing logs in the abc BigStreamer syslog?",
            "answer": "To check for missing logs in the abc BigStreamer syslog, SSH into un2 as root and inspect the rsyslog configuration using:\n```bash\ncat /etc/rsyslog.conf | more\n```\nThen, check the servers where messages are transferred by examining log rotation settings with:\n```bash\ncat /etc/logrotate.conf | more\n```",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How do you update an expired Kubernetes certificate in RAN.AI?",
            "answer": "Use `kubeadm certs renew all`, then restart kube-apiserver and controller-manager containers using `ctrctl stop <container_id>`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What logs should you check if the Piraeus Cisco VDSL2 application fails to transform SNMP files?",
            "answer": "Check the logs using `kubectl logs` for stdout messages and verify the transformation process in `/app/conf/application.yaml` under `vdsl2.dataDir`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What steps should I follow to resolve missing statistics warnings in `sai.voice_quality_hist`?",
            "answer": "To resolve missing statistics warnings in `sai.voice_quality_hist`:\n1. Check the current table statistics using Impala:\n   ```sql\n   show table stats sai.voice_quality_hist;\n   ```\n2. If statistics are missing, recompute them:\n   ```sql\n   compute stats sai.voice_quality_hist;\n   ```\n3. If errors persist, refresh the table:\n   ```sql\n   refresh sai.voice_quality_hist;\n   ```\n4. If the warning continues, verify data flow and logs for potential loading issues.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How can I determine if an Impala batch job failed due to long row key lengths in HBase?",
            "answer": "If an Impala batch job fails due to long row keys, inspect the logs for the following error:\n   ```bash\n   WARNING: java.lang.IllegalArgumentException: Row length 34XXX is > 32767\n   ```\nTo identify problematic rows, modify the query:\n   ```sql\n   SELECT * FROM your_table WHERE length(concat(default.hbase_salt_md5hex(client_username,4,TRUE), chr(0), hex(CAST(CAST(u_timestamp AS DOUBLE) * 1000000 AS bigint)), chr(0), a.id)) > 32767;\n   ```\nIf results appear, modify the job script to exclude these rows.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What should I do if Cloudera Manager stops sending automatic alert emails?",
            "answer": "If Cloudera Manager stops sending alerts, follow these steps:\n1. SSH to `un5` (Alert Publisher Node) and check the following logs for errors:\n   ```bash\n   less /var/log/messages\n   less /var/log/mail.err\n   less /var/log/mail.info\n   less /var/log/cloudera-scm-alertpublisher\n   ```\n2. Count the number of alerts generated in logs:\n   ```bash\n   for i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 'Cloudera Alert' | wc -l; done\n   ```\n3. Restart the Alert Publisher via Cloudera Manager if needed.\n(Source: X20220901-IM1957832.md)",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What are the key steps to restart a Wildfly instance in case of errors?",
            "answer": "To restart a Wildfly instance, follow these steps:\n1. Check the application logs at `/var/log/wildfly/prodrestib/server.log` for error messages.\n2. If there are frequent errors, use the provided script in `/opt/wildfly/default/prodrestib/standalone/deployments` to restart the service.\n3. If logs do not indicate a specific issue, check for Kafka performance issues.\n4. Restarting instructions can be found [here](../procedures/manage_wildfly.md).",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "Why was the HDFS failover controller role down, and how was it fixed?",
            "answer": "The HDFS failover controller role was down due to a timeout in its connection to ZooKeeper. The issue was resolved by enabling the failover controller\u2019s automatic restart in Cloudera Manager: `HDFS -> Failover Controller -> Automatically Restart Processes`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "Why is my decrypted disk not mounting after following the procedure?",
            "answer": "Ensure the fstab entry for the disk is uncommented:\n```bash\ncat /etc/fstab | grep '/data/1'\n```\nThen, try manually mounting:\n```bash\nmount -a\n```\nIf the issue persists, check system logs:\n```bash\ndmesg | tail -50\n```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What should I check if the TeMIP Main Application is not receiving alarms?",
            "answer": "First, check the application logs using `temip-tailog` for any error messages. Then, verify that the TeMIP Server is up using `ping 999.999.999.999`. If necessary, contact a TeMIP admin to investigate any server-side issues.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What are the common reasons for an HBase region server to restart unexpectedly?",
            "answer": "Common reasons for an HBase region server restart include:\n1. **Out-Of-Memory (OOM) errors**: When memory allocation exceeds available resources, the server restarts.\n2. **Table misconfiguration**: High per-region workload leading to uneven distribution among RegionServers.\n3. **Metadata inconsistencies**: If metadata for deleted tables still exists, it can cause issues.\nTo resolve, analyze logs at `/var/log/hbase/hbase-cmf-hbase-REGIONSERVER-dr1node01.mno.gr.log.out`, and if needed, remove stale metadata with `hbase hbck2 extraRegionsInMeta <TABLE_NAME> --fix`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do you troubleshoot a failed Cube Indicators execution?",
            "answer": "First, verify that the required data sources (`brond.fixed_radio_matches_unq_inp`, `radius.radacct_hist`) are populated. If necessary, rerun the indicators job manually using `./run_cube.sh <date>`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What steps should be taken when a batch job fails in Grafana for the DWH_IBank application?",
            "answer": "1. Identify the failed job in Grafana:\n   ```\n   Application: DWH_IBank\n   Job_Name: Extract\n   Component: MAN_DATE\n   ```\n2. Investigate logs to find duplicate key errors.\n3. If duplicate records exist, use the `-f` option to truncate and reinsert records:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n   ```\n4. After extraction, re-run the export script:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n   ```\n5. Validate the fix by checking the job status in Grafana.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How was the Impala daemon health issue on PR nodes resolved?",
            "answer": "The issue was identified through Cloudera logs, showing that an `Upsert to HBase` query had stopped being processed, causing Impala to stop handling other queries. The resolution steps included:\n1. Canceling the query from Cloudera UI.\n2. Restarting the Impala daemon role on pr1node01 and pr1node04.\n3. Removing `set num_nodes = 1` from the script `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`.\n4. Disabling HBase quotas for `PROD_IBANK` before re-running the script.\nThe script ran successfully after these changes.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do you reset a GROUPNET bind user password?",
            "answer": "Log in to the admin portal at `https://cne.def.gr/auth/admin`, navigate to User Federation > GROUPNET, request a password update, and update the `Bind Credential` field.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do I modify SSL settings in Apache, Nginx, and HAProxy?",
            "answer": "To modify SSL settings:\n1. **Apache (httpd)**:\n   - Backup existing config files:\n     ```bash\n     cp -ap /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.bak\n     ```\n   - Modify `/etc/httpd/conf.d/ssl.conf`:\n     ```\n     SSLProtocol +TLSv1.2\n     ```\n   - Restart Apache:\n     ```bash\n     systemctl restart httpd\n     ```\n2. **Nginx:**\n   - Backup and modify `/etc/nginx/nginx.conf`:\n     ```\n     ssl_protocols TLSv1.2;\n     ```\n   - Restart Nginx:\n     ```bash\n     systemctl restart nginx\n     ```\n3. **HAProxy:**\n   - Modify `/etc/haproxy/haproxy.cfg`:\n     ```\n     bind 999.999.999.999:25002 ssl crt no-sslv3 /opt/haproxy/security/x509/node.haproxy.pem\n     ```\n   - Restart HAProxy:\n     ```bash\n     systemctl restart haproxy\n     ```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do I create a Dell support ticket for a hardware issue?",
            "answer": "1. Retrieve the service tag from iDRAC (`Overview \u2192 Server \u2192 Logs`).\n2. Export lifecycle logs (`Overview \u2192 Server \u2192 Troubleshooting \u2192 Support Assist \u2192 Export Collection`).\n3. Open a case on Dell Support with the service tag.\n4. Send the exported TSR zip file to Dell.\n5. Follow Dell\u2019s instructions for BIOS and Lifecycle Controller updates if needed.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How is Kafka mirroring implemented for redundancy in the Online Banking system?",
            "answer": "Kafka mirroring is implemented using Kafka MirrorMaker. Each site produces messages to a topic with the `-mir` suffix, which is then mirrored to the final topic without the suffix. PR and DR sites use MirrorMakers at specific nodes (e.g., `pr1node01.mno.gr` for PR and `dr1node01.mno.gr` for DR) to replicate data between sites, ensuring redundancy.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I upgrade Java on a server running Wildfly?",
            "answer": "1. Stop Wildfly:\n```bash\nsupervisorctl stop wildfly-prodrestib\n```\n2. Install the new Java version:\n```bash\nsudo yum install java-11-openjdk\n```\n3. Update the JAVA_HOME variable:\n```bash\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk\n```\n4. Restart Wildfly:\n```bash\nsupervisorctl start wildfly-prodrestib\n```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How can I resolve an RStudio user authentication issue after resetting a password?",
            "answer": "If an RStudio user's applications fail to run after a password reset, check the LDAP authentication:\n1. SSH to `unrstudio1` and verify the `t1-svc-cnebind` account:\n   ```bash\n   ldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D 't1-svc-cnebind' -W -b 'dc=groupnet,dc=gr' '(sAMAccountName=t1-svc-cnebind)'\n   ```\n2. If the password is expired, update it in `/etc/rstudio-connect/rstudio-connect.gcfg`.\n(Source: X20220909-IM1962926.md)",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What changes are required to fix the `brond.an_rollout_data_hist` table loading issue?",
            "answer": "The issue occurs due to a script error in `000_brond_rollout_post.sh`. Modify the script as follows:\n1. Change the query in `000_brond_rollout_post.sh`:\n   ```sql\n   from:\n   ( select eett,dslam, colid,colvalue from brond.brond_rollout_data_hist where par_dt='YYYYMMDD' ) d on c.colid=d.colid\n   to:\n   ( select eett,dslam, colid colid1,colvalue from brond.brond_rollout_data_hist where par_dt='YYYYMMDD' ) d on c.colid=d.colid1\n   ```\n2. Reload missing data using:\n   ```bash\n   /shared/abc/brond/bin/000_brond_rollout_post.sh YYYYMMDD\n   ```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do you change the domain in RStudio Connect?",
            "answer": "Backup the current configuration, update `/etc/rstudio-connect/rstudio-connect.gcfg` with new LDAP settings, and restart the service with `systemctl restart rstudio-connect`.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What steps should be taken when CDSW nodes are overloaded and causing downtime?",
            "answer": "When CDSW nodes are overloaded, review system resources on the nodes (`mncdsw1, wrkcdsw1-wrkcdsw6`). If resources are maxed out, optimize workloads or scale up infrastructure. Logs can be checked via Cloudera Manager.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do you configure Viavi Kafka connectivity with BigStreamer?",
            "answer": "Modify `haproxy.cfg` to include new brokers, restart HAProxy, and update DNS records in the cluster to map internal hostnames to Viavi Kafka brokers.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How can I check the status of Cloudera Data Science Workbench (CDSW) if it is unavailable?",
            "answer": "You can check the CDSW status using:\n\n```bash\ncdsw status\nkubectl get pods -A\n```\nIf pods are not running, check Cloudera Manager for resource usage on CDSW hosts.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What should be done before manually deleting old partitions from an HDFS table?",
            "answer": "Before deleting old partitions:\n1. Check the current number of partitions:\n   ```sql\n   SHOW PARTITIONS prod_trlog_ibank.service_audit_old;\n   ```\n2. Identify the oldest partition dates.\n3. Ensure no active queries are running on those partitions.\n4. Notify monitoring teams about possible alerts.\n5. Delete partitions in controlled batches:\n   ```sql\n   ALTER TABLE service_audit_old DROP IF EXISTS PARTITION(par_dt<=20200715) PURGE;\n   ```\n6. Monitor Cloudera Manager for any unexpected issues.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How can I configure HBase quotas?",
            "answer": "Use the HBase shell:\n```bash\nhbase shell\nset_quota 'table_name', 'THROTTLE', 'WRITE', 100MB\n```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How is authentication to Impala handled in the IPVPN-SM application?",
            "answer": "Authentication is done using Kerberos with configuration files including `/etc/krb5.conf`, `/shared/abc/ip_vpn/sm-app/deployment/config/jaas.conf`, and the keytab file.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What steps should be followed to resolve a bad health issue in Cloudera Manager for a node?",
            "answer": "1. Log in to Cloudera Manager and check the status of the affected host.\n2. Navigate to `https://dr1edge01.mno.gr:7183/cmf/hardware/hosts` to inspect the host.\n3. Verify disk usage using the command:\n   ```bash\n   df -h\n   ```\n4. If the `/var` partition is full, identify large directories using:\n   ```bash\n   sudo du -sh /var/*\n   ```\n5. If necessary, delete unnecessary files, such as old Graphite logs, to free up space.\n6. Re-check disk usage and confirm that the Cloudera Manager alert is resolved.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How can Cloudera Manager memory leak issues be mitigated for idm2.bigdata.abc.gr?",
            "answer": "The memory leak issue in Cloudera Manager is associated with the Cloudera Manager Agent process (`cmf-agent`). To troubleshoot, log in to the affected node and check memory usage with `ps aux --sort -rss`. If `cmf-agent` is consuming excessive memory, verify if the node is stuck downloading parcels. If so, remove the host from the cluster using Cloudera Manager (`Hosts > All Hosts > Actions > Remove from Cluster`). Finally, restart the agent with `systemctl restart cloudera-scm-agent` to stabilize memory usage.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I renew an expired RStudio Connect license?",
            "answer": "To renew an expired RStudio Connect license:\n1. SSH into the RStudio server:\n   ```bash\n   ssh unrstudio1\n   ```\n2. Ensure the system time is correct:\n   ```bash\n   sudo timedatectl\n   sudo hwclock -w\n   ```\n3. Deactivate the existing license:\n   ```bash\n   /opt/rstudio-connect/bin/license-manager deactivate\n   ```\n4. Activate the new license:\n   ```bash\n   /opt/rstudio-connect/bin/license-manager activate <product-key>\n   ```\n5. Restart RStudio Connect:\n   ```bash\n   systemctl restart rstudio-connect\n   ```\n6. Verify the activation:\n   ```bash\n   /opt/rstudio-connect/bin/license-manager verify\n   ```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I monitor Wildfly access logs?",
            "answer": "To check access logs for `prodrestib`, run:\n```bash\ntail -f /var/log/wildfly/prodrestib/access.log\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you check for duplicate records in the Energy Efficiency dataset?",
            "answer": "Run the following Impala queries:\n```bash\nSELECT COUNT(*), par_dt FROM energy_efficiency.cell WHERE par_dt > '202111201' GROUP BY par_dt ORDER BY par_dt DESC;\nSELECT COUNT(*) FROM (SELECT DISTINCT * FROM energy_efficiency.cell WHERE par_dt='20211210') a;\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you verify the number of exported Location Mobility records?",
            "answer": "Check the reconciliation log:\n```bash\ncat /shared/abc/location_mobility/logging/LM_05_voiceInOut_reconciliation.log\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do I check HBase table quotas?",
            "answer": "Use the following HBase shell command:\n```bash\nlist_quotas\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can you remove duplicate records before running an ingestion job?",
            "answer": "1. Check for duplicate keys in the source table before ingestion.\n2. If duplicates exist, truncate the table using:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n   ```\n3. After the extract process, rerun the export process to re-populate the table:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n   ```\n4. Validate that the duplicate records are removed before processing new data.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you execute Cube Indicators processing via terminal?",
            "answer": "Login to `un1.bigdata.abc.gr`, remove the old script, fetch the new script from HDFS, modify `run_cube.sh` with the correct execution date, and execute `./run_cube.sh`.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you check failed Location Mobility exports?",
            "answer": "Run:\n```bash\ncat /shared/abc/location_mobility/logging/LM_*_reconciliation.log\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do I check the number of entries in `sai.voice_quality_hist` over the last 7 days?",
            "answer": "Use the following SQL query in Impala to check the number of entries in `sai.voice_quality_hist` over the last 7 days:\n```sql\nselect par_dt, count(*) from sai.voice_quality_hist group by par_dt order by par_dt;\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can I determine which Hive queries caused an Out-Of-Memory crash?",
            "answer": "To identify which Hive queries caused an Out-Of-Memory crash, follow these steps:\n1. Search HiveServer2 logs for Java Heap Space errors:\n```bash\ngrep Java /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node04.mno-gr.log.out\n```\n2. Identify corresponding YARN applications that failed:\n```bash\nCluster -> Yarn -> Applications -> Filter: \"application_type = MAPREDUCE\"\n```\n3. List all failed queries:\n```bash\n14:19 application_1665578283516_50081 user:E30825\n14:25 application_1665578283516_50084 user:E30825\n...\n```\n4. If multiple queries overloaded the system, consider increasing Java Heap Space or optimizing query execution strategies.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you troubleshoot performance degradation in Impala caused by concurrent client connections?",
            "answer": "When multiple 'Impala Concurrent Client Connections' alarms appear, follow these steps:\n1. Identify the root cause by checking query logs in Cloudera.\n2. If queries are stalled, cancel them from `Cloudera > Impala > Queries`.\n3. Restart the Impala daemon on affected nodes.\n4. If the issue persists, analyze memory usage and consider redistributing workload across nodes.\nThese steps ensure that Impala recovers from heavy client loads and resumes processing efficiently.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do I check HBase replication status?",
            "answer": "Use the following command:\n```bash\nhbase shell\nstatus 'replication'\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can I enable access control lists (ACLs) for Yarn and Spark?",
            "answer": "To enable ACLs for YARN and Spark:\n1. Modify the **YARN ACL Configuration**:\n   - Navigate to YARN \u2192 Configuration \u2192 Search for `ACL For Viewing A Job`.\n   - Add extra groups for map-reduce job viewing:\n     ```\n     hue WBDADMIN,WBDOPDEV,WBDOPPRO,WBDOPQA\n     ```\n   - Enable `JobHistory Server Default Group`.\n2. Modify **Spark ACL Configuration**:\n   - Search for `Spark Client Advanced Configuration Snippet`.\n   - Enable Spark ACLs:\n     ```\n     spark.acls.enable=true\n     ```\n   - Configure admin groups:\n     ```\n     spark.admins.acls.groups=WBDADMIN\n     ```\n   - Grant access to Spark History Server:\n     ```\n     spark.history.ui.admin.acls.groups=WBDADMIN\n     ```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you verify OpenLDAP replication status between kerb1 and kerb2?",
            "answer": "Use the following command to check if the `testuser` entry exists on kerb2:\n```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\" 'uid=testuser'\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can I manually update a failed job\u2019s status in Impala to allow the script to re-run?",
            "answer": "If a script fails because a previous job is stuck in 'RUNNING' status, update it in Impala as follows:\n1. Login to `dr1edge01` and change to the appropriate user: \n```bash\nsudo su - PRODUSER\n```\n2. Open Impala shell:\n```bash\nimpala-shell -k -i dr1edge01.mno.gr --ssl\n```\n3. Run the update query:\n```sql\nUPSERT INTO prod_trlog_ibank_analytical.dwh_monitoring \n(details_type, procedure_par_dt, status, start_time, end_time, description) \nVALUES ('TIME_DEPOSIT', 'EXTRACT', '20221003', 'SUCCESS', '2022-10-04', '08:32-42.000', '2022-10-04', '08-39:21.000');\n```\nAfter this, the script should be able to proceed.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "What is the command to check failed executions in the Monitoring database for IPVPN-SM?",
            "answer": "Run the following MySQL query:\n```\nSELECT * FROM jobstatus WHERE application='IPVPN-SM' AND status='FAILED' AND system_ts >= NOW() - INTERVAL 1 DAY;\n```",
            "category": "Data Management & Query Execution"
        }
    ]
}