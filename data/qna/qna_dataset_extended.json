{
    "qa_pairs": [
      {
        "question": "How does the Brond Retrains Flow handle failed monitoring jobs, and how can failures be diagnosed?",
        "answer": "The Brond Retrains Flow logs failures in the `jobstatus` table of the monitoring database. To diagnose failures, administrators can use MySQL queries to list failed monitoring messages related to 'BROND_RETRAINS'. Additionally, logs stored at `/shared/abc/brond/DataParser/scripts/log` can be searched for keywords like 'error', 'fail', or 'exception' to pinpoint the issue."
      },
      {
        "question": "What are the primary differences in data processing between the Brond Retrains Flow and Brond ADSL/VDSL Stats Flow?",
        "answer": "Both flows fetch raw files from an FTP server and process them before loading into HDFS. However, the Brond Retrains Flow primarily deals with training data for network analytics, while the Brond ADSL/VDSL Stats Flow processes DSL performance statistics. Additionally, Retrains Flow loads into `brond.brond_retrains_hist`, whereas ADSL/VDSL Stats Flow has separate tables for ADSL (`brond.brond_adsl_stats_daily`) and VDSL (`brond.brond_vdsl_stats_daily`)."
      },
      {
        "question": "How does the Piraeus Cisco VDSL2 App utilize Kubernetes for its data processing?",
        "answer": "The Piraeus Cisco VDSL2 App runs inside a Kubernetes pod under the `piraeus-cisco-vdsl2-deployment` namespace. It uses a cron-based scheduler to poll SNMP data every 5 minutes, processes the data into CSV format, merges multiple files, and then transfers them to an SFTP server and an HDFS directory."
      },
      {
        "question": "What are the main steps of the CSI-Redis Flow, and how is Redis used in the process?",
        "answer": "CSI-Redis Flow extracts and aggregates data from HDFS using Spark jobs, then exports it to Redis for real-time analytics. The key steps are: (1) Fetch data from HDFS, (2) Aggregate metrics using Spark, (3) Export data to `/user/rediscsi/docx-data/csi/parquet/`, (4) Archive and transfer data to Redis, and (5) Load data into Redis via the `102.CSI_Redis_Load_Data.sh` script."
      },
      {
        "question": "How does the DWHFixed Flow handle full and delta data processing workflows?",
        "answer": "The DWHFixed Flow has a full workflow that runs at 15:30 and 18:30 UTC and a delta workflow that runs every two hours. The full workflow extracts complete data from Oracle source tables and loads them into Hive-Impala, whereas the delta workflow identifies and processes incremental data based on changes recorded in the `SAS_VA_VIEW.V_DW_CONTROL_TABLE`."
      },
      {
        "question": "What is the purpose of the Energy-Efficiency Pollaploi Flow, and how does it integrate with Impala?",
        "answer": "The Energy-Efficiency Pollaploi Flow processes `.txt` files containing energy efficiency data. It retrieves files via SFTP, transfers them to HDFS, and then loads the data into the `energy_efficiency.pollaploi` table in Impala using `LOAD DATA INPATH` commands."
      },
      {
        "question": "How do the IPVPN Flow and IPVPN-SM Replacement process work together?",
        "answer": "The IPVPN Flow collects network performance metrics, such as CPU load, memory usage, and Quality of Service (QoS), and stores them in Impala tables. The IPVPN-SM Replacement process extracts this data, applies transformations via a Spring Boot application, and sends the computed KPIs to an external Service Quality Management (SQM) system."
      },
      {
        "question": "What are the primary logs and monitoring tools used for debugging failures in the IPVPN-SM Replacement process?",
        "answer": "Logs for the IPVPN-SM Replacement process are stored at `/shared/abc/ip_vpn/sm-replacement/log` and `/shared/abc/ip_vpn/sm-app/deployment/logs/`. Monitoring is available via MySQL queries on the `jobstatus` table and through a Grafana dashboard at `https://unc1.bigdata.abc.gr:3000/d/HiuaKlU4z/ipvpn-sm-replacement-monitoring`."
      },
      {
        "question": "What API endpoints are available in the Monitoring Application, and how are they used?",
        "answer": "The Monitoring Application provides endpoints such as `/monitoring/app/status` to check if the service is running, `/monitoring/app/lb/check` to verify the load balancer, and `/monitoring/app/lb/enable` or `/monitoring/app/lb/disable` to manage traffic routing."
      },
      {
        "question": "How does the def_NETWORK_MAP Flow synchronize data between Oracle and Big Data systems?",
        "answer": "The def_NETWORK_MAP Flow exports network data from an Oracle database, moves it to HDFS, and loads it into Hive tables. Synchronization is managed by an Oozie workflow that runs every 5 minutes, ensuring that network-related changes are reflected in the Big Data system."
      },
      {
        "question": "What steps should be taken to manually restart the IPVPN-SM application?",
        "answer": "To restart the IPVPN-SM application, execute `sudo systemctl restart ipvpn-sm` on the application server. If additional debugging is needed, check `/shared/abc/ip_vpn/sm-app/deployment/logs/application.log` for errors."
      },
      {
        "question": "What monitoring tools are used across the different flows described in the documents?",
        "answer": "Monitoring is primarily done via MySQL queries on the `jobstatus` table, Grafana dashboards, and custom log files stored in HDFS. Some flows, such as Brond Retrains and CSI-Redis, also provide API-based health checks."
      },
      {
        "question": "How does the IPVPN Flow ensure real-time data ingestion?",
        "answer": "The IPVPN Flow uses a combination of SNMP polling and Flume agents to continuously collect and ingest network performance metrics into Impala tables, ensuring near real-time processing."
      },
      {
        "question": "What is the function of the HAProxy Load Balancer in the IPVPN-SM process?",
        "answer": "The HAProxy Load Balancer (`un-vip:13001`) distributes API requests across multiple application servers, ensuring high availability and failover protection for the IPVPN-SM system."
      },
      {
        "question": "How does the Piraeus Cisco VDSL2 App transform SNMP data before storage?",
        "answer": "The app first converts SNMP output into CSV format, merges files with the same timestamp, and then transfers the processed data to an SFTP server and an HDFS directory."
      },
      {
        "question": "How does the Brond Retrains Flow handle data validation?",
        "answer": "The Brond Retrains Flow uses Impala shell commands to check the latest partitions in `brond.brond_retrains_hist` and runs SQL queries to validate data integrity before marking partitions as loaded."
      },
      {
        "question": "How does the CSI-Redis Flow leverage Spark for data aggregation?",
        "answer": "Spark jobs, such as `AggregateRdCells` and `AggregateCsiPrimary`, are executed to compute key metrics before the data is exported to Redis for further analysis."
      },
      {
        "question": "What security mechanisms are used for data transfers in the Brond ADSL/VDSL Stats Flow?",
        "answer": "Data is transferred securely using SFTP with password authentication and SSH key-based authentication for HDFS transfers."
      },
      {
        "question": "What are the main dependencies required for the Energy-Efficiency Pollaploi Flow to function properly?",
        "answer": "Dependencies include SSH access to the utility node (`un2.bigdata.abc.gr`), proper Oozie coordinator configuration, and an active SFTP connection for retrieving `.zip` files."
      },
      {
        "question": "How does the Monitoring Application ensure that logs are available for debugging?",
        "answer": "Logs are stored in `/opt/monitoring-app/logs/monitoring-api.log` and are accessible through the monitoring API at `http://999.999.999.999:12800/monitoring/app/status`."
      },
      {
        "question": "How does the Prometheus Flow ensure data consistency when importing from Oracle to Hive?",
        "answer": "The Prometheus Flow uses Sqoop to extract data from the Oracle database (DWSRC.DWH22) and stores it in an HDFS staging directory before loading it into Hive. A Hive refresh operation ensures the latest data is available in Impala. The process runs daily at 06:30 AM UTC and is monitored through logs and partition validation queries in Impala."
      },
      {
        "question": "What are the key error scenarios in the Radius Flow and how are they handled?",
        "answer": "The Radius Flow encounters errors such as missing SFTP files, Trustcenter SFTP failures, and data insertion failures. These are handled through automated alerts, log analysis, and manual retries. The monitoring system tracks failed executions using MySQL queries, while failed file transfers can be reattempted via the Trustcenter SFTP server."
      },
      {
        "question": "How does the Reference Data Flow manage historical snapshots of reference data?",
        "answer": "The Reference Data Flow runs a two-step process: first, the 210_refData_Load.sh script reads and loads new reference data into Hive LOAD tables. Then, the 220_refData_Daily_Snapshot.sh script extracts the latest snapshot from these tables and stores it separately for historical tracking. The process is scheduled daily at 00:05 UTC."
      },
      {
        "question": "How does the Streamsets Flow process and store energy efficiency data?",
        "answer": "The Streamsets Flow ingests energy efficiency data from SFTP sources, processes them using Streamsets pipelines, and stores them in HDFS directories before loading into Hive tables. The flow includes multiple subflows for different energy parameters, such as enodeb_vswr, tcu_temperatures, and baseband tests."
      },
      {
        "question": "What are the main functions of the Syzefxis Flow in terms of network monitoring?",
        "answer": "The Syzefxis Flow collects network performance data, processes SLA metrics, and transfers them to BigStreamer nodes. It categorizes KPIs into separate tables, performs transformations using Spark, and generates reports on a daily and monthly basis through Oozie workflows."
      },
      {
        "question": "What are the four key components of the TeMIP Flow, and how do they interact?",
        "answer": "The TeMIP Flow consists of four components: Initialization/Synchronization, Main Application, Move Kudu to Impala, and Alert Mail. The Main Application receives and stores alarms in Kudu, while the Move Kudu to Impala flow transfers historical alarms for extended retention. The Alert Mail system notifies administrators if no alarms are available."
      },
      {
        "question": "How does the Traffica Flow process SMS and VOICE data, and how is it scheduled?",
        "answer": "The Traffica Flow processes SMS data every 35 minutes and VOICE data every 20 minutes using a Java Spring Boot application. The flow involves transferring data to a staging area, processing it in HDFS and Hive, and ultimately storing it in Impala. The system includes automated monitoring and alert mechanisms for failures."
      },
      {
        "question": "How does the TrustCenter Flow handle data extraction for multiple applications?",
        "answer": "The TrustCenter Flow extracts data from BigStreamer for various use cases, including Location Mobility, Router Analytics, Application Data Usage Insights, and Customer Satisfaction Index. The extracted data is compressed and transferred to an exchange directory for further processing by TrustCenter."
      },
      {
        "question": "What role does Oozie play in automating the Reference Data Flow?",
        "answer": "Oozie automates the Reference Data Flow by scheduling the execution of scripts that load reference data into Hive and generate daily snapshots. This ensures consistency in historical data tracking and reduces manual intervention."
      },
      {
        "question": "What monitoring mechanisms are in place for the Prometheus Flow?",
        "answer": "The Prometheus Flow is monitored using API calls to check job execution statuses, Grafana dashboards, and Impala queries to verify partition loads. Alerts are generated if a process fails, prompting administrators to take corrective action."
      },
      {
        "question": "How does the Syzefxis Flow ensure SLA compliance for network operations?",
        "answer": "The Syzefxis Flow collects raw performance data every 5 minutes, processes SLA metrics through scheduled transformations, and categorizes KPIs into separate tables. The system runs daily and monthly KPI calculations using Oozie workflows to ensure compliance with SLA agreements."
      },
      {
        "question": "What data sources feed into the Radius Flow, and how is data validated?",
        "answer": "The Radius Flow ingests SFTP files, processes them in Hive and Impala, and exports validated files to a Trustcenter SFTP server. Data validation occurs at multiple points, including SFTP filename checks, partition updates in Impala, and cross-checks against reference tables."
      },
      {
        "question": "How does the TeMIP Flow maintain high availability for alarm processing?",
        "answer": "The TeMIP Flow uses an active-standby Wildfly server setup, where the primary server processes alarms and the secondary server acts as a backup. If the active server fails, operations are switched to the standby server, ensuring continuous alarm processing."
      },
      {
        "question": "How is the Traffica Flow designed to handle failure recovery?",
        "answer": "The Traffica Flow automatically marks successfully loaded files with a .LOADED suffix and maintains staging directories for data recovery. If failures occur, administrators can manually check logs, reattempt file transfers, or restart the flow via API commands."
      },
      {
        "question": "What are the main causes of errors in the Reference Data Flow, and how are they mitigated?",
        "answer": "Common errors in the Reference Data Flow include empty reference files, Hive/Impala service unavailability, and Kerberos authentication failures. These issues are mitigated by verifying input files, restarting services, and refreshing Kerberos tickets."
      },
      {
        "question": "How does the Streamsets Flow partition data in Hive based on file timestamps?",
        "answer": "The Streamsets Flow extracts timestamps from filenames (e.g., data_20181121123916.csv) and uses them to create partition keys in Hive. This ensures that data is efficiently organized and queried based on the date of ingestion."
      },
      {
        "question": "How does the TrustCenter Flow validate the integrity of exported files?",
        "answer": "The TrustCenter Flow maintains reconciliation logs that record execution timestamps, file names, and line counts. These logs are cross-checked against BigStreamer tables to ensure that all expected data has been extracted and transferred correctly."
      },
      {
        "question": "How does the Syzefxis Flow ensure reliable data transfer from network monitoring systems?",
        "answer": "The Syzefxis Flow transfers data using SFTP with passwordless authentication, stores it in a local staging area, and then moves it to an HDFS destination path. The process runs every 10 minutes to ensure timely synchronization."
      },
      {
        "question": "What role does the Move Kudu to Impala flow play in the TeMIP system?",
        "answer": "The Move Kudu to Impala flow is responsible for transferring alarm data from Kudu to Impala for extended storage. It runs daily at 06:00 local time and ensures that historical alarms are retained for six months."
      },
      {
        "question": "How does the Prometheus Flow handle failed executions?",
        "answer": "If the Prometheus Flow encounters a failure, alerts are generated via email, and logs can be checked in Hue. Failed partitions can be dropped and reloaded in Impala, and the import workflow can be manually rerun via Oozie."
      },
      {
        "question": "How can you diagnose and resolve missing logs in BigStreamer when using rsyslog?",
        "answer": "To diagnose missing logs in BigStreamer, first SSH into the `un2` server as root and check the `/etc/rsyslog.conf` file to verify the correct log transfer configurations. Next, inspect `/etc/logrotate.conf` to check if logs are being rotated too frequently, causing loss. If necessary, increase logging verbosity by modifying `/etc/rsyslog.d/*.conf` and restarting the rsyslog service with `systemctl restart rsyslog`."
      },
      {
        "question": "What are the key steps to securely disable root SSH access across all BigStreamer nodes while maintaining administrative operations?",
        "answer": "To disable root SSH access securely, first inspect all nodes' SSH configurations using `salt '*' cmd.run 'grep \"^PermitRootLogin\" /etc/ssh/sshd_config'`. Identify any cron jobs and scripts that rely on root SSH by searching for `ssh`, `scp`, and `rsync` commands within `/root` and `/usr/local/bin`. Then, create a non-root user (`backup_user`) with the necessary privileges, update ACLs using `setfacl`, and modify scripts to use `backup_user`. Finally, apply the changes using SaltStack scripts and reload SSHD: `salt '*' cmd.script salt://disable_root_login.sh`."
      },
      {
        "question": "What steps should be taken to troubleshoot missing RA_Dsession and RA_Dtraffic exports?",
        "answer": "Start by logging into `un2.bigdata.abc.gr` and switching to `mtuser`. Check export logs using `less /shared/abc/location_mobility/log/ra_export_bs_01.oozie.<date>.log`. If logs indicate missing partitions, validate with Impala queries such as `SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '<date>'`. If the export is missing, manually trigger the export using `./export_ra_bs_01.sh -t <date>`."
      },
      {
        "question": "How does Flume handle CPU and memory usage metric ingestion in BigStreamer?",
        "answer": "Flume ingests CPU and memory usage metrics by transferring raw files from NNM nodes to a local spool directory every 5 minutes. The process involves parsing and transforming records using Morphline, which extracts the correct timestamp (`field_min_5`). Issues with timestamp rounding can be fixed by modifying `/shared/abc/ip_vpn/flume/nnm_component_metrics/morphline_nnmMetricsCsvToRecord_ipvpn_user.conf` to use `rounding:mathematical`. Flume restarts are required for the changes to take effect."
      },
      {
        "question": "How do you debug missing CPU_LOAD and MEMORY_USAGE files in BigStreamer?",
        "answer": "First, inspect export logs using `less /shared/abc/ip_vpn/log/initiate_export_components.cron.<date>.log`. If missing, check Flume logs for the input data transfer (`/var/log/flume-ng/flume-cmf-flume5-AGENT-*.log`). Next, verify Impala queries are fetching the correct timestamps: `SELECT * FROM bigcust.nnm_ipvpn_componentmetrics_hist WHERE par_dt='<date>'`. If queries return no results, restart the Flume agent and inspect whether the timestamps align with expected intervals."
      },
      {
        "question": "How can you reduce latency in Impala queries for exported IPVPN component metrics?",
        "answer": "Latency can be reduced by optimizing table schemas and partitioning. Start by checking if excessive partition refreshes are slowing down queries. In Hue, analyze query execution times with `PROFILE` and identify slow steps. Consider optimizing schema definitions, indexing frequently queried columns, and reducing partition fragmentation by consolidating small partitions."
      },
      {
        "question": "What troubleshooting steps should be followed when a Spark job in the Geolocation pipeline fails?",
        "answer": "First, check Spark job logs and error codes (`yarn logs -applicationId <app_id>`). Verify if HDFS contains older failure logs in `/ez/warehouse/geolocation.db/geo_<technology>_fail/`. If the issue persists, delete older logs beyond the retention period (2 days for named, 1 day for anonymous geolocation tables). If resource exhaustion is suspected, increase executor memory and core settings in the Spark configuration."
      },
      {
        "question": "How do you resolve Impala stale metadata issues in Hive-managed tables?",
        "answer": "When Impala reports stale metadata (e.g., `invalid version number`), first check if the error is partition-specific: `SELECT * FROM <table> WHERE par_dt='<date>';`. If confirmed, refresh metadata using `REFRESH <table> PARTITION (par_dt='<date>');`. If the problem persists, inspect HDFS file formats using `parquet-tools meta <file>`, and if corruption is found, move corrupted files out of the dataset and rerun table refresh commands."
      },
      {
        "question": "How can you ensure correct partition alignment between source and exported CSI_fix files?",
        "answer": "Validate partition alignment by checking `brond.cube_indicators` and its dependencies (`brond.brond_retrains_hist`, `radius.radacct_hist`) using Impala queries such as `SELECT COUNT(*) FROM brond.brond_retrains_hist WHERE par_dt >= '<date>';`. If missing partitions are found, manually rerun the Cube Indicators workflow (`Coord_Cube_Spark_Indicators`) in Hue and then verify data counts before exporting missing CSI_fix files."
      },
      {
        "question": "How can you force a re-export of missing CSI_fix files for specific dates?",
        "answer": "If CSI_fix files were empty due to missing partitions, first rerun the `Coord_Cube_Spark_Indicators` workflow in Hue to regenerate missing data. Then, execute `/shared/abc/export_sai_csi/export_csi_fix.sh <date>` sequentially for each missing date. Verify successful exports by checking `/shared/abc/export_sai_csi/logging/CSI_fix_reconciliation.log`."
      },
      {
        "question": "What steps should be taken to diagnose and fix server CPU issues causing cluster failures?",
        "answer": "Begin by checking iDRAC logs (`Overview-->Server-->Logs`). Export lifecycle logs (`Overview-->Server-->Troubleshooting-->Support Assist`). If hardware issues are suspected, update BIOS and iDRAC firmware using CLI-based updates. Run `ipmitool mc getsysinfo system_fw_version` to verify firmware versions. If the issue persists, open a Dell support case and provide TSR logs."
      },
      {
        "question": "How can you verify if Osix SIP ingestion has stalled due to a topology failure?",
        "answer": "First, SSH into `unosix1`, switch to `osix`, and authenticate via Kerberos (`kinit -kt osix.keytab osix`). Check if the topology is down using `yarn application -list | grep OSIX-SIP-NORM`. If it is not running, resubmit it with `./submit_sip_norm.sh`. Verify successful ingestion by executing `SELECT count(*), par_dt FROM osix.sip WHERE par_dt > '<date>'` in Impala."
      },
      {
        "question": "What automated mechanisms exist to handle Spark job failures in the Geolocation pipeline?",
        "answer": "Geolocation jobs use a `Failure Handling Manual Mechanism`, where failed executions are logged and retried automatically if necessary. Older failure logs stored in `/ez/warehouse/geolocation.db/geo_<technology>_fail/` can be cleaned up if they exceed retention limits. Manual intervention is rarely needed unless repeated failures occur."
      },
      {
        "question": "How does the Osix SIP monitoring mechanism detect and restart stalled ingestion jobs?",
        "answer": "Osix SIP monitoring checks listener health at `http://172.25.37.251/dashboard/#osix_listeners`. If `listen_sip_core` messages drop below 12K per second, an alert is triggered. If ingestion is stalled, `coord_OsixStreaming_SIP_MonitorResubmit` automatically restarts the pipeline. If the topology does not restart, it must be manually resubmitted using `./submit_sip_norm.sh`."
      },
      {
        "question": "What Impala query can be used to diagnose delayed exports in IPVPN component metrics?",
        "answer": "To diagnose delays, execute: `SELECT count(*) FROM bigcust.nnm_ipvpn_componentmetrics_hist WHERE min_5='<timestamp>' AND par_dt='<date>';`. If the count is unexpectedly low, inspect the ingestion process (`less /shared/abc/ip_vpn/log/nnm_component_metrics.cron.<date>.log`). If input metrics were loaded late, queries may return incorrect results due to outdated metadata."
      },
      {
        "question": "What logs should be analyzed when IPVPN export files are delayed beyond expected transfer times?",
        "answer": "First, check Flume logs (`/var/log/flume-ng/flume-cmf-flume5-AGENT-*.log`). Then inspect export component logs (`less /shared/abc/ip_vpn/log/initiate_export_components.cron.<date>.log`). If file transfers are slow, examine disk I/O using `iostat -xm 1` and check Cloudera Manager for increased disk queue lengths."
      },
      {
        "question": "How can you identify and correct missing partitions in the `refdata.rd_cells_load` table using Impala and HDFS?",
        "answer": "To identify missing partitions, first check the existing ones in Impala:\n```bash\nshow partitions refdata.rd_cells_load;\n```\nIf an expected partition is missing or has significantly fewer rows, inspect the corresponding HDFS files:\n```bash\nhdfs dfs -ls /ez/warehouse/refdata.db/rd_cells_load/\n```\nIf a missing partition is detected, copy data from an adjacent partition and rename the previous file:\n```bash\nhdfs dfs -cp /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/\nhdfs dfs -mv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/.cells_20201110.csv\n```\nFinally, refresh Impala metadata:\n```bash\nimpala-shell -q \"refresh refdata.rd_cells_load;\"\n```"
      },
      {
        "question": "How do you debug an energy efficiency data pipeline failure in the `pollaploi` table?",
        "answer": "Start by verifying the workflow status in Hue:\n```bash\ngrep -i 'energy_efficiency_load_pollaploi' /shared/abc/energy_efficiency/logs/\n```\nThen, inspect the latest logs to determine if the failure is due to missing files or query execution errors:\n```bash\nless /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log\n```\nIf data ingestion is stalled, compare the expected row count with Impala:\n```bash\nselect count(*) from energy_efficiency.pollaploi;\n```\nIf the counts mismatch, re-execute the workflow manually:\n```bash\nsudo -iu intra /shared/abc/energy_efficiency/load_pollaploi.sh\n```"
      },
      {
        "question": "What steps should be taken if BigStreamer services fail due to an HDFS Namenode failure?",
        "answer": "If a Namenode is in standby mode and HDFS, Impala, or Oozie services are unhealthy:\n1. Log in to Cloudera Manager:\n```bash\nhttps://172.25.37.232:7183\n```\n2. Restart the inactive Namenode:\n```bash\nhdfs haadmin -transitionToActive nn2\n```\n3. Verify active Namenode status:\n```bash\nhdfs haadmin -getServiceState nn2\n```\n4. Check if Oozie workflows resume automatically:\n```bash\nhttps://172.25.37.236:8888/oozie/list_oozie_workflows/\n```"
      },
      {
        "question": "How can you backfill missing `radius.radacct_hist` data from SFTP when ingestion fails?",
        "answer": "Identify missing data using HDFS:\n```bash\nhdfs dfs -ls /ez/warehouse/radius.db/radacct_orig_files/ | grep '20201220'\n```\nThen, fetch missing files from SFTP:\n```bash\nsftp prdts@79.128.178.35\nget radacct_2020-12-20_04-30.csv.bz2\n```\nMove the retrieved files to the correct location and update ingestion settings:\n```bash\nmv radacct_2020-12-20_04-30.csv.bz2 /shared/radius_repo/cdrs/\n```\nManually trigger the ingestion process:\n```bash\n/shared/abc/radius/bin/000_radius_ops.sh\n```"
      },
      {
        "question": "How can you force the regeneration of missing `CSI_fix` data due to failed partition loads?",
        "answer": "Verify missing partitions in the `brond.dsl_stats_week_xdsl_hist` table:\n```bash\nimpala-shell -q \"select count(*), par_dt from brond.dsl_stats_week_xdsl_hist where par_dt >= '20210115' group by 2 order by 2;\"\n```\nIf data is missing, re-execute insert queries stored in:\n```bash\n/user/intra/brond_dsl_stats/impala-shell/populate_dsl_stats.sql\n```\nUpdate conditions to include missing dates (`20210119`) and re-run in Impala:\n```bash\nimpala-shell -f /user/intra/brond_dsl_stats/impala-shell/populate_dsl_stats.sql\n```"
      },
      {
        "question": "How do you resolve high resource consumption in Cloudera Data Science Workbench (CDSW) that prevents job scheduling?",
        "answer": "If jobs remain in a `Scheduled` state with no available CPU/GPU:\n1. Access the CDSW admin panel:\n```bash\nhttps://mncdsw1.bigdata.abc.gr/\n```\n2. Identify PENDING pods:\n```bash\nkubectl get pods | grep Pending\n```\n3. Force delete stuck jobs:\n```bash\nkubectl get pods | grep Pending | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n```"
      },
      {
        "question": "What steps are needed to test the integration of NNMi services with `nnmprd01`?",
        "answer": "1. Establish SSH connectivity:\n```bash\nssh custompoller@nnmprd01\n```\n2. If password is required, set up passwordless authentication:\n```bash\nssh-copy-id custompoller@nnmprd01\n```\n3. Execute the IPVPN Custompoller service:\n```bash\n/home/custompoller/ipvpn/run/java -Xms1024m -Xmx4096m -DlogFilename=/home/custompoller/ipvpn/log/ipvpn -cp bigstreamer-snmp-tools.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner\n```\n4. Check log output for successful execution:\n```bash\nless /home/custompoller/ipvpn/log/ipvpn-2021-04-21.log\n```"
      },
      {
        "question": "How can you resolve a `No such file or directory` error when running SNMP queries on NNMi?",
        "answer": "The issue is likely due to missing directories or incorrect symlinks. Check if paths exist:\n```bash\nls -l /opt/OV/_CUSTOM_POLLER/ipvpn\n```\nIf they do not exist, recreate them (requires administrator privileges):\n```bash\nsudo mkdir -p /opt/OV/_CUSTOM_POLLER/ipvpn\nsudo chown custompoller:custompoller /opt/OV/_CUSTOM_POLLER/ipvpn\n```"
      },
      {
        "question": "How can you reprocess missing `agama` schema data from SFTP?",
        "answer": "First, check if the primary data exists:\n```bash\ncrontab -l | grep agama\n```\nIf logs indicate missing data, inspect the SFTP server:\n```bash\nsftp sftp_server\nls -l /shared/abc/agama/bin/\n```\nModify the ingestion script with static date variables:\n```bash\nvim /shared/abc/agama/bin/load_agama.sh\n```\nUncomment `dt_sftp` and `dt`, then rerun ingestion:\n```bash\n/shared/abc/agama/bin/load_agama.sh\n```"
      },
      {
        "question": "How do you diagnose and recover missing NNMi custompoller data for IPVPN?",
        "answer": "Check scheduled jobs:\n```bash\ncrontab -l | grep ipvpn\n```\nManually execute the SNMP collection process:\n```bash\n/home/custompoller/ipvpn/run/java -Xms1024m -Xmx4096m -cp bigstreamer-snmp-tools.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner\n```\nEnsure that output files are correctly written:\n```bash\nls -l /home/custompoller/ipvpn/out/\n```"
      },
      {
        "question": "How do you resolve a missing data issue in the `brond.brond_retrains_hist` table caused by incorrect SFTP file naming?",
        "answer": "When data is missing in `brond.brond_retrains_hist`, check if the corresponding SFTP files were incorrectly named. The script `/shared/abc/brond/DataParser/scripts/brond_retrains.pl` determines partition names based on uploaded files with the expected format:\n```\nCounter_Collection_24H.<number>_<yyyy>_<mm>_<dd>.csv\n```\nIf files are not named correctly, the script cannot process them. Steps to resolve:\n1. Verify missing partitions in Impala:\n```sql\nSELECT count(*), par_dt FROM brond.brond_retrains_hist WHERE par_dt >= '20220101' GROUP BY par_dt ORDER BY par_dt;\n```\n2. Check SFTP for incorrectly named files:\n```bash\nsftp intra@172.16.166.30\nls -l /shared/abc/brond/sftp/\n```\n3. Request abc to re-upload files with correct names, then rerun:\n```bash\n/shared/abc/brond/bin/000_brond_retrains_ops.sh\n```"
      },
      {
        "question": "How can you reprocess missing data in `huawei_tv.rel_play_tv_hist` due to empty files being received?",
        "answer": "When `huawei_tv.rel_play_tv_hist` data is missing, first verify missing partitions in Impala:\n```sql\nSELECT count(*), par_dt FROM huawei_tv.rel_play_tv_hist WHERE par_dt BETWEEN '20210831' AND '20210901' GROUP BY par_dt ORDER BY par_dt;\n```\nIf a partition is missing, check the SFTP source:\n```bash\nsftp bigdata@172.28.128.150:/export\nls -l 20210901\n```\nIf files are empty:\n```bash\n/shared/abc/huawei_tv/bin/huawei_tv_load.sh 20210831\n```"
      },
      {
        "question": "What steps should be taken if `prometheus.dwh22_last` is empty, affecting `prometheus.prom_total_subscrs`?",
        "answer": "If `prometheus.dwh22_last` is empty, it affects `prometheus.prom_total_subscrs`. Steps to resolve:\n1. Check the latest logs:\n```bash\nless /shared/abc/prometheus/log/Cron_Prometheus_Load.date_of_issue.log\n```\n2. If the job failed, rerun it:\n```bash\n/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh\n```\n3. Verify data in Impala:\n```sql\nSELECT count(*), par_dt FROM prometheus.dwh22_last WHERE par_dt >= '<issue_date>' GROUP BY par_dt;\n```"
      },
      {
        "question": "How do you fix corrupted statistics warnings in `sai.voice_quality_hist`, `sai.sms_raw`, and `temip.temip_impala_terminated_alarms`?",
        "answer": "1. Check if tables exist in HDFS:\n```bash\nhdfs dfs -du -h -s /ez/warehouse/sai.db/voice_quality_hist\n```\n2. Check Impala statistics:\n```sql\nSHOW TABLE STATS sai.voice_quality_hist;\n```\n3. If stats are missing, recompute:\n```sql\nCOMPUTE STATS sai.voice_quality_hist;\n```\n4. If the issue persists, refresh:\n```sql\nREFRESH sai.voice_quality_hist;\n```"
      },
      {
        "question": "How do you manually backfill `radius.radarchive_hist` when a daily file is missing?",
        "answer": "To backfill missing data in `radius.radarchive_hist`, follow these steps:\n1. Check if the file exists in SFTP:\n```bash\nsftp intra@79.128.184.153\nls -l radarchive_2021-08-31.csv.bz2\n```\n2. If missing, request reupload. If present, download and extract:\n```bash\nsftp> get radarchive_2021-08-31.csv.bz2\nbzip2 -d radarchive_2021-08-31.csv.bz2\n```\n3. Move file to the correct directory and reload data:\n```bash\nmv radarchive_2021-08-31.csv /shared/radius_repo/cdrs/\n/shared/abc/radius/bin/000_radius_ops.sh\n```"
      },
      {
        "question": "What is the root cause of decreasing entries in `sai.voice_quality_hist`, and how can it be addressed?",
        "answer": "When fewer entries appear in `sai.voice_quality_hist`, verify the number of received files:\n```bash\nls -l /data/1/trafficaftp/Traffica/TrafRTTE_Voice_Quality_abc_TRAVQTNES_20211128* | wc -l\n```\nCompare against previous days:\n```bash\nls -l /data/1/trafficaftp/Traffica/TrafRTTE_Voice_Quality_abc_TRAVQTNES_20211121* | wc -l\n```\nIf fewer files were received, abc needs to investigate the source system as there is no local issue."
      },
      {
        "question": "How can you troubleshoot and restart a failed RStudio Connect license activation?",
        "answer": "If RStudio Connect shows a 'License Expired' error:\n1. Verify system time:\n```bash\nsudo timedatectl\nsudo hwclock -w\n```\n2. Deactivate and reactivate the license:\n```bash\n/opt/rstudio-connect/bin/license-manager deactivate\n/opt/rstudio-connect/bin/license-manager activate <product-key>\n```\n3. Restart RStudio Connect if activation succeeds:\n```bash\nsystemctl restart rstudio-connect\n```"
      },
      {
        "question": "How do you resolve missing `CSI_MOBILE` data in `sai.sub_aggr_csi_it`?",
        "answer": "If `CSI_MOBILE` data is missing:\n1. Check log files for failed aggregation:\n```bash\nless /shared/abc/export_sai_csi/log/sai_csi.cron.<date>.log\n```\n2. Verify table row count:\n```sql\nSELECT count(*), par_dt FROM sai.sub_aggr_csi_it WHERE par_dt >= '20210xxx' GROUP BY par_dt;\n```\n3. If data is missing, manually reprocess:\n```bash\n/shared/abc/csi/bin/csi_weekly_load.sh.manual 2\n```"
      },
      {
        "question": "How can you manually reload missing data for `brond.an_rollout_data_hist`?",
        "answer": "If `brond.an_rollout_data_hist` stops loading:\n1. Check last successful loads:\n```sql\nSELECT par_dt, count(*) FROM brond.an_rollout_data_hist GROUP BY par_dt ORDER BY par_dt DESC LIMIT 10;\n```\n2. Inspect logs for errors:\n```bash\nless /shared/abc/brond/log/brond_rollout_cron.*\n```\n3. If missing data is found, rerun:\n```bash\n/shared/abc/brond/bin/000_brond_rollout_post.sh 20211007\n```"
      },
      {
        "question": "What change is required in `brond_rollout.pl` to prevent ambiguous column errors after a system upgrade?",
        "answer": "After an upgrade, an ambiguous column reference error occurs in `brond_rollout.pl`. Modify:\n```sql\nSELECT eett, dslam, colid, colvalue FROM brond.brond_rollout_data_hist WHERE par_dt='20210927'\n```\nTo use an alias:\n```sql\nSELECT eett, dslam, colid AS colid1, colvalue FROM brond.brond_rollout_data_hist WHERE par_dt='20210927'\n```"
      },
      {
        "question": "How do you resolve a missing data issue in the `brond.brond_retrains_hist` table caused by incorrect SFTP file naming?",
        "answer": "When data is missing in `brond.brond_retrains_hist`, check if the corresponding SFTP files were incorrectly named. The script `/shared/abc/brond/DataParser/scripts/brond_retrains.pl` determines partition names based on uploaded files with the expected format:\n```\nCounter_Collection_24H.<number>_<yyyy>_<mm>_<dd>.csv\n```\nIf files are not named correctly, the script cannot process them. Steps to resolve:\n1. Verify missing partitions in Impala:\n```sql\nSELECT count(*), par_dt FROM brond.brond_retrains_hist WHERE par_dt >= '20220101' GROUP BY par_dt ORDER BY par_dt;\n```\n2. Check SFTP for incorrectly named files:\n```bash\nsftp intra@172.16.166.30\nls -l /shared/abc/brond/sftp/\n```\n3. Request abc to re-upload files with correct names, then rerun:\n```bash\n/shared/abc/brond/bin/000_brond_retrains_ops.sh\n```"
      },
      {
        "question": "How can you reprocess missing data in `huawei_tv.rel_play_tv_hist` due to empty files being received?",
        "answer": "When `huawei_tv.rel_play_tv_hist` data is missing, first verify missing partitions in Impala:\n```sql\nSELECT count(*), par_dt FROM huawei_tv.rel_play_tv_hist WHERE par_dt BETWEEN '20210831' AND '20210901' GROUP BY par_dt ORDER BY par_dt;\n```\nIf a partition is missing, check the SFTP source:\n```bash\nsftp bigdata@172.28.128.150:/export\nls -l 20210901\n```\nIf files are empty:\n```bash\n/shared/abc/huawei_tv/bin/huawei_tv_load.sh 20210831\n```"
      },
      {
        "question": "What steps should be taken if `prometheus.dwh22_last` is empty, affecting `prometheus.prom_total_subscrs`?",
        "answer": "If `prometheus.dwh22_last` is empty, it affects `prometheus.prom_total_subscrs`. Steps to resolve:\n1. Check the latest logs:\n```bash\nless /shared/abc/prometheus/log/Cron_Prometheus_Load.date_of_issue.log\n```\n2. If the job failed, rerun it:\n```bash\n/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh\n```\n3. Verify data in Impala:\n```sql\nSELECT count(*), par_dt FROM prometheus.dwh22_last WHERE par_dt >= '<issue_date>' GROUP BY par_dt;\n```"
      },
      {
        "question": "How do you fix corrupted statistics warnings in `sai.voice_quality_hist`, `sai.sms_raw`, and `temip.temip_impala_terminated_alarms`?",
        "answer": "1. Check if tables exist in HDFS:\n```bash\nhdfs dfs -du -h -s /ez/warehouse/sai.db/voice_quality_hist\n```\n2. Check Impala statistics:\n```sql\nSHOW TABLE STATS sai.voice_quality_hist;\n```\n3. If stats are missing, recompute:\n```sql\nCOMPUTE STATS sai.voice_quality_hist;\n```\n4. If the issue persists, refresh:\n```sql\nREFRESH sai.voice_quality_hist;\n```"
      },
      {
        "question": "How do you manually backfill `radius.radarchive_hist` when a daily file is missing?",
        "answer": "To backfill missing data in `radius.radarchive_hist`, follow these steps:\n1. Check if the file exists in SFTP:\n```bash\nsftp intra@79.128.184.153\nls -l radarchive_2021-08-31.csv.bz2\n```\n2. If missing, request reupload. If present, download and extract:\n```bash\nsftp> get radarchive_2021-08-31.csv.bz2\nbzip2 -d radarchive_2021-08-31.csv.bz2\n```\n3. Move file to the correct directory and reload data:\n```bash\nmv radarchive_2021-08-31.csv /shared/radius_repo/cdrs/\n/shared/abc/radius/bin/000_radius_ops.sh\n```"
      },
      {
        "question": "What is the root cause of decreasing entries in `sai.voice_quality_hist`, and how can it be addressed?",
        "answer": "When fewer entries appear in `sai.voice_quality_hist`, verify the number of received files:\n```bash\nls -l /data/1/trafficaftp/Traffica/TrafRTTE_Voice_Quality_abc_TRAVQTNES_20211128* | wc -l\n```\nCompare against previous days:\n```bash\nls -l /data/1/trafficaftp/Traffica/TrafRTTE_Voice_Quality_abc_TRAVQTNES_20211121* | wc -l\n```\nIf fewer files were received, abc needs to investigate the source system as there is no local issue."
      },
      {
        "question": "How can you troubleshoot and restart a failed RStudio Connect license activation?",
        "answer": "If RStudio Connect shows a 'License Expired' error:\n1. Verify system time:\n```bash\nsudo timedatectl\nsudo hwclock -w\n```\n2. Deactivate and reactivate the license:\n```bash\n/opt/rstudio-connect/bin/license-manager deactivate\n/opt/rstudio-connect/bin/license-manager activate <product-key>\n```\n3. Restart RStudio Connect if activation succeeds:\n```bash\nsystemctl restart rstudio-connect\n```"
      },
      {
        "question": "How do you resolve missing `CSI_MOBILE` data in `sai.sub_aggr_csi_it`?",
        "answer": "If `CSI_MOBILE` data is missing:\n1. Check log files for failed aggregation:\n```bash\nless /shared/abc/export_sai_csi/log/sai_csi.cron.<date>.log\n```\n2. Verify table row count:\n```sql\nSELECT count(*), par_dt FROM sai.sub_aggr_csi_it WHERE par_dt >= '20210xxx' GROUP BY par_dt;\n```\n3. If data is missing, manually reprocess:\n```bash\n/shared/abc/csi/bin/csi_weekly_load.sh.manual 2\n```"
      },
      {
        "question": "How can you manually reload missing data for `brond.an_rollout_data_hist`?",
        "answer": "If `brond.an_rollout_data_hist` stops loading:\n1. Check last successful loads:\n```sql\nSELECT par_dt, count(*) FROM brond.an_rollout_data_hist GROUP BY par_dt ORDER BY par_dt DESC LIMIT 10;\n```\n2. Inspect logs for errors:\n```bash\nless /shared/abc/brond/log/brond_rollout_cron.*\n```\n3. If missing data is found, rerun:\n```bash\n/shared/abc/brond/bin/000_brond_rollout_post.sh 20211007\n```"
      },
      {
        "question": "What change is required in `brond_rollout.pl` to prevent ambiguous column errors after a system upgrade?",
        "answer": "After an upgrade, an ambiguous column reference error occurs in `brond_rollout.pl`. Modify:\n```sql\nSELECT eett, dslam, colid, colvalue FROM brond.brond_rollout_data_hist WHERE par_dt='20210927'\n```\nTo use an alias:\n```sql\nSELECT eett, dslam, colid AS colid1, colvalue FROM brond.brond_rollout_data_hist WHERE par_dt='20210927'\n```"
      },
      {
        "question": "How do you resolve an issue where `refdata.rd_cells` is not loading the latest data from `refdata.rd_cells_load`?",
        "answer": "1. Verify the latest partitions in both tables:\n```sql\nSELECT max(par_dt) FROM refdata.rd_cells_load;\nSELECT max(refdate) FROM refdata.rd_cells;\n```\n2. If `rd_cells` is behind `rd_cells_load`, check the execution logs:\n```bash\nls -ltr /shared/abc/refdata/log/\n```\n3. The root cause is a synchronization issue where `220_refData_Daily_Snapshot.sh` runs before `210_refData_Load.sh` completes. Fix it by adding:\n```sql\nSET SYNC_DDL=1;\n```\nto the relevant scripts.\n4. Restart the Cloudera coordinator and re-run the daily snapshot script:\n```bash\n/shared/abc/refdata/bin/220_refData_Daily_Snapshot.sh\n```"
      },
      {
        "question": "How can you debug and resolve an HDFS failover controller failure on Cloudera nodes `mn1` and `mn2`?",
        "answer": "1. Check failover controller logs:\n```bash\nless /var/log/hadoop-hdfs/hdfs-zkfc-mn1.log\n```\n2. Look for timeout errors related to Zookeeper connections.\n3. Check Zookeeper logs for connection failures:\n```bash\nless /var/log/zookeeper/zookeeper.log\n```\n4. If you see repeated connection losses, verify latency using Cloudera Manager’s RPC Latency alerts.\n5. Increase Zookeeper tick time to prevent premature timeouts:\n```bash\necho 'tickTime=4000' >> /etc/zookeeper/conf/zoo.cfg\n```\n6. Restart Zookeeper and Failover Controllers:\n```bash\nsystemctl restart zookeeper\nsystemctl restart hadoop-hdfs-zkfc\n```"
      },
      {
        "question": "How do you investigate missing data in the `aums.archive_metadata` table due to unprocessed SFTP files?",
        "answer": "1. Log in to Streamsets and check the pipeline status:\n```bash\nsdc-cli status --pipeline AUMS_Metadata_File_Feed\n```\n2. If the pipeline is running but not processing files, check SFTP for unprocessed data:\n```bash\nsftp bigd@172.16.166.30\nls -ltr aums/\n```\n3. Verify missing partitions in Impala:\n```sql\nSELECT count(*) FROM aums.archive_metadata WHERE par_dt = '20230811';\n```\n4. Manually insert missing data:\n```bash\n/shared/abc/aums/bin/load_aums_metadata.sh 20230811\n```\n5. Refresh Impala metadata and confirm data has loaded:\n```sql\nREFRESH aums.archive_metadata;\n```"
      },
      {
        "question": "How do you resolve a Cloudera Data Science Workbench (CDSW) unavailability issue caused by resource exhaustion?",
        "answer": "1. Check available system resources:\n```bash\nkubectl get nodes -o wide\nkubectl top nodes\n```\n2. Identify overloaded nodes:\n```bash\ncdsw status\nkubectl get pods -A | grep Pending\n```\n3. If high memory usage is detected, reduce allocated resources:\n```yaml\nspec:\n  containers:\n  - name: cdsw\n    resources:\n      limits:\n        memory: \"8Gi\"\n        cpu: \"4\"\n```\n4. Restart affected pods:\n```bash\nkubectl delete pod -n default-user-XXX pod_name\n```"
      },
      {
        "question": "How do you investigate a `Disk I/O error` in BigStreamer’s Location Mobility pipeline?",
        "answer": "1. Locate the failing query:\n```bash\ngrep 'Disk I/O error' /var/log/impala/impalad.log\n```\n2. Identify missing HDFS files:\n```bash\nhdfs fsck /ez/warehouse/npce.db/eea_hour/ -files -blocks\n```\n3. If files are missing, manually trigger a reprocessing job:\n```bash\n/shared/abc/location_mobility/bin/export_Location_Mobility.sh\n```\n4. Refresh metadata in Impala:\n```sql\nREFRESH npce.eea_hour;\n```"
      },
      {
        "question": "How do you fix intermittent SSH authentication failures on `virtualop`?",
        "answer": "1. Verify `/var/log/secure` for rejected connections:\n```bash\ngrep 'refused connect' /var/log/secure\n```\n2. If Cyberark’s IP is blocked, whitelist it in `/etc/hosts.allow`:\n```bash\necho 'sshd: 10.53.134.71' >> /etc/hosts.allow\n```\n3. If Cyberark uses multiple IPs, request them from the customer and add them.\n4. Restart SSH service:\n```bash\nsystemctl restart sshd\n```"
      },
      {
        "question": "How can you optimize Spark resource allocation for an Energy Bills job that consumes too much RAM?",
        "answer": "1. Locate the Spark configuration in the failing job:\n```python\nspark = SparkSession.builder \\\n    .master(\"yarn\") \\\n    .config(\"spark.executor.instances\", \"100\") \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .getOrCreate()\n```\n2. Reduce resource usage by modifying it to:\n```python\n.config(\"spark.executor.instances\", \"50\") \\\n.config(\"spark.executor.memory\", \"2g\") \\\n```\n3. Restart the job and monitor system resources."
      },
      {
        "question": "How do you recover from a bad health state in BigStreamer node `mn3`?",
        "answer": "1. Check Cloudera Manager for node status.\n2. Attempt to ping the node:\n```bash\nping mn3\n```\n3. If unresponsive, reboot via iDRAC:\n```bash\nssh admin@10.255.242.85\nreboot\n```\n4. After reboot, ensure Cloudera services are running:\n```bash\nsystemctl status cloudera-scm-agent\n```"
      },
      {
        "question": "How do you manually backfill missing `sai.voice_quality_hist` data?",
        "answer": "1. Verify missing partitions:\n```sql\nSELECT count(*) FROM sai.voice_quality_hist WHERE par_dt = '20230530';\n```\n2. Check if raw files exist:\n```bash\nls -ltr /data/1/trafficaftp/Traffica/TrafRTTE_Voice_Quality_abc_*\n```\n3. If files are missing, request a re-upload and reprocess:\n```bash\n/shared/abc/traffica/bin/load_voice_quality.sh 20230530\n```"
      },
      {
        "question": "How do you diagnose Cloudera Data Science Workbench jobs stuck in `Pending` state?",
        "answer": "1. Check if pods are waiting for resources:\n```bash\nkubectl get pods | grep Pending\n```\n2. Identify the reason:\n```bash\nkubectl describe pod pod_name\n```\n3. If there is insufficient memory, adjust Kubernetes settings:\n```yaml\nspec:\n  containers:\n  - name: cdsw\n    resources:\n      limits:\n        memory: \"8Gi\"\n```"
      },
      {
        "question": "How do you troubleshoot missing Location Mobility files after a failed export?",
        "answer": "1. Verify missing files in the logging directory:\n```bash\nls -ltr /shared/abc/location_mobility/logging\n```\n2. Check script execution status:\n```bash\nssh -o \"StrictHostKeyChecking no\" -i ./id_rsa mtuser@un-vip.bigdata.abc.gr \"script\"\n```\n3. If permissions were an issue, check Oozie workflow for `mtuser`:\n```bash\nLogin to Hue Server -> Oozie Editor -> Search for mtuser workflows\n```"
      },
      {
        "question": "How do you fix a Neighbor Tool failure due to an expired password in Solvatio?",
        "answer": "1. Identify expired credentials by checking API logs:\n```bash\nekltaillog\n```\n2. Change the password in `ImpalaURL` under `BigStreamerApiJmxConfig`:\n```bash\njdbc:impala://172.25.37.237:21090;SSL=1;AuthMech=3;UID=neighb_user;PWD=NEW_PASSWORD;\n```\n3. Restart the Wildfly service:\n```bash\nekl-start\n```"
      },
      {
        "question": "What steps are needed to recover a failing `airflow-scheduler` pod due to persistent volume errors?",
        "answer": "1. Check pod status:\n```bash\nkubectl get pods -n ranai-geo\n```\n2. Identify PVC stuck in deleting state:\n```bash\nkubectl logs instance-manager-r-28535c55 -n instance-manager-r\n```\n3. Delete problematic PVC:\n```bash\nkubectl delete pvc pvc-c826d577-e764-470e-9904-3986042810aa -n ranai-geo\n```"
      },
      {
        "question": "How do you fix an expired Kerberos ticket preventing Hue and Impala queries from executing?",
        "answer": "1. Find Hue's Kerberos crontab entry:\n```bash\ncrontab -l | grep hue\n```\n2. Locate the missing Kerberos keytab directory:\n```bash\nls -ltra /var/run/cloudera-scm-agent/process/\n```\n3. Update `crontab` with the correct directory and refresh:\n```bash\nsudo -u hue /usr/bin/kinit -k -t /var/run/cloudera-scm-agent/process/159836-hue-KT_RENEWER/hue.keytab -c /var/run/hue/hue_krb5_ccache hue/un2.bigdata.abc.gr@BIGDATA.abc.GR\n```"
      },
      {
        "question": "How do you resolve an unhealthy Cloudera Manager node (`idm2.bigdata.abc.gr`) due to excessive memory consumption?",
        "answer": "1. Identify memory usage:\n```bash\nps aux --sort -rss\n```\n2. Locate Cloudera parcel download failures:\n```bash\ncd /var/lib/cloudera-scm-agent/parcels\nls -ltr\n```\n3. Remove affected hosts:\n```bash\nCloudera Manager -> Hosts -> Remove `idm1.bigdata.abc.gr`, `idm2.bigdata.abc.gr`\n```"
      },
      {
        "question": "What actions should be taken to resolve CDSW jobs failing with `Engine exited with status 34`?",
        "answer": "1. Identify failing pods:\n```bash\nkubectl get pods -w -A\n```\n2. Check for network plugin failures:\n```bash\nkubectl get events -n default-user-XXX\n```\n3. Restart Cloudera Docker service:\n```bash\n/opt/cloudera/cm-agent/bin/supervisorctl restart 145071-cdsw-CDSW_DOCKER\n```"
      },
      {
        "question": "How do you troubleshoot Cloudera Data Science Workbench (CDSW) jobs failing due to `dial unix /run/cloudera/data-science-workbench/port-forwarder/port-forwarder.sock: connect: connection refused`?",
        "answer": "1. Check SparkPortForwarder logs:\n```bash\nkubectl logs spark-port-forwarder-thrr9 -n <namespace>\n```\n2. Identify failing requests:\n```bash\ncdsw status | grep wrkcdsw1.bigdata.abc.gr\n```\n3. Restart Cloudera services:\n```bash\nCloudera Manager -> CDSW -> Restart `Docker Daemon Worker` on `wrkcdsw1`\n```"
      },
      {
        "question": "How do you disable Nagios notifications permanently?",
        "answer": "1. Edit Nagios configuration:\n```bash\nvi /etc/nagios/nagios.cfg\n```\n2. Set `enable_notifications=0` and restart Nagios:\n```bash\nsystemctl restart nagios\n```"
      },
      {
        "question": "How do you recover an unresponsive Cloudera Data Science Workbench job that is stuck in `Pending`?",
        "answer": "1. Identify pending jobs:\n```bash\nkubectl get pods | grep Pending\n```\n2. Restart affected pods:\n```bash\nkubectl delete pod -n default-user-XXX pod_name\n```"
      },
      {
        "question": "How do you manually reset an expired LDAP password in Cloudera Manager?",
        "answer": "1. Log into Cloudera Manager:\n```bash\nhttps://172.25.37.247:8543/serviceweaver/jmx/\n```\n2. Navigate to:\n```\nservers -> com.xyz.abc_nts.bigstreamer_api.util -> attributes -> ImpalaURL\n```\n3. Update credentials and restart Cloudera Manager Agent:\n```bash\nsystemctl restart cloudera-scm-agent\n```"
      },
      {
        "question": "How do you resolve failed mount issues in an Airflow scheduler pod?",
        "answer": "1. Identify failing volumes:\n```bash\nkubectl describe pod airflow-scheduler-0 -n ranai-geo\n```\n2. Delete corrupted PVC:\n```bash\nkubectl delete pvc pvc-c826d577-e764-470e-9904-3986042810aa -n ranai-geo\n```"
      },
      {
        "question": "How do you investigate missing email notifications from Nagios?",
        "answer": "1. Check Nagios logs:\n```bash\nless /var/log/nagios/nagios.log\n```\n2. Verify email configuration:\n```bash\nvi /etc/nagios/nagios.cfg\n```\n3. If notifications should be disabled:\n```bash\nsystemctl restart nagios\n```"
      },
      {
        "question": "How do you fix an Oozie workflow that fails to trigger due to incorrect user permissions?",
        "answer": "1. Verify the `mtuser` account has correct permissions:\n```bash\nls -ltr /home/mtuser/.ssh\n```\n2. Check the Oozie workflow status in Hue:\n```bash\nOozie Editor -> Search for mtuser workflows\n```"
      },
      {
        "question": "How do you restart a failing Wildfly service in the Neighbor Tool?",
        "answer": "1. Check logs for password authentication issues:\n```bash\nekltaillog\n```\n2. Reset expired credentials:\n```bash\njdbc:impala://172.25.37.237:21090;SSL=1;AuthMech=3;UID=neighb_user;PWD=NEW_PASSWORD;\n```\n3. Restart Wildfly:\n```bash\nekl-start\n```"
      },
      {
        "question": "How do you restore an Airflow scheduler pod stuck in `CrashLoopBackOff`?",
        "answer": "1. Verify logs and PVC status:\n```bash\nkubectl get pods -n ranai-geo\nkubectl describe pod airflow-scheduler-0 -n ranai-geo\n```\n2. Delete problematic PVC:\n```bash\nkubectl delete pvc pvc-c826d577-e764-470e-9904-3986042810aa -n ranai-geo\n```"
      },
      {
        "question": "How do you install a Certificate Authority (CA) using SaltStack?",
        "answer": "To install a CA using SaltStack:\n1. Move the given certificate:\n   ```bash\n   mv /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.cer /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.crt\n   ```\n2. Verify that the `.crt` file is base64-encoded:\n   ```bash\n   cat /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.crt | head -n 1\n   ```\n   Ensure it starts with `-----BEGIN CERTIFICATE-----`.\n3. Test the installation:\n   ```bash\n   salt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os test=True\n   ```\n4. Apply the installation:\n   ```bash\n   salt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os\n   ```\n5. Install `jssecacerts` if Java is present:\n   ```bash\n   salt 'node_name' state.apply tls.internal_certificate.install_root_certificate_jssecacerts\n   ```"
      },
      {
        "question": "How do you deploy a new war file for the Permanent Anonymization & Retention UI?",
        "answer": "To deploy a new war file:\n1. SSH into the backend server:\n   ```bash\n   ssh user@unc2\n   sudo -i\n   ```\n2. Backup the old war file:\n   ```bash\n   ssh user@unekl1\n   cp -rp /opt/trustcenter/wf_cdef_trc/standalone/deployments/wftrust-landing-web.war /opt/trustcenter/wf_cdef_trc/standalone/wftrust-landing-web.war.bkp\n   ```\n3. Set permissions on the new file:\n   ```bash\n   chown trustuser:trustcenter <new_war_file>\n   chmod 644 <new_war_file>\n   ```\n4. Move the new war file:\n   ```bash\n   mv <new_war_file> /opt/trustcenter/wf_cdef_trc/standalone/deployments/\n   ```\n5. Check that Wildfly redeploys the war file automatically:\n   ```bash\n   su - trustuser\n   trust-status\n   ```\n6. Repeat steps for `unekl2`."
      },
      {
        "question": "How can you check if the Retention process executed successfully?",
        "answer": "To check the Retention process:\n1. Login to `un2` as `intra`:\n   ```bash\n   ssh user@un2\n   sudo -i\n   ```\n2. Check script execution status:\n   ```bash\n   grep \"Script Status\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.202012.log | tail -n1\n   ```\n   - If `Status != 0`, then there is a problem.\n3. If an issue is detected, extract logs:\n   ```bash\n   egrep -i '(error|problem|excep|fail)' /shared/abc/cdo/log/Retention/*<SnapshotID>*.log\n   ```\n   - If the output contains more than 10 errors, further investigation is needed."
      },
      {
        "question": "How do you increase Java Heap Space for Streamsets?",
        "answer": "To increase Java Heap Space for Streamsets:\n1. Update Java options in Cloudera Manager:\n   ```bash\n   cluster -> Streamsets -> Configuration -> Java options: `-Xmx32768m -Xms32768m -server`\n   ```\n2. Remove old configurations:\n   ```bash\n   cluster-> Streamsets -> Configuration -> Data Collector Advanced Configuration Snippet\n   ```\n3. Restart Streamsets:\n   ```bash\n   cluster -> Streamsets -> Restart\n   ```\n4. Verify new settings:\n   ```bash\n   ps -ef | grep -i streamsets | grep -i xmx\n   ```"
      },
      {
        "question": "How do you create a keytab in NYMA?",
        "answer": "To create a keytab in NYMA:\n1. SSH into `kerb1` as root:\n   ```bash\n   ssh kerb1\n   sudo -i\n   ```\n2. Open Kerberos administration tool:\n   ```bash\n   kadmin.local\n   ```\n3. Check if a principal exists:\n   ```bash\n   listprincs <username>@CNE.abc.GR\n   ```\n4. Create a principal if missing:\n   ```bash\n   addprinc <username>@CNE.abc.GR\n   ```\n5. Generate a keytab:\n   ```bash\n   ktadd -k /tmp/<username>.keytab -norandkey <username>@CNE.abc.GR\n   ```"
      },
      {
        "question": "How do you set up a Kubernetes user environment?",
        "answer": "To set up Kubernetes tools:\n1. Install `kubectl`:\n   ```bash\n   curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl\n   chmod +x kubectl\n   sudo mv kubectl /usr/local/bin/\n   ```\n2. Enable bash completion:\n   ```bash\n   mkdir -p /etc/bash_completion.d\n   kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl\n   ```"
      },
      {
        "question": "How do you execute a Cube Indicators Spark job manually?",
        "answer": "To execute a Cube Indicators job:\n1. SSH into `un1` and switch to `intra`:\n   ```bash\n   ssh user@un1\n   sudo -i\n   su - intra\n   ```\n2. Change directory:\n   ```bash\n   cd projects/cube_ind\n   ```\n3. Fetch the latest Spark job:\n   ```bash\n   hdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\n   ```\n4. Edit execution date in the script:\n   ```bash\n   vim run_cube.sh\n   ```\n5. Run the job:\n   ```bash\n   ./run_cube.sh\n   ```"
      },
      {
        "question": "How do you solve duplicate records in an Impala table?",
        "answer": "To resolve duplicates in `energy_efficiency.cell`:\n1. Backup the table:\n   ```bash\n   CREATE TABLE energy_efficiency.cell_bak LIKE energy_efficiency.cell;\n   INSERT INTO TABLE energy_efficiency.cell_bak PARTITION (par_dt) SELECT * FROM energy_efficiency.cell;\n   ```\n2. Remove duplicates:\n   ```bash\n   INSERT OVERWRITE TABLE energy_efficiency.cell partition (par_dt)\n   SELECT DISTINCT * FROM energy_efficiency.cell WHERE par_dt BETWEEN '20211210' AND '20211215';\n   ```"
      },
      {
        "question": "How do you fix OpenLDAP replication after a password change?",
        "answer": "To fix OpenLDAP replication:\n1. Backup LDAP configuration:\n   ```bash\n   slapcat -n 0 -l config.ldif\n   slapcat -n 2 -l data.ldif\n   ```\n2. Modify replication settings:\n   ```bash\n   ldapmodify -H ldaps://kerb1.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\n   ldapmodify -H ldaps://kerb2.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\n   ```\n3. Verify replication:\n   ```bash\n   ldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=testuser'\n   ```"
      },
      {
        "question": "How do you verify if SSL certificates for the Groupnet domain are imported?",
        "answer": "To verify SSL certificates:\n1. SSH into the target server:\n   ```bash\n   ssh root@un5\n   ```\n2. Run the OpenSSL command:\n   ```bash\n   openssl s_client -connect PVDCAHR01.groupnet.gr:636\n   ```\n3. If the certificate is missing, import it using SaltStack:\n   ```bash\n   salt-call state.apply admin:etc/salt/salt/tls/certificate_authority/import_ca.sls\n   ```"
      },
      {
        "question": "How do you back up the SpagoBI database before making changes?",
        "answer": "To back up the SpagoBI MySQL database:\n1. SSH into the database server:\n   ```bash\n   ssh root@db01\n   ```\n2. Run the following command to create a backup:\n   ```bash\n   mysqldump -u root -p spagobi --single-transaction > /tmp/spagobi.sql\n   ```"
      },
      {
        "question": "How do you update the HAProxy configuration to allow Groupnet AD access?",
        "answer": "To update HAProxy:\n1. Edit the configuration file:\n   ```bash\n   vi /etc/haproxy/haproxy.cfg\n   ```\n2. Add the following lines:\n   ```bash\n   listen def-ad-ldaps\n       bind *:863 ssl crt /opt/security/haproxy/node.pem\n       mode tcp\n       balance source\n       server def_ad1 PVDCAHR01.groupnet.gr:636 ssl check ca-file /etc/ssl/certs/ca-bundle.crt\n   ```\n3. Validate and reload HAProxy:\n   ```bash\n   haproxy -f /etc/haproxy/haproxy.cfg -c\n   systemctl reload haproxy\n   ```"
      },
      {
        "question": "How do you change bind users' passwords in R-Studio Connect?",
        "answer": "To update the bind user password:\n1. SSH into `unrstudio1.bigdata.abc.gr`:\n   ```bash\n   ssh root@unrstudio1\n   ```\n2. Edit the configuration file:\n   ```bash\n   vi /etc/rstudio-connect/rstudio-connect.gcfg\n   ```\n3. Update the `BindPassword` field with the new password.\n4. Restart the service:\n   ```bash\n   systemctl restart rstudio-connect\n   ```\n5. Verify LDAP authentication:\n   ```bash\n   systemctl status rstudio-connect\n   ```"
      },
      {
        "question": "How do you delete a user in R-Studio Connect?",
        "answer": "To delete a user:\n1. Stop R-Studio Connect:\n   ```bash\n   systemctl stop rstudio-connect\n   ```\n2. List users and find the GUID of the user:\n   ```bash\n   /opt/rstudio-connect/bin/usermanager list --users | grep -i <username>\n   ```\n3. Delete the user:\n   ```bash\n   /opt/rstudio-connect/bin/usermanager delete --users --user-guid <GUID>\n   ```\n4. Start R-Studio Connect:\n   ```bash\n   systemctl start rstudio-connect\n   ```"
      },
      {
        "question": "How do you merge duplicate user accounts in R-Studio Connect?",
        "answer": "To merge duplicate users:\n1. Stop R-Studio Connect:\n   ```bash\n   systemctl stop rstudio-connect\n   ```\n2. Find user IDs:\n   ```bash\n   /opt/rstudio-connect/bin/usermanager list --users | grep -i <username>\n   ```\n3. Transfer ownership:\n   ```bash\n   /opt/rstudio-connect/bin/usermanager transfer -source-user-id <old_id> -target-user-id <new_id>\n   ```\n4. Start R-Studio Connect:\n   ```bash\n   systemctl start rstudio-connect\n   ```"
      },
      {
        "question": "How do you move users from `central-domain.root.def.gr` to `groupnet` in SpagoBI?",
        "answer": "To migrate users:\n1. SSH into the database server:\n   ```bash\n   ssh root@db01\n   ```\n2. Open MySQL:\n   ```bash\n   mysql -u root -p\n   ```\n3. Run the following queries:\n   ```sql\n   USE spagobi;\n   SELECT * FROM SBI_USER WHERE USER_ID LIKE '%@central-domain%';\n   UPDATE SBI_USER SET USER_ID = REPLACE(USER_ID,'@central-domain','@groupnet') WHERE USER_ID LIKE '%@central-domain%';\n   ```"
      },
      {
        "question": "How do you check certificate expiration in Kubernetes?",
        "answer": "To check Kubernetes certificate expiration:\n1. SSH into the master node:\n   ```bash\n   ssh root@kubemaster1\n   ```\n2. Run the command:\n   ```bash\n   kubeadm certs check-expiration\n   ```"
      },
      {
        "question": "How do you renew Kubernetes certificates?",
        "answer": "To renew certificates:\n1. Back up existing certificates:\n   ```bash\n   cp -ar /etc/kubernetes /tmp/\n   ```\n2. Run the renewal command:\n   ```bash\n   kubeadm certs renew all\n   ```\n3. Verify expiration:\n   ```bash\n   kubeadm certs check-expiration\n   ```"
      },
      {
        "question": "How do you reset Nagios alarms for SSH connection errors?",
        "answer": "To fix SSH connection errors:\n1. Edit `commands.cfg`:\n   ```bash\n   vi /usr/local/nagios/etc/objects/commands.cfg\n   ```\n2. Modify the `check_by_ssh` command to:\n   ```bash\n   $USER1$/check_by_ssh -E 8 -o StrictHostKeyChecking=no -H $HOSTADDRESS$ -t 30 -C \"/usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$ -p $ARG3$\"\n   ```\n3. Restart Nagios:\n   ```bash\n   service nagios restart\n   ```"
      },
      {
        "question": "How do you open a ticket with Dell for server issues?",
        "answer": "To open a ticket with Dell:\n1. Find the server’s management IP:\n   ```bash\n   ipmitool lan print | grep -i 'IP Address'\n   ```\n2. Connect to iDRAC using VNC and retrieve the Service Tag from:\n   ```\n   Server -> Overview -> Server Information\n   ```\n3. Call Dell support at `2108129800` and provide the Service Tag.\n4. Follow the instructions to generate TSR logs:\n   ```bash\n   ssh user@server\n   cd /home/cloudera/Downloads/\n   ```\n5. Send logs to Dell support."
      },
      {
        "question": "How do you manage IDM (FreeIPA) replication between two nodes?",
        "answer": "To manage IDM replication:\n1. List replication targets:\n   ```bash\n   ipa-replica-manage list -v\n   ```\n2. Force synchronization:\n   ```bash\n   ipa-replica-manage force-sync --from idm2.bigdata.abc.gr\n   ```"
      },
      {
        "question": "How do you manage Kafka connectivity using HAProxy?",
        "answer": "To configure HAProxy for Kafka:\n1. Edit HAProxy configuration:\n   ```bash\n   vi /etc/haproxy/haproxy.cfg\n   ```\n2. Add a new Kafka listener:\n   ```conf\n   listen viavi-megafeed-kafka1_ssl\n       bind 999.999.999.999:9093\n       mode tcp\n       balance leastconn\n       server viavi-megafeed-kafka1 999.999.999.999:9093\n   ```\n3. Restart HAProxy:\n   ```bash\n   systemctl restart haproxy\n   ```"
      },
      {
        "question": "How can you diagnose and resolve a failure in the UC4 agent responsible for executing `sched_extract_details.sh` for Transfer Extract jobs?",
        "answer": "To diagnose and resolve a failure in the UC4 agent executing `sched_extract_details.sh`, follow these steps:\n\n1. **Verify UC4 agent logs**:\n   ```bash\n   tail -n 50 /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\n   ```\n   Look for errors such as `command not found` or `permission denied`.\n\n2. **Check if the agent is running**:\n   ```bash\n   ps aux | grep UC4\n   ```\n   If the agent is not running, restart it:\n   ```bash\n   sudo systemctl restart uc4-agent\n   ```\n\n3. **Validate Spark job execution logs**:\n   - Open **YARN Resource Manager UI** in Firefox on `dr1edge01.mno.gr` or `pr1edge01.mno.gr`\n   - Search for the job **`PROD_IBank_DWH_transfer_YYYYMMDD-YYYYMMDD`** and review logs.\n\n4. **Manually rerun the extraction job for a specific date**:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t transfer -p 20240212\n   ```\n   If the job completes successfully, the issue was likely transient. If it fails again, check HDFS for missing or corrupted data:\n   ```bash\n   hdfs dfs -ls /datawarehouse/ibank/prod_trlog_ibank_analytical/dwh_details_transfer/\n   ```\n5. **Contact `mno UC4 administrators`** if the script logs indicate a persistent issue.\n"
      },
      {
        "question": "What are the key failure points in the Sqoop export process for `dwh_details_payment` and how can you address them?",
        "answer": "To troubleshoot and resolve failures in the Sqoop export process for `dwh_details_payment`, follow these steps:\n\n1. **Check Sqoop job status**:\n   ```bash\n   yarn application -list | grep PROD_IBank_DWH_EXPORT_PaymentDetails\n   ```\n   If the job is missing, check if it was scheduled in the UC4 agent:\n   ```bash\n   tail -n 50 /var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log\n   ```\n\n2. **Check Impala staging table for missing data**:\n   ```sql\n   SELECT COUNT(*) FROM prod_trlog_ibank_analytical.dwh_details_payment_stg WHERE par_dt = '20240212';\n   ```\n   If count is zero, the extract job failed—rerun it first.\n\n3. **Verify Sqoop connection to MSSQL Server**:\n   ```bash\n   sqoop eval --connect 'jdbc:sqlserver://mssqlserver.mno.gr;database=InternetBankingDW' --username PRODUSER --password 'password' --query \"SELECT COUNT(*) FROM InternetBankingDW.PaymentDetails WHERE date_key = '20240212'\"\n   ```\n   If connection fails, verify MSSQL availability:\n   ```bash\n   telnet mssqlserver.mno.gr 1433\n   ```\n\n4. **Manually rerun the export job for a specific date**:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t payment -p 20240212\n   ```\n\n5. **Confirm data consistency after rerun**:\n   ```sql\n   SELECT COUNT(*) FROM InternetBankingDW.PaymentDetails WHERE date_key = '20240212';\n   ```"
      },
      {
        "question": "How do you debug a retention mechanism failure that does not delete old partitions in `dwh_details_transfer`?",
        "answer": "If the retention mechanism fails to delete partitions older than 10 days in `prod_trlog_ibank_analytical.dwh_details_transfer`, follow these steps:\n\n1. **Check execution logs for errors**:\n   ```bash\n   tail -n 50 /opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily.log\n   ```\n   Look for `impala-shell` connection errors or SQL syntax issues.\n\n2. **Verify if partitions exist beyond the retention window**:\n   ```sql\n   SHOW PARTITIONS prod_trlog_ibank_analytical.dwh_details_transfer;\n   ```\n   If partitions older than 10 days exist, manually attempt deletion:\n   ```sql\n   ALTER TABLE prod_trlog_ibank_analytical.dwh_details_transfer DROP PARTITION (par_dt='20240130');\n   ```\n   If this fails, check if partition dropping is blocked by active queries:\n   ```sql\n   SELECT * FROM sys.jobs WHERE object_name = 'dwh_details_transfer';\n   ```\n\n3. **Check if retention script execution is suspended**:\n   - If retention was deactivated (`Incident IM2220978`), **DO NOT TROUBLESHOOT**.\n   - If active, rerun manually:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily_STABLE.sh\n   ```\n   - Verify if partitions were deleted:\n   ```sql\n   SHOW PARTITIONS prod_trlog_ibank_analytical.dwh_details_transfer;\n   ```"
      },
      {
        "question": "How can you troubleshoot and recover from a failure in the Kafka MirrorMaker process used for PR replication in Internet Banking?",
        "answer": "To troubleshoot and recover from a failure in the Kafka MirrorMaker process for PR replication, follow these steps:\n\n1. **Verify the status of MirrorMaker processes**:\n   ```bash\n   ps aux | grep -i 'mirror' | grep -v grep\n   ```\n   If the process is not running, restart it using Cloudera Manager or manually:\n   ```bash\n   sudo systemctl restart kafka-mirrormaker\n   ```\n\n2. **Check MirrorMaker logs for errors**:\n   ```bash\n   tail -n 50 /var/log/kafka/mirrormaker.log\n   ```\n   Look for connectivity issues such as `Broker not available` or `Leader not found`.\n\n3. **Test connectivity between Kafka brokers**:\n   - From `pr1node01.mno.gr`, test the connection to the target Kafka topic:\n   ```bash\n   kafka-console-consumer --bootstrap-server pr1node04.mno.gr:9092 --topic prod-trlog-ibank-ingest-stream-mir --from-beginning --max-messages 10\n   ```\n   - If there are connection errors, verify firewall rules and broker health:\n   ```bash\n   nc -zv pr1node04.mno.gr 9092\n   ```\n\n4. **Manually restart MirrorMaker and verify message flow**:\n   ```bash\n   kafka-mirror-maker --consumer.config /etc/kafka/mirrormaker-consumer.properties --producer.config /etc/kafka/mirrormaker-producer.properties --whitelist 'prod-trlog-ibank-ingest-stream-mir' &\n   ```\n   - Monitor logs for successful replication confirmation.\n\n5. **Confirm replication in the target Kafka topic**:\n   ```bash\n   kafka-console-consumer --bootstrap-server pr1node05.mno.gr:9092 --topic prod-trlog-ibank-ingest-stream --from-beginning --max-messages 10\n   ```\n   - If messages appear, replication is restored; otherwise, check offsets and rebalance consumers.\n\n6. **If failure persists, escalate to the Kafka administrator and check Zookeeper health**:\n   ```bash\n   zookeeper-shell pr1node01.mno.gr:2181 ls /brokers/ids\n   ```\n   If brokers are missing, a cluster rebalance might be needed."
      },
      {
        "question": "What are the critical failure points in the `Prod_IBANK_IngestStream` Spark Streaming pipeline, and how can they be resolved?",
        "answer": "To troubleshoot failures in the `Prod_IBANK_IngestStream` Spark Streaming pipeline, follow these steps:\n\n1. **Check YARN logs for errors**:\n   ```bash\n   yarn logs -applicationId <application_id> | tail -n 50\n   ```\n   Look for errors related to resource allocation, memory limits, or connectivity.\n\n2. **Check Kafka topic ingestion status**:\n   - Verify if the Kafka topic has active producers:\n   ```bash\n   kafka-topics --describe --bootstrap-server pr1node01.mno.gr:9092 --topic prod-trlog-ibank-ingest-stream\n   ```\n   If the topic has no active partitions, restart Kafka producers.\n\n3. **Verify Kudu and HBase health**:\n   - Check Kudu table availability:\n   ```bash\n   kudu table scan kudu://pr1node01.mno.gr:7051/prod_trlog_ibank.service_audit_stream\n   ```\n   - Check HBase region status:\n   ```bash\n   echo 'status' | hbase shell\n   ```\n   If regions are in transition, restart the HBase Master:\n   ```bash\n   sudo systemctl restart hbase-master\n   ```\n\n4. **Restart the Spark job manually if necessary**:\n   ```bash\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\n   ```\n   - Verify that a new application is running in YARN:\n   ```bash\n   yarn application -list | grep IBANK_IngestStream\n   ```\n\n5. **Check logs for excessive batch processing delay**:\n   ```bash\n   grep 'batch delay' /var/log/ingestion/PRODREST/ibank/log/ibank_stream.log\n   ```\n   If processing time exceeds expected thresholds, check for skewed data causing executor imbalance.\n\n6. **If Spark jobs keep failing due to out-of-memory errors, increase executor memory**:\n   - Modify `submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` to adjust:\n   ```bash\n   --executor-memory 4G\n   --driver-memory 2G\n   ```\n   - Restart the job and monitor memory usage.\n\nIf issues persist, escalate to the Big Data engineering team for further debugging."
      },
      {
        "question": "How do you diagnose and recover from data inconsistencies in `service_audit` caused by failures in `MergeBatch`?",
        "answer": "Data inconsistencies in `service_audit` can occur when `MergeBatch` fails due to Spark execution errors, data duplication, or incomplete deletion from Kudu/HBase. Follow these steps to diagnose and recover:\n\n1. **Verify that `MergeBatch` completed successfully**:\n   ```bash\n   yarn application -list | grep PROD_IBank_MergeBatch\n   ```\n   If no job is running, check the latest logs:\n   ```bash\n   tail -n 50 /var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log\n   ```\n\n2. **Check for duplicate records in `service_audit`**:\n   ```sql\n   SELECT client_username, COUNT(*) FROM prod_trlog_ibank.service_audit\n   WHERE par_dt = '20240212'\n   GROUP BY client_username HAVING COUNT(*) > 1;\n   ```\n   If duplicates exist, they might be remnants of a failed MergeBatch.\n\n3. **Manually re-run `MergeBatch` for a specific date**:\n   ```bash\n   /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh\n   ```\n   - Ensure the process runs successfully in YARN.\n\n4. **Check if orphaned records exist in Kudu/HBase**:\n   - Count records in Kudu:\n   ```sql\n   SELECT COUNT(*) FROM prod_trlog_ibank.service_audit_stream WHERE par_dt = '20240212';\n   ```\n   - Count records in Impala:\n   ```sql\n   SELECT COUNT(*) FROM prod_trlog_ibank.service_audit WHERE par_dt = '20240212';\n   ```\n   If counts mismatch, some records were not cleaned up properly.\n\n5. **Manually clean up duplicate Kudu partitions if necessary**:\n   ```sql\n   DELETE FROM prod_trlog_ibank.service_audit_stream WHERE par_dt = '20240212';\n   ```\n\n6. **Mark MergeBatch as complete in HBase if all data is correct**:\n   ```bash\n   echo \"put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20240212', 'MergaBatchState:drop_hbase', 'true'\" | hbase shell\n   ```\n   This ensures that the next scheduled run will not process this date again unnecessarily.\n\nBy following these steps, you can recover from `MergeBatch` failures and ensure consistency in `service_audit`."
      },
      {
        "question": "How can you troubleshoot and recover from a failure in the Kafka MirrorMaker process used for PR replication in Online Banking?",
        "answer": "To troubleshoot and recover from a failure in the Kafka MirrorMaker process for PR replication, follow these steps:\n\n1. **Verify the status of MirrorMaker processes**:\n   ```bash\n   ps aux | grep -i 'mirror' | grep -v grep\n   ```\n   If the process is not running, restart it using Cloudera Manager or manually:\n   ```bash\n   sudo systemctl restart kafka-mirrormaker\n   ```\n\n2. **Check MirrorMaker logs for errors**:\n   ```bash\n   tail -n 50 /var/log/kafka/mirrormaker.log\n   ```\n   Look for connectivity issues such as `Broker not available` or `Leader not found`.\n\n3. **Test connectivity between Kafka brokers**:\n   - From `pr1node01.mno.gr`, test the connection to the target Kafka topic:\n   ```bash\n   kafka-console-consumer --bootstrap-server pr1node04.mno.gr:9092 --topic prod-trlog-online-ingest-stream-mir --from-beginning --max-messages 10\n   ```\n   - If there are connection errors, verify firewall rules and broker health:\n   ```bash\n   nc -zv pr1node04.mno.gr 9092\n   ```\n\n4. **Manually restart MirrorMaker and verify message flow**:\n   ```bash\n   kafka-mirror-maker --consumer.config /etc/kafka/mirrormaker-consumer.properties --producer.config /etc/kafka/mirrormaker-producer.properties --whitelist 'prod-trlog-online-ingest-stream-mir' &\n   ```\n   - Monitor logs for successful replication confirmation.\n\n5. **Confirm replication in the target Kafka topic**:\n   ```bash\n   kafka-console-consumer --bootstrap-server pr1node05.mno.gr:9092 --topic prod-trlog-online-ingest-stream --from-beginning --max-messages 10\n   ```\n   - If messages appear, replication is restored; otherwise, check offsets and rebalance consumers.\n\n6. **If failure persists, escalate to the Kafka administrator and check Zookeeper health**:\n   ```bash\n   zookeeper-shell pr1node01.mno.gr:2181 ls /brokers/ids\n   ```\n   If brokers are missing, a cluster rebalance might be needed."
      },
      {
        "question": "What are the critical failure points in the `Prod_Online_IngestStream` Spark Streaming pipeline, and how can they be resolved?",
        "answer": "To troubleshoot failures in the `Prod_Online_IngestStream` Spark Streaming pipeline, follow these steps:\n\n1. **Check YARN logs for errors**:\n   ```bash\n   yarn logs -applicationId <application_id> | tail -n 50\n   ```\n   Look for errors related to resource allocation, memory limits, or connectivity.\n\n2. **Check Kafka topic ingestion status**:\n   - Verify if the Kafka topic has active producers:\n   ```bash\n   kafka-topics --describe --bootstrap-server pr1node01.mno.gr:9092 --topic prod-trlog-online-ingest-stream\n   ```\n   If the topic has no active partitions, restart Kafka producers.\n\n3. **Verify Kudu and HBase health**:\n   - Check Kudu table availability:\n   ```bash\n   kudu table scan kudu://pr1node01.mno.gr:7051/prod_trlog_online.service_audit_stream\n   ```\n   - Check HBase region status:\n   ```bash\n   echo 'status' | hbase shell\n   ```\n   If regions are in transition, restart the HBase Master:\n   ```bash\n   sudo systemctl restart hbase-master\n   ```\n\n4. **Restart the Spark job manually if necessary**:\n   ```bash\n   /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\n   ```\n   - Verify that a new application is running in YARN:\n   ```bash\n   yarn application -list | grep ONLINE_IngestStream\n   ```\n\n5. **Check logs for excessive batch processing delay**:\n   ```bash\n   grep 'batch delay' /var/log/ingestion/PRODREST/online/log/online_stream.log\n   ```\n   If processing time exceeds expected thresholds, check for skewed data causing executor imbalance.\n\n6. **If Spark jobs keep failing due to out-of-memory errors, increase executor memory**:\n   - Modify `submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` to adjust:\n   ```bash\n   --executor-memory 4G\n   --driver-memory 2G\n   ```\n   - Restart the job and monitor memory usage.\n\nIf issues persist, escalate to the Big Data engineering team for further debugging."
      },
      {
        "question": "How do you diagnose and recover from data inconsistencies in `service_audit` caused by failures in `MergeBatch`?",
        "answer": "Data inconsistencies in `service_audit` can occur when `MergeBatch` fails due to Spark execution errors, data duplication, or incomplete deletion from Kudu/HBase. Follow these steps to diagnose and recover:\n\n1. **Verify that `MergeBatch` completed successfully**:\n   ```bash\n   yarn application -list | grep PROD_Online_MergeBatch\n   ```\n   If no job is running, check the latest logs:\n   ```bash\n   tail -n 50 /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log\n   ```\n\n2. **Check for duplicate records in `service_audit`**:\n   ```sql\n   SELECT client_username, COUNT(*) FROM prod_trlog_online.service_audit\n   WHERE par_dt = '20240212'\n   GROUP BY client_username HAVING COUNT(*) > 1;\n   ```\n   If duplicates exist, they might be remnants of a failed MergeBatch.\n\n3. **Manually re-run `MergeBatch` for a specific date**:\n   ```bash\n   /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh\n   ```\n   - Ensure the process runs successfully in YARN.\n\n4. **Check if orphaned records exist in Kudu/HBase**:\n   - Count records in Kudu:\n   ```sql\n   SELECT COUNT(*) FROM prod_trlog_online.service_audit_stream WHERE par_dt = '20240212';\n   ```\n   - Count records in Impala:\n   ```sql\n   SELECT COUNT(*) FROM prod_trlog_online.service_audit WHERE par_dt = '20240212';\n   ```\n   If counts mismatch, some records were not cleaned up properly.\n\n5. **Manually clean up duplicate Kudu partitions if necessary**:\n   ```sql\n   DELETE FROM prod_trlog_online.service_audit_stream WHERE par_dt = '20240212';\n   ```\n\n6. **Mark MergeBatch as complete in HBase if all data is correct**:\n   ```bash\n   echo \"put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20240212', 'MergaBatchState:drop_hbase', 'true'\" | hbase shell\n   ```\n   This ensures that the next scheduled run will not process this date again unnecessarily.\n\nBy following these steps, you can recover from `MergeBatch` failures and ensure consistency in `service_audit`."
      },
      {
        "question": "A user with ID E70529 is unable to execute queries in Hue or Impala-shell. The user appears to have the correct permissions. How would you diagnose and fix the issue?",
        "answer": "1. Log in to dr1node3 and check the user groups: `id E70529 | grep 871556062`. If the group name is empty, proceed with fixing.\n2. Clear SSSD cache: `sss_cache -E; id E70529 | grep 871556062`.\n3. If the group is still missing, move SSSD cache files and restart the service across nodes:\n   ```\n   dcli -c 'dr1node03, dr1node05, dr1node06, dr1node07, dr1node08, dr1node09, dr1node10' 'mv /var/lib/sss/db/* /tmp; systemctl restart sssd'\n   ```\n4. Verify the group again: `id E70529 | grep -v \"CMS Way4Manager PROD RDS DevTOOLS\"`."
      },
      {
        "question": "You are troubleshooting a failed IBank_Ingestion batch job. The monitoring tool shows an 'OOM' (Out of Memory) error. How do you resolve the issue?",
        "answer": "1. Log in to `dr1edge01` and access YARN UI.\n2. Search for `PRODREST`, sort by End Date, and find the failed job.\n3. Execute the Merge Batch in three parts to reduce memory usage:\n   ```\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2022-02-28 00:00:00\" \"2022-02-28 12:00:00\"\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2022-02-28 12:00:00\" \"2022-02-28 18:00:00\"\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2022-02-28 18:00:00\" \"2022-03-01 00:00:00\"\n   ```\n4. Update monitoring PostgreSQL database to reflect the job as successful."
      },
      {
        "question": "How can you check if a user’s group assignments are correctly synchronized across multiple nodes?",
        "answer": "Use the following command:\n```bash\nid E70529 | grep -v \"CMS Way4Manager PROD RDS DevTOOLS\"\n```\nIf the output is empty or incorrect, synchronize across nodes:\n```bash\ndcli -c 'dr1node03, dr1node05, dr1node06, dr1node07, dr1node08, dr1node09, dr1node10' 'mv /var/lib/sss/db/* /tmp; systemctl restart sssd'\n```\nThen re-check the group assignment."
      },
      {
        "question": "A Spark job is failing due to an OOM error in a batch process. What is a common approach to rerun the job in smaller parts?",
        "answer": "Split the job into smaller time ranges and execute them separately:\n```bash\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"YYYY-MM-DD 00:00:00\" \"YYYY-MM-DD 12:00:00\"\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"YYYY-MM-DD 12:00:00\" \"YYYY-MM-DD 18:00:00\"\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"YYYY-MM-DD 18:00:00\" \"YYYY-MM-DD 23:59:59\"\n```"
      },
      {
        "question": "Which system is primarily affected when an IBank_Ingestion batch job fails?",
        "answer": "The affected system is the 'Disaster Site IBank Batch' hosted on `dr1edge01.mno.gr`."
      },
      {
        "question": "If a Hue user group synchronization crashes, what might be a possible cause?",
        "answer": "A missing or improperly assigned user group in the system. Checking the user's group membership using `id E70529 | grep 871556062` and manually synchronizing the group can help resolve the issue."
      },
      {
        "question": "How can you restart the SSSD service across multiple nodes?",
        "answer": "Run the following command:\n```bash\ndcli -c 'dr1node03, dr1node05, dr1node06, dr1node07, dr1node08, dr1node09, dr1node10' 'mv /var/lib/sss/db/* /tmp; systemctl restart sssd'\n```"
      },
      {
        "question": "What command can you use to check the failed batch job logs in YARN?",
        "answer": "Log in to `dr1edge01`, open Firefox, and check YARN UI for `PRODREST`. Sort by End Date and inspect the logs."
      },
      {
        "question": "After rerunning a batch job, how do you verify that it has been successfully recorded in Grafana?",
        "answer": "Update the monitoring PostgreSQL database with a success entry. Then, check Grafana to ensure no failed batch job entries remain for that day."
      },
      {
        "question": "What is the purpose of the `dcli` command used in the troubleshooting steps?",
        "answer": "It allows executing commands on multiple nodes simultaneously, ensuring configuration consistency across all relevant servers."
      },
      {
        "question": "A batch job fails during the 'Distinct join to Service Audit' step. How would you rerun it?",
        "answer": "Modify the master script to remove earlier steps and execute the process starting from the Distinct join step."
      },
      {
        "question": "How would you identify an issue with user group synchronization?",
        "answer": "Run:\n```bash\nid <username> | grep <group_id>\n```\nIf the group name is missing, refresh SSSD cache:\n```bash\nsss_cache -E\nid <username>\n```"
      },
      {
        "question": "How do you ensure a user has the correct Impala privileges to run queries?",
        "answer": "Check the user’s role in Impala:\n```sql\nSHOW GRANT ROLE <role_name>\n```\nIf needed, grant required permissions:\n```sql\nGRANT SELECT ON DATABASE <db_name> TO ROLE <role_name>;\n```"
      },
      {
        "question": "If a Spark job fails due to memory limits, what tuning parameter can be adjusted?",
        "answer": "Increase executor memory in the Spark submit script by modifying the `--executor-memory` parameter."
      },
      {
        "question": "Which log file should you check when investigating an IBank batch job failure?",
        "answer": "Check the logs in YARN UI and the following log file:\n```bash\n/var/log/ingestion/PRODREST/ibank/log/ibank_daily_batch_jobs.log\n```"
      },
      {
        "question": "If you need to migrate traffic to the DR site, where can you find the procedure?",
        "answer": "Follow the instructions in `../procedures/failover.md`."
      },
      {
        "question": "How do you verify that the IBank_Ingestion job succeeded after rerunning it?",
        "answer": "Check Grafana for no failed batch jobs and query PostgreSQL monitoring database for successful execution logs."
      },
      {
        "question": "What is a common cause of failed synchronization between user permissions and Hue?",
        "answer": "A missing user group assignment or an outdated SSSD cache."
      },
      {
        "question": "How do you manually trigger a batch ingestion process?",
        "answer": "Execute the ingestion script with appropriate date parameters:\n```bash\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"YYYY-MM-DD 00:00:00\" \"YYYY-MM-DD 23:59:59\"\n```"
      },
      {
        "question": "How can you troubleshoot a failed job in Grafana related to the EXPORT step of DWH_IBank?",
        "answer": "1. Login to `https://dr1edge01.mno.gr:3000` and check `Monitoring/Monitoring PR/DR` for failed Datawarehouse flows. 2. Check logs using `less /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`. 3. Identify if `Impala Insert` is still running, potentially blocked by another query. 4. Use Cloudera Manager (`Clusters > Impala > Queries`) to check for resource-intensive queries. 5. If `COMPUTE STATS` on `prod_trlog_ibank.service_audit` is hogging resources, consider disabling it."
      },
      {
        "question": "What does Cloudera's critical alarm for HiveServer2 Pause Duration indicate?",
        "answer": "The alarm typically indicates that the Java Heap Space is exhausted, leading to long garbage collection (GC) pauses. Logs can be checked using `grep GC /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node04.mno-gr.log.out`. If OutOfMemory errors occur, restarting HiveServer2 may be a temporary workaround."
      },
      {
        "question": "How can you identify and resolve an Impala query that is consuming excessive resources?",
        "answer": "1. Login to Cloudera Manager. 2. Navigate to `Clusters > Impala > Queries`. 3. Filter for queries with high memory usage. 4. If `COMPUTE STATS` is consuming excessive resources, advise stakeholders to disable it for large tables like `prod_trlog_ibank.service_audit`. 5. Restart affected services if necessary."
      },
      {
        "question": "What are the primary failure points in the `EXTRACT` job of DWH_IBank, and how can you troubleshoot them?",
        "answer": "The `EXTRACT` job consists of `Impala Insert` and `Sqoop Export`. If the job times out (`Code: 6`), check if another query is blocking Impala. Use `less /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log` and `Clusters > Impala > Queries` in Cloudera Manager to analyze resource allocation."
      },
      {
        "question": "How can you diagnose a memory-related failure in HiveServer2?",
        "answer": "1. Check Heap Memory Usage and GC Pause Time in Cloudera Manager. 2. Use `grep Java /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node04.mno-gr.log.out` to locate OutOfMemory errors. 3. Identify failed jobs using `Cluster -> Yarn -> Applications -> Filter: application_type = MAPREDUCE`. 4. Restart HiveServer2 if needed."
      },
      {
        "question": "What troubleshooting steps should be followed when multiple queries crash with a Java Heap Space error in Hive?",
        "answer": "1. Identify affected queries (`application_1665578283516_50081` to `application_1665578283516_50096`). 2. Confirm if they target `dev_trlog_card.pmnt_response_stg_0`. 3. Check GC logs to verify prolonged memory retention. 4. Restart HiveServer2 and analyze memory settings for improvement."
      },
      {
        "question": "How does Cloudera handle resource allocation for Impala queries, and how can this impact job performance?",
        "answer": "Impala queries are assigned memory from resource pools. If a query like `COMPUTE STATS` consumes excessive memory, it may delay or block other jobs, leading to timeouts. Monitoring `Clusters > Impala > Queries` can help identify such cases."
      },
      {
        "question": "What is the impact of running `COMPUTE STATS` on large Impala tables, and how can it be mitigated?",
        "answer": "`COMPUTE STATS` locks resources and may delay metadata refresh, causing job failures. To mitigate, it can be disabled for non-critical tables. This has been previously observed for `prod_trlog_ibank.service_audit`."
      },
      {
        "question": "How can Cloudera Manager be used to analyze failures in MapReduce jobs?",
        "answer": "1. Navigate to `Cluster -> Yarn -> Applications`. 2. Use filters such as `application_type = MAPREDUCE`. 3. Match failed applications with HiveServer2 logs using timestamps."
      },
      {
        "question": "How can you confirm if a failed job was caused by a long-running Impala query?",
        "answer": "Check `less /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log` to see if `Impala Insert` was still running. Then, use `Clusters > Impala > Queries` to check for resource-heavy queries."
      },
      {
        "question": "How can you check and restart a failed Sqoop job?",
        "answer": "1. Use `less /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log` to locate the issue. 2. If Sqoop Export is stuck, restart it manually. 3. Check if there is an underlying issue with the `Impala Insert` phase."
      },
      {
        "question": "What is the significance of Java Heap Space errors in HiveServer2 logs?",
        "answer": "They indicate memory exhaustion, leading to OutOfMemory exceptions. Affected services should be restarted, and memory allocation should be adjusted."
      },
      {
        "question": "How can Garbage Collection (GC) pauses affect Cloudera services, and how do you troubleshoot them?",
        "answer": "Long GC pauses can freeze applications. Use `grep GC /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node04.mno-gr.log.out` to analyze logs. Restarting services may help."
      },
      {
        "question": "How can you analyze Cloudera logs to correlate failed queries with system performance issues?",
        "answer": "Compare timestamps of failed queries in `HiveServer2` logs with resource metrics from Cloudera Manager."
      },
      {
        "question": "What steps should be taken if HiveServer2 crashes repeatedly?",
        "answer": "1. Identify queries causing the issue. 2. Check GC logs for memory retention. 3. Increase heap memory allocation if necessary."
      },
      {
        "question": "How do you diagnose and resolve timeouts in Impala query execution?",
        "answer": "Check resource utilization in Cloudera Manager. If a query is consuming excessive memory, consider killing it and rerunning jobs."
      },
      {
        "question": "How can you determine if an OutOfMemory error was caused by an inefficient query?",
        "answer": "Use `grep Java /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node04.mno-gr.log.out` to find errors and correlate them with running queries."
      },
      {
        "question": "How do you verify if a specific Hive query has caused a system crash?",
        "answer": "Check failed queries in `Cluster -> Yarn -> Applications` and compare them to logs from `HiveServer2`."
      },
      {
        "question": "What are the best practices to prevent recurring memory-related crashes in Cloudera services?",
        "answer": "Monitor resource-intensive queries, optimize memory allocation, and consider limiting the execution of high-memory operations like `COMPUTE STATS`."
      },
      {
        "question": "How do you safely rerun failed Cloudera jobs without causing further instability?",
        "answer": "Ensure the root cause (e.g., memory leaks, heavy queries) has been addressed before rerunning. Use Cloudera Manager to monitor performance."
      },
      {
        "question": "How can you manually authenticate a Kerberos user using a keytab file in a RHEL environment?",
        "answer": "You can authenticate a Kerberos user manually using the `kinit` command. For example:\n\n```bash\nkinit DEVUSER@BANK.CENTRAL.mno.GR -kt /way4/DEVUSER.keytab\n```\nIf successful, running `klist` should show a valid ticket cache."
      },
      {
        "question": "What change should be made in `/etc/krb5.conf` to ensure Kerberos ticket caching works correctly?",
        "answer": "Modify the `default_ccache_name` in `/etc/krb5.conf` under `[libdefaults]`:\n\n```conf\ndefault_ccache_name = FILE:/tmp/krb5cc_%{uid}\n```\n\nAdditionally, remove `sssd-kcm` if installed:\n\n```bash\nyum remove sssd-kcm\n```"
      },
      {
        "question": "What was the root cause of the Kerberos authentication error with Way4Streams?",
        "answer": "The issue was that the keytab contained deprecated `rc4-hmac` encryption, which is not supported by OpenJDK 11 by default. Enabling weak encryption in `/etc/krb5.conf` resolved the issue:\n\n```conf\nallow_weak_crypto = true\n```"
      },
      {
        "question": "How can you check if a Kudu table has been successfully created, and what commands should you run to verify it?",
        "answer": "To check if a Kudu table has been successfully created, follow these steps:\n      1. Login to `dr1edge01.mno.gr` and navigate to `dr1node01.mno.gr`.\n      2. Move to the Cloudera process folder: \n         ```bash\n         cd /var/run/cloudera-scm-agent/process/\n         ```\n      3. Identify the latest process folder, e.g., `12200-kudu-KUDU_TSERVER`, and navigate to it:\n         ```bash\n         cd 12200-kudu-KUDU_TSERVER\n         ```\n      4. Use the keytab file:\n         ```bash\n         kinit -kt kudu.keytab kudu/`hostname`\n         ```\n      5. Run the following command to check if the table exists in Kudu:\n         ```bash\n         kudu cluster ksck dr1node04.mno.gr dr1node05.mno.gr dr1node06.mno.gr | grep -i prod_trlog_card_analytical\n         ```\n      6. Alternatively, verify via Impala Shell:\n         ```bash\n         impala-shell -i dr1edge01 -k --ssl\n         use prod_trlog_card_analytical;\n         show tables;\n         ```"
      },
      {
        "question": "What steps should be taken if a 'Thrift SASL frame is too long' error occurs when creating a Kudu table?",
        "answer": "If you encounter a 'Thrift SASL frame is too long' error (e.g., 338.01M/100.00M), follow these steps:\n      1. Login to Cloudera Manager at the DR site.\n      2. Go to Kudu > Instances > Click on Master > Select Tab Configuration.\n      3. Search for 'safety value' and add the following flag in 'Master Advanced Configuration Snippet (Safety Valve for gflagfile)':\n         ```bash\n         --hive_metastore_max_message_size_bytes=858993459\n         ```\n      4. Apply this setting on all three Kudu masters.\n      5. Restart the three Kudu master nodes (one at a time)."
      },
      {
        "question": "How can you shut down specific ingestion streams before restarting Kudu tablets?",
        "answer": "Before restarting Kudu tablets, ensure that ingestion streams are shut down properly using the following commands:\n      ```bash\n      hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\n      hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown_marker/\n      hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\n      ```"
      },
      {
        "question": "How can you manually verify that the Hive Metastore notification log listener is caught up?",
        "answer": "To check the Hive Metastore notification log listener, follow these steps:\n1. Login to Cloudera Manager.\n2. Navigate to Hive > Instances > Metastore Logs.\n3. Look for logs related to 'failed to get Hive Metastore next notification'.\n4. If necessary, restart the Hive Metastore service."
    },
    {
        "question": "A Spark Streaming job is experiencing delays in processing messages from Kafka. What steps should be taken to diagnose and resolve this issue?",
        "answer": "To diagnose and resolve Spark Streaming job delays:\n1. Check the YARN Resource Manager UI to see if the job is still running.\n2. Inspect the application logs on the cluster:\n   ```bash\n   yarn logs -applicationId <application_id>\n   ```\n3. Look for any errors related to Kafka message consumption.\n4. Ensure that Kafka topics are receiving messages by running:\n   ```bash\n   kafka-console-consumer --bootstrap-server <kafka_broker> --topic <topic_name> --from-beginning\n   ```\n5. Restart the Spark Streaming job if needed."
    },
    {
        "question": "What Impala query would you run to check if a table exists in a specific database?",
        "answer": "To check if a table exists in a specific database using Impala, run:\n   ```sql\n   USE <database_name>;\n   SHOW TABLES LIKE '<table_name>';\n   ```"
    },
    {
        "question": "What configuration change should be applied to Kudu to increase the Hive Metastore message size limit?",
        "answer": "Modify the 'Master Advanced Configuration Snippet' in Cloudera Manager and add:\n   ```bash\n   --hive_metastore_max_message_size_bytes=858993459\n   ```\nThen restart the Kudu master nodes."
    },
    {
        "question": "How can you verify if a Kafka topic is correctly mirrored between sites?",
        "answer": "Run the following command to consume messages from the mirrored topic:\n   ```bash\n   kafka-console-consumer --bootstrap-server <mirror_broker> --topic <topic_name> --from-beginning\n   ```\nCompare this output with the primary Kafka topic."
    },
    {
        "question": "How can you manually force the deletion of old Kudu table partitions?",
        "answer": "Use the Impala query:\n   ```sql\n   ALTER TABLE <table_name> DROP PARTITION (par_dt <= 'YYYYMMDD') PURGE;\n   ```"
    },
    {
        "question": "How can you restart a failed Oozie job from the last failed step?",
        "answer": "Login to the Oozie UI, navigate to the failed job, and select 'Rerun' from the last failed action. Alternatively, use the command:\n   ```bash\n   oozie job -oozie http://<oozie-server>:11000/oozie -rerun <job_id> -action <failed_action_name>\n   ```"
    },
    {
        "question": "How can you manually authenticate a Kerberos user using a keytab file in a RHEL environment?",
        "answer": "You can authenticate a Kerberos user manually using the `kinit` command. For example:\n   ```bash\n   kinit DEVUSER@BANK.CENTRAL.mno.GR -kt /way4/DEVUSER.keytab\n   ```\nIf successful, running `klist` should show a valid ticket cache."
    },
    {
        "question": "What change should be made in `/etc/krb5.conf` to ensure Kerberos ticket caching works correctly?",
        "answer": "Modify the `default_ccache_name` in `/etc/krb5.conf` under `[libdefaults]`:\n   ```conf\n   default_ccache_name = FILE:/tmp/krb5cc_%{uid}\n   ```\nAdditionally, remove `sssd-kcm` if installed:\n   ```bash\n   yum remove sssd-kcm\n   ```"
    },
    {
        "question": "What was the root cause of the Kerberos authentication error with Way4Streams?",
        "answer": "The issue was that the keytab contained deprecated `rc4-hmac` encryption, which is not supported by OpenJDK 11 by default. Enabling weak encryption in `/etc/krb5.conf` resolved the issue:\n   ```conf\n   allow_weak_crypto = true\n   ```"
    },
    {
        "question": "How can you check the encryption types used in a keytab file?",
        "answer": "Use the following command:\n   ```bash\n   klist -kte /way4/DEVUSER.keytab\n   ```\nThe output will display the encryption types used. If you see `DEPRECATED:arc4-hmac`, this encryption type is outdated and may cause authentication issues."
    },
    {
        "question": "How do you restart a failed Grafana batch job for DWH_IBank?",
        "answer": "First, check the logs to identify the issue. If the error is related to duplicate keys, rerun the extract and export scripts with the `-f` option:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n   ```"
    },
    {
        "question": "How can you check if a Kerberos ticket is correctly obtained?",
        "answer": "Run the following command:\n   ```bash\n   klist\n   ```\nA valid ticket cache should display the principal name and expiration details."
    },
    {
        "question": "What is the impact of running the extract script with the `-f` flag in DWH_IBank?",
        "answer": "Running the script with `-f` forces truncation of the table before inserting records, which helps resolve duplicate key issues but will remove existing records:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n   ```"
    },
    {
        "question": "What command should be used to manually clear an expired Kerberos ticket and reauthenticate?",
        "answer": "Use the following commands:\n   ```bash\n   kdestroy\n   kinit DEVUSER@BANK.CENTRAL.mno.GR -kt /way4/DEVUSER.keytab\n   ```"
    },
    {
        "question": "How can you determine the status of a failed DWH_IBank batch job in Grafana?",
        "answer": "Check Grafana logs for batch job failures, particularly in `MAN_DATE` extract and export logs. If the error mentions duplicate keys, execute:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n   ```"
    },
    {
        "question": "How do you modify Kerberos configuration to allow deprecated encryption types?",
        "answer": "Edit `/etc/krb5.conf` and add the following under `[libdefaults]`:\n   ```conf\n   allow_weak_crypto = true\n   ```"
    },
    {
        "question": "How can you verify that a Kerberos ticket cache is using `FILE` storage instead of `KCM`?",
        "answer": "Run `klist` and check the output. If it says `KCM:1500`, modify `/etc/krb5.conf` to use:\n   ```conf\n   default_ccache_name = FILE:/tmp/krb5cc_%{uid}\n   ```"
    },
    {
        "question": "What error indicated the need to reconfigure Kerberos authentication for Way4Streams?",
        "answer": "The error `/way4/DEVUSER.keytab does not contain any keys for DEVUSER@BANK.CENTRAL.mno.GR` indicated an issue with the keytab encryption."
    },
    {
        "question": "What steps should be taken when an IBank_Ingestion MergeBatch job fails due to a memory fault?",
        "answer": "1. Log in to `dr1edge01` using:\n   ```bash\n   su - PRODREST\n   ```\n2. Check the script logs for errors:\n   ```bash\n   vi /var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log\n   ```\n3. If a memory fault error is found, update the Spark submission script:\n   ```bash\n   vi /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh\n   ```\n   Change `coalesce` from `6` to `12` and save the changes.\n4. Ensure that no records are present in `prod_trlog_ibank.service_audit_old`:\n   ```bash\n   impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select count(*) from prod_trlog_ibank.service_audit where par_dt='YYYYMMDD';\"\n   ```\n5. If no records exist, rerun the batch job for the previous day:\n   ```bash\n   /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n   ```"
    },
    {
        "question": "What was the root cause of the Online_Ingestion MergeBatch failure and how was it resolved?",
        "answer": "The failure was due to `Permission Denied on HDFS directory`. The root cause was an unavailability of MySQL, preventing sentry permissions verification. The issue was resolved by:\n1. Checking for existing records in the table:\n   ```bash\n   impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select count(*) from prod_trlog_online.service_audit where par_dt='20230223';\"\n   ```\n2. If no records exist, running the MergeBatch script manually:\n   ```bash\n   /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_Online_MergeBatch LOCK_ONLINE_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_Online_MergeBatch /opt/ingestion/PRODREST/online/lock/ FULL 1800 \"`date --date='-1 day' '+%Y-%m-%d 00:00:00'`\" \"`date '+%Y-%m-%d 00:00:00'`\" >> /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log 2>&1 &\n   ```"
    },
    {
        "question": "What is the purpose of running the ‘Distinct join to Service Audit’ step after rerunning the MergeBatch job?",
        "answer": "After rerunning the MergeBatch job, the ‘Distinct join to Service Audit’ step ensures that duplicate records are removed and that the final dataset contains only unique records before further processing. This step is crucial for data integrity and prevents redundancy in analytics."
    },
    {
        "question": "How can you verify if a failed Online_Ingestion MergeBatch job has successfully completed after rerunning it?",
        "answer": "To verify if the rerun was successful:\n1. Check the log file:\n   ```bash\n   tail -f /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log\n   ```\n2. Ensure that the job runs for a few hours and completes without errors.\n3. Run a query to check if data has been properly ingested:\n   ```bash\n   impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select count(*) from prod_trlog_online.service_audit where par_dt='YYYYMMDD';\"\n   ```"
    },
    {
        "question": "Why does the IBank_Ingestion MergeBatch job run significantly longer under heavy load, and how can execution time be optimized?",
        "answer": "The IBank_Ingestion MergeBatch job runs longer under heavy load due to the large volume of data being processed and memory-intensive operations. To optimize execution time:\n1. Increase the Spark partition coalescing value in the script:\n   ```bash\n   vi /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh\n   ```\n   Change `coalesce` from `6` to `12`.\n2. Run the process in screen mode to prevent session loss.\n3. Monitor YARN Resource Manager UI to identify resource bottlenecks.\n4. If required, allocate additional resources or optimize queries to reduce load."
    },
    {
        "question": "How can you renew an expiring SSL certificate for a Wildfly server?",
        "answer": "Follow these steps:\n1. Generate a new key pair and CSR:\n   ```bash\n   keytool -genkeypair -alias server -keyalg RSA -keysize 2048 -keystore keystore.jks -validity 365\n   keytool -certreq -alias server -file server.csr -keystore keystore.jks\n   ```\n2. Submit the CSR to a certificate authority.\n3. Once issued, import the certificate:\n   ```bash\n   keytool -import -trustcacerts -alias root -file rootCA.crt -keystore keystore.jks\n   keytool -import -trustcacerts -alias server -file server.crt -keystore keystore.jks\n   ```\n4. Restart Wildfly to apply the new certificate."
      },
      {
        "question": "What are the key steps involved in a failover procedure for a primary site failure?",
        "answer": "1. Verify that the primary site is down by checking monitoring alerts and logs.\n2. Manually switch traffic to the disaster recovery (DR) site:\n   ```bash\n   /opt/failover/scripts/switch_to_dr.sh\n   ```\n3. Confirm that critical services (Kafka, Wildfly, HBase) are running on the DR site.\n4. Update DNS records to point to the DR infrastructure.\n5. Monitor the DR site for anomalies before resuming full-scale operations."
      },
      {
        "question": "What should be done before upgrading Grafana to prevent data loss?",
        "answer": "1. Backup the Grafana database:\n   ```bash\n   sudo cp -r /var/lib/grafana /var/lib/grafana.bak\n   ```\n2. Backup the configuration file:\n   ```bash\n   sudo cp /etc/grafana/grafana.ini /etc/grafana/grafana.ini.bak\n   ```\n3. Check installed plugins and note any dependencies.\n4. Stop Grafana services:\n   ```bash\n   sudo systemctl stop grafana-server\n   ```\n5. Proceed with the upgrade and verify that dashboards are intact afterward."
      },
      {
        "question": "How can you benchmark HBase read and write performance using YCSB?",
        "answer": "1. Install YCSB and configure it for HBase:\n   ```bash\n   git clone https://github.com/brianfrankcooper/YCSB.git\n   cd YCSB\n   mvn -pl site.ycsb:hbase-binding -am clean package\n   ```\n2. Load test data:\n   ```bash\n   ./bin/ycsb load hbase10 -P workloads/workloada -p table=usertable\n   ```\n3. Run a benchmark test:\n   ```bash\n   ./bin/ycsb run hbase10 -P workloads/workloada -p table=usertable\n   ```\n4. Analyze results and tune HBase configurations accordingly."
      },
      {
        "question": "What steps are required to upgrade Java in a production environment with minimal downtime?",
        "answer": "1. Download and install the new Java version:\n   ```bash\n   sudo yum install java-11-openjdk\n   ```\n2. Set the new Java version as default:\n   ```bash\n   sudo alternatives --config java\n   ```\n3. Verify Java version:\n   ```bash\n   java -version\n   ```\n4. Restart Java-dependent applications:\n   ```bash\n   sudo systemctl restart wildfly\n   sudo systemctl restart kafka\n   ```\n5. Monitor logs to detect any compatibility issues."
      },
      {
        "question": "How can you set an HBase quota to limit user space consumption?",
        "answer": "1. Set a space quota:\n   ```bash\n   hbase shell\n   set_quota 'testTable', SPACE, '10G'\n   ```\n2. Verify quota settings:\n   ```bash\n   list_quotas\n   ```\n3. If a quota is exceeded, adjust it or clean up data:\n   ```bash\n   alter 'testTable', METHOD => 'delete'\n   ```"
      },
      {
        "question": "How do you restart a MirrorMaker process in a Kafka cluster?",
        "answer": "1. Check the running MirrorMaker process:\n   ```bash\n   ps -ef | grep MirrorMaker\n   ```\n2. Stop the existing process:\n   ```bash\n   kill -9 <pid>\n   ```\n3. Restart MirrorMaker:\n   ```bash\n   nohup kafka-mirror-maker --consumer.config consumer.properties --producer.config producer.properties --whitelist '.*' &\n   ```"
      },
      {
        "question": "What command is used to deploy a new Wildfly application?",
        "answer": "1. Copy the `.war` file to the deployment directory:\n   ```bash\n   cp myapp.war /opt/wildfly/standalone/deployments/\n   ```\n2. Restart Wildfly:\n   ```bash\n   sudo systemctl restart wildfly\n   ```\n3. Verify deployment status using the management CLI:\n   ```bash\n   /opt/wildfly/bin/jboss-cli.sh --connect --command='deployment-info'\n   ```"
      },
      {
        "question": "How do you decrypt a disk encrypted with Navencrypt?",
        "answer": "1. Unlock the encrypted disk:\n   ```bash\n   sudo navencrypt-unlock -d /dev/sdb1\n   ```\n2. Verify the decryption status:\n   ```bash\n   sudo navencrypt-status\n   ```"
      },
      {
        "question": "What should you check before applying a new SSL certificate?",
        "answer": "1. Verify the certificate chain:\n   ```bash\n   openssl verify -CAfile chain.pem cert.pem\n   ```\n2. Ensure the private key matches the certificate:\n   ```bash\n   openssl x509 -noout -modulus -in cert.pem | openssl md5\n   openssl rsa -noout -modulus -in privkey.pem | openssl md5\n   ```"
      },
      {
        "question": "How do you change the default heap size of Wildfly?",
        "answer": "Edit `standalone.conf`:\n   ```bash\n   export JAVA_OPTS=\"-Xms2G -Xmx4G\"\n   ```\nRestart Wildfly:\n   ```bash\n   sudo systemctl restart wildfly\n   ```"
      },
      {
        "question": "How do you force a ZooKeeper leader election manually?",
        "answer": "1. Identify the current leader:\n   ```bash\n   echo stat | nc localhost 2181\n   ```\n2. Stop the leader node:\n   ```bash\n   systemctl stop zookeeper\n   ```\n3. Observe leader re-election by running:\n   ```bash\n   echo stat | nc localhost 2181\n   ```"
      },
      {
        "question": "How can you set up automatic Wildfly deployments?",
        "answer": "1. Enable the deployment scanner:\n   ```bash\n   /opt/wildfly/bin/jboss-cli.sh --connect --command='deploy scanner --enabled=true'\n   ```\n2. Copy `.war` files to `/opt/wildfly/standalone/deployments/`."
      },
      {
        "question": "What steps should be followed before restarting a Kafka broker?",
        "answer": "1. Verify there is no under-replicated partition:\n   ```bash\n   kafka-topics --describe --bootstrap-server localhost:9092\n   ```\n2. Stop the broker:\n   ```bash\n   systemctl stop kafka\n   ```\n3. Restart and check logs:\n   ```bash\n   systemctl start kafka\n   journalctl -u kafka -n 50\n   ```"
      },
      {
        "question": "How do you trigger a failover in a Hadoop NameNode?",
        "answer": "Run:\n   ```bash\n   hdfs haadmin -failover active standby\n   ```"
      },
      {
        "question": "How can you enable ACLs in both YARN and Spark to control access to Spark logs?",
        "answer": "To enable ACLs:\n\n1. **In YARN:**\n   - Navigate to YARN → Configuration.\n   - Search for 'ACL For Viewing A Job'.\n   - Add extra groups to allow viewing MapReduce jobs, e.g., `hue WBDADMIN,WBDOPDEV,WBDOPPRO,WBDOPQA`.\n   - Enable 'Job ACL JobHistory Server Default Group'.\n\n2. **In Spark:**\n   - Navigate to Spark → Configuration.\n   - Search for 'Spark Client Advanced Configuration Snippet'.\n   - Enable Spark ACLs with:\n     ```bash\n     spark.acls.enable=true\n     ```\n   - Set admin groups:\n     ```bash\n     spark.admins.acls.groups=WBDADMIN\n     ```\n   - Give permissions to the Spark History Server:\n     ```bash\n     spark.history.ui.admin.acls.groups=WBDADMIN\n     ```\n   - Define UI view access:\n     ```bash\n     spark.ui.view.acls.groups=WBDOPDEV,WBDOPPRO,WBDOPQA\n     ```"
    },
    {
        "question": "How can you repair a broken MySQL replication process?",
        "answer": "Follow these steps:\n\n1. **Check replication status:**\n   ```bash\n   mysql -u root -p\n   SHOW SLAVE STATUS\\G;\n   ```\n   If `Slave_IO_Running` or `Slave_SQL_Running` is 'No', the replication is broken.\n\n2. **Stop the slave:**\n   ```bash\n   STOP SLAVE;\n   ```\n\n3. **Restore from the latest MySQL dump:**\n   ```bash\n   cd /backup\n   tar -zxvf /backup/DRBDA_year-month-day.tar.gz mysql_backup_yearmonthday.sql.gz\n   gunzip mysql_backup_yearmonthday.sql.gz\n   mysql -uroot -p < mysql_backup_yearmonthday.sql\n   ```\n\n4. **Restart the replication:**\n   ```bash\n   START SLAVE;\n   SHOW SLAVE STATUS\\G;\n   ```\n   Ensure `Slave_IO_Running` and `Slave_SQL_Running` are both 'Yes' and `Seconds_Behind_Master` is 0."
    },
    {
        "question": "What is the procedure to upgrade Java in a production environment?",
        "answer": "1. Download the required Java version.\n2. Backup the existing Java installation:\n   ```bash\n   tar -cvzf /backup/java_backup.tar.gz /usr/java/\n   ```\n3. Install the new Java version:\n   ```bash\n   rpm -ivh jdk-<version>.rpm\n   ```\n4. Update alternatives:\n   ```bash\n   alternatives --config java\n   ```\n5. Verify the installation:\n   ```bash\n   java -version\n   ```"
    },
    {
        "question": "How can you manually fail over a WildFly instance?",
        "answer": "1. Stop the active WildFly instance:\n   ```bash\n   systemctl stop wildfly\n   ```\n2. Update the DNS records or load balancer to point to the secondary WildFly server.\n3. Start the standby WildFly instance:\n   ```bash\n   systemctl start wildfly\n   ```\n4. Verify logs to ensure a successful failover:\n   ```bash\n   tail -f /var/log/wildfly/server.log\n   ```"
    },
    {
        "question": "How can you configure a new repository in Nexus?",
        "answer": "1. Log in to Nexus.\n2. Navigate to 'Repositories' and click 'Create repository'.\n3. Choose the repository type (e.g., 'maven2-hosted').\n4. Configure storage and cleanup policies.\n5. Save and validate the configuration."
    },
    {
        "question": "What is the recommended procedure for an HBase benchmarking test?",
        "answer": "1. Run the YCSB benchmarking tool:\n   ```bash\n   ./bin/ycsb load hbase10 -P workloads/workloada -p table=usertable\n   ```\n2. Execute the workload:\n   ```bash\n   ./bin/ycsb run hbase10 -P workloads/workloada -p table=usertable\n   ```\n3. Analyze results using `hbase hbck`."
    },
    {
        "question": "How do you manually renew an SSL certificate in Apache?",
        "answer": "1. Obtain a new certificate from the CA.\n2. Replace the existing certificate files in `/etc/httpd/conf/ssl.crt/`.\n3. Restart Apache:\n   ```bash\n   systemctl restart httpd\n   ```"
    },
    {
        "question": "How can you synchronize MySQL databases between two sites?",
        "answer": "1. Stop replication:\n   ```bash\n   STOP SLAVE;\n   ```\n2. Copy the latest MySQL dump to the secondary site.\n3. Restore the dump:\n   ```bash\n   mysql -u root -p < mysql_backup.sql\n   ```\n4. Restart replication:\n   ```bash\n   START SLAVE;\n   ```"
    },
    {
        "question": "How can you enable Job ACLs in YARN?",
        "answer": "Navigate to YARN → Configuration → 'ACL For Viewing A Job' and add the required groups."
    },
    {
        "question": "How do you benchmark HBase write performance?",
        "answer": "Use the `hbase` command-line tool:\n   ```bash\n   hbase pe --table testTable --rows=100000 --nomapred sequentialWrite 10\n   ```"
    },
    {
        "question": "How can you configure a failover for Apache?",
        "answer": "Use a secondary Apache server and enable Keepalived."
    },
    {
        "question": "What steps should be taken before upgrading PostgreSQL?",
        "answer": "1. Backup the database using pg_dump.\n2. Stop the PostgreSQL service.\n3. Install the new version.\n4. Run `pg_upgrade`."
    },
    {
        "question": "How do you add users to Spark ACLs?",
        "answer": "Modify `spark.acls.enable` and `spark.ui.view.acls.groups` in Spark configuration."
    },
    {
        "question": "What command is used to manually decrypt a NaVencrypt-protected disk?",
        "answer": "Use `navdecrypt`:\n   ```bash\n   navdecrypt --decrypt /dev/sdb1\n   ```"
    },
    {
        "question": "How can you verify that MySQL replication is caught up?",
        "answer": "Run:\n   ```bash\n   SHOW SLAVE STATUS\\G;\n   ```\n   Ensure `Seconds_Behind_Master` is 0."
    },
    {
        "question": "How do you upgrade YARN configurations for better ACL handling?",
        "answer": "Modify the 'ACL For Viewing A Job' and 'Job ACL JobHistory Server Default Group' in YARN configuration."
    },
    {
        "question": "How can you confirm that a Kerberos principal has a valid ticket?",
        "answer": "Run:\n   ```bash\n   klist\n   ```"
    },
    {
        "question": "What is the recommended approach for setting HBase quotas?",
        "answer": "Use `hbase shell`:\n   ```bash\n   set_quota '10G', TYPE => SPACE, TABLE => 'mytable'\n   ```"
    },
    {
        "question": "How do you restart MirrorMaker manually?",
        "answer": "Run:\n   ```bash\n   systemctl restart mirrormaker\n   ```"
    }
    ]
  }
  