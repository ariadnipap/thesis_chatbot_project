{
    "qa_pairs": [
        {
            "question": "How do I verify SSH connectivity between a custompoller user and nnmprd01 before running IP-VPN services?",
            "answer": "To verify SSH connectivity:\n```bash\nssh custompoller@nnmprd01\n```\nIf prompted for a password, execute:\n```bash\nssh-copy-id custompoller@nnmprd01\n```\nThen, test again with:\n```bash\nssh custompoller@nnmprd01\n```\nOnce SSH is passwordless, proceed with running IP-VPN services.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What are the steps involved in the Move Kudu to Impala Flow in TeMIP?",
            "answer": "The Move Kudu to Impala Flow consists of an Oozie Coordinator (`TeMIP_kudu_2_Impala_CO`) that runs daily at 06:00. It moves terminated alarms from `temip.temip_kudu_terminated_alarms` to `temip.temip_impala_terminated_alarms` and historic events from `temip.temip_kudu_historic_events` to `temip.temip_impala_historic_events`.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the purpose of the IBank Migration Historical JOB, and how can I verify if it has been executed successfully?",
            "answer": "The IBank Migration Historical JOB is responsible for transferring historical data related to IBank from one system to another. To verify its execution, run the following query:\n```sql\nSELECT * FROM prod_trlog_ibank.monitor_sched_jobs WHERE par_dt=20210429;\n```\nIf the job has run successfully, you will see an entry in the results. Additionally, you can check Grafana for job status and logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How do you open a support ticket with Dell for BigStreamer issues?",
            "answer": "To open a ticket with Dell:\n1. SSH into the affected node.\n2. Use `ipmitool lan print | grep -i 'IP Address'` to find the Management IP.\n3. Connect via VNC, open Firefox, and enter the IP.\n4. Retrieve the Service Tag from 'Server > Overview > Server Information'.\n5. Call Dell support at 2108129800 with the Service Tag.\n6. Follow Dell\u2019s instructions to collect TSR logs from IDRAC.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How is IDM replication managed across nodes?",
            "answer": "IDM (FreeIPA) replication uses GSSAPI authentication via Kerberos and LDAP service principal names (SPNs). The replication is push-based, ensuring both nodes remain synchronized.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How does the anonymization process work in data retention?",
            "answer": "The anonymization process runs the script `100.Anonymize_Data_Main.sh`, logs execution status, and checks for errors. The retention process verifies script success using `grep \"Script Status\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.202012.log`.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the purpose of the `ztab` file in the navencrypt disk decryption process?",
            "answer": "The `ztab` file lists encrypted mount points. When decrypting a disk, check the file using:\n```bash\ncat /etc/navencrypt/ztab | grep /data/1\n```\nIf the entry exists, it should be commented out before proceeding.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the purpose of the Streamsets Java Heap Space configuration?",
            "answer": "The Java Heap Space configuration in Streamsets ensures proper memory allocation by setting `-Xmx32768m -Xms32768m` in Cloudera Manager. This prevents memory allocation issues when running data processing pipelines.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can a Kerberos keytab file be created for user authentication?",
            "answer": "Log in to the `kerb1` node, use `kadmin.local`, check if the principal exists, and create it if necessary using `addprinc <username>@CNE.abc.GR`. Generate the keytab file using `ktadd -k /tmp/<username>.keytab -norandkey <username>@CNE.abc.GR` and move it to the appropriate directory.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can I reload missing data into the `radius.radarchive_hist` table?",
            "answer": "To reload missing data into the `radius.radarchive_hist` table, follow these steps:\n1. Refresh the table using Impala:\n   ```sql\n   refresh radius.radarchive_hist;\n   select count(*), par_dt from radius.radarchive_hist where par_dt = 'YYYYMMDD' group by par_dt;\n   ```\n2. If the issue persists, check the crontab to confirm the scheduled scripts are running.\n3. Verify if the required file exists in the SFTP server:\n   ```bash\n   sftp intra@<sftp-server-ip>\n   ls -l radarchive_YYYYMMDD.csv.bz2\n   ```\n4. If missing, retrieve and extract it:\n   ```bash\n   get radarchive_YYYYMMDD.csv.bz2\n   bzip2 -d radarchive_YYYYMMDD.csv.bz2\n   ```\n5. Load the missing data by executing:\n   ```bash\n   /shared/abc/radius/DataParser/scripts/radius.pl -l -d -D -o\n   /shared/abc/radius/bin/000_radius_ops.sh\n   ```",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can I prevent root SSH login on all servers?",
            "answer": "To disable root SSH login on all servers, modify the SSH configuration. First, check the current configuration:\n```bash\nsalt '*' cmd.run 'grep \"^PermitRootLogin\" /etc/ssh/sshd_config'\n```\nThen, modify the configuration to disallow root login:\n```bash\nsed -i -e 's/^PermitRootLogin.*/PermitRootLogin no/' /etc/ssh/sshd_config\n```\nApply the changes using:\n```bash\nsalt '*' cmd.script salt://disable_root_login.sh\nsalt '*' cmd.run 'service sshd reload'\n```",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can we resolve a 'Batch Job Failed' error in Grafana for the DWH_IBank application?",
            "answer": "The failure is due to duplicate keys in the srcib.MandateDetails table. The resolution involves running the extract script with the `-f` flag to truncate the table before inserting new records:\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n```\nAfter extraction, the export script must also be executed with the `-f` flag:\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n```\nThis ensures that the extracted records are properly inserted and exported.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the function of the Main Application in the TeMIP Flow?",
            "answer": "The Main Application in the TeMIP Flow is a Java application hosted on a Wildfly server. It receives TeMIP alarms and stores them into Apache Kudu for near real-time CRUD operations. The stored data is later moved to Apache Impala for extended retention of six months.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How does the Cube Indicators Pipeline process data?",
            "answer": "The Cube Indicators Pipeline runs the `Coord_Cube_Spark_Indicators` coordinator, which processes data with a two-day delay. It populates the `brond.cube_indicators` table using data from various dependent tables like `brond.fixed_radio_matches_unq_inp`, `radius.radacct_hist`, and `brond.brond_retrains_hist`.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How do you change the OpenLDAP Manager password?",
            "answer": "Generate a new password using `slappasswd -h {SSHA}`. Store the output and create `changepwconfig.ldif` and `changepwmanager.ldif` files with the new password. Apply changes using `ldapmodify -H ldapi:// -Y EXTERNAL -f changepwmanager.ldif`.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can I add a new repository in Nexus?",
            "answer": "To add a new repository in Nexus, follow these steps:\n1. Login to an edge node using SSH:\n   ```bash\n   ssh -X xedge0x\n   ```\n2. Open Firefox and navigate to `https://999.999.999.999:8081/`.\n3. Login with Nexus credentials.\n4. Click on the gear icon, then go to **Repositories** and select **Create repository**.\n5. Choose **yum (proxy)** and configure the repository with values such as `Name`, `Remdef storage`, and `Clean up policies`.\n6. Click **Create repository**.\n7. Add the repository configuration on the node:\n   ```bash\n   vi /etc/yum.repos.d/name_of_repo.repo\n   ```\n8. Check and add the new repository using:\n   ```bash\n   yum clean all\n   yum check-update > /tmp/test-repo.txt\n   yum repolist\n   ```",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How does Viavi Kafka connectivity work within BigStreamer?",
            "answer": "Viavi Kafka connectivity is established via HAProxy on a single node (Incelligent). This ensures traffic isolation to prevent network congestion. The connection involves internal DNS resolution mapping to Kafka broker IPs using VLAN 300.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What are the main KPI categories computed by the IPVPN-SM replacement process?",
            "answer": "The three major KPI categories computed by the IPVPN-SM replacement process are Components (CPU, Memory), Interfaces (IF), and SLA (Availability, Quality of Service).",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How do I manually verify the registration of new measurement points in BigStreamer?",
            "answer": "To manually verify if new measurement points are registered in BigStreamer:\n1. Query the `bigcust.nnmcp_ipvpn_slametrics_hist` table:\n   ```sql\n   select distinct qa_probe_name from bigcust.nnmcp_ipvpn_slametrics_hist where par_dt='20230202' and customer = 'fraport';\n   ```\n2. Compare with the list of expected probes.\n3. If missing, check `custompoller` logs for extraction issues.\n(Source: X20230205-IM2076207.md)",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How is the `Piraeus Cisco VDSL2` application scheduled and executed?",
            "answer": "The application is executed within a Kubernetes pod and is scheduled to run every 5 minutes using a Cron job.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can I restart a failed Spark streaming topology in the BigStreamer environment?",
            "answer": "To restart a failed Spark streaming topology in the BigStreamer environment, login to `dr1edge01`, then execute the following command: \n```bash\n/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\n```\nThis will resubmit the topology, and you should verify the status via Grafana and Cloudera Manager to ensure there are no ongoing failures.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How do I check if the Kafka and Kudu services are running before decrypting a disk?",
            "answer": "To check if Kafka and Kudu are running, navigate to Cloudera Manager and:\n1. Go to `Kafka > Status`.\n2. Go to `Kudu > Status`.\nEnsure they are stopped before proceeding with disk decryption.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How does the Initialization/Synchronization flow in TeMIP work?",
            "answer": "The Initialization/Synchronization Flow in TeMIP is managed by the Oozie Coordinator `TeMIP_Synchronization_CO`. It establishes a connection between the Wildfly Server running the TeMIP application and the TeMIP Server. This coordinator must be run manually after every restart or deployment of the Wildfly Server.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "What is the function of a load balancer in the Wildfly setup?",
            "answer": "The load balancer (NetScaler) manages SSL offloading and routes traffic to Wildfly instances based on predefined rules. For example, requests that start with `/trlogibank` are routed to `prodrestib`, while other requests are directed to `prodrestibmetrics`.",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How can I check for missing logs in the abc BigStreamer syslog?",
            "answer": "To check for missing logs in the abc BigStreamer syslog, SSH into un2 as root and inspect the rsyslog configuration using:\n```bash\ncat /etc/rsyslog.conf | more\n```\nThen, check the servers where messages are transferred by examining log rotation settings with:\n```bash\ncat /etc/logrotate.conf | more\n```",
            "category": "Application Functionality & Flow"
        },
        {
            "question": "How do you manually rerun a failed location mobility export?",
            "answer": "You can manually execute the script using the `--max-files` flag to catch up on missed exports. For example, to export six missing files, run:\n```bash\n/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh --max-files 6\n```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "Why did the EXPORT job for the Data Warehouse fail with Code: 6?",
            "answer": "The EXPORT job failure with Code: 6 indicates that the control script timed out while monitoring the EXTRACT script. The failure might be caused by another Impala query consuming all available resources. To diagnose:\n1. Check logs: \n```bash\nless /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\n```\n2. Identify resource-intensive queries from Cloudera Manager:\n```bash\nClusters > Impala > Queries\n```\n3. Look for compute-intensive queries such as `COMPUTE STATS` running on large tables.\nIf this is the case, rerun the failed job and consider disabling statistics computation for non-essential tables.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do you troubleshoot a failed Cube Indicators execution?",
            "answer": "First, verify that the required data sources (`brond.fixed_radio_matches_unq_inp`, `radius.radacct_hist`) are populated. If necessary, rerun the indicators job manually using `./run_cube.sh <date>`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do you restart the Monitoring application in case it stops unexpectedly?",
            "answer": "To restart the Monitoring application, use:\n```\ndocker start monitoring-app-{version}\n```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What should you check if OpenLDAP replication is broken?",
            "answer": "First, verify if the replication issue is due to a password change by checking the `Manager` credentials. If needed, update the credentials in the replication configuration file and apply changes using `ldapmodify`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What steps should I follow to resolve missing statistics warnings in `sai.voice_quality_hist`?",
            "answer": "To resolve missing statistics warnings in `sai.voice_quality_hist`:\n1. Check the current table statistics using Impala:\n   ```sql\n   show table stats sai.voice_quality_hist;\n   ```\n2. If statistics are missing, recompute them:\n   ```sql\n   compute stats sai.voice_quality_hist;\n   ```\n3. If errors persist, refresh the table:\n   ```sql\n   refresh sai.voice_quality_hist;\n   ```\n4. If the warning continues, verify data flow and logs for potential loading issues.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What caused the Spark History Server on dr1node03 to exit unexpectedly, and how was it resolved?",
            "answer": "The Spark History Server on dr1node03 exited due to an `OutOfMemoryError`. The investigation showed that the Java heap size for the server was set to 512MB, whereas the PR Site had it set to 2GB. The resolution involved increasing the Java heap size to 2GB and restarting the History Server, which resolved the issue without any further disruptions.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What are the common reasons for an HBase region server to restart unexpectedly?",
            "answer": "Common reasons for an HBase region server restart include:\n1. **Out-Of-Memory (OOM) errors**: When memory allocation exceeds available resources, the server restarts.\n2. **Table misconfiguration**: High per-region workload leading to uneven distribution among RegionServers.\n3. **Metadata inconsistencies**: If metadata for deleted tables still exists, it can cause issues.\nTo resolve, analyze logs at `/var/log/hbase/hbase-cmf-hbase-REGIONSERVER-dr1node01.mno.gr.log.out`, and if needed, remove stale metadata with `hbase hbck2 extraRegionsInMeta <TABLE_NAME> --fix`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do I fix a broken MySQL replication?",
            "answer": "To repair MySQL replication:\n1. Check the replication status on the slave:\n   ```bash\n   mysql -u root -p\n   SHOW SLAVE STATUS\\G;\n   ```\n2. If `Slave_IO_Running` or `Slave_SQL_Running` is `No`, stop the slave:\n   ```bash\n   STOP SLAVE;\n   ```\n3. Restore from the latest MySQL dump backup:\n   ```bash\n   cd /backup\n   tar -zxvf backup.tar.gz mysql_backup.sql.gz\n   gunzip mysql_backup.sql.gz\n   mysql -uroot -p < mysql_backup.sql\n   ```\n4. Restart the slave and verify:\n   ```bash\n   START SLAVE;\n   SHOW SLAVE STATUS\\G;\n   ```\n5. Ensure `Seconds_Behind_Master` is 0.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How can I determine if an Impala batch job failed due to long row key lengths in HBase?",
            "answer": "If an Impala batch job fails due to long row keys, inspect the logs for the following error:\n   ```bash\n   WARNING: java.lang.IllegalArgumentException: Row length 34XXX is > 32767\n   ```\nTo identify problematic rows, modify the query:\n   ```sql\n   SELECT * FROM your_table WHERE length(concat(default.hbase_salt_md5hex(client_username,4,TRUE), chr(0), hex(CAST(CAST(u_timestamp AS DOUBLE) * 1000000 AS bigint)), chr(0), a.id)) > 32767;\n   ```\nIf results appear, modify the job script to exclude these rows.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do you fix Viavi Kafka HAProxy connection issues?",
            "answer": "Check HAProxy logs using `systemctl status haproxy`. If needed, restart with `systemctl restart haproxy`. Verify connectivity with `nc -zv <broker-ip> 9093`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What logs should you check if the Piraeus Cisco VDSL2 application fails to transform SNMP files?",
            "answer": "Check the logs using `kubectl logs` for stdout messages and verify the transformation process in `/app/conf/application.yaml` under `vdsl2.dataDir`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What steps should be taken to reprocess missing radius.radacct_hist records if ingestion failed?",
            "answer": "1. Verify that the original files exist in `radius.radacct_orig_files`:\n```bash\nhdfs dfs -ls /ez/warehouse/radius.db/radacct_orig_files/\n```\n2. Copy the missing files to the load table:\n```bash\nhdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_YYYY-MM-DD_HH-MM.csv.utc /ez/warehouse/radius.db/radacct_load/\n```\n3. Re-run the radius processing script:\n```bash\n/shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.YYYYMMDD.log.manual 2>&1\n```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How can I resolve a failed batch job due to a duplicate key in MSSQL?",
            "answer": "If a batch job fails due to a duplicate key in MSSQL (e.g., `Cannot insert duplicate key in object srcib.MandateDetails`), execute the following steps:\n1. Run the extract script with the `-f` option to truncate the table before inserting new records:\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n```\n2. Run the export script to ensure the truncated data is exported correctly:\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n```\nThis ensures the duplicate records are removed and reinserted cleanly.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do you update an expired Kubernetes certificate in RAN.AI?",
            "answer": "Use `kubeadm certs renew all`, then restart kube-apiserver and controller-manager containers using `ctrctl stop <container_id>`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What should I do if Kafka does not restart after disk decryption?",
            "answer": "Ensure that the disk decryption process was completed successfully. If Kafka fails to start, try restarting manually from Cloudera Manager. If the issue persists, check logs under:\n```bash\n/var/log/kafka/server.log\n```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How can I troubleshoot a failed Wildfly deployment?",
            "answer": "Check the Wildfly application logs using:\n```bash\ntail -f /var/log/wildfly/prodrestib/server.log\n```\nIf the issue is related to configuration, verify `standalone.xml` and check for missing dependencies in the deployments folder:\n```bash\nls -lh /opt/wildfly/default/prodrestib/standalone/deployments/\n```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How can you restart the Traffica Flow after resolving an issue?",
            "answer": "After identifying and resolving the root cause, enable normal operation by running:\n```bash\ncurl -X PUT 'http://unc2.bigdata.abc.gr:11483/traffica/app/operations/lifecycle/enable'\n```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do you resolve Nagios 'ssh_exchange_identification: Connection closed by remdef host' errors?",
            "answer": "Modify `/usr/local/nagios/etc/objects/commands.cfg`, changing SSH options to `-E 8 -o StrictHostKeyChecking=no`. Restart Nagios using `service nagios restart`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do I modify SSL settings in Apache, Nginx, and HAProxy?",
            "answer": "To modify SSL settings:\n1. **Apache (httpd)**:\n   - Backup existing config files:\n     ```bash\n     cp -ap /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.bak\n     ```\n   - Modify `/etc/httpd/conf.d/ssl.conf`:\n     ```\n     SSLProtocol +TLSv1.2\n     ```\n   - Restart Apache:\n     ```bash\n     systemctl restart httpd\n     ```\n2. **Nginx:**\n   - Backup and modify `/etc/nginx/nginx.conf`:\n     ```\n     ssl_protocols TLSv1.2;\n     ```\n   - Restart Nginx:\n     ```bash\n     systemctl restart nginx\n     ```\n3. **HAProxy:**\n   - Modify `/etc/haproxy/haproxy.cfg`:\n     ```\n     bind 999.999.999.999:25002 ssl crt no-sslv3 /opt/haproxy/security/x509/node.haproxy.pem\n     ```\n   - Restart HAProxy:\n     ```bash\n     systemctl restart haproxy\n     ```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How do I resolve Impala stale metadata issues?",
            "answer": "If an Impala query fails due to stale metadata, try refreshing the affected table:\n```sql\nREFRESH osix.sip;\n```\nFor partition-specific issues, refresh the specific partition:\n```sql\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='REGISTER');\n```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "Why is my decrypted disk not mounting after following the procedure?",
            "answer": "Ensure the fstab entry for the disk is uncommented:\n```bash\ncat /etc/fstab | grep '/data/1'\n```\nThen, try manually mounting:\n```bash\nmount -a\n```\nIf the issue persists, check system logs:\n```bash\ndmesg | tail -50\n```",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "Why was the HDFS failover controller role down, and how was it fixed?",
            "answer": "The HDFS failover controller role was down due to a timeout in its connection to ZooKeeper. The issue was resolved by enabling the failover controller\u2019s automatic restart in Cloudera Manager: `HDFS -> Failover Controller -> Automatically Restart Processes`.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What should I do if Cloudera Manager stops sending automatic alert emails?",
            "answer": "If Cloudera Manager stops sending alerts, follow these steps:\n1. SSH to `un5` (Alert Publisher Node) and check the following logs for errors:\n   ```bash\n   less /var/log/messages\n   less /var/log/mail.err\n   less /var/log/mail.info\n   less /var/log/cloudera-scm-alertpublisher\n   ```\n2. Count the number of alerts generated in logs:\n   ```bash\n   for i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 'Cloudera Alert' | wc -l; done\n   ```\n3. Restart the Alert Publisher via Cloudera Manager if needed.\n(Source: X20220901-IM1957832.md)",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "What steps should be taken when a batch job fails in Grafana for the DWH_IBank application?",
            "answer": "1. Identify the failed job in Grafana:\n   ```\n   Application: DWH_IBank\n   Job_Name: Extract\n   Component: MAN_DATE\n   ```\n2. Investigate logs to find duplicate key errors.\n3. If duplicate records exist, use the `-f` option to truncate and reinsert records:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n   ```\n4. After extraction, re-run the export script:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n   ```\n5. Validate the fix by checking the job status in Grafana.",
            "category": "Troubleshooting & Issue Resolution"
        },
        {
            "question": "How is authentication to Impala handled in the IPVPN-SM application?",
            "answer": "Authentication is done using Kerberos with configuration files including `/etc/krb5.conf`, `/shared/abc/ip_vpn/sm-app/deployment/config/jaas.conf`, and the keytab file.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I free up CPU and memory resources in a CDSW cluster that is stuck due to resource exhaustion?",
            "answer": "If CDSW jobs are stuck in 'Scheduling' due to insufficient CPU, memory, or GPU, delete all pending pods:\n```bash\nkubectl get pods | grep Pending | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\nkubectl get pods | grep 'Init:0/1' | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n```\nMonitor compute and memory resource usage to prevent future scheduling issues.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do you install a certificate authority using SaltStack?",
            "answer": "Move the given certificate to `/etc/salt/salt/tls/internal_certificate/root_certificate/`, rename it if necessary, and install it using SaltStack with:\n```bash\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os\n```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I check available HBase disk quotas?",
            "answer": "Run the following command in HBase shell:\n```bash\nlist_quotas\n```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I resolve a Cloudera CDSW startup failure due to persistent storage issues?",
            "answer": "If CDSW fails to start due to persistent storage issues, check the disk I/O usage:\n1. In Cloudera Manager, check if disk I/O is at 100%.\n2. Identify the affected storage volume:\n   ```bash\n   lsblk | grep cdsw-var\n   ```\n3. Check Kubernetes persistent volumes:\n   ```bash\n   kubectl get pv\n   ```\n4. If the issue persists, restart the CDSW services via Cloudera Manager.\n(Source: X20230130-IM2073052.md)",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "Which components form the infrastructure of the IPVPN-SM replacement process?",
            "answer": "The infrastructure consists of the IPVPN-SM application, Impala data sources, Kerberos authentication, HAProxy load balancer, and the external SQM server.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I update the OS on an edge node?",
            "answer": "To update the OS on an edge node:\n1. SSH into the edge node:\n   ```bash\n   ssh Exxxx@XXXedgeXX\n   sudo -i\n   ```\n2. Clean YUM cache and check for updates:\n   ```bash\n   yum clean all\n   yum check-update\n   ```\n3. Apply updates and reboot:\n   ```bash\n   yum update\n   systemctl reboot\n   ```\n4. Verify the OS version:\n   ```bash\n   cat /etc/oracle-release\n   ```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How can I safely restart HiveServer2 after an Out-Of-Memory error?",
            "answer": "If HiveServer2 crashes due to an Out-Of-Memory error, follow these steps:\n1. Check memory usage and pause duration from Cloudera Manager: \n```bash\ncluster -> Hive -> HiveServer2 -> Charts\n```\n2. Identify failing jobs that caused memory exhaustion:\n```bash\ngrep Java /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node04.mno-gr.log.out\n```\n3. Restart the HiveServer2 service:\n```bash\nsudo service hive-server2 restart\n```\n4. Verify that the service is running smoothly by monitoring resource usage and garbage collection warnings.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What steps should be followed to upgrade PostgreSQL?",
            "answer": "To upgrade PostgreSQL:\n1. Put the edge node on standby:\n   ```bash\n   ssh Exxxx@XXXedgeXX\n   sudo -i\n   ```\n2. Stop the running PostgreSQL service:\n   ```bash\n   sudo -iu postgres\n   systemctl stop postgresql-9.5.service\n   ```\n3. Backup data:\n   ```bash\n   pg_dumpall > edgeXX_postgres_backup\n   ```\n4. Download PostgreSQL 14 RPMs and create the repository:\n   ```bash\n   createrepo .\n   ```\n5. Update PostgreSQL:\n   ```bash\n   yum install --disablerepo=* --enablerepo=postgres14 postgresql14 postgresql14-server\n   ```\n6. Initialize and start the new PostgreSQL version:\n   ```bash\n   /usr/pgsql-14/bin/postgresql-14-setup initdb\n   systemctl enable --now postgresql-14\n   ```\n7. Restore the database backup:\n   ```bash\n   psql -f edgeXX_postgres_backup postgres\n   ```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I benchmark HBase performance?",
            "answer": "Use the built-in HBase performance testing tool:\n```bash\nhbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 10\n```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What steps should be taken when CDSW nodes are overloaded and causing downtime?",
            "answer": "When CDSW nodes are overloaded, review system resources on the nodes (`mncdsw1, wrkcdsw1-wrkcdsw6`). If resources are maxed out, optimize workloads or scale up infrastructure. Logs can be checked via Cloudera Manager.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What are the steps to perform a Wildfly failover?",
            "answer": "1. Ensure the secondary site is ready.\n2. Update the load balancer to redirect traffic.\n3. Restart Wildfly instances on the backup site:\n```bash\nsupervisorctl start wildfly-prodreston\n```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What should I do if an HBase table has been deleted but metadata remains?",
            "answer": "If an HBase table has been deleted but metadata remains, follow these steps:\n1. Identify extra regions in meta:\n```bash\nhbase hbck -j hbase-hbck2-1.2.0.jar extraRegionsInMeta PROD_BANK:TAX_FREE_20220404 --fix\n```\n2. Verify that the table does not appear in `hbase:meta`:\n```bash\nhbase shell\nscan 'hbase:meta',{FILTER=>\"PrefixFilter('PROD_BANK:TAX_FREE_20220404')\"}\n```\n3. Remove the table from HDFS:\n```bash\nhdfs dfs -rm -r hdfs://DRBDA-ns/hbase/data/PROD_BANK/TAX_FREE_20220404\nhdfs dfs -ls hdfs://DRBDA-ns/hbase/data/PROD_BANK/\n```\n4. Restart HBase and check health:\n```bash\nhbase hbck\n```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do you check the status of OpenLDAP replication?",
            "answer": "Run `ldapsearch -H ldaps://kerb1.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"` to verify if replication has taken effect.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do you set up a new Kerberos keytab for authentication?",
            "answer": "Create a keytab using `ktadd -k /tmp/<username>.keytab -norandkey <username>@CNE.abc.GR`, then move it to `/home/users/skokkoris/` and set ownership using `chown skokkoris. /home/users/skokkoris/<username>.keytab`.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I renew an expired RStudio Connect license?",
            "answer": "To renew an expired RStudio Connect license:\n1. SSH into the RStudio server:\n   ```bash\n   ssh unrstudio1\n   ```\n2. Ensure the system time is correct:\n   ```bash\n   sudo timedatectl\n   sudo hwclock -w\n   ```\n3. Deactivate the existing license:\n   ```bash\n   /opt/rstudio-connect/bin/license-manager deactivate\n   ```\n4. Activate the new license:\n   ```bash\n   /opt/rstudio-connect/bin/license-manager activate <product-key>\n   ```\n5. Restart RStudio Connect:\n   ```bash\n   systemctl restart rstudio-connect\n   ```\n6. Verify the activation:\n   ```bash\n   /opt/rstudio-connect/bin/license-manager verify\n   ```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I update the BIOS and Lifecycle Controller on a Dell PowerEdge C6320?",
            "answer": "1. Download the latest iDRAC and BIOS firmware in `.bin` format.\n2. Transfer them to `/tmp/` on the server.\n3. Grant execution permission:\n```bash\nchmod +x BIOS_CCTDP_LN64_2.13.0.BIN\nchmod +x iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\n```\n4. Execute updates:\n```bash\n./iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\n./BIOS_CCTDP_LN64_2.13.0.BIN\n```\n5. Reboot the system to complete the updates.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do I create a Dell support ticket for a hardware issue?",
            "answer": "1. Retrieve the service tag from iDRAC (`Overview \u2192 Server \u2192 Logs`).\n2. Export lifecycle logs (`Overview \u2192 Server \u2192 Troubleshooting \u2192 Support Assist \u2192 Export Collection`).\n3. Open a case on Dell Support with the service tag.\n4. Send the exported TSR zip file to Dell.\n5. Follow Dell\u2019s instructions for BIOS and Lifecycle Controller updates if needed.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How can I resolve an RStudio user authentication issue after resetting a password?",
            "answer": "If an RStudio user's applications fail to run after a password reset, check the LDAP authentication:\n1. SSH to `unrstudio1` and verify the `t1-svc-cnebind` account:\n   ```bash\n   ldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D 't1-svc-cnebind' -W -b 'dc=groupnet,dc=gr' '(sAMAccountName=t1-svc-cnebind)'\n   ```\n2. If the password is expired, update it in `/etc/rstudio-connect/rstudio-connect.gcfg`.\n(Source: X20220909-IM1962926.md)",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do you increase Java heap memory allocation for Streamsets?",
            "answer": "Modify Java options in Cloudera Manager:\n```bash\ncluster -> Streamsets -> Configuration -> Java options: `-Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow`\n```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How can Cloudera Manager memory leak issues be mitigated for idm2.bigdata.abc.gr?",
            "answer": "The memory leak issue in Cloudera Manager is associated with the Cloudera Manager Agent process (`cmf-agent`). To troubleshoot, log in to the affected node and check memory usage with `ps aux --sort -rss`. If `cmf-agent` is consuming excessive memory, verify if the node is stuck downloading parcels. If so, remove the host from the cluster using Cloudera Manager (`Hosts > All Hosts > Actions > Remove from Cluster`). Finally, restart the agent with `systemctl restart cloudera-scm-agent` to stabilize memory usage.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What are the prerequisites for changing the domain in RCPE?",
            "answer": "Ensure SSL certificates are imported, update `/etc/hosts`, perform an LDAP search for user validation, and confirm an active user exists in the new domain before proceeding.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How is the TeMIP application deployed?",
            "answer": "The TeMIP application is deployed on a Wildfly server located at `/opt/wf_cdef_temip/standalone/deployments`. The deployment is automated, and a `.war.deployed` file is created automatically when a new war file is placed.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "Which database and table does the Energy-Efficiency Pollaploi workflow load data into?",
            "answer": "The workflow loads data into the `energy_efficiency` database and the `pollaploi` table.",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "What firewall and iptables modifications are needed to redirect database connections from nnmdis01 to nnmprd01?",
            "answer": "To redirect connections to a different database server, update iptables rules:\n```bash\n-A PREROUTING -i bond0.300 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\n-A OUTPUT -d 10.255.240.13/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\nservice iptables reload\n```",
            "category": "Infrastructure & Deployment"
        },
        {
            "question": "How do you manually sync IDM replication?",
            "answer": "Run `ipa-replica-manage force-sync --from idm2.bigdata.abc.gr` to push changes immediately.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can you execute an Impala query to check the number of records in `bigcust.nnmcp_ipvpn_slametrics_hist`?",
            "answer": "Run the following command in Impala:\n```\nSELECT COUNT(*) FROM bigcust.nnmcp_ipvpn_slametrics_hist;\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you query the number of rows in the `nnmnps.nnmcp_qametrics_hist` table?",
            "answer": "Use the following Impala query:\n```sql\nSELECT COUNT(*) FROM nnmnps.nnmcp_qametrics_hist;\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can I check if a missing CSI_fix file was caused by missing data in the brond.dsl_stats_week_xdsl_hist table?",
            "answer": "To check for missing data in the brond.dsl_stats_week_xdsl_hist table, run the following query in Impala:\n```sql\nSELECT count(*), par_dt FROM brond.dsl_stats_week_xdsl_hist WHERE par_dt >= 'YYYYMMDD' GROUP BY 2 ORDER BY 2;\n```\nIf the count is zero for the missing date, investigate the coordinator job that populates this table (`coord_brond_load_dsl_daily_stats`) and ensure its source tables (`brond.brond_vdsl_stats_week` and `brond.brond_adsl_stats_week`) contain data.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do I check the number of entries in `sai.voice_quality_hist` over the last 7 days?",
            "answer": "Use the following SQL query in Impala to check the number of entries in `sai.voice_quality_hist` over the last 7 days:\n```sql\nselect par_dt, count(*) from sai.voice_quality_hist group by par_dt order by par_dt;\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you execute Cube Indicators processing via terminal?",
            "answer": "Login to `un1.bigdata.abc.gr`, remove the old script, fetch the new script from HDFS, modify `run_cube.sh` with the correct execution date, and execute `./run_cube.sh`.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can I enable access control lists (ACLs) for Yarn and Spark?",
            "answer": "To enable ACLs for YARN and Spark:\n1. Modify the **YARN ACL Configuration**:\n   - Navigate to YARN \u2192 Configuration \u2192 Search for `ACL For Viewing A Job`.\n   - Add extra groups for map-reduce job viewing:\n     ```\n     hue WBDADMIN,WBDOPDEV,WBDOPPRO,WBDOPQA\n     ```\n   - Enable `JobHistory Server Default Group`.\n2. Modify **Spark ACL Configuration**:\n   - Search for `Spark Client Advanced Configuration Snippet`.\n   - Enable Spark ACLs:\n     ```\n     spark.acls.enable=true\n     ```\n   - Configure admin groups:\n     ```\n     spark.admins.acls.groups=WBDADMIN\n     ```\n   - Grant access to Spark History Server:\n     ```\n     spark.history.ui.admin.acls.groups=WBDADMIN\n     ```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "What are the key steps for debugging and fixing an 'Upsert to HBase' failure in the IBank application?",
            "answer": "To debug and fix an 'Upsert to HBase' failure:\n1. Check the Impala query logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`.\n2. Identify the stalled query from `Cloudera > Impala > Queries`.\n3. Cancel the query via Cloudera UI or its monitoring URL.\n4. Restart the Impala daemon on affected nodes.\n5. Disable HBase quotas for `PROD_IBANK` before retrying the job.\n6. Ensure `set num_nodes = 1` is removed from the script `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`.\n7. Re-run the script after reverting to parallel execution.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can I check for missing partitions in the `huawei_tv.rel_play_tv_hist` table?",
            "answer": "To check for missing partitions in the `huawei_tv.rel_play_tv_hist` table:\n1. Open the Impala Editor in Hue.\n2. Run the following SQL query:\n   ```sql\n   select count(*), par_dt from huawei_tv.rel_play_tv_hist where par_dt between 'YYYYMMDD_START' and 'YYYYMMDD_END' group by par_dt order by par_dt;\n   ```\n3. Identify any missing partitions and reload the necessary data.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can you verify missing records in the IBank ingestion system?",
            "answer": "To verify missing records in IBank ingestion, use the following Impala command to check if data exists for a specific date:\n```bash\nimpala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select count(*) from prod_trlog_ibank.service_audit where par_dt='YYYYMMDD';\"\n```\nIf no records exist, re-run the ingestion script:\n```bash\n/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh YYYYMMDD\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you manually move Kudu alarms to Impala in TeMIP?",
            "answer": "Run the script manually using:\n```bash\nhdfs:/user/temip/temip_kudu_to_impala.sh\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do I monitor Wildfly access logs?",
            "answer": "To check access logs for `prodrestib`, run:\n```bash\ntail -f /var/log/wildfly/prodrestib/access.log\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you fetch and update the Streamsets configuration in Cloudera Manager?",
            "answer": "Go to `cluster -> Streamsets -> Configuration`, modify the necessary Java heap memory settings, and restart Streamsets to apply the new configuration.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do I check HBase replication status?",
            "answer": "Use the following command:\n```bash\nhbase shell\nstatus 'replication'\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "Which command is used to check Oracle table statistics before performing data exports in the `dwhfixed` workflow?",
            "answer": "Run the following command in Impala:\n```\nSHOW TABLE STATS dwhfixed.v_box_dim_hist;\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can you verify the number of records in the `pollaploi` table using Impala?",
            "answer": "Run the following command:\n```\nSELECT COUNT(*) FROM energy_efficiency.pollaploi;\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can I check if data has been inserted into the osix.sip table?",
            "answer": "Use Impala to check if data exists for a specific date:\n```sql\nSELECT count(*), par_dt FROM osix.sip WHERE par_dt > '20201124' GROUP BY par_dt;\n```\nIf no data is found, restart the topology:\n```bash\nssh unosix1\nsudo -iu osix\n./submit_sip_norm.sh\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can I enable ACLs for Spark and YARN?",
            "answer": "To enable ACLs for Spark and YARN:\n1. **YARN Configuration:**\n   - Go to YARN \u2192 Configuration \u2192 Search for `ACL For Viewing A Job`.\n   - Add the groups allowed to view map-reduce jobs (e.g., `hue WBDADMIN,WBDOPDEV,WBDOPPRO,WBDOPQA`).\n   - Enable `JobHistory Server Default Group`.\n2. **Spark Configuration:**\n   - Go to Spark \u2192 Configuration \u2192 Search for `Spark Client Advanced Configuration Snippet`.\n   - Add the following lines:\n     ```\n     spark.acls.enable=true\n     spark.admins.acls.groups=WBDADMIN\n     spark.history.ui.admin.acls.groups=WBDADMIN\n     spark.ui.view.acls.groups=WBDOPDEV,WBDOPPRO,WBDOPQA\n     ```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How can you remove duplicate records before running an ingestion job?",
            "answer": "1. Check for duplicate keys in the source table before ingestion.\n2. If duplicates exist, truncate the table using:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n   ```\n3. After the extract process, rerun the export process to re-populate the table:\n   ```bash\n   /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n   ```\n4. Validate that the duplicate records are removed before processing new data.",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "What is the command to check failed executions in the Monitoring database for IPVPN-SM?",
            "answer": "Run the following MySQL query:\n```\nSELECT * FROM jobstatus WHERE application='IPVPN-SM' AND status='FAILED' AND system_ts >= NOW() - INTERVAL 1 DAY;\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you list all Kafka topics available in Viavi Kafka?",
            "answer": "Use the command:\n```bash\nkafka-topics.sh --list --bootstrap-server <broker-ip>:9093\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you manually trigger the Piraeus Cisco VDSL2 application?",
            "answer": "You can manually trigger the application by running:\n```\nkubectl exec -it <pod_name> -- /app/run_vdsl2.sh\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you verify OpenLDAP replication status between kerb1 and kerb2?",
            "answer": "Use the following command to check if the `testuser` entry exists on kerb2:\n```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\" 'uid=testuser'\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do I manually re-export RA_Dsession and RA_Dtraffic files in abc BigStreamer?",
            "answer": "If data has already been exported but is missing, manually trigger an export for a specific date:\n```bash\ncd /shared/abc/location_mobility/run\n./export_ra_bs_01.sh -t 20201115\n./export_ra_bs_02.sh -t 20201115\n```",
            "category": "Data Management & Query Execution"
        },
        {
            "question": "How do you check the last exported Traffica record?",
            "answer": "Run the following query in Impala:\n```sql\nSELECT MAX(event_time) FROM sai.sms_raw;\n```",
            "category": "Data Management & Query Execution"
        }
    ]
}