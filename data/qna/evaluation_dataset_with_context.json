{
    "qa_pairs": [
        {
            "question": "How does the Brond Retrains pipeline handle raw file ingestion and Hive loading?",
            "answer": "The Brond Retrains pipeline retrieves `.csv.gz` files via SFTP, renames them with `.LOADED`, parses them using scripts in `/shared/abc/brond/DataParser/scripts/`, uploads them to HDFS at `/ez/warehouse/brond.db/landing_zone/brond_retrains`, and loads them into the `brond.brond_retrains_hist` Hive table via Oozie workflow `Brond_Load_Retrains_WF_NEW`.",
            "category": "Application Functionality & Flow",
            "files": "Brond_Retrains_Flow.md",
            "context": "---\ntitle: Brond Retrains Flow - End-to-End File Ingestion and Hive Loading via Oozie\ndescription: Detailed documentation of the Brond Retrains pipeline, including SFTP retrieval, parsing, HDFS loading, Hive integration, monitoring via MySQL, and Oozie-based orchestration across BigStreamer infrastructure.\ntags:\n  - mno\n  - bigstreamer\n  - brond\n  - retrains\n  - oozie\n  - sftp\n  - hive\n  - impala\n  - beeline\n  - data pipeline\n  - monitoring\n  - ftp ingestion\n  - kerberos\n  - hdfs\n  - compute stats\n  - metadata ingestion\n  - jobstatus\n  - partition management\n  - manual rerun\n  - alert resolution\nlast_updated: 2025-05-01\nauthor: ilpap\ncontext:\n  job_name: BROND_RETRAINS\n  component: MAIN\n  target_table: brond.brond_retrains_hist\n  host: un-vip.bigdata.abc.gr\n  coordinator: Brond_Load_Retrains_Coord_NEW\n  workflow: Brond_Load_Retrains_WF_NEW\n  owner: brond\n  system: BigStreamer\n  root_cause_keywords:\n    - no raw files found\n    - missing files\n    - hive partition missing\n    - failed workflow execution\n    - kerberos expiration\n  monitoring_db_host: 999.999.999.999\n  ssh_script_host: un-vip.bigdata.abc.gr\n  oozie_main_script_path: /user/brond/000.Brond_Retrains_Oozie_Main.sh\n  hive_db: brond\n  hive_table: brond_retrains_hist\n  log_file_pattern: 002.Brond_Retrains_Load.<YYYYMMDD>.log\n  manual_triggerable: true\n  default_schedule: [04:10, 05:10, 06:10, 10:10 UTC]\n---\n# Brond Retrains Flow\n## Installation info\nThis section outlines the setup details for the Brond Retrains pipeline, including input files, directories, scripts, logging, and Oozie scheduling.\n### Data Source File\nDetails on the raw input files retrieved via SFTP, including naming patterns, SFTP credentials, and local/HDFS paths.\n- Source system: FTP Server  \n  - host :`999.999.999.999`\n  - port :`22`\n  - protocol :`SFTP`\n  - user : `bigd`\n  - spool area : `/ADSL_Brond`\n  - file_type : `Counter_Collection_24H.*.csv.gz`\n  - load_suffix : `LOADED`\n- Local FileSystem Directories\n\t- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n\t- landing_zone : `/data/1/brond_retr_LZ`\n\t- archive_dir : `/data/1/brond_retr_LZ/archives`\n\t- work_dir : `/shared/brond_retr_repo`\n- HDFS Directories\n\t- hdfs_Bin : `/user/brond`\n\t- hdfs_dir : `/ez/warehouse/brond.db/landing_zone/brond_retrains`\n\t- hdfs_pending_dir : `/ez/warehouse/brond.db/landing_zone/brond_retrains/not_loaded`\n\t- hdfs_stats_dir : `/ez/warehouse/brond.db/landing_zone/brond_retrains/stats`\n### Scripts-Configuration Location\nPaths to parsing scripts and configuration `.trn` files used for file handling and flow control.\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- scripts path : `/shared/abc/brond/DataParser/scripts`\n-\tconfigurations path : `/shared/abc/brond/DataParser/scripts/transferlist/*.trn` (i.e. brond_retrains.trn)\n### Logs Location\nLocation and naming convention for logs generated by each retrains load run.\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- path : `/shared/abc/brond/DataParser/scripts/log`\n- log file: `002.Brond_Retrains_Load.<YYYYMMDD>.log`\n### Oozie Scheduling\nInformation on the Oozie coordinator, workflow, execution schedule, and trigger script.\n- user : `brond`\n- Coordinator :`Brond_Load_Retrains_Coord_NEW`  \n\truns at : `04:10, 05:10, 06:10, 10:10 UTC`\n- Workflow : `Brond_Load_Retrains_WF_NEW`  \n- Main script : `HDFS:/user/brond/000.Brond_Retrains_Oozie_Main.sh`\n- SSH Identity file : `HDFS:/user/brond/id_rsa`\nNdef: **Main Script** runs `oozie_brond_retrains.sh` located on `un-vip.bigdata.abc.gr` using **ssh** as user **brond**  \n`$ ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa brond@un-vip.bigdata.abc.gr \"/shared/abc/brond/DataParser/scripts/oozie_brond_retrains.sh\"`\n### Hive Tables\nThe target Hive table used for storing parsed retrains data.\n- Target Database: `brond`\n- Target Tables: `brond.brond_retrains_hist`\n### Beeline-Impala Shell commands\nCommands for querying and managing data using Hive (Beeline) and Impala.\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/default;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala-shell: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k`\n## Data process: How raw FTP files get into Hive\nStep-by-step process of how retrains data files move from the FTP server to the final Hive table, including renaming, parsing, HDFS upload, and Hive loading.\n1. sftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```bash\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999\n\nsftp> ls -l ADSL_Brond\n-rw-r--r-- 0 507 500 21902115 Nov 28 07:02 ADSL_Brond/Counter_Collection_24H.328_2022_11_28.csv.gz.LOADED\n-rw-r--r-- 0 507 500 21925249 Nov 29 06:22 ADSL_Brond/Counter_Collection_24H.329_2022_11_29.csv.gz.LOADED\n-rw-r--r-- 0 507 500 22107252 Nov 30 06:52 ADSL_Brond/Counter_Collection_24H.330_2022_11_30.csv.gz\n```\n2. rename the raw file(s) in remdef SFTP server by adding the suffix .LOADED\n`echo \"rename /ADSL_Brond/Counter_Collection_24H.330_2022_11_30.csv.gz /ADSL_Brond/Counter_Collection_24H.330_2022_11_30.csv.gz.LOADED\" | sftp -oport=22 bigd@999.999.999.999`\n3. unzip raw files using `gzip -d` command in `/data/1/brond_retr_LZ`\n4. parsing raw files in `/data/1/brond_retr_LZ`\n- removes the headers (1st line)\n- removes double-qudefs chars\n- defines the PAR_DT value from the filename (i.e. Counter_Collection_24H.330_2022_11_30.csv.gz convert to 20221130)\n- add the prefix `RETR___` to raw file\n- add the suffix `<load time>.parsed` to raw file  \nLoad time format:`<YYYYMMDD_HHMISS>`  \ni.e. `RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed`\n5. put raw files into HDFS landingzone\n`hdfs dfs -put /data/1/brond_retr_LZ/RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed /ez/warehouse/brond.db/landing_zone/brond_retrains/RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed`\n6. clean-up any copy of the raw files from local filesystem  \n`/data/1/brond_retr_LZ`  \n`/shared/brond_retr_repo`  \n7. load HDFS files into hive table `brond.brond_retrains_hist`\n`beeline -e \"LOAD DATA INPATH '/ez/warehouse/brond.db/landing_zone/brond_retrains/RETR___Counter_Collection_24H.330_2022_11_30.csv.20221201_061005.parsed' OVERWRITE INTO TABLE brond.brond_retrains_hist PARTITION (par_dt='20221130')\"`\n8. execute compute stats using impala-shell  \n```bash\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\t\ncompute incremental stats brond.brond_retrains_hist;\n```\nIn the event where multiple files are transfered (files refer to the same data), we proceed with overwriting data in brond table. Deletion of those multiple files is abc's responsibility.\n## Monitoring\nHow the pipeline’s execution is tracked using monitoring logs and component-level messages.\n### Monitoring connection details\nDatabase connection info for querying monitoring logs.\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \nConnection command: `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n### Monitoring Message list\nExample logs recorded during successful pipeline executions.\nFor each load the following set of messages will be recorded in the Monitoring database.\n```\nid    | execution_id | application | job            | component                        | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n------+--------------+-------------+----------------+----------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n15807 | 1659939004   | BROND       | BROND_RETRAINS | MAIN                             | 20220808            | SUCCESS | 2022-08-08 09:10:04 | 2022-08-08 09:10:48 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n15809 | 1659939004   | BROND       | BROND_RETRAINS | GET_RAW_RETRAIN_FILES            | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n15811 | 1659939004   | BROND       | BROND_RETRAINS | RENAME_FILES_@SFTP_SERVER        | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     |                        | brond | un2.bigdata.abc.gr\n15813 | 1659939004   | BROND       | BROND_RETRAINS | UNZIP_FILES                      | 20220808            | SUCCESS | 2022-08-08 09:10:07 |                     |                        | brond | un2.bigdata.abc.gr\n15815 | 1659939004   | BROND       | BROND_RETRAINS | PARSING_FILES                    | 20220808            | SUCCESS | 2022-08-08 09:10:13 |                     |                        | brond | un2.bigdata.abc.gr\n15817 | 1659939004   | BROND       | BROND_RETRAINS | LOAD_HDFS_LANDINGZONE            | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n15819 | 1659939004   | BROND       | BROND_RETRAINS | CLEAN-UP_THE_INPUT_FILES         | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n15821 | 1659939004   | BROND       | BROND_RETRAINS | LOAD_HDFS_FILES_INTO_HIVE_TABLES | 20220808            | SUCCESS | 2022-08-08 09:10:37 |                     |                        | brond | un2.bigdata.abc.gr\n15823 | 1659939004   | BROND       | BROND_RETRAINS | POST_SCRIPT                      | 20220808            | SUCCESS | 2022-08-08 09:10:48 |                     |                        | brond | un2.bigdata.abc.gr\n```\n### Monitoring Component list\nDescriptions of each component and what task it performs during execution.\n|Component | Description \n|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED| \n|GET_RAW_RETRAIN_FILES|sftp-get the raw files from the remdef server.<br />i.e.<br />Counter_Collection_24H.218_2022_08_08.csv.gz\n|RENAME_FILES_@SFTP_SERVER| Rename the raw files in remdef SFTP server by adding the suffix .LOADED<br />i.e.<br />Counter_Collection_24H.218_2022_08_08.csv.gz.LOADED\n|UNZIP_FILES| unzip the raw files using `gzip -d` command\n|PARSING_FILES| removes any control chars (if any) from the raw files\n|LOAD_HDFS_LANDINGZONE|PUT the parsing files into HDFS landingzone `/ez/warehouse/brond.db/landing_zone/brond_retrains`\n|CLEAN-UP_THE_INPUT_FILES|Clean-up any copy of the raw files from the filesystem (`/data/1/brond_retr_LZ`, `/shared/brond_retr_repo`)\n|LOAD_HDFS_FILES_INTO_HIVE_TABLE| Load raw data (files) into the tables<br />`brond.brond_retrains_hist`\n|POST_SCRIPT| Execute Compute Statistics using impala-shell.<br />`compute incremental stats brond.brond_retrains_hist;`\n### Monitoring database Queries\nSQL queries to retrieve logs from the monitoring database for recent or failed loads.\n- List messages of the last load  \n`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n```sql\nselect \nexecution_id, id, application, job, component, operative_partition,  \nstatus, system_ts, system_ts_end, message, user,host   \nfrom jobstatus a where upper(job) like 'BROND_RETRAINS%'   \nand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'BROND_RETRAINS%');\n\texecution_id | id    | application | job            | component                        | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n\t-------------+-------+-------------+----------------+----------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n\t1659939004   | 15807 | BROND       | BROND_RETRAINS | MAIN                             | 20220808            | SUCCESS | 2022-08-08 09:10:04 | 2022-08-08 09:10:48 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n\t1659939004   | 15809 | BROND       | BROND_RETRAINS | GET RAW RETRAIN FILES            | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n\t1659939004   | 15811 | BROND       | BROND_RETRAINS | RENAME FILES @SFTP SERVER        | 20220808            | SUCCESS | 2022-08-08 09:10:05 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15813 | BROND       | BROND_RETRAINS | UNZIP FILES                      | 20220808            | SUCCESS | 2022-08-08 09:10:07 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15815 | BROND       | BROND_RETRAINS | PARSING FILES                    | 20220808            | SUCCESS | 2022-08-08 09:10:13 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15817 | BROND       | BROND_RETRAINS | LOAD HDFS LANDINGZONE            | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15819 | BROND       | BROND_RETRAINS | CLEAN-UP THE INPUT FILES         | 20220808            | SUCCESS | 2022-08-08 09:10:26 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15821 | BROND       | BROND_RETRAINS | LOAD HDFS FILES INTO HIVE TABLES | 20220808            | SUCCESS | 2022-08-08 09:10:37 |                     |                        | brond | un2.bigdata.abc.gr\n\t1659939004   | 15823 | BROND       | BROND_RETRAINS | POST SCRIPT                      | 20220808            | SUCCESS | 2022-08-08 09:10:48 |                     |                        | brond | un2.bigdata.abc.gr\n\t```\n### Monitoring Health-Check\nHow to verify the health of the monitoring application and steps to restart if needed.\n- Check Monitoring status.  \n```\tbash\n$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\n{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n```  \n- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\nCommon issues, log search patterns, and diagnostic steps when the retrains flow fails.\n- An email will be sent by the system with the point of failure.\n- Check the log file for errors/exceptions  \n`egrep -i 'error|fail|exception|problem' /shared/abc/brond/DataParser/scripts/log/002.Brond_Retrains_Load.<YYYYMMDD>.log`\n- List Failed Monitoring messages of the last load  \n```bash\n/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\nselect * from jobstatus where upper(job) like 'BROND_RETRAINS%' \nand status='FAILED'\nand operative_partition=(select max(operative_partition) from jobstatus where upper(job) like 'BROND_RETRAINS%' and operative_partition regexp '[0-9]{8}')\n\torder by id;\n\tid    | execution_id | application | job            | component             | operative_partition | status | system_ts           | system_ts_end       | message            | user  | host                  \n\t------+--------------+-------------+----------------+-----------------------+---------------------+--------+---------------------+---------------------+--------------------+-------+-----------------------\n\t14621 |              | BROND       | BROND_RETRAINS | MAIN                  | 20220801            | FAILED | 2022-08-01 16:13:13 | 2022-08-01 16:13:14 | No raw files found | brond | un2.bigdata.abc.gr\n\t14623 |              | BROND       | BROND_RETRAINS | GET RAW RETRAIN FILES | 20220801            | FAILED | 2022-08-01 16:13:14 |                     | No raw files found | brond | un2.bigdata.abc.gr\n\t```\n### Common errors  \n- `No raw files found`, there are no raw files available for loading at remdef server.  \nA WARNING message will be sent to abc by email.\nNo actions required from OBSS. Responsible abc. \n- Other factors not related to the specific flow\n- impala/hive availability\n- Kerberos authentication (A.  \n> Ndef: The flow checks if the ticket is still active before any HDFS action.  \nIn case of expiration the flow performs a `kinit` command*\n## Manually triggering the workflow\nInstructions for rerunning the workflow when files are uploaded after the scheduled execution time.\nThere are cases where abc might upload new files after the scheduled workflow timing, and request these files to be\nprocessed in the same day. This can only be done by manually triggering the workflow. Before doing so, you need to make some checks first:\n### Check workflow logs\n1. Login to https://999.999.999.999:8888/hue/accounts/login?next=/hue using the brond account\n2. Go to \"Jobs\" > \"Workflows\"\nIf all workflow executions (\"Brond_Load_Retrains_WF_NEW\") were successful, you can proceed by checking that the file(s)\nabc added, were copied after the scheduled timings of the workflow\n### Check added files\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond_DWH\n-rw-r--r--    0 507      500      150589497 Nov 29 05:34 ADSL_Brond_DWH/DWH_VDSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      150823890 Nov 29 13:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_29.csv.gz\n```\nThe second file was indeed added after the scheduled time and has not been picked up by the workflow.\n### Trigger workflow\nYou can now proceed to manually trigger the workflow:\n1. Go to HUE and select \"Jobs\"\n2. Go to \"Workflow\" and select \"Brond_Load_Retrains_WF_NEW\"\n3. In the next screen, select \"Rerun\"\n4. Wait for the workflow to successfully end\n5. If no errors occur, proceed with:\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond_DWH\n-rw-r--r--    0 507      500      150589497 Nov 29 05:34 ADSL_Brond_DWH/DWH_VDSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      150823890 Nov 29 13:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_29.csv.gz\n```\n## Data Check\nSteps to validate that retrains data has been properly loaded and partitioned in the Hive table.\n- **Check final tables for new partitions**:\n- Impala-shell: \n```bash\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\nrefresh brond.brond_retrains_hist;  \nshow partitions brond.brond_retrains_hist;  \n\t\n\tpar_dt   | #Rows   | #Files | Size     | Bytes Cached | Cache Replication | Format | Incremental stats | Location                                                                     \n\t---------+---------+--------+----------+--------------+-------------------+--------+-------------------+------------------------------------------------------------------------------\n\t20221130 | 2784494 |      1 | 146.16MB | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/brond.db/brond_retrains_hist/par_dt=20221130\n\tTotal    | 5569421 |      1 | 146.16MB | 0B           |                   |        |                   |                                                                              \n```\n- **Check the amount of data in final tables**:\n- Impala-shell: \n```bash\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\nselect par_dt, count(*) as cnt from brond.brond_retrains_hist group by par_dt order by 1;\n\tpar_dt   | cnt    \n\t---------+--------\n\t20221130 | 2784494\n```"
        },
        {
            "question": "What are the differences in data flow between the Brond ADSL and VDSL stats ingestion pipelines?",
            "answer": "Both pipelines ingest `.csv.gz` files via SFTP from `/ADSL_Brond_DWH`, parse and stage them, and load to respective Hive tables. The ADSL stats flow targets `brond.brond_adsl_stats_daily`, while the VDSL stats flow targets `brond.brond_vdsl_stats_daily`. Each uses staging tables and logs in `/shared/abc/brond_dsl_stats/DataParser/scripts/log`.",
            "category": "Application Functionality & Flow",
            "files": "Brond_xDSL_Stats_Flow.md",
            "context": "---\ntitle: Brond ADSL/VDSL Flow - Daily xDSL Statistics Ingestion Pipeline\ndescription: Ingestion flow for ADSL and VDSL metrics via SFTP to Hive using Oozie scheduling. Includes FTP retrieval, parsing, staging, HDFS upload, Hive table overwrite, monitoring, and error resolution for brond_adsl_stats_daily and brond_vdsl_stats_daily tables.\njob_name: BROND_ADSL_STATS / BROND_VDSL_STATS\ncomponent: MAIN\nsystem: BigStreamer\nhost: un-vip.bigdata.abc.gr\nowner: brond\ncoordinator: Brond_Load_xDSL_Coord_NEW\nworkflow: Brond_Load_xDSL_WF_NEW\ntarget_tables:\n  - brond.brond_adsl_stats_daily\n  - brond.brond_vdsl_stats_daily\nstaging_tables:\n  - brond.brond_adsl_stats_daily_stg\n  - brond.brond_vdsl_stats_daily_stg\nhdfs_landingzone: /ez/warehouse/brond.db/landing_zone/brond_dsl_stats\nschedule: [04:00, 05:00, 06:00, 10:00 UTC]\nload_type: daily\nretry_policy: manual rerun supported via HUE\nlast_updated: 2025-05-01\nkeywords:\n  - adsl\n  - vdsl\n  - bigstreamer\n  - brond\n  - stats\n  - SFTP\n  - HDFS\n  - Hive\n  - Oozie\n  - impala\n  - beeline\n  - daily partitioning\n  - monitoring\n  - jobstatus\n  - load overwrite\n  - parsing\n  - staging\n  - automation\n  - file suffix loaded\n  - xDSL metrics\n  - manual triggering\n  - file landing\n  - kerberos\n  - workflow rerun\n  - log tracing\n  - partition validation\n  - brond_dsl_stats\n---\n# Brond ADSL/VDSL Flow\n## Installation info\nThis section provides configuration details about the source systems, local and HDFS directories, scripts, logging paths, and job scheduling for the ADSL/VDSL load pipeline.\n### Data Source File\nInformation about the raw data files retrieved via SFTP, including naming patterns and filesystem structure.\n- Source system: FTP Server  \n  - host :`999.999.999.999`\n  - port :`22`\n  - protocol :`SFTP`\n  - user : `bigd`\n  - spool area : `/ADSL_Brond_DWH`\n  - file_type : `DWH_ADSL*.csv.gz` and `DWH_VDSL*.csv.gz`\n  - load_suffix : `LOADED`\n- Local FileSystem Directories\n\t- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n\t- landing_zone : `/data/1/brond_dsl_stats_LZ`\n\t- archive_dir= : `/data/1/brond_dsl_stats_LZ/archives`\n\t- work_dir= : `/shared/abc/brond_dsl_stats/repo`\n- HDFS Directories\n\t- hdfs_Bin : `/user/brond`\n\t- hdfs_dir : `/ez/warehouse/brond.db/landing_zone/brond_dsl_stats`\n\t- hdfs_pending_dir : `/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/not_loaded`\n\t- hdfs_stats_dir : `/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/stats`\n### Scripts-Configuration Location\nLocations of the scripts and .trn configuration files used to manage the flow.\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- scripts path : `/shared/abc/brond_dsl_stats/DataParser/scripts`\n-\tconfigurations path : `/shared/abc/brond_dsl_stats/DataParser/scripts/transferlist/*.trn` (i.e. brond_retrains.trn)\n### Logs Location\nLog file path and format used to trace each data loading run.\n- node : `un-vip.bigdata.abc.gr (999.999.999.999)`\n- user : `brond`\n- path : `/shared/abc/brond_dsl_stats/DataParser/scripts/log`\n- log file: `002.Brond_xDSL_Load.<YYYYMMDD>.log`\n### Oozie Scheduling\nDetails about the Oozie coordinator, workflow, and execution commands.\n- user : `brond`\n- Coordinator :`Brond_Load_xDSL_Coord_NEW`  \n\truns at : `04:00, 05:00, 06:00, 10:00 UTC`\n- Workflow : `Brond_Load_xDSL_WF_NEW`  \n- Main script : `HDFS:/user/brond/000.Brond_xDSL_Oozie_Main.sh`\n- SSH Identity file : `HDFS:/user/brond/id_rsa`\nNdef: **Main Script** runs `oozie_brond_xdsl.sh` located on `un-vip.bigdata.abc.gr` using **ssh** as user **brond**  \n`$ ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa brond@un-vip.bigdata.abc.gr \"/shared/abc/brond_dsl_stats/DataParser/scripts/oozie_brond_xdsl.sh\"`\n### Hive Tables\nStaging and final target tables used for storing the cleaned ADSL/VDSL metrics.\n- Target Database: `brond`\n- Staging Tables: `brond.brond_adsl_stats_daily_stg, brond.brond_vdsl_stats_daily_stg`\n- Target Tables: `brond.brond_adsl_stats_daily, brond.brond_vdsl_stats_daily`\n### Beeline-Impala Shell commands\nUseful commands to interact with Hive and Impala for loading and querying data.\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/default;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala-shell: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k`\n## Data process\nStep-by-step breakdown of how raw files are transferred, parsed, staged, and loaded into Hive tables.\n1. sftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_dsl_stats_LZ`\n```bash\necho \"ls -l ADSL_Brond_DWH\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond_DWH\n-rw-r--r--    0 507      500      35399779 Nov 27 06:19 ADSL_Brond_DWH/DWH_ADSL.327_2022_11_27.csv.gz.LOADED\n-rw-r--r--    0 507      500      35440542 Nov 28 06:57 ADSL_Brond_DWH/DWH_ADSL.328_2022_11_28.csv.gz.LOADED\n-rw-r--r--    0 507      500      35360378 Nov 29 06:20 ADSL_Brond_DWH/DWH_ADSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      35415258 Nov 30 06:48 ADSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz\n-rw-r--r--    0 507      500      150757798 Nov 27 05:33 ADSL_Brond_DWH/DWH_VDSL.327_2022_11_27.csv.gz.LOADED\n-rw-r--r--    0 507      500      150728306 Nov 28 06:26 ADSL_Brond_DWH/DWH_VDSL.328_2022_11_28.csv.gz.LOADED\n-rw-r--r--    0 507      500      150589497 Nov 29 05:34 ADSL_Brond_DWH/DWH_VDSL.329_2022_11_29.csv.gz.LOADED\n-rw-r--r--    0 507      500      150823890 Nov 30 06:21 ADSL_Brond_DWH/DWH_VDSL.330_2022_11_30.csv.gz\n```\n2. rename the raw file(s) in remdef SFTP server by adding the suffix .LOADED\n`echo \"rename /ADSL_Brond_DWH/ADSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz /ADSL_Brond_DWH/ADSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz.LOADED\" | sftp -oport=22 bigd@999.999.999.999`\n`echo \"rename /ADSL_Brond_DWH/VDSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz /ADSL_Brond_DWH/VDSL_Brond_DWH/DWH_ADSL.330_2022_11_30.csv.gz.LOADED\" | sftp -oport=22 bigd@999.999.999.999`\n3. parsing raw files in `/data/1/brond_dsl_stats_LZ`\n- removes the headers (1st line)\n- removes double-qudefs chars\n- defines the PAR_DT value from the filename (i.e. DWH_ADSL.330_2022_11_30.csv.gz convert to 20221130)\n- add the prefix `HDFS___` to raw file\n- add the suffix `<load time>` to raw file  \nLoad time format:`<YYYYMMDD_HHMISS>`  \ni.e. `HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz`\n4. put raw files into HDFS landingzone\n```bash\nhdfs dfs -put /data/1/brond_dsl_stats_LZ/HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz /ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz`\nhdfs dfs -put /data/1/brond_dsl_stats_LZ/HDFS___DWH_VDSL.330_2022_11_30.csv.gz.20221201_060005.gz /ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_VDSL.330_2022_11_30.csv.gz.20221201_060005.gz`\n```\n5. clean-up any copy of the raw files from local filesystem  \n`/data/1/brond_dsl_stats_LZ`  \n`/shared/abc/brond_dsl_stats/repo`\n6. load HDFS files into hive staging tables  \n`brond.brond_adsl_stats_daily_stg` and `brond.brond_vdsl_stats_daily_stg`  \n```bash\nbeeline -e \"LOAD DATA INPATH '/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_ADSL.330_2022_11_30.csv.gz.20221201_060005.gz' OVERWRITE INTO TABLE brond.brond_adsl_stats_daily_stg PARTITION (par_dt='20221130')\"\nbeeline -e \"LOAD DATA INPATH '/ez/warehouse/brond.db/landing_zone/brond_dsl_stats/HDFS___DWH_VDSL.330_2022_11_30.csv.gz.20221201_060005.gz' OVERWRITE INTO TABLE brond.brond_vdsl_stats_daily_stg PARTITION (par_dt='20221130')\"\n```\n> Ndef: Once the load completed, the staging tables should contain no data.*\t\n7. update hive tables with filtered columns  \nscript: `/shared/abc/brond_dsl_stats/DataParser/scripts/003.Brond_xDSL_Post.sh`\n- `brond_adsl_stats_daily`\n```sql\n\t\tset hive.exec.dynamic.partition.mode=nonstrict;\n\t\tinsert overwrite table brond.brond_adsl_stats_daily partition (par_dt) \n\t\tselect \n\t\t\tserv_tel,\n\t\t\tserv_siid,\n\t\t\tne_name,\n\t\t\tne_port,\n\t\t\tcard_tehn,\n\t\t\tcard_type,\n\t\t\tinv_port,\n\t\t\tmeasure_date,\n\t\t\tlast_change,\n\t\t\taif_adm,\n\t\t\taif_oper,\n\t\t\topmod_annex,\n\t\t\tup_sign_attn,\n\t\t\tup_snr,\n\t\t\tup_crt_rate,\n\t\t\tup_max_rate,\n\t\t\tdn_sign_attn,\n\t\t\tdn_snr,\n\t\t\tdn_crt_rate,\n\t\t\tdn_max_rate,\n\t\t\tprf_name,\n\t\t\tradius,\n\t\t\tsproto,\n\t\t\tpar_dt\n\t\tfrom brond.brond_adsl_stats_daily_stg \n\t\twhere 1=1\n\t\t;\n\t\t```\n- `brond_vdsl_stats_daily`\n```sql\n\t\tset hive.exec.dynamic.partition.mode=nonstrict;\n\t\tinsert overwrite table brond.brond_vdsl_stats_daily partition (par_dt) \n\t\tselect \n\t\t\tserv_tel,\n\t\t\tserv_siid,\n\t\t\tne_name,\n\t\t\tne_port,\n\t\t\tcard_tehn,\n\t\t\tcard_type,\n\t\t\tinv_port,\n\t\t\tmeasure_date,\n\t\t\tcustid,\n\t\t\tlast_change,\n\t\t\taif_adm,\n\t\t\taif_oper,\n\t\t\topmod_annex,\n\t\t\tup_sign_attn,\n\t\t\tup_snr,\n\t\t\tup_max_rate,\n\t\t\tup_crt_rate,\n\t\t\tdn_sign_attn,\n\t\t\tdn_snr,\n\t\t\tdn_max_rate,\n\t\t\tdn_crt_rate,\n\t\t\tprf_name,\n\t\t\tradius,\n\t\t\tsproto,\n\t\t\tpar_dt\n\t\tfrom brond.brond_vdsl_stats_daily_stg \n\t\twhere 1=1\n\t\t;\n```\nIn the event where multiple files are transfered (files refer to the same data), we proceed with overwriting data in brond table. Deletion of those multiple files is abc's responsibility.\n## Monitoring\nInstructions for checking job execution status, component-level logs, and tracking metadata.\n### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \nConnection command: `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n### Monitoring Message list\nFor each type of load (ADSL or VDSL) the following set of messages will be recorded in the Monitoring database.\n```\nid    | execution_id | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n------+--------------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n15675 | 1659931204   | BROND       | BROND_ADSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:00:04 | 2022-08-08 07:01:38 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n15677 | 1659931204   | BROND       | BROND_ADSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:00:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n15679 | 1659931204   | BROND       | BROND_ADSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n15681 | 1659931204   | BROND       | BROND_ADSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n15683 | 1659931204   | BROND       | BROND_ADSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr\n15685 | 1659931204   | BROND       | BROND_ADSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr\n15687 | 1659931204   | BROND       | BROND_ADSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:00:26 |                     |                        | brond | un2.bigdata.abc.gr\n15689 | 1659931204   | BROND       | BROND_ADSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:01:38 |                     |                        | brond | un2.bigdata.abc.gr\n\nid    | execution_id | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n------+--------------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n15691 | 1659931204   | BROND       | BROND_VDSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:01:38 | 2022-08-08 07:03:51 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n15693 | 1659931204   | BROND       | BROND_VDSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:01:40 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n15695 | 1659931204   | BROND       | BROND_VDSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n15697 | 1659931204   | BROND       | BROND_VDSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n15699 | 1659931204   | BROND       | BROND_VDSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr\n15701 | 1659931204   | BROND       | BROND_VDSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr\n15703 | 1659931204   | BROND       | BROND_VDSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:02:05 |                     |                        | brond | un2.bigdata.abc.gr\n15705 | 1659931204   | BROND       | BROND_VDSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:03:51 |                     |                        | brond | un2.bigdata.abc.gr\n```\n### Monitoring Component list\nDescriptions of each logical step in the ADSL/VDSL processing pipeline as represented in monitoring.\n|Component | Description \n|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED| \n|GET RAW XDSL FILES|sftp-get the raw files from the remdef server.<br />i.e.<br />DWH_ADSL.197_2022_07_18.csv.gz<br />DWH_VDSL.197_2022_07_18.csv.gz\n|RENAME FILES @SFTP SERVER| Rename the raw files in remdef server by adding the suffix .LOADED<br />i.e.<br />DWH_ADSL.197_2022_07_18.csv.gz.LOADED<br />DWH_VDSL.197_2022_07_18.csv.gz.LOADED\n|PARSING FILES| removes any control chars (if any) from the raw files\n|LOAD HDFS LANDINGZONE|PUT the parsing files into HDFS\n|CLEAN-UP THE INPUT FILES|Clean-up any copy of the raw files from the filesystem\n|LOAD HDFS FILES INTO HIVE STAGING TABLES| Load raw data (files) into the staging tables<br />`brond.brond_adsl_stats_daily_stg`<br />`brond.brond_vdsl_stats_daily_stg`\n|UPDATE HIVE TABLES WITH FILTERED COLUMNS| Update final tables with the necessary columns only.<br />`brond.brond_adsl_stats_daily`<br />`brond.brond_vdsl_stats_daily`\n### Monitoring database Queries\nSample SQL queries to check logs and monitoring status for the latest executions.\n- List messages of the last load  \n`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n```sql\nselect \n  execution_id, id, application, job, component, operative_partition,  \n  status, system_ts, system_ts_end, message, user,host   \nfrom jobstatus a where upper(job) like 'BROND__DSL%'   \nand execution_id=(select max(execution_id) from jobstatus where upper(job) like 'BROND__DSL%');\nexecution_id | id    | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n-------------+-------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n1659931204   | 15675 | BROND       | BROND_ADSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:00:04 | 2022-08-08 07:01:38 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n1659931204   | 15677 | BROND       | BROND_ADSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:00:05 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n1659931204   | 15679 | BROND       | BROND_ADSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15681 | BROND       | BROND_ADSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:00:06 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15683 | BROND       | BROND_ADSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15685 | BROND       | BROND_ADSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:00:13 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15687 | BROND       | BROND_ADSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:00:26 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15689 | BROND       | BROND_ADSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:01:38 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15691 | BROND       | BROND_VDSL_STATS | MAIN                                     | 20220808            | SUCCESS | 2022-08-08 07:01:38 | 2022-08-08 07:03:51 | Succesfully Completed. | brond | un2.bigdata.abc.gr\n1659931204   | 15693 | BROND       | BROND_VDSL_STATS | GET RAW XDSL FILES                       | 20220808            | SUCCESS | 2022-08-08 07:01:40 |                     | Single raw file found  | brond | un2.bigdata.abc.gr\n1659931204   | 15695 | BROND       | BROND_VDSL_STATS | RENAME FILES @SFTP SERVER                | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15697 | BROND       | BROND_VDSL_STATS | PARSING FILES                            | 20220808            | SUCCESS | 2022-08-08 07:01:41 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15699 | BROND       | BROND_VDSL_STATS | LOAD HDFS LANDINGZONE                    | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15701 | BROND       | BROND_VDSL_STATS | CLEAN-UP THE INPUT FILES                 | 20220808            | SUCCESS | 2022-08-08 07:01:51 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15703 | BROND       | BROND_VDSL_STATS | LOAD HDFS FILES INTO HIVE STAGING TABLES | 20220808            | SUCCESS | 2022-08-08 07:02:05 |                     |                        | brond | un2.bigdata.abc.gr\n1659931204   | 15705 | BROND       | BROND_VDSL_STATS | UPDATE HIVE TABLES WITH FILTERED COLUMNS | 20220808            | SUCCESS | 2022-08-08 07:03:51 |                     |                        | brond | un2.bigdata.abc.gr\n-------------+-------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n```\n### Monitoring Health-Check\n- Check Monitoring status.  \n```bash\t\n$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\t\n{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n```  \n- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\nCommon issues, log inspection tips, and diagnostic commands for resolving errors in the flow.\n- An email will be sent by the system with the point of failure.\n- Check the log file for errors/exceptions  \n`egrep -i 'error|fail|exception|problem' /shared/abc/brond_dsl_stats/DataParser/scripts/log/002.Brond_xDSL_Load.YYYYMMDD.log`\n- List Failed Monitoring messages of the last load\n```bash\n/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n\n\tselect * from jobstatus where upper(job) like 'BROND__DSL' \n\tand status='FAILED'\n\tand operative_partition=(select max(operative_partition) from jobstatus where upper(job) like 'BROND__DSL' and operative_partition regexp '[0-9]{8}')\n\torder by id;\n\texecution_id | id    | application | job              | component                                | operative_partition | status  | system_ts           | system_ts_end       | message                | user  | host                  \n\t-------------+-------+-------------+------------------+------------------------------------------+---------------------+---------+---------------------+---------------------+------------------------+-------+-----------------------\n\t1659946615   | 15825 | BROND       | BROND_ADSL_STATS | MAIN                                     | 20220808            | FAILED  | 2022-08-08 11:16:55 | 2022-08-08 11:16:55 | No raw files found     | brond | un2.bigdata.abc.gr\n\t1659946615   | 15827 | BROND       | BROND_ADSL_STATS | GET RAW XDSL FILES                       | 20220808            | FAILED  | 2022-08-08 11:16:55 |                     | No raw files found     | brond | un2.bigdata.abc.gr\n\t1659946615   | 15829 | BROND       | BROND_VDSL_STATS | MAIN                                     | 20220808            | FAILED  | 2022-08-08 11:16:56 | 2022-08-08 11:16:56 | No raw files found     | brond | un2.bigdata.abc.gr\n\t1659946615   | 15831 | BROND       | BROND_VDSL_STATS | GET RAW XDSL FILES                       | 20220808            | FAILED  | 2022-08-08 11:16:56 |                     | No raw files found     | brond | un2.bigdata.abc.gr\n\t```\n### Common errors  \n- `No raw files found`, there are no raw files available for loading at remdef server.  \nA WARNING message will be sent to abc by email.\nNo actions required from OBSS. Responsible abc. \n- Other factors not related to the specific flow\n- impala/hive availability\n- Kerberos authentication (A.  \n> Ndef: The flow checks if the ticket is still active before any HDFS action.  \nIn case of expiration the flow performs a `kinit` command*\n## Manually triggering the workflow\nProcedure to rerun the Oozie workflow manually in case files are uploaded after the scheduled runs.\nThere are cases where abc might upload new files after the scheduled workflow timing, and request these files to be\nprocessed in the same day. This can only be done by manually triggering the workflow. Before doing so, you need to make some checks first:\n### Check workflow logs\n1. Login to https://999.999.999.999:8888/hue/accounts/login?next=/hue using the brond account\n2. Go to \"Jobs\" > \"Workflows\"\nIf all workflow executions (\"Brond_Load_xDSL_WF_NEW\") were successful, you can proceed by checking that the file(s)\nabc added, were copied after the scheduled timings of the workflow\n### Check added files\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```bash\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond\n-rw-r--r-- 0 507 500 21925249 Nov 29 06:22 ADSL_Brond/Counter_Collection_24H.329_2022_11_29.csv.gz.LOADED\n-rw-r--r-- 0 507 500 22107252 Nov 29 13:52 ADSL_Brond/Counter_Collection_24H.330_2022_11_29.csv.gz\n```\nThe second file was indeed added after the scheduled time and has not been picked up by the workflow.\n### Trigger workflow\nYou can now proceed to manually trigger the workflow:\n1. Go to HUE and select \"Jobs\"\n2. Go to \"Workflow\" and select \"Brond_Load_xDSL_WF_NEW\"\n3. In the next screen, select \"Rerun\"\n4. Wait for the workflow to successfully end\n5. If no errors occur, proceed with:\nsftp get raw files (*.csv.gz) from FTP Server to `/data/1/brond_retr_LZ`\n```bash\necho \"ls -l ADSL_Brond\" | sftp bigd@999.999.999.999\nsftp> ls -l ADSL_Brond\n-rw-r--r-- 0 507 500 21925249 Nov 29 06:22 ADSL_Brond/Counter_Collection_24H.329_2022_11_29.csv.gz.LOADED\n-rw-r--r-- 0 507 500 22107252 Nov 29 13:52 ADSL_Brond/Counter_Collection_24H.330_2022_11_29.csv.gz.LOADED\n```\n## Data Check\nValidation steps to confirm data has been properly loaded and partitioned in final Hive tables.\n- **Check final tables for new partitions**:\n- Impala-shell: \n```bash\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\n\n\trefresh brond.brond_adsl_stats_daily;  \n\tshow partitions brond.brond_adsl_stats_daily;  \n\t\n\tpar_dt   | #Rows  | #Files | Size     | Bytes Cached | Cache Replication | Format | Incremental stats | Location                                                                        \n\t---------+--------+--------+----------+--------------+-------------------+--------+-------------------+---------------------------------------------------------------------------------\n\t20221130 | 629397 |      1 | 155.09MB | NOT CACHED   | NOT CACHED        | TEXT   | false             | hdfs://nameservice1/ez/warehouse/brond.db/brond_adsl_stats_daily/par_dt=20220808\n\tTotal    |     -1 |      1 | 155.09MB | 0B           |                   |        |                   |                                                                                 \n\n\n\trefresh brond.brond_vdsl_stats_daily;\n\tshow partitions brond.brond_vdsl_stats_daily\n\t\n\tpar_dt   | #Rows   | #Files | Size     | Bytes Cached | Cache Replication | Format | Incremental stats | Location                                                                        \n\t---------+---------+--------+----------+--------------+-------------------+--------+-------------------+---------------------------------------------------------------------------------\n\t20221130 | 2157413 |      1 | 588.26MB | NOT CACHED   | NOT CACHED        | TEXT   | false             | hdfs://nameservice1/ez/warehouse/brond.db/brond_vdsl_stats_daily/par_dt=20220808\n\tTotal    |      -1 |      1 | 588.26MB | 0B           |                   |        |                   |                                                                                 \n```\n- **Check the amount of data in final tables**:\n- Impala-shell: \n\t```bash\n\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k\n\tSELECT par_dt, count(*) as cnt from brond.brond_adsl_stats_daily group by par_dt order by 1;\n\tpar_dt   | cnt   \n\t---------+-------\n\t20221130 | 629397\n\n\tSELECT par_dt, count(*) as cnt from brond.brond_vdsl_stats_daily group by par_dt order by 1;\n\tpar_dt   | cnt    \n\t---------+--------\n\t20221130 | 2157413\n\t```"
        },
        {
            "question": "What should I check if the CSI Redis flow fails to load data into Redis?",
            "answer": "Verify if the Spark jobs completed successfully and examine logs under `/user/rediscsi/log`. If there’s a failure, look for tar.gz files named `csiRedis.<date>.<execution ID>.tar.gz`. Also ensure that `/home/bigstreamer/bin/102.CSI_Redis_Load_Data.sh` executed correctly.",
            "category": "Troubleshooting & Issue Resolution",
            "files": "CSI-Redis_Flow.md",
            "context": "---\ntitle: CSI-Redis Flow – Daily Export of CSI Metrics to Redis\ndescription: Spark-based ETL pipeline for collecting, aggregating, and exporting CSI metrics from HDFS to Redis via Oozie, with daily scheduling, monitoring integration, and Redis SFTP delivery and load execution.\njob_name: CSI_REDIS\ncomponent: MAIN\nsystem: BigStreamer\nhost: un-vip.bigdata.abc.gr\ntarget_system: Redis\ntarget_vm: 999.999.999.999\ntarget_port: 2223\ntarget_script: /home/bigstreamer/bin/102.CSI_Redis_Load_Data.sh\ncoordinator: Redis-CSI_Coordinator\nworkflow: Redis-CSI_Workflow\nowner: rediscsi\nexecution_schedule: 20:00 UTC daily\ndata_source_paths:\n  - /ez/warehouse/npce.db/yak_cells/*\n  - /ez/warehouse/csi.db/csi_cell_dashboard_primary_dly/*\n  - /ez/warehouse/csi.db/csi_cell_daily_v3/*\nexport_hdfs_path: /user/rediscsi/docx-data/csi/parquet/\nlogs_hdfs_path: /user/rediscsi/log\nmonitoring_db: monitoring\nmonitoring_host: 999.999.999.999\nmonitoring_table: jobstatus\nspark_jobs:\n  - AggregateRdCells\n  - AggregateCsiPrimary\n  - CSIAveragePerCellId\n  - AverageCsi\n  - PLMNCsiCellDistri\n  - TopWorstCsiCellTableAndMap\n  - CSIPerLocTimeCharts\n  - TopWorstDeltaCsiCellTableAndMap\nload_type: daily\ndelivery_target: Redis\nlast_updated: 2025-05-01\nkeywords:\n  - bigstreamer\n  - redis\n  - csi\n  - spark\n  - parquet\n  - metrics\n  - oozie\n  - hdfs\n  - sftp\n  - ssh\n  - jobstatus\n  - monitoring\n  - kafka\n  - impala\n  - hive\n  - beeline\n  - aggregation\n  - json configs\n  - top worst csi\n  - network metrics\n  - data delivery\n  - telecom\n  - plmn\n  - dashboards\n  - error tracing\n  - logs\n  - execution_id\n  - cxi-etl\n  - yaml\n  - tar.gz\n---\n# CSI-Redis Flow\n## Installation info\nSetup configuration for the CSI-Redis Flow, including input data paths, working directories, job scheduling, and tools.\n### Data Source\nDetails about the source data files, their HDFS locations, and relevant working directories used in the pipeline.\n- Source system: HDFS  \n  - user : `rediscsi`\n  - Parquet files:  \n\t\t- `/ez/warehouse/npce.db/yak_cells/*`  \n\t\t- `/ez/warehouse/csi.db/csi_cell_dashboard_primary_dly/*`  \n\t\t- `/ez/warehouse/csi.db/csi_cell_daily_v3/*`\n- Local FileSystem Directories\n  - user : `rediscsi`\n\t- exec node : defined by Oozie\n\t- work dir : defined by Oozie\n\t- export dir: `/csiRedis_exp_data`\n- HDFS Directories\n\t- Export dir : `/user/rediscsi/docx-data/csi/parquet/`\n\t- Status dir : `/user/rediscsi/docx-data/metatdata/checkpoints`\n#### Scripts-Configuration Location\nPaths for locating scripts and configuration files used in the CSI-Redis flow.\n- node : `HDFS`\n- user : `rediscsi`\n- scripts path : `hdfs:/user/rediscsi`\n-\tconfigurations path : `hdfs:/user/rediscsi`\n#### Logs Location\nInformation about where log files are stored and how they are named for each flow execution.\n- node : `HDFS`\n- user : `rediscsi`\n- path : `/user/rediscsi/log`\n- log file: `csiRedis.<partition data>.<execution ID>.tar.gz`  \n\t*i.e. `csiRedis.20230420.20230420_230010.tar.gz`*\n#### Oozie Scheduling\nDetails about the Oozie coordinator, workflow, script execution, and schedule for the CSI-Redis flow.\n- user : rediscsi\n- Coordinator :`Redis-CSI_Coordinator`  \n- Workflow : `Redis-CSI_Workflow`  \n- Shell : `/user/rediscsi/100.CSI_Main.sh`\n- runs at : `20:00 UTC Daily`\n#### Database CLI commands\nCommand-line tools for accessing Hive, Impala, and MySQL used in various validation and debugging steps.\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/def_network_map;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr -d def_network_map --ssl -k`\n- MySql*: `mysql -u monitoring -p -h 999.999.999.999 monitoring`\n*\\*The password for the MySql database can be found [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)*\n### Data target\nRedis target information including VM details and script used to load processed data into Redis.\n- Redis VM:`999.999.999.999`\n- Port Forward:`un-vip.bigdata.abc.gr:2223`\n- user: `bigstreamer`\n- scripts path: `/home/bigstreamer/bin`\n-\tLoad Script: `102.CSI_Redis_Load_Data.sh`\n## Data process\nStep-by-step breakdown of the data processing flow from HDFS extraction to Redis loading, including Spark job execution and file management.\n### Set HDFS Export Path\nReplaces placeholders in configuration files with the actual export path for the current execution.\nDefines the export path in HDFS and updates the json configuration files.  \nReplaces the key-word `HDFS_PATH_YYYYMMDD` with the `/user/rediscsi/docx-data/csi/parquet/<execution ID>`  \ni.e. `/user/rediscsi/docx-data/csi/parquet/20230401_102030`  \n### Data Preparation\nExecutes Spark jobs that generate the intermediate data needed for aggregation.\nExecute the Data preparation Spark jobs\n- AggregateRdCells  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.aggregator.AggregateRdCells cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./aggregate_rd_cells_full.json`\n- AggregateCsiPrimary  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.aggregator.AggregateCsiPrimary cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar aggregate_csi_primary_inc.json`  \n**IMPORTANT: if any of the above Spark jobs fails then the procedure stops.**\n### Data Aggregation\nExecutes Spark jobs for metric calculations, averages, and top/worst CSI indicators.\nExecute the Data Aggregation Spark jobs\n- CSIAveragePerCellId  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.csiarea.CSIAveragePerCellId ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./csi_average_per_cell_id_metrics_predef_all.json`\n- AverageCsi  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.avgcsi.AverageCsi ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./avg_csi_metrics_predef_all.json`\n- PLMNCsiCellDistri  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.plmncsicelldistribution.PLMNCsiCellDistri ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./plmn_csi_cell_distri_metrics_predef_all.json`\n- TopWorstCsiCellTableAndMap  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.topworstcsi.TopWorstCsiCellTableAndMap ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./top_worst_csi_metrics_predef_all.json`\n- CSIPerLocTimeCharts  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.csibyloc.CSIPerLocTimeChartsToMongo ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./avg_csi_by_loc_metrics_predef_all_mongo.json`\n- TopWorstDeltaCsiCellTableAndMap  \n`spark-submit --verbose --master yarn --deploy-mode client --principal \"rediscsi@CNE.abc.GR\" --keytab \"./rediscsi.keytab\" --jars ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar --class de.telekom.cxi.dashboard.metrics.data.topworstdeltacsi.TopWorstDeltaCsiCellTableAndMap ./cxi-etl-1.0-SNAPSHOT-jar-with-dependencies.jar ./top_worst_delta_csi_metrics_inc.json`\n**Ndef: The Spark jobs above create the export file in HDFS under `/user/rediscsi/docx-data/csi/parquet/<execution ID>`**\n### Get export files from HDFS\nRetrieves the processed export files from HDFS to local storage.\nCopy the export files from HDFS to the slave node's local filesystem  \nThe working slave node is defined by Oozie\n`hdfs dfs -get /user/rediscsi/docx-data/csi/parquet/<execution ID>/* ./csiRedis_exp_data/<execution ID>`  \ni.e. `hdfs dfs -get /user/rediscsi/docx-data/csi/parquet/20230401_102030/* ./csiRedis_exp_data/20230401_102030`\n### Archive export files\nArchives export files into a single `.tar.gz` for transfer.\ncreates a compressed tar file which contains all the log files\n`tar cvfz ./csiRedis_exp_data/<execution ID>/redisCSI.<execution ID>.tar.gz ./csiRedis_exp_data/<execution ID>`  \ni.e. `tar cvfz ./csiRedis_exp_data/<execution ID>/redisCSI.20230401_102030.tar.gz ./csiRedis_exp_data/20230401_102030`\n### Transfer Archived file to Redis VM\nTransfers the archived export data to the Redis VM using SFTP.\nTranfers the Archived file to Redis VM using `SFTP PUT`  \n`echo \"put ./csiRedis_exp_data/<execution ID>/redisCSI.<execution ID>.tar.gz <redis_LZ>\" | sftp -o \"StrictHostKeyChecking no\" -i ./id_rsa -P$<redis_Port> $<redis_User>@$<redis_Node>`  \ni.e. `echo \"put ./csiRedis_exp_data/20230401_102030/redisCSI.20230401_102030.tar.gz ./CSI_LZ\" | sftp -o \"StrictHostKeyChecking no\" -i ./id_rsa -P2223 bigstreamer@un-vip.bigdata.abc.gr`\n### Load Data to Redis DB\nRuns the load script on the Redis VM to ingest the data into the Redis database.\nExtracts the parquet files from the Archived file and load them into the Redis database  \nExecute the load script `Redis VM:/home/bigstreamer/bin/102.CSI_Redis_Load_Data.sh` remdefly.\n`ssh -o \"StrictHostKeyChecking no\" -i ./id_rsa -P2223 bigstreamer@un-vip.bigdata.abc.gr \"/home/bigstreamer/bin/102.CSI_Redis_Load_Data.sh`\n## Monitoring\nDetails on monitoring connections, execution message logs, and tracked job components.\n### Monitoring connection details\nDatabase credentials and paths for querying execution logs.\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \n### Monitoring Message list\nExample log messages that indicate successful execution of each flow component.\nFor each load the following set of messages will be recorded in the Monitoring database.\n```sql\n+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+\n| execution_id    | component                       | job              | operative_partition | status  | system_ts               |\n+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+\n| 20230420_230010 | MAIN_START                      | JOB_BEGIN        | 20230420            | SUCCESS | 2023-04-20 23:00:10.000 |\n| 20230420_230010 | UPDATE_HDFS_EXPORT_PATH         | PRE_TASK         | 20230420            | SUCCESS | 2023-04-20 23:00:10.000 |\n| 20230420_230010 | AGGREGATERDCELLS                | DATA_PREPARATION | 20230420            | SUCCESS | 2023-04-20 23:02:14.000 |\n| 20230420_230010 | AGGREGATECSIPRIMARY             | DATA_PREPARATION | 20230420            | SUCCESS | 2023-04-20 23:06:30.000 |\n| 20230420_230010 | COREKPIANDCSIBYLEVEL            | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:09:29.000 |\n| 20230420_230010 | CSIAVERAGEPERCELLID             | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:12:14.000 |\n| 20230420_230010 | AVERAGECSI                      | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:15:49.000 |\n| 20230420_230010 | PLMNCSICELLDISTRI               | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:17:17.000 |\n| 20230420_230010 | TOPWORSTCSICELLTABLEANDMAP      | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:24:42.000 |\n| 20230420_230010 | CSIPERLOCTIMECHARTSTOMONGO      | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:27:58.000 |\n| 20230420_230010 | TOPWORSTDELTACSICELLTABLEANDMAP | DATA_AGGREGATION | 20230420            | SUCCESS | 2023-04-20 23:29:34.000 |\n| 20230420_230010 | GET_EXP_FILES_FROM_HDFS         | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:29:55.000 |\n| 20230420_230010 | TAR_EXP_FILES                   | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:30:09.000 |\n| 20230420_230010 | SFTP_PUT_EXP_FILE_TO_REDIS      | POST_TASK        | 20230420            | SUCCESS | 2023-04-20 23:30:13.000 |\n| 20230420_230010 | LOAD_DATA_TO_REDIS_DB           | LOAD_REDIS       | 20230420            | SUCCESS | 2023-04-20 23:33:52.000 |\n| 20230420_230010 | MAIN_END                        | JOB_END          | 20230420            | SUCCESS | 2023-04-20 23:36:01.000 |\n+-----------------+---------------------------------+------------------+---------------------+---------+-------------------------+\n```\n### Monitoring Component list\nDescriptions of job components recorded in the monitoring logs and what each component does.\n```sql\n+---------------------------------+------------------+--------------------------------------------------------------------------------------------------\n| Component                       | Job              | Description\n+---------------------------------+------------------+--------------------------------------------------------------------------------------------------\n| MAIN_START                      | JOB_BEGIN        | Procedure Started\n| UPDATE_HDFS_EXPORT_PATH         | PRE_TASK         | Set the HDFS path in Json Configuration files\n| AGGREGATERDCELLS                | DATA_PREPARATION | Data preparation: `spark-submit` using `aggregate_rd_cells_full.json` config file\n| AGGREGATECSIPRIMARY             | DATA_PREPARATION | Data preparation: `spark-submit` using `aggregate_csi_primary_inc.json` config file\n| COREKPIANDCSIBYLEVEL            | DATA_AGGREGATION | Data aggregation: `spark-submit` using `core_kpi_and_csi_by_level_metrics_predef_all` config file\n| CSIAVERAGEPERCELLID             | DATA_AGGREGATION | Data aggregation: `spark-submit` using `csi_average_per_cell_id_metrics_predef_all` config file\n| AVERAGECSI                      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `avg_csi_metrics_predef_all` config file\n| PLMNCSICELLDISTRI               | DATA_AGGREGATION | Data aggregation: `spark-submit` using `plmn_csi_cell_distri_metrics_predef_all` config file\n| TOPWORSTCSICELLTABLEANDMAP      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `top_worst_csi_metrics_predef_all` config file\n| CSIPERLOCTIMECHARTSTOMONGO      | DATA_AGGREGATION | Data aggregation: `spark-submit` using `avg_csi_by_loc_metrics_predef_all_mongo` config file\n| TOPWORSTDELTACSICELLTABLEANDMAP | DATA_AGGREGATION | Data aggregation: `spark-submit` using `top_worst_delta_csi_metrics_inc` config file\n| GET_EXP_FILES_FROM_HDFS         | POST_TASK        | hdfs copyToLocal the export files\n| TAR_EXP_FILES                   | POST_TASK        | Archive the export files\n| SFTP_PUT_EXP_FILE_TO_REDIS      | POST_TASK        | Tranfers the Archived file to Redis VM\n| LOAD_DATA_TO_REDIS_DB           | LOAD_REDIS       | Upload the Archived file into Redis database \n| MAIN_END                        | JOB_END          | Procedure Completed\n+---------------------------------+------------------+--------------------------------------------------------------------------------------------------\n```\n### Monitoring database Queries\nMySQL queries to retrieve messages from the most recent CSI-Redis execution.\n- List messages of the last load  \n`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n```sql\n    select \n      execution_id, component, job, operative_partition,  \n      status, system_ts, substr(message,1,50) msg\n    from jobstatus a where 1=1\n    and upper(application)='CSI'\n    and execution_id in (select max(execution_id) from jobstatus where upper(application)='CSI' and upper(job)='DATA_PREPARATION')\n    order by a.id\n    ;\n```\n### Monitoring Health-Check\nHow to check the status of the monitoring application and restart it if needed.\n- Check Monitoring status.  \n```\tbash\n$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'\t\n{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n```  \n- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\nHow to respond to errors or failed jobs, including checking logs and identifying root causes.\nAn email will be sent by the system with the point of failure.  \ni.e.\n<pre>\nFrom: abc_bigd@abc.gr  \nSubject: CSI - DATA_AGGREGATION: FAILED  \n<b>\nData preparation:top_worst_delta_csi_metrics_inc\nExec_id:20230401_102030\n</b>\nThis is an automated e-mail.  \nPlease do not reply.  \n</pre>\n**Actions**  \n1. Write down the value of `Exec_id` described in the alert email  \n\ti.e. Exec_id:`1673849411`\n2. Log files are stored in HDFS in archived files.  \n```bash\n$ hdfs dfs -ls /user/rediscsi/log/\n-rw-r--r--   3 rediscsi rediscsi  366865842 2023-04-20 23:35 /user/rediscsi/log/csiRedis.20230420.20230420_230010.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  361801963 2023-04-21 23:38 /user/rediscsi/log/csiRedis.20230421.20230421_230010.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  358913487 2023-04-22 23:41 /user/rediscsi/log/csiRedis.20230422.20230422_230013.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  364564867 2023-04-23 23:42 /user/rediscsi/log/csiRedis.20230423.20230423_230010.tar.gz\n-rw-r--r--   3 rediscsi rediscsi  359603322 2023-04-24 23:38 /user/rediscsi/log/csiRedis.20230424.20230424_230009.tar.gz\n```\nEach archived file contains a set of logs related to the specific flow run (execution ID).  \nThe filename contains info about the `<partition data>` and the `<execution ID>`  \n`csiRedis.<partition data>.<execution ID>.tar.gz`\n3. Get the log file  \n- create a new dir to store the log file\n```bash\nmkdir -p /tmp/csi_redis_log\ncd /tmp/csi_redis_log\n```\n- Copy from the HDFS log dir the proper log file according to the `<execution ID>` mentioned in the alert email\n`hdfs dfs -get /user/rediscsi/log/csiRedis.20230401.20230401_102030.tar.gz`\n- Extract the archived log file\n`tar xvfz ${archFile} --strip-components 9 -C .`\n- Searches for Exception messages in log files  \n`egrep -i '(Exception:|Caused by)' *.log`  \n4. In case of failure, the flow will try to load the data in the next run.  \njkl-Telecom is not aware of how the data files are produced or the contents in them.  \nThe Spark jobs that are used by the flow, have been developed by a partner of abc (an India company).  \njkl-Telecom is responsible for \n- the execution of Spark jobs to produce the export data files, \n- the collection of the export data files (if any), \n- the transfer of them in Redis node \n- and finally the loading of the export files into the Redis database (using specific Spark jobs)."
        },
        {
            "question": "How can I verify successful Oracle to Hive data transfers in the def_NETWORK_MAP flow?",
            "answer": "Check Hive import logs in `/user/def_network_maps/log`, especially `104.OneTicket_OraData_Import_Hive.<UNIX-TIME>.log`. Additionally, query the Oracle control table:\n```sql\nSELECT * FROM def_NETWORK_MAP.EXPORT_CTL WHERE TARGET='ACTIVITY';\n```",
            "category": "Troubleshooting & Issue Resolution",
            "files": "NETWORK_MAP_Support_Notes.md",
            "context": "---\ntitle: def_NETWORK_MAP ETL Flow (OneTicket)\nsystem: BigStreamer\ncomponent: OneTicket\njob_name: Oracle_to_Hive_OneTicket_Load\nsource_system: Oracle\nsource_tables:\n  - def_NETWORK_MAP.ACTIVITY\n  - def_NETWORK_MAP.AFFECTED_CUSTOMERS\n  - def_NETWORK_MAP.AFFECTED_OCT_WTT\n  - def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n  - def_NETWORK_MAP.OPEN_MW\n  - def_NETWORK_MAP.OPEN_NTT\n  - def_NETWORK_MAP.OPEN_OCT\n  - def_NETWORK_MAP.OPEN_WTT\ndestination_system: Hive\ndestination_tables:\n  - def_network_map.activity\n  - def_network_map.affected_customers\n  - def_network_map.affected_oct_wtt\n  - def_network_map.defective_netw_element\n  - def_network_map.open_mw\n  - def_network_map.open_ntt\n  - def_network_map.open_oct\n  - def_network_map.open_wtt\nschedule: every 5 minutes\ncoordinator: def_NETWORK_MAP_Coordinator\nworkflow: def_NETWORK_MAP_Workflow\nscript_path: HDFS:/user/def_network_maps/100.OneTicket_Main.sh\nmonitoring_table: monitoring.jobstatus\nowner: def_network_maps\ntags:\n  - OneTicket\n  - Oracle to Hive ETL\n  - def_NETWORK_MAP\n  - BigStreamer\n  - Monitoring\n  - Oozie\n  - HDFS\n  - Impala\n  - Beeline\n  - Troubleshooting\n  - Log Analysis\n---\n# def_NETWORK_MAP Flow (OneTicket)\nThis document describes the ETL process that exports operational data from Oracle to Hive every 5 minutes using the OneTicket flow. It covers installation details, process phases, monitoring mechanisms, and troubleshooting steps. The data is primarily used for network defect tracking and service impact analysis.\n## Installation & Setup\nConfiguration paths, database sources, and execution environment for the OneTicket ETL process.\n### Data Source Tables\n- Source system: Oracle Database 11g Enterprise Edition (999.999.999.999.0)  \n\t- Server:`999.999.999.999:1521`\n\t- Port Forward:`999.999.999.999:1521`\n\t- User:`def_network_map`\n\t- SID:`defsblf_rw`\n\t- Oracle Tables: \n\t\t- def_NETWORK_MAP.ACTIVITY\n\t\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t\t- def_NETWORK_MAP.OPEN_MW\n\t\t- def_NETWORK_MAP.OPEN_NTT\n\t\t- def_NETWORK_MAP.OPEN_OCT\n\t\t- def_NETWORK_MAP.OPEN_WTT\n\t\t- def_NETWORK_MAP.EXPORT_CTL\n### Scripts-Configuration Location in HDFS\n- user : `def_network_maps`\n- scripts path : `HDFS:/user/def_network_maps/`\n-\tconfiguration path : `HDFS:/user/def_network_maps/`\n\t- `oneTicket_env.sh`, The environment of the flow\n\t- `OraExpData.tables`, List of tables which are going to be exported/imported from Oracle to Hive\n\t- `monitoring.config`, The `Monitoring` connection details\n\t- `oraclecmd.config`, The Oracle connection details\n\t- `oneticket.keystore`, The Oracle password file\n- Temp dir : `HDFS:/ez/landingzone/tmp/oneTicket`\n### Export Data Location\n- node : Dynamically defined by the Oozie service  \n\ti.e. `sn95.bigdata.abc.gr`\n- Directory : Dynamically defined by the Oozie service\n\ti.e. `/data/2/yarn/nm/usercache/def_network_maps/appcache/application_1668434520231_277391/container_e276_1668434520231_277391_01_000001`\n### Logs Location\n- user : `def_network_maps`\n- logs path : `/user/def_network_maps/log`\n- log files: \n\t- `101.OneTicket_OraMetaData.<YYYYMM>.log`\n\t- `102.OneTicket_OraData_CTRL.<YYYYMM>.log`\n\t- `103.OneTicket_OraData_Export_Import.<TABLE_NAME>.<UNIX-TIME>.log`\n\t- `104.OneTicket_OraData_Import_Hive.<UNIX-TIME>.log`\n\t`<UNIX-TIME>` is the timestamp of the load in unix-epoch format  \n\t`<TABLE_NAME>` list of values:  \n\t- def_NETWORK_MAP.ACTIVITY\n\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t- def_NETWORK_MAP.OPEN_MW\n\t- def_NETWORK_MAP.OPEN_NTT\n\t- def_NETWORK_MAP.OPEN_OCT\n\t- def_NETWORK_MAP.OPEN_WTT\n\ti.e.  \n\t```\n\t$ hdfs dfs -ls -t -r /user/def_network_maps/log\n\t\n\t101.OneTicket_OraMetaData.202302.log\n\t102.OneTicket_OraData_CTRL.202302.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.ACTIVITY.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.AFFECTED_CUSTOMERS.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.AFFECTED_OCT_WTT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_MW.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_NTT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_OCT.1675939511.log\n\t103.OneTicket_OraData_Export_Import.def_NETWORK_MAP.OPEN_WTT.1675939511.log\n\t104.OneTicket_OraData_Import_Hive.1675939511.log\t\n\t```\n### Oozie Scheduling\n- user : def_network_maps\n- Coordinator :`def_NETWORK_MAP_Coordinator`  \n\truns at : every 5 minutes on a Daily basis  \n\t\t`0,5,10,15,20,25,30,35,40,45,50,55 * * * *` \n- Workflow : `def_NETWORK_MAP_Workflow`  \n\tBash script : `HDFS:/user/def_network_maps/100.OneTicket_Main.sh`\n### Hive Tables\n- Target Database: `def_network_map`\n- Target Tables: \n\t- def_NETWORK_MAP.ACTIVITY\n\t- def_NETWORK_MAP.AFFECTED_CUSTOMERS\n\t- def_NETWORK_MAP.AFFECTED_OCT_WTT\n\t- def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT\n\t- def_NETWORK_MAP.OPEN_MW\n\t- def_NETWORK_MAP.OPEN_NTT\n\t- def_NETWORK_MAP.OPEN_OCT\n\t- def_NETWORK_MAP.OPEN_WTT\n### Database CLI commands\n- Beeline: `/usr/bin/beeline -u \"jdbc:hive2://un-vip.bigdata.abc.gr:10000/def_network_map;principal=hive/_HOST@CNE.abc.GR;ssl=true;sslTrustStore=/usr/java/latest/jre/lib/security/jssecacerts;trustStorePassword=changeit\"`\n- Impala: `/usr/bin/impala-shell -i un-vip.bigdata.abc.gr -d def_network_map --ssl -k`\n- Oracle*: `sqlplus -s def_network_map/<PASSWORD>@999.999.999.999:1521/defsblf_rw`\n- MySql*: `mysql -u monitoring -p -h 999.999.999.999 monitoring`\n*\\*The passwords for the Oracle and MySql databases can be found [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx)*\n## Data Process: Oracle to Hive ETL Steps\nStep-by-step breakdown of how data is exported from Oracle and ingested into Hive tables.\n### In General\nThe flow consist of two basic procedures and one control Oracle table.  \n\t- the **Export** procedure, which is running at the remdef Oracle server (Responsible def/abc),  \n\t- the **Import** procedure, which is running at the BigStreamer cluster,  \n\t- the `def_NETWORK_MAP.EXPORT_CTL` table, used to synchronize the **Export** procedure with the **Import** procedure.  \nThe data in `def_NETWORK_MAP.EXPORT_CTL` is similar to the following\nConnect to Oracle (see [Database CLI commands](#database-cli-commands))  \n```\nselect * from def_NETWORK_MAP.EXPORT_CTL;  \nEXPORT_SEQUENCE | TARGET                 | EXPORT_START_DT     | EXPORT_END_DT       | ROW_COUNT | IMPORT_START_DT     | IMPORT_END_DT      \n----------------+------------------------+---------------------+---------------------+-----------+---------------------+--------------------\n              0 | TOTAL                  | 2022-11-15 16:50:01 | 2022-11-15 17:07:09 |           | 2022-11-22 09:28:24 | 2022-11-22 09:29:12\n              5 | ACTIVITY               | 2022-11-15 16:51:27 | 2022-11-15 17:04:36 |     73211 |                     |                    \n              6 | AFFECTED_CUSTOMERS     | 2022-11-15 17:04:36 | 2022-11-15 17:04:54 |     14438 |                     |                    \n              7 | OPEN_OCT               | 2022-11-15 17:04:54 | 2022-11-15 17:07:05 |     58338 |                     |                    \n              8 | OPEN_WTT               | 2022-11-15 17:07:05 | 2022-11-15 17:07:09 |      3690 |                     |                    \n              1 | OPEN_MW                | 2022-11-15 17:10:05 | 2022-11-15 17:10:42 |       249 |                     |                    \n              2 | OPEN_NTT               | 2022-11-15 17:10:42 | 2022-11-15 17:11:03 |      6957 |                     |                    \n              3 | AFFECTED_OCT_WTT       | 2022-11-15 17:11:03 | 2022-11-15 17:11:20 |      1782 |                     |                    \n              4 | DEFECTIVE_NETW_ELEMENT | 2022-11-15 17:11:20 | 2022-11-15 17:11:21 |      6236 |                     |                    \n```\n  Ndef: We are interesting for the 1st row only `EXPORT_SEQUENCE=0`  \n### Phased\n1. The **Export** procedure is implemented by def/abc.  \n\tIt is responsible to prepare the data in Oracle tables (see Oracle Table list in [Data Source Tables](#data-source-tables))  \n\tOnce completed, it updates the `def_NETWORK_MAP.EXPORT_CTL.EXPORT_START_DT` column with the current system's timestamp.   \n\tNdef: It is not known how often the **Export** procedure runs.\n2.  The **Import** procedure is implemented by jkl.  \n\tIt checks periodically if the value of `def_NETWORK_MAP.EXPORT_CTL.EXPORT_START_DT` has been updated.  \n\t- In case of new value, the procedure exports the data from the Oracle tables  \n\t`./oracle_cmd.sh \"select * from <table>\" > ./<table>.exp`  \n\t- stores them into HDFS  \n\t`hdfs dfs -moveFromLocal ./<table>.exp .`\n\t- and, consequently, load them into the corresponding [Hive Tables](#hive-tables).  \n\t*Connect to Beeline (see [Database CLI commands](#database-cli-commands))*\n\t`Beeline <connection> -e \"load data inpath './<table>.exp' overwrite into table <table>;\"`\n\t- Once the Import procedure completed, the `IMPORT_START_DT` column will be updated with the current system's timestamp.   \n\t*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*\n\t`update def_NETWORK_MAP.EXPORT_CTL.EXPORT_START_DT`\n\t- Compute table statistics using impala-shell\t \n\t*Connect to Impala (see [Database CLI commands](#database-cli-commands))*\n\t`compute incremental stats brond.brond_retrains_hist;`\n3. In case the `EXPORT_START_DT` value isn't updated, the **Import** procedure exists doing nothing.\n## Monitoring\nDescribes how load jobs are tracked in the monitoring.jobstatus table and validated via logs and queries.\n### Monitoring connection details\n|Field|Value|\n|-|-|\n|Database Type| mysql  \n|Host| 999.999.999.999  \n|DB Name| monitoring  \n|DB User| monitoring  \n|Table| jobstatus  \nConnection command: `/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n### Monitoring Message list → Monitoring Messages in MySQL\nFor each load for each TABLE the following set of messages will be recorded in the Monitoring database.\n```sql\nexecution_id | id     | application | job             | component                 | operative_partition | status  | system_ts           | system_ts_end       | param0 | message                                                              | user             | host                    \n-------------+--------+-------------+-----------------+---------------------------+---------------------+---------+---------------------+---------------------+--------+----------------------------------------------------------------------+------------------+-------------------------\n1670509202   | 402171 | ONETICKET   | def_NETWORK_MAP | MAIN                      | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 | 2022-12-08 16:22:05 |        | Procedure Started                                                    | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402173 | ONETICKET   | def_NETWORK_MAP | CHECK_FOR_AVAILABLE_DATA  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 |                     |        | New data found: 2022-12-08 15:50:04                                  | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402207 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-<TABLE-NAME>  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     | 6987   | Oracle export def_NETWORK_MAP.OPEN_NTT data. Rows:6987               | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402209 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-<TABLE-NAME> | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402211 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-<TABLE-NAME>    | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     |        | Move def_NETWORK_MAP.OPEN_NTT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402241 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-<TABLE-NAME>    | 20221208_1620       | SUCCESS | 2022-12-08 16:21:32 |                     |        | Load def_NETWORK_MAP.OPEN_NTT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr\n1670509202   | 402247 | ONETICKET   | def_NETWORK_MAP | COMPUTE_STATS             | 20221208_1620       | SUCCESS | 2022-12-08 16:22:04 |                     |        | Compute statistics                                                   | def_network_maps | un-vip.bigdata.abc.gr\n```\n### Monitoring Component list\n|Component | Description \n|-|-|\n|MAIN|Indicates the status of the whole load. <br />Status:RUNNING, SUCCESS, FAILED| \n|CHECK_FOR_AVAILABLE_DATA| Check the Oracle table EXPORT_CTL if there are new data to export\n|EXPORT_DATA-\\<TABLE-NAME\\>| Exports data from Oracle to `/shared/abc/oneTicket/exp`\n|DATA_PARSING-\\<TABLE-NAME\\>| Change column separator and remove the string \"null\"\n|HDFS_MOVE-\\<TABLE-NAME\\>| Move export file from local file system to HDFS `/ez/landingzone/tmp/oneTicket`\n|LOAD_DATA-\\<TABLE-NAME\\>| Load export file from HDFS `/ez/landingzone/tmp/oneTicket` into the HIVE table (i.e. `def_NETWORK_MAP.OPEN_NTT`)\n|COMPUTE_STATS| performs compute statistics on HIVE tables using impala-shell\n### Monitoring database Queries → Sample Monitoring DB Queries\n- List messages of the last load  \n`/usr/bin/mysql -u monitoring -p -h 999.999.999.999 monitoring`\n```sql\n  select \n    execution_id, id, application, job, component, operative_partition,  \n    status, system_ts, system_ts_end, param0, message, user,  host\n  from jobstatus a where 1=1\n  and upper(application)='ONETICKET' and upper(job) like 'def_NETWORK_MAP'   \n  and execution_id=(\n    select max(execution_id) execution_id from jobstatus where 1=1\n    and upper(application)='ONETICKET' and upper(job) like 'def_NETWORK_MAP'   \n    and lower(message) not like '%no new export found%' \n    and component='CHECK_FOR_AVAILABLE_DATA'\n    and system_ts>=date(now())-30)\n  order by id\n  ;\n\n  execution_id | id     | application | job             | component                                           | operative_partition | status  | system_ts           | system_ts_end       | param0 | message                                                              | user             | host                    \n  -------------+--------+-------------+-----------------+-----------------------------------------------------+---------------------+---------+---------------------+---------------------+--------+----------------------------------------------------------------------+------------------+-------------------------\n  1670509202   | 402171 | ONETICKET   | def_NETWORK_MAP | MAIN                                                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 | 2022-12-08 16:22:05 |        | Procedure Started                                                    | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402173 | ONETICKET   | def_NETWORK_MAP | CHECK_FOR_AVAILABLE_DATA                            | 20221208_1620       | SUCCESS | 2022-12-08 16:20:02 |                     |        | New data found: 2022-12-08 15:50:04                                  | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402183 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.AFFECTED_OCT_WTT        | 20221208_1620       | SUCCESS | 2022-12-08 16:20:12 |                     | 1867   | Oracle export def_NETWORK_MAP.AFFECTED_OCT_WTT data. Rows:1867       | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402185 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.AFFECTED_OCT_WTT       | 20221208_1620       | SUCCESS | 2022-12-08 16:20:12 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402187 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.AFFECTED_CUSTOMERS      | 20221208_1620       | SUCCESS | 2022-12-08 16:20:15 |                     | 17397  | Oracle export def_NETWORK_MAP.AFFECTED_CUSTOMERS data. Rows:17397    | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402189 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.AFFECTED_OCT_WTT          | 20221208_1620       | SUCCESS | 2022-12-08 16:20:15 |                     |        | Move def_NETWORK_MAP.AFFECTED_OCT_WTT data in HDFS                   | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402191 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.AFFECTED_CUSTOMERS     | 20221208_1620       | SUCCESS | 2022-12-08 16:20:15 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402193 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_MW                 | 20221208_1620       | SUCCESS | 2022-12-08 16:20:17 |                     | 238    | Oracle export def_NETWORK_MAP.OPEN_MW data. Rows:238                 | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402195 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_MW                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:17 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402197 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:18 |                     | 6035   | Oracle export def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT data. Rows:6035 | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402199 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT | 20221208_1620       | SUCCESS | 2022-12-08 16:20:18 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402201 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.AFFECTED_CUSTOMERS        | 20221208_1620       | SUCCESS | 2022-12-08 16:20:18 |                     |        | Move def_NETWORK_MAP.AFFECTED_CUSTOMERS data in HDFS                 | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402203 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_MW                   | 20221208_1620       | SUCCESS | 2022-12-08 16:20:21 |                     |        | Move def_NETWORK_MAP.OPEN_MW data in HDFS                            | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402205 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT    | 20221208_1620       | SUCCESS | 2022-12-08 16:20:21 |                     |        | Move def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT data in HDFS             | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402207 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_NTT                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     | 6987   | Oracle export def_NETWORK_MAP.OPEN_NTT data. Rows:6987               | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402209 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_NTT               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:24 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402211 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_NTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     |        | Move def_NETWORK_MAP.OPEN_NTT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402213 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_WTT                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     | 3621   | Oracle export def_NETWORK_MAP.OPEN_WTT data. Rows:3621               | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402215 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_WTT               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:27 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402217 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_WTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:30 |                     |        | Move def_NETWORK_MAP.OPEN_WTT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402219 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.ACTIVITY                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:36 |                     | 74433  | Oracle export def_NETWORK_MAP.ACTIVITY data. Rows:74433              | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402221 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.ACTIVITY               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:37 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402223 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.ACTIVITY                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:40 |                     |        | Move def_NETWORK_MAP.ACTIVITY data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402225 | ONETICKET   | def_NETWORK_MAP | EXPORT_DATA-def_NETWORK_MAP.OPEN_OCT                | 20221208_1620       | SUCCESS | 2022-12-08 16:20:40 |                     | 60164  | Oracle export def_NETWORK_MAP.OPEN_OCT data. Rows:60164              | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402227 | ONETICKET   | def_NETWORK_MAP | DATA_PARSING-def_NETWORK_MAP.OPEN_OCT               | 20221208_1620       | SUCCESS | 2022-12-08 16:20:40 |                     |        | Change separator                                                     | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402229 | ONETICKET   | def_NETWORK_MAP | HDFS_MOVE-def_NETWORK_MAP.OPEN_OCT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:43 |                     |        | Move def_NETWORK_MAP.OPEN_OCT data in HDFS                           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402231 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.ACTIVITY                  | 20221208_1620       | SUCCESS | 2022-12-08 16:20:57 |                     |        | Load def_NETWORK_MAP.ACTIVITY data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402233 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.AFFECTED_CUSTOMERS        | 20221208_1620       | SUCCESS | 2022-12-08 16:21:04 |                     |        | Load def_NETWORK_MAP.AFFECTED_CUSTOMERS data into Hive               | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402235 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.AFFECTED_OCT_WTT          | 20221208_1620       | SUCCESS | 2022-12-08 16:21:11 |                     |        | Load def_NETWORK_MAP.AFFECTED_OCT_WTT data into Hive                 | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402237 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT    | 20221208_1620       | SUCCESS | 2022-12-08 16:21:18 |                     |        | Load def_NETWORK_MAP.DEFECTIVE_NETW_ELEMENT data into Hive           | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402239 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_MW                   | 20221208_1620       | SUCCESS | 2022-12-08 16:21:25 |                     |        | Load def_NETWORK_MAP.OPEN_MW data into Hive                          | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402241 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_NTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:21:32 |                     |        | Load def_NETWORK_MAP.OPEN_NTT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402243 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_OCT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:21:39 |                     |        | Load def_NETWORK_MAP.OPEN_OCT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402245 | ONETICKET   | def_NETWORK_MAP | LOAD_DATA-def_NETWORK_MAP.OPEN_WTT                  | 20221208_1620       | SUCCESS | 2022-12-08 16:21:46 |                     |        | Load def_NETWORK_MAP.OPEN_WTT data into Hive                         | def_network_maps | un-vip.bigdata.abc.gr\n  1670509202   | 402247 | ONETICKET   | def_NETWORK_MAP | COMPUTE_STATS                                       | 20221208_1620       | SUCCESS | 2022-12-08 16:22:04 |                     |        | Compute statistics                                                   | def_network_maps | un-vip.bigdata.abc.gr\n  ```\n### Monitoring Health-Check\n- Check Monitoring status.  \n```bash  \n$ curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/app/check'  \n{\"code\":0,\"info\":\"App is up and running. Current time:20220803 06:46:57.708 +0000\"}\n```\n- In case of Monitoring is stopped then follow the instructions of `start monitoring-app` procedure described in [Monitoring application](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage-monitoring-app.md#procedure) doc\n## Troubleshooting\nActions to follow in case of failure, based on alert messages and log investigation.\nAn email will be sent by the system with the point of failure.  \ni.e.\n<pre>\nFrom: abc_bigd@abc.gr  \nSubject: ONETICKET - def_NETWORK_MAP: FAILED  \nLoad <b>def_NETWORK_MAP.ACTIVITY</b> data into Hive (1673849411)  \n<b>Exec_id:1673849411</b>  \nThis is an automated e-mail.  \nPlease do not reply.  \n</pre>\n**Actions**  \n1. Write down the values of the `Table name` and `Exec_id` described in the alert email  \ni.e. \n- Table name: `def_NETWORK_MAP.ACTIVITY`\n- Exec_id:`1673849411`\n2. Copy from HDFS the folowing log files which contains the specific `Table name` and `Exec_id` in its filename.\n- 103.OneTicket_OraData_Export_Import.\\<Table name\\>.\\<Exec_id\\>.log\n- 104.OneTicket_OraData_Import_Hive.\\<Exec_id\\>.log\n<pre>\nhdfs dfs -get /user/def_network_maps/hdfs dfs -get 103.OneTicket_OraData_Export_Import.<b>def_NETWORK_MAP.ACTIVITY.1673849411</b>.log\nhdfs dfs -get /user/def_network_maps/hdfs dfs -get 104.OneTicket_OraData_Import_Hive.<b>1673849411</b>.log\n</pre>\n3. Searches for Exception messages in log files  \n`egrep '(Exception:|Coused by)' 10[1-4].OneTicket_OraData*.log`  \ni.e.\n<pre>\n$ egrep '(Exception:|Coused by)' 10[1-4].OneTicket_OraData*.log\n104.OneTicket_OraData_Import_Hive.1673849411.log:javax.security.sasl.SaslException: GSS initiate failed\n104.OneTicket_OraData_Import_Hive.1673849411.log:Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find\n</pre>\n### Common errors  \n  - impala/hive availability\n  - Kerberos authentication\n  *Ndef: The flow checks if the ticket is still active before any HDFS action.  \n  In case of expiration the flow performs a `kinit` command*\n## Data Check\nOptional validation queries for verifying data completeness and load success.\nThe data checks below are provided for informational purposes only.  \nIf any of them returns wrong data, then no actions need to be taken from the support team.  \nThe flow runs periodically over the day and every time overwrites the data.   \n### Check Load Status.\nif the difference between `EXPORT_START_DT` and `IMPORT_START_DT` is greater than 2 hours it is considered as a problem in loading procedure.  \n*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*  \n\t<pre>\n\tselect \n\t  EXPORT_START_DT, IMPORT_START_DT,\n\t  case when 24*(EXPORT_START_DT-IMPORT_START_DT)>2 then 'ERROR' else 'OK' end Load_Status\n\tfrom EXPORT_CTL where EXPORT_SEQUENCE=0;\n\t</pre>\n\t<pre>\n\tEXPORT_START_DT     | IMPORT_START_DT     | LOAD_STATUS\n\t--------------------+---------------------+------------\n\t<b>2022-12-02 10:46:11 | 2022-12-02 07:48:26 | ERROR      </b>#in case of load issue\n\t\n\tEXPORT_START_DT     | IMPORT_START_DT     | LOAD_STATUS\n\t--------------------+---------------------+------------\n\t2022-12-02 10:46:11 | 2022-12-02 10:48:26 | OK         #under normal circumstances\n\t</pre>\n### Check data in Hive-Impala tables\n*Connect to Impala (see [Database CLI commands](#database-cli-commands))*  \n```sql\nselect * from (\n  select distinct  'activity' tbl, upd_ts from def_network_map.activity union all\n  select distinct  'affected_customers', upd_ts from def_network_map.affected_customers union all\n  select distinct  'affected_oct_wtt', upd_ts from def_network_map.affected_oct_wtt union all\n  select distinct  'defective_netw_element', upd_ts from def_network_map.defective_netw_element union all\n  select distinct  'open_mw', upd_ts from def_network_map.open_mw union all\n  select distinct  'open_ntt', upd_ts from def_network_map.open_ntt union all\n  select distinct  'open_oct', upd_ts from def_network_map.open_oct union all\n  select distinct  'open_wtt', upd_ts from def_network_map.open_wtt\n)a order by tbl\n;\n\n+------------------------+---------------------+\n| tbl                    | upd_ts              |\n+------------------------+---------------------+\n| activity               | 2022-12-16 10:50:18 |\n| affected_customers     | 2022-12-16 10:50:18 |\n| affected_oct_wtt       | 2022-12-16 10:50:18 |\n| defective_netw_element | 2022-12-16 10:50:18 |\n| open_mw                | 2022-12-16 10:50:18 |\n| open_ntt               | 2022-12-16 10:50:18 |\n| open_oct               | 2022-12-16 10:50:18 |\n| open_wtt               | 2022-12-16 10:50:18 |\n+------------------------+---------------------+\nFetched 8 row(s) in 6.10s\n```\n`upd_ts` should have the same value *(+- 10 seconds)* as the one in `IMPORT_START_DT` from Oracle table `EXPORT_CTL`  \ni.e.  \n*Connect to Oracle (see [Database CLI commands](#database-cli-commands))*  \n`select IMPORT_START_DT from EXPORT_CTL where EXPORT_SEQUENCE=0;`\n```sql\nIMPORT_START_DT     \n--------------------\n2022-12-16 10:50:19\n```\n*Conclusion: Hive/Impala tables contain the correct data according to the control table*\n```\n     upd_ts     = 2022-12-16 10:50:18 #Hive/Impala tables\nIMPORT_START_DT = 2022-12-16 10:50:19 #Oracle Control Table\n```"
        },
        {
            "question": "How can I manually validate the latest data loaded by the DWHFixed ETL pipeline?",
            "answer": "Query the latest partitions in Hive:\n```sql\nSELECT MAX(partition_column) FROM dwhfixed.v_kv_dim_hist;\n```\nOr review full/delta logs in HDFS under `/user/dwhfixed/log`.",
            "category": "Data Management & Query Execution",
            "files": "dwhfixed.md",
            "context": "---\ntitle: DWHFixed Full and Delta Load – Oracle to Hive/Impala ETL\ndescription: Daily and bi-hourly ETL flows for ingesting data from Oracle SAS_VA views to Hive and Impala using Sqoop, Beeline, and Impala-shell, with monitoring, logging, retry mechanisms, and Grafana dashboards.\njob_name: FULL / DELTA\ncomponent: DWHFixed\nsystem: BigStreamer\nhost: un-vip.bigdata.abc.gr\nsource: Oracle (SAS_VA_VIEW)\ntarget_system: Hive / Impala\ntarget_tables: dwhfixed.*_hist\ncoordinator_full: DWHFixed - Full Coordinator\nworkflow_full: DWHFixed - Full Workflow\ncoordinator_delta: DWHFixed - Delta Coordinator\nworkflow_delta: DWHFixed - Delta Workflow\noracle_user: dm_sas_va\noracle_password_link: https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx\nexecution_schedule_full: Daily at 15:30 & 18:30 UTC\nexecution_schedule_delta: Every 2 hours from 01:30 to 23:30 UTC\nhdfs_paths:\n  full: /user/dwhfixed/full\n  delta: /user/dwhfixed/delta\n  config: /user/dwhfixed/dwhfixed.config\n  log: /user/dwhfixed/log\n  retention: /user/dwhfixed/HDFS_LOG_Retention\nmonitoring:\n  application: DWHFIXED\n  api_url: http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find\n  dashboard: https://unc1.bigdata.abc.gr:3000/d/DNM-sBo4z/dwhfixed-dashboard?orgId=1&from=now-2d&to=now\nhue_login_user: dwhfixed\nalert_subject_format: DWHFIXED - {FULL|DELTA}: FAILED\nalert_source_system: Oracle\nalert_target_system: Hive / Impala\nretention_policy_days: 9\nowner: dwhfixed\nkeywords:\n  - oracle\n  - hive\n  - impala\n  - sqoop\n  - beeline\n  - impala-shell\n  - hdfs\n  - monitoring\n  - grafana\n  - full load\n  - delta load\n  - partitioned data\n  - devpasswd\n  - SAS_VA_VIEW\n  - jdbc\n  - load failure alerts\n  - control table\n  - jceks\n  - oozie\n  - retry\n  - logging\n  - alerting\n  - automation\n  - curl monitoring\n  - log retention\n  - hue workflows\n---\n# Full Load\nThis section describes the full data pipeline executed twice daily, transferring data from Oracle SAS_VA views to partitioned Hive/Impala history tables via Sqoop and Beeline.\n- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - Full Workflow`, \n- **COORDINATOR**: `DWHFixed - Full Coordinator`\n- **HDFS path**: `/user/dwhfixed/full`\n- **Runs**: `15:30, 18:30 (UTC)` \n- **Config file**: `hdfs: /user/dwhfixed/dwhfixed.config`\n- **Tables config/list**: `hdfs: /user/dwhfixed/full/tables_full.config`\n- **Oracle user**: `dm_sas_va`\n- **Oracle pass**: `https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx`. You need password to open this file and get the user's password.\nMoves data from the source tables to a `yesterday` partition of the target tables.\n## Oracle Tables (source)\nThese are the source tables in Oracle used in the Full Load process. The data from each table is transferred to Hive/Impala with daily partitions.\n- `SAS_VA_VIEW.V_BOX_DIM`\n- `SAS_VA_VIEW.V_FIXED_CABLE_DIM`\n- `SAS_VA_VIEW.V_KV_DIM`\n- `SAS_VA_VIEW.V_DSLAM_DIM`\n- `SAS_VA_VIEW.V_SR_TYPE_DIM`\n- `SAS_VA_VIEW.V_SRIA_ADDINFO_DIM`\n- `SAS_VA_VIEW.V_SRIA_STATUS_DIM`\n- `SAS_VA_VIEW.V_SRIA_SERV_PRODUCT_CAT_DIM`\n- `SAS_VA_VIEW.V_SRIA_PRIORITY_DIM`\n- `SAS_VA_VIEW.V_PROVIDER_DIM`\n- `SAS_VA_VIEW.V_def_NETWORK_DIM`\n- `SAS_VA_VIEW.V_def_DIVISION_DIM`\n- `SAS_VA_VIEW.V_SRIA_TYPE_DIM`\n- `SAS_VA_VIEW.V_SRIA_ACT_STATUS_DIM`\n- `SAS_VA_VIEW.V_SRIA_SUBAREA_DIM`\n- `SAS_VA_VIEW.V_POSITION_DIM`\n- `SAS_VA_VIEW.V_CAUSE_DIM`\n- `SAS_VA_VIEW.V_ACTION_DIM`\n- `SAS_VA_VIEW.V_ADSL_DIM`\n- `SAS_VA_VIEW.V_SRIA_AREA_DIM`\n## Hive - Impala Tables (target)\nThese are the target history tables in Hive and Impala, where data is written using LOAD DATA INPATH and REFRESH statements.\n- `dwhfixed.v_box_dim_hist`\n- `dwhfixed.v_fixed_cable_dim_hist`\n- `dwhfixed.v_kv_dim_hist`\n- `dwhfixed.v_dslam_dim_hist`\n- `dwhfixed.v_sr_type_dim_hist`\n- `dwhfixed.v_sria_addinfo_dim_hist`\n- `dwhfixed.v_sria_status_dim_hist`\n- `dwhfixed.v_sria_serv_product_cat_dim_hist`\n- `dwhfixed.v_sria_priority_dim_hist`\n- `dwhfixed.v_provider_dim_hist`\n- `dwhfixed.v_def_network_dim_hist`\n- `dwhfixed.v_def_division_dim_hist`\n- `dwhfixed.v_sria_type_dim_hist`\n- `dwhfixed.v_sria_act_status_dim_hist`\n- `dwhfixed.v_sria_subarea_dim_hist`\n- `dwhfixed.v_position_dim_hist`\n- `dwhfixed.v_cause_dim__hist`\n- `dwhfixed.v_action_dim_hist`\n- `dwhfixed.v_adsl_dim_hist`\n- `dwhfixed.v_sria_area_dim_hist`\n## Data Flow\nThe diagram below illustrates the end-to-end data movement from Oracle to Hive and Impala using Sqoop, Beeline, and Impala-shell.\n``` mermaid\n    graph TD\n        A1(<b>Oracle Table</b>)\n        A2(\"<b>HDFS</b> <br> /user/hive/warehouse/dwhfixed.db/landing_zone/{table_name}\")\n        A3(<b>Hive table</b>)\n        A4(\"<b>Impala table</b>\")\n\n        A1 --> |\"\"\"<b>sqoop: import</b> <br> \n                jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD <br>\n                user: dm_sas_va <br>\n                password file jceks://hdfs/user/dwhfixed/dwhfixed <br>\n                password-alias: dm_sas_va.pass\"\"\"| A2\n        A2 --> |\"<b>beeline: load data inpath</b>  <br>\n                 un-vip.bigdata.abc.gr:10000/default\"| A3\n        A3 --> |\"<b>impala-shell: refresh</b> <br>\n                 un-vip.bigdata.abc.gr\"| A4\n```\n## Logs\nLogs for each execution of the workflow can be viewed in the Hue interface under the dwhfixed user, or in HDFS log paths for historical runs.\nApplication logs can be found in each Workflow in Hue as user `dwhfixed`. Oozie Coordinator result can be seen into HUE (login as `dwhfixed` user) and go to `Jobs -> Workflows` and look for `DWHFixed - Full Workflow`\n## Monitoring messages\nThis section explains how monitoring tracks execution status for each table and component. Each job run has a unique executionId.\n- All monitoring messages have the following constant values:\\\n  **application=DWHFIXED**\\\n  **job=FULL**\n- All monitoring messages of the same execution have a **unique executionId**\n- Every component of one execution has a unique row that is updated between the following status values: RUNNING to SUCCESS or FAILED.\n```mermaid\n    graph TD\n        A1(\"component=MAIN_START_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A2(\"component=SQOOP_IMPORT_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A3(\"component=BEELINE_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A4(\"component=IMPALA_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A5(\"component=MAIN_END_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n\n        A1 --> A2\n        A2 --> A3\n        A3 --> A4\n        A4 --> A5\n```\n- Check monitoring app for successful executions:  \nFrom `un2` with personal account:\n``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=FULL$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'\n```\n- Check monitoring app for failed executions:  \nFrom `un2` with personal account:\n``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=FULL$status=FAILED&operativePartition=<timestamp e.g.:20220518>'\n```\n- Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n### Grafana dashboard\nA Grafana dashboard provides visual monitoring for DWHFixed executions and error trends. Use the link below to access it.\nGrafana: `https://unc1.bigdata.abc.gr:3000/d/DNM-sBo4z/dwhfixed-dashboard?orgId=1&from=now-2d&to=now`\n## Alerts (Mail)\nEmail alerts are triggered automatically in case of failure in any component — Oracle, Hive, Impala. The subject and body format is detailed below.\n**Subject**: `DWHFIXED - FULL: FAILED`  \n**Body**: \n- `${ORACLE_TABLE}: failed to perform Oracle query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to fetch data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: fetched no data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to execute beeline query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n`${ORACLE_TABLE}`: Can be any of the tables from section `1.1. Oracle (source)`  \n`${datetime_now}`: Current datetime in `%Y-%m-%d %H:%M` format\nThe application sends an email in each case of the following failures (for each table):\n### Oracle failure\n- Sqoop eval failure.\n- Sqoop import failure.\n- Sqoop has fetched no data.\nHow to check Oracle:\n```bash\nssh <username>@un2\nsh -l intra\njsec_file=jceks://hdfs/ez/intra.Sqoop.Creds.jceks\nsqoop eval \\\n-Dhadoop.security.credential.provider.path=${jsec_file} \\\n--password-alias dm_sas_va.pass \\\n--connect jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD \\\n--username dm_sas_va \\\n--query \"select * FROM SAS_VA_VIEW.V_DW_CONTROL_TABLE where 1=1\" ;\n```\nAlternative way of checking Oracle:\n```bash\nssh <username>@undt1\nsu -l intra\nexport ORACLE_HOME=/usr/lib/oracle/11.2/client64\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:${LD_LIBRARY_PATH}\nexport PATH=$ORACLE_HOME/bin:${PATH}\nexport SQLPATH=$ORACLE_HOME/lib:${SQLPATH}\nexport NLS_DATE_FORMAT='yyyy-mm-dd hh24:mi:ss'\nexport HISTSIZE=0\nsqlplus dm_sas_va/<Oracle-pass>@999.999.999.999:6644/DWHPRD\n```\n### Hive/Impala failure\n- Beeline load data inpath has failed.\n- Impala refresh has failed.\nHow to check Hive/Impala:\n- In the Hue UI mentioned above, click on editor -> Hive/Impala\n- Run the following query on each:`select * from dwhfixed.v_sria_subarea_dim_hist limit 3;`\n### Actions\n//TODO\n# Delta Load\nThe delta load flow runs every 2 hours and loads only new partitions by consulting the control table in Oracle before running the main pipeline.\n- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - Delta Workflow`, \n- **COORDINATOR**: `DWHFixed - Delta Coordinator`\n- **HDFS path**: `/user/dwhfixed/delta`\n- **Runs**: `1:30,3:30,5:30,7:30,9:30,11:30,13:30,15:30,19:30,21:30,23:30 (UTC)`\n- **Config file**: `hdfs: /user/dwhfixed/dwhfixed.config`\n- **Tables config/list**: `hdfs: /user/dwhfixed/delta/tables_delta.config`\n- **Oracle user**: `dm_sas_va`\n- **Oracle pass**: `https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx`. You need password to open this file and get the user's password.\nRuns every 2h, checks control table: `SAS_VA_VIEW.V_DW_CONTROL_TABLE` in order to evaluate if a new partition has been added to the source tables. After that, it runs data flow.\n## Oracle (source)\n- `SAS_VA_VIEW.V_SRIA_SERVICE_REQUESTS_FCT`\n- `SAS_VA_VIEW.V_ACS_MODEMS_ACTIVE_FCT`\n- `SAS_VA_VIEW.V_LL_DIM`\n- `SAS_VA_VIEW.V_SRIA_INTERACT_ACTIVITY_FCT`\n- `SAS_VA_VIEW.V_DW_CONTROL_TABLE`\n- `SAS_VA_VIEW.V_FAULT_NTT_NETWORK_ELEM_FCT`\n- `SAS_VA_VIEW.V_SRIA_SITE_DIM`\n- `SAS_VA_VIEW.V_SR_AFF_CUST_FCT`\n## Hive - Impala (target)\n- `dwhfixed.v_sria_service_requests_fct_hist`\n- `dwhfixed.v_acs_modems_active_fct_hist`\n- `dwhfixed.v_ll_dim_hist`\n- `dwhfixed.v_sria_interact_activity_fct_hist`\n- `dwhfixed.v_fault_ntt_network_elem_fct_hist`\n- `dwhfixed.v_sria_site_dim_hist`\n- `dwhfixed.v_sr_aff_cust_fct_hist`\n## Data Flow\n``` mermaid\n    graph TD\n        A1(<b>Oracle Table</b>)\n        A2(\"<b>HDFS</b> <br> /user/hive/warehouse/dwhfixed.db/landing_zone/{table_name}\")\n        A3(<b>Hive table</b>)\n        A4(\"<b>Impala table</b>\")\n\n        A1 --> |\"\"\"<b>sqoop: import</b> <br> \n                jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD <br>\n                user: dm_sas_va <br>\n                password file jceks://hdfs/user/dwhfixed/dwhfixed <br>\n                password-alias: dm_sas_va.pass\"\"\"| A2\n        A2 --> |\"<b>beeline: load data inpath</b>  <br>\n                 un-vip.bigdata.abc.gr:10000/default\"| A3\n        A3 --> |\"<b>impala-shell: refresh</b> <br>\n                 un-vip.bigdata.abc.gr\"| A4\n```\n## Logs\nApplication logs can be found in each Workflow in Hue as user `dwhfixed`. Oozie Coordinator result can be seen into HUE (login as `dwhfixed` user) and go to `Jobs -> Workflows` and look for `DWHFixed - Delta Workflow`\n## Monitoring messages\n- All monitoring messages have the following constant values:\\\n  **application=DWHFIXED**\\\n  **job=DELTA**\n- All monitoring messages of the same execution have a **unique executionId**\n- Every component of one execution has a unique row that is updated between the following status values: RUNNING to SUCCESS or FAILED.\n```mermaid\n    graph TD\n        A1(\"component=MAIN_START_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A2(\"component=SQOOP_IMPORT_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A3(\"component=BEELINE_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A4(\"component=IMPALA_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n        A5(\"component=MAIN_END_`ORACLE_TABLE`, operativePartition=`par_dt`, status=SUCCESS/FAILED\", param0='ORACLE_TABLE')\n\n        A1 --> A2\n        A2 --> A3\n        A3 --> A4\n        A4 --> A5\n```\n- Check monitoring app for successful executions:  \nFrom `un2` with personal account:\n``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=DELTA$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'\n```\n- Check monitoring app for failed executions:  \nFrom `un2` with personal account:\n``` bash\ncurl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=DWHFIXED&job=DELTA$status=FAILED&operativePartition=<timestamp e.g.:20220518>'\n```\n- Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n### Grafana dashboard\nGrafana: `https://unc1.bigdata.abc.gr:3000/d/DNM-sBo4z/dwhfixed-dashboard?orgId=1&from=now-2d&to=now`\n## Alerts (Mail)\n**Subject**: `DWHFIXED - DELTA: FAILED`  \n**Body**: \n- `${ORACLE_TABLE}: failed to perform Oracle query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to fetch data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: fetched no data through Sqoop.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: failed to execute beeline query.\\nTime: ${datetime_now}`\n- `${ORACLE_TABLE}: to connect or execute impala-shell query.\\nTime: ${datetime_now}`\n`${ORACLE_TABLE}`: Can be any of the tables from section `1.1. Oracle (source)`  \n`${datetime_now}`: Current datetime in `%Y-%m-%d %H:%M` format\nThe application sends an email in each case of the following failures (for each table):\n### Oracle failure\n- Sqoop eval failure.\n- Sqoop import failure.\n- Sqoop has fetched no data.\nHow to check Oracle:\n```bash\nssh <username>@un2\nsh -l intra\njsec_file=jceks://hdfs/ez/intra.Sqoop.Creds.jceks\nsqoop eval \\\n-Dhadoop.security.credential.provider.path=${jsec_file} \\\n--password-alias dm_sas_va.pass \\\n--connect jdbc:oracle:thin:@999.999.999.999:6644/DWHPRD \\\n--username dm_sas_va \\\n--query \"select * FROM SAS_VA_VIEW.V_DW_CONTROL_TABLE where 1=1\" ;\n```\nAlternative way of checking Oracle:\n```bash\nssh <username>@undt1\nsu -l intra\nexport ORACLE_HOME=/usr/lib/oracle/11.2/client64\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:${LD_LIBRARY_PATH}\nexport PATH=$ORACLE_HOME/bin:${PATH}\nexport SQLPATH=$ORACLE_HOME/lib:${SQLPATH}\nexport NLS_DATE_FORMAT='yyyy-mm-dd hh24:mi:ss'\nexport HISTSIZE=0\nsqlplus dm_sas_va/<Oracle-pass>@999.999.999.999:6644/DWHPRD\n```\n### Hive/Impala failure\n- Beeline load data inpath has failed.\n- Impala refresh has failed.\nHow to check Hive/Impala:\n- In the Hue UI mentioned above, click on editor -> Hive/Impala\n- Run the following query on each:`select * from dwhfixed.v_sria_subarea_dim_hist limit 3;`\n### Actions\nIn case of any of alert, do nothing. The flow will try to re-run in 2 hours. If everything is OK, then it will load successfully the partitions. After 2-2:30 hours check the status of the next run. \nIf the error persists:\n- In case of infastructure error (ex. HDFS, Hive are down), do nothing.\n- In case of other error, contact BigData Developer team.\n# HDFS Log Files Retention \nA daily HDFS workflow archives execution logs for historical reference. Default retention is 9 days.\nArchiving of the log files produced in every run of the flow and store them in HDFS directory.  \nIt is useful for investigation in case of errors.  \nThe retention of the log files is configurable. Default value: 9 days  \n- **USER**: `dwhfixed`\n- **HUE WORKFLOW**: `DWHFixed - HDFS_Log_Retention_Workf`, \n- **COORDINATOR**: `DWHFixed - HDFS_Log_Retention_Coord`\n- **Runs**: `${coord:days(1)} 23:00 (Europe/Athens)`\n- **HDFS Retention path**: `/user/dwhfixed/HDFS_LOG_Retention`\n- **HDFS Log path**: `/user/dwhfixed/log`\n- **HDFS Log Files**: `DWHFIXED.*.Archive.tar.gz`\n# Useful Links\n[Home](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/dwhfixed/-/wikis/home)\n[Infrastructure](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/dwhfixed/-/wikis/Infrastructure)"
        },
        {
            "question": "Which scripts and configurations drive KPI export in the IPVPN-SM Replacement flow?",
            "answer": "The scripts like `initiate_export_CPU.sh` and `compute_metrics_via_sm_app.sh` in `/shared/abc/ip_vpn/sm-replacement/scripts` call a Spring Boot app configured via `/shared/abc/ip_vpn/sm-app/deployment/config/`, which executes Impala queries and exports data to the SQM server.",
            "category": "Data Management & Query Execution",
            "files": "ipvpn_sm_replacement.md",
            "context": "---\ntitle: IPVPN-SM Replacement ETL & App  \ndescription: Spring Boot-based application and ETL scripts that extract KPIs from BigStreamer and export them to the SQM server every 5 minutes. Replaces legacy IPVPN-SLA pipeline components for CPU, Memory, QoS, Availability, and Interface metrics.  \nsystem: BigStreamer  \ncomponent: IPVPN-SM  \njob_name: Export_IPVPN_KPIs_to_SQM  \nowner: ipvpn  \nsource_tables:\n  - bigcust.nnm_ipvpn_componentmetrics_hist\n  - bigcust.perf_interfacemetrics_ipvpn_hist\n  - bigcust.nnmcp_ipvpn_slametrics_hist\n  - nnmnps.nms_node\n  - bigcust.customer_pl\n  - bigcust.customer_sla_config_ipvpn\n  - bigcust.sla_configurations\ndestination_system: SQM Server  \ndestination_endpoint: /inventory/measurements  \napi_ingestion_endpoint: /ingest-query  \nschedule:\n  frequency: every 5 minutes  \n  retry_policy: up to 5 retries per interval  \ndeployment:\n  application_host_group:\n    - un1.bigdata.abc.gr\n    - un2.bigdata.abc.gr\n  haproxy_vip: un-vip.bigdata.abc.gr\n  haproxy_port: 13001\n  http_port: 13000\n  jmx_port: 13800\n  scripts_path: /shared/abc/ip_vpn/sm-replacement/scripts\n  config_path: /shared/abc/ip_vpn/sm-app/deployment/config\n  logs_path: /shared/abc/ip_vpn/sm-app/deployment/logs\nauthentication:\n  kerberos_keytab: /home/users/ipvpn/ipvpn.keytab\n  jaas_config: /shared/abc/ip_vpn/sm-app/deployment/config/jaas.conf\n  token_auth_keystore: /shared/abc/ip_vpn/sm-app/deployment/config/credentials.keystore\nmonitoring:\n  grafana_dashboard: https://unc1.bigdata.abc.gr:3000/d/HiuaKlU4z/ipvpn-sm-replacement-monitoring?orgId=1\n  mysql_monitoring_db_host: 999.999.999.999\n  mysql_user: monitoring\n  mysql_database: monitoring\nfailure_handling:\n  retries: 5\n  logging_paths:\n    - /shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*\n    - /shared/abc/ip_vpn/sm-app/deployment/logs/sm-server-requests.log\n    - /shared/abc/ip_vpn/sm-app/deployment/logs/application.log\n    - /shared/abc/ip_vpn/sm-app/deployment/logs/tomcat/access_log.log\nscripts:\n  orchestrators:\n    - initiate_export_CPU.sh\n    - initiate_export_MEM.sh\n    - initiate_export_QOS.sh\n    - initiate_export_AV.sh\n    - initiate_export_IF.sh\n  helpers:\n    - compute_metrics_via_sm_app.sh\n    - query_sm.sh\n    - sm-replacement-call-repeater.sh\ntags:\n  - ipvpn\n  - ipvpn-sm\n  - sqm\n  - bigstreamer\n  - kpi\n  - sla\n  - availability\n  - qos\n  - memory\n  - cpu\n  - interface\n  - impala\n  - spring boot\n  - haproxy\n  - kerberos\n  - token authentication\n  - export\n  - metrics\n  - monitoring\n  - failure handling\n  - retry\n  - ingestion\n  - flume\n  - hive\n  - bash scripts\n  - automation\n  - crontab\n  - 5-minute jobs\n  - grafana\n  - mysql\n---\n# Introduction\nOverview of the replacement flow that calculates and exports KPIs (CPU, Memory, QoS, etc.) from BigStreamer to SQM via IPVPN-SM app.\nThis support document outlines the steps that compose the IPVPN-SM replacement process that computes Key Performance Indicators (KPIs) regarding the network quality of abc. The ETL process involves a series of scripts and a Spring Boot application that work together to retrieve, transform, and load data from Impala tables into a remdef server.\nThere are three major KPI categories, *Components*, *SLA* and *Interfaces*, which consist of the following subcategories:\n|KPI category|Metrics involved| Relevant Impala tables|\n|---|---|---|\n|Components|CPU, Memory (MEM)|bigcust.nnm_ipvpn_componentmetrics_hist,<br>  nnmnps.nms_node |\n|Interfaces|Interfaces (IF)|bigcust.perf_interfacemetrics_ipvpn_hist,<br>  bigcust.pe_interfaces,<br>  nnmnps.nms_node|\n|SLA|Availability (AV), Quality of Service (QOS)|bigcust.nnmcp_ipvpn_slametrics_hist,<br> bigcust.customer_pl,<br> bigcust.customer_sla_config_ipvpn,<br> bigcust.sla_configurations|\n# Application Flow\nStep-by-step description of how the ETL pipeline is executed using scripts and Spring Boot app.\n## Scripts & IPVPN-SM App operations\nThe metrics computation of each of the three categories (sla, components, interfaces) is triggered by the `un2:/shared/abc/ip_vpn/run/initiate_export_*.sh` script that runs as part of the IPVPN-SLA [cronjob](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/blob/master/PROD/crontab/crontab_per_user.txt), scheduled to run every 5 minutes, and the computation refers to a full five-minute interval.\nThe ETL process follows the following flow:\n### IPVPN-SLA\n- [IPVPN-SLA Support Doc](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ip_vpn.md)\n### IPVPN-SM\n- [IPVPN-SM Dev Wiki](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/home)\n3. The `export_*_kpis.sh` calls the IPVPN-SM App to execute the Impala queries. The curl request to the IPVPN-SM App is executed asynchronously, allowing the IPVPN-SM pipeline to continue with other scheduled 5 min intervals without waiting for the query results.\n4. The IPVPN-SM App executes the Impala queries, applies transformations on the result set based on the agreed schema specifications, and sends the results to the SQM server.\n## IPVPN-SM Application\nThe IPVPN-SM Application executes the Impala queries specified in the `/shared/abc/ip_vpn/sm-app/deployment/config/field-mappings.yml`, applies transformations on the result set based on the agreed [schema specifications](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Data-Transmission-Schema) , and sends the results to the SQM server.\n### IPVPN-SM Endpoint: /ingest-query\nThe endpoint for the IPVPN-SM application is `/ingest-query`. This endpoint should be called with a POST request.\nDetails on the request body fields and examples [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Application-details#request-body-1)\n### Check application status\n```bash\ncurl --location --request GET 'un-vip:13001/ipvpn-sm/app/status'\n```\n## Application Flow Diagram\n```mermaid\nflowchart TD;\n  A(\"/shared/abc/ip_vpn/run/initiate_export_*.sh <br> ipvpn@un2\") --> B(\"/shared/abc/ip_vpn/run/export_*_kpis.sh <br> ipvpn@un2\")\n  B -- 1. Calls Sync ----> C(\"/shared/abc/ip_vpn/sm-replacement/scripts/refresh_*_kpis.sh <br> ipvpn@un2\")\n  B -- 2. Calls Async ---> I(\"/shared/abc/ip_vpn/sm-replacement/scripts/compute_metrics_via_sm_app.sh <br> ipvpn@un2\")--> J(\"/shared/abc/ip_vpn/sm-replacement/scripts/query_sm.sh <br> ipvpn@un2\")\n  J -- /ingest-query request--->IPVPN-SM_Replacement_Application\n  subgraph IPVPN-SM_Replacement_Application[\"root@un1/un2\"]\n\t E(Impala Queries)\n\t F(Data with agreed schema)\n  end\n  E -- data transformation --> F\n  F -- POST --> G[SQM Server]\n  style A fill:#F8F8F8\n  style B fill:#F8F8F8\n  style IPVPN-SM_Replacement_Application fill:#DCDCDC\n  style I fill:#DCDCDC\n  style C fill:#DCDCDC\n  style E fill:#DCDCDC\n  style F fill:#DCDCDC\n  style J fill:#DCDCDC\n  style G fill:darkgray\n```\n# Infrastructure\nStep-by-step description of how the ETL pipeline is executed using scripts and Spring Boot app.\nThe ETL pipeline infrastructure includes the following components:\n## Application\n- un1/un2 files\n\t- **Deployment path**: `/shared/abc/ip_vpn/sm-app/deployment`\n\t- **Configuration**: `/shared/abc/ip_vpn/sm-app/deployment/config`\n\t\t- [application.yml](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/blob/main/PROD/deployment/config/application.yml)\n\t- **Logs**: `/shared/abc/ip_vpn/sm-app/deployment/logs`\n- Data sources:\n\t- **Impala**:  `un-vip.bigdata.abc.gr:21050`\n\t- **hive database**: `bigcust`\n- Authentication:\n  The authentication to Impala is done with Kerberos. The files used are:\n    - `/etc/krb5.conf`\n    - `/shared/abc/ip_vpn/sm-app/deployment/config/jaas.conf`\n    - `/home/users/ipvpn/ipvpn.keytab`\n- IPVPN-SM app:\n  The application uses HAProxy, a load balancer that distributes incoming requests to the deployed application nodes.\n  - **HAProxy**:\n  Host: `un-vip` ,\n  Port: `13001`\n  - **Application**:\n    Hosts: `un1/un2`,\n    Active Ports:\n      - HTTP port : `13000`\n      - JMX port : `13800`\n- External SQM server:\n\t- **host**: `gw-prod-sqm.aksprd.abc.gr`\n\t- **port**: `443`\n\t- **endpoint**: `/inventory/measurements`\n## Bash scripts\n| Script | Description | Location |\n|--------|-------------|----------|\n| `initiate_export_*.sh` | Triggers category-specific KPI exports | `/shared/abc/ip_vpn/run/` |\n| `compute_metrics_via_sm_app.sh` | Prepares & posts payload to SM App | `/shared/abc/ip_vpn/sm-replacement/scripts/` |\n| `query_sm.sh` | Constructs the payload for SM App | Same as above |\n| `sm-replacement-call-repeater.sh` | Manual runner across intervals | Same as above |\nList of helper scripts used by the SLA cronjob and their role in preparing and triggering requests.\nIn addition to the IPVPN-SM application, IPVPN-SM utilizes certain bash scripts. Their primary purpose is to generate the appropriate input data that is subsequently passed as the body parameter in the POST requests to the IPVPN-SM application.\nThey are invoked by the IPVPN-SLA `export_*_kpis.sh` [scripts](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/blob/master/PROD/nnmnps/bin/export_component_kpis.sh).\n- **Node**: `un2`\n- **User**: `ipvpn`\n- **Scripts**: `/shared/abc/ip_vpn/sm-replacement/scripts`\n- **IPVPN-SM App host & port configured in file** : `/shared/abc/ip_vpn/sm-replacement/scripts/config.properties`\n- **Logs**: `/shared/abc/ip_vpn/sm-replacement/log`\nThe specific order in which they are called is the following:\n```mermaid\ngraph LR\nA[\"export_*_kpis.sh<br><i>IPVPN-SLA</i>\"] -->B[compute_metrics_via_sm_app.sh]\nB -->C[query_sm.sh]\nC --/ingest-query-->D(SM Application)\nstyle A fill:darkgray\nstyle B fill:#ECECEC\nstyle C fill:#ECECEC\n```\n## Deployment Instructions\nStop/Start guide on [abc_BigStreamer_Apps_MoP_Restart.docx](https://ghi.sharepoint.com/:w:/t/abcBigData/EZsK97GxFyVHkeBoFTFjuRYBtPF7lK86cTA44wMhayNlNA?e=Gjh0ZO)  \nCheck project wiki [here](https://metis.ghi.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpn-sm-devops/-/wikis/Application-details#deployment-instructions)\n## SQM Token Authentication\nThe authentication on the SQM server is done with the HTTP Token Auth in IPVPN-SM application. It requires providing the keystore file, where the token is stored.\n# Failure Handling\nHow to debug failed requests, interpret logs, and what to check first when something breaks.\n## Logs\n### Failure Log Locations\n#### Script Logs on un2\nThe scripts which refresh the Impala tables and which call the IPVPN-SM application write logs under the directories:\n1. The `refresh_*_.sh script` Logs\n`/shared/abc/ip_vpn/sm-replacement/log/refresh_*_kpis`\n2. The `compute_metrics_via_sm_app.sh` Logs\n`/shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*`\nThe asterisk is used to dendef the type of the particular category.\n#### Application Logs on un1/un2\nIn case of failure, the logs related to the IPVPN-SM app are the following:\n1. **Access Logs**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/tomcat/access_log.log`\n2. **Application Logs**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/application.log`\n3. **Requests sent to SQM Server**\n`/shared/abc/ip_vpn/sm-app/deployment/logs/sm-server-requests.log`\n#### IPVPN-SLA Logs on `un2`\nAside from the above logs, the operation of the IPVPN-SLA scripts is also monitored by the original implementation. Details about the logging of IPVPN-SLA can be found [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/ip_vpn.md)\n### Auto-retry mechanism\nDuring the schedules runs, if the call to the IPVPN-SM app fails, the call to the application is repeated. If the application fails on all 5 attemps, the script exits. On each failed attempt, an email alert is sent via the monitoring app. This operation is done inside the `un2:/shared/abc/ip_vpn/sm-replacement/scripts/compute_metrics_via_sm_app.sh` script.\n# Support\nWhere to check system health (Grafana, MySQL), common exceptions and their causes, and support scripts.\n## Check request status via Monitoring\n### Visual Dashboards (Grafana)\nCheck grafana dashboard for failed requests:\n`https://unc1.bigdata.abc.gr:3000/d/HiuaKlU4z/ipvpn-sm-replacement-monitoring?orgId=1&refresh=5s`\n### Monitoring DataBase (MySQL)\nIPVPN-SM App uses Monitoring App in order to record statistics regarding the status of each metric computation request. Querying monitoring DB allows us to investigate pdefntial failed requests.\n1. Connect to monitoring DB\n`mysql -umonitoring -p -h 999.999.999.999`\n2. `use monitoring;`\n#### See failed requests in the past 24h excluding EmptyQuery Exceptions\nOften persistent EmptyQuery alerts demand further investigation in the CustomPoller component of IPVPN-SLA application and the associated metrics cannot be retrieved from the Impala tables.   \nTo identify failed metrics in the past 24h due to errors other than EmptyQuery exceptions, we can use the following query\n```sql\nmysql> select distinct x.job, x.5min_interval_ts, x.sent_to_SQM, y.request_repetitions, x.exception from ( select d.job, DATE_FORMAT( STR_TO_DATE( operative_partition, '%Y%m%d%H%i%s' ), '%Y-%m-%d %H:%i' ) as 5min_interval_ts, c.operative_partition, c.status AS 'sent_to_S\nQM', DATE_FORMAT( metric_ts_start, '%Y-%m-%d %H:%i:%s' ) as metric_ts_start, DATE_FORMAT( c.system_ts, '%Y-%m-%d %H:%i:%s' ) as metric_ts_end, SUBSTRING_INDEX( c.message, 'com.jkl.bigstreamer.abc.ipvpn.sm.exception.', -1 ) as exception from ( SELECT job, compone\nnt as start_comp, status as started, operative_partition as start_oppart, system_ts as metric_ts_start FROM jobstatus WHERE application = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_START' and message = 'n_attempt=0' order by system_ts desc,\njob ) d inner join ( select b.* from ( SELECT MAX(system_ts) as max_ts FROM jobstatus WHERE application = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_END' group by operative_partition, job ) a inner join ( SELECT * FROM jobstatus WHERE applic\nation = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_END' ) b on a.max_ts = b.system_ts ) c on d.job = c.job and d.start_oppart = c.operative_partition order by 5min_interval_ts desc ) x left join ( select count(*)-1 as 'request_repetitions',\njob, operative_partition from jobstatus where application = \"IPVPN-SM\" and system_ts >= now() - INTERVAL 1 DAY and component = 'MAIN_START' group by operative_partition, job ) y on x.operative_partition = y.operative_partition and x.job = y.job where sent_to_SQM=\"FAILED\"\n and SUBSTRING_INDEX( exception, ':', 1 )!=\"AppEmptyQueryException\" order by 5min_interval_ts desc;\n\n+-----+------------------+-------------+---------------------+-------------------+\n| job | 5min_interval_ts | sent_to_SQM | request_repetitions | exception         |\n+-----+------------------+-------------+---------------------+-------------------+\n| AV  | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| CPU | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| MEM | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| IF  | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| QOS | 2023-05-17 13:30 | FAILED      |                   2 | SMSystemException |\n| AV  | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| CPU | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| IF  | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| MEM | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n| QOS | 2023-05-17 13:25 | FAILED      |                   2 | SMSystemException |\n+-----+------------------+-------------+---------------------+-------------------+\n\n```\nThese are the requests that should be manually handled following the actions described next.\n#### Inspect specific metric request\nIn case of alerts, we should first check the status of the particular requests on the monitoring DB.\nUsing the metric category and the reference interval associated with a request to IPVPN-SM App we can query the Monitoring DB\nby passing the corresponding job and operative_partition values.\n- `select application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\"IPVPN-SM\" and job=\"<category>\" and operative_partition=\"<YYYYMMDDhhmmss>\" order by system_ts;`\nFor example. if we want to check the status of a request for QOS metric category and `2023-05-10 06:55:00` reference interval:\n```sql\nmysql> select application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\"IPVPN-SM\" and job=\"QOS\" and operative_partition=\"20230510065500\" order by system_ts;\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\n| application | job | component  | operative_partition | status  | message                                                                    | system_ts               |\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\n| IPVPN-SM    | QOS | MAIN_START | 20230510065500      | SUCCESS | n_attempt=0                                                                | 2023-05-10 07:04:51.000 |\n| IPVPN-SM    | QOS | MAIN_END   | 20230510065500      | FAILED  | com.jkl.bigstreamer.abc.ipvpn.sm.exception.AppEmptyQueryException | 2023-05-10 07:05:02.000 |\n| IPVPN-SM    | QOS | MAIN_START | 20230510065500      | SUCCESS | n_attempt=1                                                                | 2023-05-10 07:05:22.000 |\n| IPVPN-SM    | QOS | MAIN_END   | 20230510065500      | SUCCESS |                                                                            | 2023-05-10 07:05:35.000 |\n+-------------+-----+------------+---------------------+---------+----------------------------------------------------------------------------+-------------------------+\n4 rows in set (0.01 sec)\n```\nAccording to the above results, we conclude that the computation of the QOS metrics for the selected 5min interval failed on the first attempt with AppEmptyQueryException error. However the request was successful on the second attempt.\n#### Inspect overall requests' status\nIf we want to check if any of the requests has failed, for instance in the past 24h\n```\nselect application, job,component, operative_partition, status, message, system_ts from jobstatus where application=\"IPVPN-SM\" and component=\"MAIN_END\" and status=\"FAILED\" and system_ts >= now() - INTERVAL 1 DAY;\n```\n##  Pdefntial Error Cases\n### AppEmptyQueryException (Impala Query Returned No Data)\nThis exception occurs when the executed Impala query fails. The underlying reason behind this message can be one the following:\n1. The query did not retrieve any records for the `specified 5min interval`. For category **IF**, this type of failure of a 5min interval may cause the failure of `the following 5min interval` as well. This occurs because an aggregation between two consecutive intervals is performed. \n2. Inability to open one of the HDFS files that constitute a table associated with the particular Impala query.\n3. Unreachable Impala deamon.\n4. Inability to allocate Memory (OutOfMemoryError: unable to create new native thread).\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\nIf the `bigcust` tables are empty, manually adding metrics won't be effective. In such cases, it's crucial to conduct a detailed investigation to pinpoint the specific step within the IPVPN-SLA process where the failure occurred. Such an investigation is documented [here](../procedures/ipvpn_sm_AppEmptyQuery_resolution_MoP.md)\n### SMSystemException (SQM Ingestion Failure)\nThe above exception is solely due to SQM server's inability to ingest the information sent by IPVPN-SM App.\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n### SMValidationException (Schema Mismatch)\nThis occurs in the case when the schema of the data sent does not comply with the agreed schema.\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n### AppQueryIngestionException (Transformation Failure)\nThis can occur due to inability to apply the particular transformation on the retrieved data.\n**Support Action:** [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n### SMAuthException (Token Auth Failure)\nThis occurs if the token used as basic HTTP authentication to the SQM endpoint is rejected.\n**Support Actions:**\n1. [Update keystore with the new credentials](#update-http-auth-token)\n2. [Add metrics manually](#call-the-ipvpn-sm-app-manually-on-un2)\n## Actions\n### Update HTTP Auth Token\n1. Create keystore\n```\nkeytool -importpass -alias sm_token -storetype JCEKS -validity 365000 -keystore credentials.keystore\n```\n2. Enter <keystore_pw>\n2. Save the given credentials in the format: `username:password`\n#### Instructions to update the file if given new credentials\n1. Update keystore\n`keytool -keypasswd -keystore credentials.keystore -alias sm_token`\n2. Enter <keystore_pw>\n3. Enter new credentials in the format: `username:password`\n### Manual Retry: sm-replacement-call-repeater.sh {#manual-call}\nIn case we want to bypass the scripts running on un2 we can post a request directly on the IPVPN-SM application, either by performing a curl request specifying the [request body](#request-body) details or by running the script `/shared/abc/ip_vpn/sm-replacement/scripts/sm-replacement-call-repeater.sh` providing the arguments:\n- `category`, one the valid category types {AV,QOS,CPU,MEM,IF}\n- `reference-start-time` in the format yyyyMMddHHmm\n- `reference-end-time` in the format yyyyMMddHHmm (inclusive)\nThe script will successively call the `refresh_*_.sh` and  `compute_metrics_via_sm_app.sh` scripts for the given category and the period specified by the end and start arguments.\n1. In case we want perform the execution for just **one 5m interval**, we provide the same argument for `reference-start-time` and `reference-end-time` fields. E.g. for 5m period of 18/12/2022 10:10:\n```\n./sm-replacement-call-repeater.sh CPU 202212181010 202212181010\n```\n2. If we want to perform requests for **5 multiple consecutive** 5min intervals E.g. for 5m periods of 18/12/2022 10:10 to 10:30 (inclusive):\n```\n./sm-replacement-call-repeater.sh CPU 202212181010 202212181030\n```\nThe logs of this operation are appended to the log files in `/shared/abc/ip_vpn/sm-replacement/log/compute_via_sm_app_*`."
        },
        {
            "question": "How does the Prometheus ETL flow handle daily ingestion from Oracle to Hive?",
            "answer": "Prometheus runs a daily Oozie workflow at 06:30 UTC that extracts data from Oracle table `DWSRC.DWH22` using Sqoop, stages it in HDFS at `/ez/warehouse/prometheus.db/tmp_sqoop_jobs/`, and loads it into Hive table `prometheus.dwh22`. Afterwards, an Impala `REFRESH` is issued to reflect the new data.",
            "category": "Application Functionality & Flow",
            "files": "prometheus.md",
            "context": "---\ntitle: Prometheus Oracle to Hive ETL Flow\nsystem: BigStreamer\ncomponent: Prometheus\njob_name: Prometheus-Import-Workflow\nsource_system: Oracle\nsource_tables:\n  - DWSRC.DWH22\ndestination_system: Hive\ndestination_tables:\n  - prometheus.dwh22\nschedule: daily at 06:30 UTC\ncoordinator: Prometheus-Coordinator\nworkflow: Prometheus-Import-Workflow\nscript_path: /user/prometheus/flows\nmonitoring_table: monitoring.jobstatus\nowner: prometheus\ntags:\n  - Prometheus\n  - Oracle to Hive\n  - ETL\n  - BigStreamer\n  - Monitoring\n  - Oozie\n  - Impala\n  - Workflow Troubleshooting\n  - Partition Drop\n  - Grafana\n---\n# Prometheus\nThis document describes the Prometheus ETL flow that extracts data from Oracle table DWSRC.DWH22 into the Hive table prometheus.dwh22 using a daily Oozie workflow. It includes scheduling details, partition management, and troubleshooting guidelines in case of failures.\n## Useful Links\nReferences to internal documentation related to infrastructure and monitoring of the Prometheus ETL flow.\n[Infrastructure](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/Infastructure)  \n[Monitoring](https://metis.ghi.com/obss/bigdata/abc/etl/prometheus/prometheus-devops/-/wikis/home#monitoring)  \n## Oozie workflow / ETL Flow: Oracle to Hive\nDescription of the Oracle-to-Hive import flow, configuration details, and how to monitor and troubleshoot job execution.\n``` mermaid\n  graph TD\n    A[Oracle DB table DWSRC.DWH22  <br> via Port Forward 999.999.999.999:6634] -->|Sqoop Import| B[HDFS Staging Directory]\n    B -->|Hive Load| C[Hive: prometheus.dwh22]\n    C -->|Impala Refresh| D[Impala: prometheus.dwh22]\n```\nRuns every day at `06:30 AM UTC`\n**User**: `prometheus`  \n**Coordinator**: `Prometheus-Coordinator`  \n**Workflow**: `Prometheus-Import-Workflow`  \n**Source Database**:  \n- **Host**: `999.999.999.999`  \n- **Port**: `1521`  \n- **SID**: `A7`\n- **User**: `bigstreamer`  \n**Target Table**: `prometheus.dwh22`  \n**HDFS Installation Directory**: `/user/prometheus/flows`  \n**HDFS Staging Directory**: `/ez/warehouse/prometheus.db/tmp_sqoop_jobs/`\n**Alerts**:\n- Mail with subject: `Prometheus Flow failed`\n**Troubleshooting Steps**:\n- Check messages written to Monitoring App\n    - Check monitoring app for successful executions:  \n        - From `un2` with personal account:\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=SUCCESS&operativePartition=<timestamp e.g.:20220518>'`\n    - Check monitoring app for failed executions:  \n        - From `un2` with personal account:\n        - `curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=PROMETHEUS$status=FAILED&operativePartition=<timestamp e.g.:20220518>'`\n    - Get all the available fields [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n    - Grafana link: `https://unc1.bigdata.abc.gr:3000/d/PcKYyfTVz/prometheus-dashboard?orgId=1&from=now-2d&to=now`\n- Check if partition is loaded:\n  From `Hue` as `prometheus` in `Impala Editor`:\n  ``` sql\n  SHOW PARTITIONS prometheus.dwh22;\n  SELECT COUNT(*) FROM prometheus.dwh22 WHERE par_dt='<par_dt>';\n  ```\n- Check logs for failed steps:  \n  From `Hue` as `prometheus` in `Workflows`:\n  - Search for `Prometheus-Import-Workflow` and filter for failed\n  - Go to logs and check both stdout and stderr\n- In case a partition has partially been inserted into the final table `prometheus.dwh22` and the error that caused the failure has been resolved:\n  From `Hue` as `prometheus` in `Impala Editor`:\n    ``` sql\n    ALTER TABLE prometheus.dwh22 DROP IF EXISTS PARTITION (par_dt='<par_dt>');\n    ```\n  - For the previous day:\n    From `Hue` as `prometheus` in `Workflows`:\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n  - For the previous day:\n    From `Hue` as `prometheus` in `Workflows`:\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n  - For partitions older than yesterday:\n    From `Hue` as `prometheus` in `File Browser`:\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and set `days_back` to the number of days back needed to reach the partition\n    From `Hue` as `prometheus` in `Workflows`:\n    - Search for `Prometheus-Import-Workflow` and filter for `failed`\n    - Re-run it\n    From `Hue` as `prometheus` in `File Browser`:\n    - Edit `/user/prometheus/flows/config/settings_prod.ini` and restore `days_back` to `1`"
        },
        {
            "question": "What should I do if Radius fails to rename a file after processing?",
            "answer": "Check for alerts in emails with subjects like `Could not rename file`. Then verify the HDFS file list `/user/radius/unrenamed_files` and reprocess or manually rename the affected files.",
            "category": "Troubleshooting & Issue Resolution",
            "files": "radius.md",
            "context": "---\ntitle: Radius ETL Flow and Kudu Housekeeping\nsystem: BigStreamer\ncomponent: Radius\njob_name: Radius_Load_Workflow\nsource_system: SFTP Server (prdts)\nsource_tables:\n  - radarchive CSV files\n  - radacct CSV files\ndestination_system: Hive & Kudu\ndestination_tables:\n  - radius.radarchive\n  - radius.radacct\n  - radius.radreference\nschedule: every 1h and 30min\ncoordinator: Radius_Load_Coordinator\nworkflow: Radius_Load_Workflow\nscript_path: /user/radius\nmonitoring_table: monitoring.jobstatus\nowner: radius\ntags:\n  - Radius\n  - ETL\n  - radarchive\n  - radacct\n  - Kudu\n  - Hive\n  - BigStreamer\n  - SFTP\n  - Monitoring\n  - Oozie\n  - Retention\n  - Impala Stats\n  - Workflow Troubleshooting\n---\n# Radius\nThe Radius ETL process ingests user accounting data and session logs from the Trustcenter SFTP server. It transforms and loads the files into Hive and Kudu tables through hourly workflows and maintains data freshness using regular housekeeping and statistics recomputation.\n## Main Flow\nThis section describes the main ETL pipeline of the Radius flow, which loads and processes radarchive and radacct CSVs from an external SFTP server into Hive and Kudu tables.\n``` mermaid\n    graph TD\n    subgraph AA[Startup]\n      direction TB\n      AA1(\"Cleanup HDFS Folder:\" /ez/warehouse/radius.db/tmp)-->\n      AA2(\"Create Local Folder:\" ./sftp_files)-->\n      AA3(\"Create Local Folder:\" ./exported_files)-->\n      AA4(\"Housekeep Trustcenter SFTP server\")\n    end\n    subgraph AB[Load]\n      AB1(\"SFTP Server\") -->|Excluding:<br>Non-valid filenames<br>Files with suffix .LOADED<br>Files older than `days_back` according to filename | AB2(\"Temporary Directory on Nodemanager: ./sftp_files\")\n      AB1 -->|Add suffix .LOADED to<br>filenames in HDFS file:<br>/user/radius/unrenamed_files|AB1\n      AB2 -->|Decompress| AB3(\"Temporary Directory on Nodemanager: ./sftp_files\")\n    end\n    subgraph AC[radarchive]\n      AC1(\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\") -->|Hive LOAD|AC2(\"Hive: radius.radarchive_stg\")\n      AC2 -->|Refresh/Impala Insert|AC3(\"Impala: radius.radarchive\")\n      AC2 -->|Refresh/Impala Upsert<br>Reduced fields|AC4(\"Kudu: radius.radreference\")\n    end\n    subgraph AD[radacct]\n      subgraph ADA[File Export]\n        direction TB\n        AD1(\"HDFS Staging Directory: /ez/warehouse/radius.db/tmp\") -->|Hive LOAD|AD2(\"Hive: radius.radacct_stg\")\n        AD6(\"npce.fixed_super_repo<br>Responsibility: abc\") -->AD8\n        AD7(\"demo.dummy_radius_dslams<br>Responsibility: abc\") -->AD8\n        AD2 --> AD8(\"Join\")\n        AD8 -->|Refresh/Impala Select|AD9(\"Temporary Directory on Nodemanager:<br> ./exported_files/radacct_enriched_yearMonthDay_startHour-endHour.csv\")\n        AD9 -->|SFTP Put|AD10(\"Trustcenter SFTP\")\n      end\n      subgraph ADB[Populate Table]\n        direction TB\n        AD3(\"Kudu: radius.radreference\") -->AD4\n        AD11(\"Hive: radius.radacct_stg\") -->AD4(\"Join on username\")\n        AD4 -->|Refresh/Impala Insert|AD5(\"Impala: radius.radacct\")\n      end\n      AD10 -.->|On successful SFTP PUT<br>Populate the table| AD3\n    end\n    subgraph AE[Finish]\n      AE1(\"SFTP Server: Add suffix .LOADED to file\")\n    end\n    AA4 --> AB1\n    AB3 -->|Filename: radarchive_yearMonthDay_startHour-endHour.csv|AC1\n    AB3 -->|Filename: radacct_yearMonthDay_startHour-endHour.csv|AD1\n    AC4 -->|On Success|AE1\n    AD5 -->|On Success|AE1\n```\n- **User**: `radius`  \n- **Coordinator**: `Radius_Load_Coordinator`  \n- **Workflow**: `Radius_Load_Workflow`\n- **HDFS path**: `/user/radius`\n- **Runs**: `every 1h and 30mins`\n- **Config file**: `HDFS: /user/radius/config/settings_prod.ini`\n- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n- **Input SFTP Server**:\n  - **Host**: `999.999.999.999`\n  - **Port**: `22`\n  - **User**: `prdts`\n  - **Remdef Files Folder**: `/home/prdts/transfer`\n  - **Port Forward**:\n    - **Host**: `un-vip.bigdata.abc.gr`\n    - **Port**: `2222`\n- **Trustcenter SFTP Server**:\n  - **Host**: `unc2.bigdata.abc.gr`\n  - **Port**: `22`\n  - **User**: `trustcenterftp`\n  - **Remdef Files Folder**: `/rd`\n**Alerts**:\n- Mail\n  - Subject: Radius Flow failed\n  - Alerts that indicate problem with Input SFTP server:\n    - Body starts with: `No upcoming files for more than 3h`\n    - Body starts with: `Files found with a late timestamp.`\n    - Body starts with: `Could not rename file`\n  - Alerts that indicate problem with Trustcenter SFTP Server:\n    - Body starts with: `Could not perform radacct_enriched housekeeping on sftp server`\n  - Alerts that indicate general failures without specific cause:\n    - Body starts with: `Insert data failed` and then lists the status for each file\n**Troubleshooting Steps**:\n- If an alert has been received, use the message from the e-mail to determine the root cause for the failure.\n  - For failures with Input SFTP server:\n    - Inform abc in order to check the Input SFTP Server\n    - If abc does not detect any problems, check connectivity:\n      From `un2.bigdata.abc.gr` with personal user:\n      ``` bash\n      su - radius\n      sftp prdts@999.999.999.999\n      # Check for files\n      sftp> ls -l\n      ```\n  - For failures with Trustcenter SFTP Server:\n    - Check `unc2.bigdata.abc.gr` for any errors\n- If no alerts have been received or the problem is not specified determine the root cause for the failure with the following steps:\n  - Check for failed executions\n    From `un2.bigdata.abc.gr` with personal user:\n    ```bash\n    curl --location --request GET 'http://un-vip.bigdata.abc.gr:12800/monitoring/api/jobstatus/find?application=RADIUS&status=FAILED&operativePartition=<date in YYYYMMDD e.g.:20220518>'\n    ```\n    For additional query parameters check [here](https://metis.ghi.com/obss/bigdata/common-dev/apps/monitoring/monitoring-devops/-/wikis/API-Functional-Spec#fields)\n    or check via the corresponding [Grafana dashboard](https://unc1.bigdata.abc.gr:3000/d/J4KPyBoVk/radius-dashboard?orgId=1&from=now-2d&to=now)\n  - Check the logs for the failed execution.\n- After the underlying problem has been corrected the next execution of the flow will load any files that have been delayed or failed to be loaded in a previous execution.\n- If the customer requests to load specific files after they have been processed by the flow. **In order to avoid duplicate data we have to reload the whole partition.**\n  - Ensure the files are still available on the SFTP server\n    From `un2.bigdata.abc.gr` with personal user:\n    ``` bash\n    su - radius\n    sftp prdts@999.999.999.999\n    # Check for files covering the whole date affected\n    sftp> ls -l *.LOADED\n    ```\n  - Suspend coordinator `Radius_Load_Coordinator`\n  - Drop partitions that were not properly loaded from following tables\n    From `un2.bigdata.abc.gr` with personal user:\n    ``` bash\n    su - radius\n    kinit -kt /home/users/radius/radius.keytab radius\n    impala-shell -i un-vip.bigdata.abc.gr -k --ssl\n    ```\n    - In case of radarchive category of file:\n      ```sql\n      alter table radius.radarchive drop partitions (par_dt=\"<date in YYYYMMDD e.g.: 20220915>\")\n      ```\n    - In case of radacct category of file:\n      ```sql\n      alter table radius.radacct drop partitions (par_dt=\"<date in YYYYMMDD e.g.: 20220915>\")\n      ```\n  - Connect to SFTP server and rename files with filename timestamp equal to above-mentioned partition(s) and the same category:\n    From `un2.bigdata.abc.gr` with personal user:\n    ``` bash\n    su - radius\n    sftp prdts@999.999.999.999\n    # For every file you need to reload\n    sftp> rename <filename>.LOADED <filename>\n    ```\n  - Resume coordinator `Radius_Load_Coordinator`\n## Kudu Housekeeping and Compute Statistics Flow\nThis secondary flow enforces retention on the Kudu table radius.radreference and recomputes Impala statistics for radarchive and radacct daily.\n```mermaid\n  graph TD\n  A[Apply 5-day rention to<br>Kudu: radius.radreference]\n  B[Compute Impala Statistics for<br>radius.radarchive<br>radius.radacct]\n```\n- **User**: `radius`  \n- **Coordinator**: `Radius_Kudu_Retention_Coordinator` \n- **Workflow**: `Radius_Kudu_Retention_Workflow`\n- **HDFS path**: `/user/radius`\n- **Runs**: `once a day at 2:25 (UTC)`\n- **Config file**: `hdfs: /user/radius/Kudu_Retention_And_Compute_Stats/settings.ini`\n- **Logs**: From Hue go to `Job Browser -> Workflows` and filter with the workflow name\n**Alerts**:\n- Not monitored\n**Troubleshooting Steps**:\n- Check the logs for the failed execution.\n- After the underlying problem has been corrected the next execution of the flow will apply the correct retention to the Kudu table and compute the missing statistics."
        },
        {
            "question": "How is the TeMIP Wildfly application deployed and monitored on BigStreamer?",
            "answer": "The Java-based TeMIP app runs on Wildfly under `/opt/wf_cdef_temip/` and logs to `/opt/wf_cdef_temip/standalone/log/`. It receives alarms and stores them in Kudu. Check `server.log` or use `temip-tailog` for real-time error tracking.",
            "category": "Infrastructure & Deployment",
            "files": "temip.md",
            "context": "---\ndocument_owner: \"Intra team\"\nsystems_involved:\n  - TeMIP\n  - BigStreamer\n  - Kudu\n  - Impala\n  - Wildfly\n  - Oozie\n  - Oracle\n  - Sqoop\nscheduling:\n  - oozie\ndata_sources:\n  - TeMIP server (live alarms)\n  - Oracle TeMIP DB (archived alarms)\ntarget_tables:\n  - temip.temip_kudu_active_alarms\n  - temip.temip_kudu_terminated_alarms\n  - temip.temip_kudu_historic_events\n  - temip.temip_impala_terminated_alarms\n  - temip.temip_impala_historic_events\n  - temip.temip_alert_table\nscripts_location:\n  - hdfs:/user/temip/\n  - /shared/abc/temip_oozie_production_scripts/\n  - /usr/icom/scripts/\nresponsible_users:\n  - temip\nmonitored: false\nalerts_handling: \"Email alerts if no alarms received (via Alert Mail flow)\"\nsummary: >\n  This document details the end-to-end flow for ingesting TeMIP alarms into BigStreamer, including the Java-based live feed, Oozie-scheduled data migration \n  from Kudu to Impala, alert notification checks, manual recovery procedures from Oracle in case of data loss, and administration of the Wildfly server.\n---\n# TeMIP\n## Overview\nThe `abc TeMIP alarms live feed to BigStreamer` application is a Java application hosted on a Wildfly application server. The objective of the application is to receive and store (in near real time) the TeMIP alarms (from specific TeMIP Operation Contexts) into the BigStreamer™ ecosystem. The `Apache Kudu` storage engine was selected in order to achieve near real time CRUD operations (Create, Read, Update, Delete). The `Apache Impala` is used for extended data retention (6 months). The `Apache Oozie` scheduler  is used in order to automatically run the necessary scripts.\n- **Ndef:** All the needed **passwords** can be found [**here**](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/devpasswd.kdbx).\n## Flows\nThe `TeMIP Flow` consists of 4 components/flows:\n1. Initialization/Synchronization flow\n1. Main Application flow\n1. Move Kudu to Impala flow\n1. Alert Mail flow\n### Main Application\nHandles live ingestion of alarms from TeMIP server into Kudu tables via a Java app hosted on Wildfly.\nThe `Main Application Flow` contains our `TeMIP application` deployed to the `Wildfly Server` which receives the TeMIP alarms and stores them into Kudu tables.\n``` mermaid\n  flowchart TD\n  A[TeMIP Server] \n  B[Wildfly Server]\n  A --> |Sends TeMIP alarms| B\n  B --> |Stores TeMIP alarms| D[(Kudu Storage Engine)]\n  D --- E[Kudu: temip.temip_kudu_active_alarms]\n  D --- Z[Kudu: temip.temip_kudu_terminated_alarms]\n  D --- K[Kudu: temip.temip_kudu_historic_events]\n  style A fill: #45b39d\n```\n- **TeMIP Server**\n  - **Host:** `999.999.999.999`\n  - **Port:** `7180`\n- **Wildfly Server**\n  - **Servers:**\n    - `temip1 (999.999.999.999)` Standby Server\n    - `temip2 (999.999.999.999)` Active Server\n  - **User:** `temip`\n  - **Installation Path:** `/opt/wf_cdef_temip/`\n  - **Deployments Path:** `/opt/wf_cdef_temip/standalone/deployments`\n  - **Application Logs:** `/opt/wf_cdef_temip/standalone/log/server.log`\n  - **Access Logs:** `/opt/wf_cdef_temip/standalone/log/access.log`\n  - **Configuration:** `/opt/wf_cdef_temip/standalone/configuration/BigStreamer/config/`\n    - **File:** `temip.properties`\n**Alerts:**\n- **Mail executed by [Alert Mail](#alert-mail)**\n  - **Subject:** `\"[ Temip ] No alarms available.\"`\n  - **Body:** `\"There are no Temip alarms  available for the last hour. Corrective action may be needed.\"`\n**Troubleshooting Steps:**\n1. Check `logs` (application and access) with `temip-tailog` for any `ERROR` message that can occur.  \nIf TeMIP Server is running correctly, we should see lines like the following:  \n`INFO [com.jkl.bigstreamer.abc.temip.core.service.TemipService] (default task-173) Counter= 3064020, handle= 968, batchName= batch_2, timesRestartedFromLastSync= 1, aoExtractDataList size= 1`\n1. Check if `TeMIP Server` is up by executing `ping 999.999.999.999`.\n1. Contact a `TeMIP admin` to see if there are any server side related issues\n### Initialization/Synchronization\nEnsures communication between Wildfly and TeMIP server is established post-deployment via Oozie.\nThe `Initialization/Synchronization Flow` consists of an OOZIE Coordinator called `TeMIP_Synchronization_CO`. The coordinator is responsible for establishing the **connection** and **communication** of the `Wildfly Server` (containing our TeMIP Application) with the `TeMIP Server`.\nEvery time the `Main Application` is successfully deployed to `Wildfly Server` or gets restarted, this `coordinator` **must be run manually** to initiate the above procedure, it does not happen automatically. If the `Wildfly Server` is up and running, the `coordinator` executes on specific days of every month to perform maintenance tasks.\n``` mermaid\n  flowchart TD\n  A[OOZIE Server] -->|SHELL Action as user temip| B[un-vip.bigdata.abc.gr <br> User: temip]\n  B -->|REST message protocol| C[Main Application]\n  C <--> |SOAP message protocol| D[TeMIP Server]\n  style C fill: #45b39d\n```\n- **Oozie Coordinator**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_Synchronization_CO`\n    - **Execution:** `2,7,12,17,22,27 of every month at 03:00 local time`\n    - **Approximate Duration:** `45 minutes`\n    - **Workflow:** `TeMIP_Synchronization_WF`\n      - **Master Script:** `hdfs:/user/temip/100.TeMIP_Synchronization_Oozie_Main.sh`\n      - **Remdef Script:** `un-vip:/shared/abc/temip_oozie_production_scripts/101.temip_synchronization_Main.sh`\n      - **Server:** `un-vip.bigdata.abc.gr`\n      - **SSH User:** `temip`\n      - **Logs:** `un-vip:/shared/abc/temip_oozie_production_scripts/log/102.temip_synchronization.$(date '+%Y%m%d').log`\n**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**\n1. Check logs for any errors\n1. If workflow `TeMIP_Synchronization_WF` has been run manually, login to `Hue` with `temip` user `after 45 minutes` and execute the following `impala query` editor: `select * from temip.temip_kudu_configs`.  \nIt should return `15 rows`. If not, re run the `TeMIP_Synchronization_WF` workflow\n### Move Kudu to Impala\nDaily Oozie flow to offload older alarm data from Kudu to Impala for long-term storage.\nThe `Move Kudu to Impala` flow consists of a coordinator called `TeMIP_kudu_2_Impala_CO` which executes once a day and is responsible for moving the alarms from kudu to the equivalent impala table.\n``` mermaid\n flowchart TD\n  A[OOZIE Job] \n  Z[Kudu: temip.temip_kudu_terminated_alarms]\n  K[Kudu: temip.temip_kudu_historic_events]\n  A --> Z\n  A --> K\n  Z --> |Move older alarms to impala|H[Impala: temip.temip_impala_terminated_alarms]\n  K --> |Move older events to impala|L[Impala: temip.temip_impala_historic_events]\n```\n- **Oozie Coordinator**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_kudu_2_Impala_CO`\n    - **Execution:** `everyday at 06:00 local time`\n    - **Approximate Duration:** `15 minutes`\n    - **Workflow:** `TeMIP_kudu_2_Impala_WF`\n      - **Script:** `hdfs:/user/temip/temip_kudu_to_impala.sh`\n      - **Logs:** `Through Oozie Job in HUE`\n**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**\n1. Check that workflow `TeMIP_kudu_2_Impala_WF` runs successfully. This can be done through accessing `Hue` with `temip` user and selecting `Jobs`. Then filter the jobs with `user:temip` and look for a `job` with the below properties:\n    - **Name:**  `oozie:launcher:T=shell:W=temip_kudu_to_impala:A=shell-661a:*`\n    - **Type:** `Oozie Launcher`\n    - **Execution Time:** `06:00 AM`\n    Select the desired shell execution, view its logs and search for any ERRORS.\n1. After the root cause of the problem has been identified, re-rerun the failed execution. This can be done through accessing `Hue` with `temip` user.\n### Alert Mail\nHourly job that checks for gaps in alarm reception and sends notifications if needed.\nThe `Alert Mail` flow consists of a coordinator called `TeMIP_Alert_Mail_CO` which runs every hour and checks if the application receives any alarms from the TeMIP Server. The `TeMIP Server` sends alarms continuously. If in the last hour, the application has not detected any new alarms, an email is sent to jkl Engineers to inform that there might be a issue. The check is performed by comparing the number of alarms from the previous execution stored in `temip.temip_alert_table` table with the current one.\n``` mermaid\n flowchart TD\n  A[OOZIE Server] -->|SHELL Action as user temip| B[un-vip.bigdata.abc.gr <br> User: temip]\n  B --> C[201.temip_alert_mechanism_Main.sh]\n```\n- **Oozie Scheduler**\n  - **Hue:** `https://un-vip.bigdata.abc.gr:8888`\n  - **User:** `temip`\n  - **Coordinator:** `TeMIP_Alert_Mail_CO`\n    - **Execution:** `every hour`\n    - **Workflow:** `TeMIP_Alert_Mail_WF`\n      - **Master Script:** `hdfs:/user/temip/200.TeMIP_Alert_Mail_Oozie_Main.sh`\n      - **Remdef Script:** `un-vip/shared/abc/temip_oozie_production_scripts/201.temip_alert_mechanism_Main.sh`\n      - **Server:** `un2.bigdata.abc.gr`\n      - **SSH User:** `temip`\n      - **Logs:** `un-vip:/shared/abc/temip_oozie_production_scripts/log/202.temip_alert_mechanism.$(date '+%Y%m%d').log`\n**Alerts:**\n- **Not Monitored**\n**Troubleshooting Steps:**\n1. Check for any failed executions. This can be done through accessing `Hue` with `temip` user\n1. Check for any cluster related problems during the failed execution.\n## Manual Actions\n### Restart Wildfly Server\nManual steps to safely restart the application server and coordinate Oozie flows.\n---\n**_Ndef:_** TEMIP runs only in one node. Second node is in standby mode.\n1. **Only if requested by TeMip Administrators** In `Hue` with `temip` user, clear table `temip.temip_kudu_active_alarms` by executing `delete from temip.temip_kudu_active_alarms;`\n1. `Shutdown Wildfly Server`\n    1. Login as `temip` user in `temip2`.\n    1. Execute `/bin/bash` and then run `temip-stop` to stop wildfly and check logs with `temip-tailog`.\n    1. Suspend the temip Coordinators (`Hue` with `temip` user):\n        - `TeMIP_kudu_2_Impala_CO`\n        - `TeMIP_Synchronization_CO`\n        - `TeMIP_Alert_Mail_CO`\n    1. Clear table `temip.temip_kudu_active_alarms` by executing `delete from temip.temip_kudu_active_alarms;` in `Hue` as `temip` user \n1. `Startup Wildfly Server`\n    1. Login as `temip` user in `temip2`.\n    1. Start wildfly by executing `/bin/bash` and then running `temip-start` and check logs with `temip-tailog`.\n    1. Resume the three temip coordinators (`Hue` with `temip` user):\n        - `TeMIP_kudu_2_Impala_CO`\n        - `TeMIP_Synchronization_CO`\n        - `TeMIP_Alert_Mail_CO`\n    1. Workflows:\n        - The two workflows `TeMIP_kudu_2_Impala_WF` and `TeMIP_Alert_Mail_WF` should run automatically when oozie scheduler detects that it was suspended.\n        - The third workflow `TeMIP_Synchronization_WF` should be run manually. Specifically, `login` as `temip` to `Hue` and run manually with no parameters. Make sure that it will not also be executed by the corresponding coordinator.\n    1. At `HUE` with `temip` user, open the impala editor and execute the following command in order to refresh e-mail alert script:  \n      `insert overwrite temip.temip_alert_table values(1);`\n1. `Sanity Checks`\n    1. Login as `temip` user in `temip2`\n    1. Check `logs` with `temip-tailog` and search for any errors.\n    1. After `45 minutes`, login to `Hue` with `temip` user and execute the following impala query editor:  \n    `select * from temip.temip_kudu_configs`  \n      It should return 15 rows. If not, `re run` the `TeMIP_Synchronization_WF` workflow.\n    1. Login to `Hue` with `temip` and perform the below impala queries with a temip admin (Ioanna Bekiari) in order to established if everything is running okay. If the results are the same or really similar, the synchronization is considered successful.\n        ``` sql\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ENM_BASEBAND%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ERICOSS2G%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%ERICOSS-LTE%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%NOKIA3G%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%.ATHENS-OC%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%NNM_FIXED%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%U2000-OC%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%1350OMS%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%HUAWEI_IMS%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%AUMS-OC%';\n        select count(*) from temip_kudu_active_alarms where operation_context like '.def.A5529.A5520_AMS-OC';\n        select count(*) from temip_kudu_active_alarms where operation_context like '%2000_DSLAM%';\n        ```\n### Load Terminated Alarms from TeMIP Oracle Database\nProcedure to backfill alarm data in case of outages by extracting historical records from the Oracle TeMIP DB.\n#### In case of data loss\n---\nIn case there is a loss of alarms for any reason, eg our application or TeMIP outage, we may be asked to load historical data directly from TeMIP Oracle Database into our terminated alarms table. In order to start this operation we must wait for some days, so that all alarms are transferred to the Oracle table. Whole procedure is described in detail below:\n1. Wait `7 days` from the day you want to `re-load` in order for terminated alarms to be refreshed in Oracle table.\n1. Connect as `temip` in `un2` and run `ping 999.999.999.999`, in order to see if `Temip Server` is up and running.\n1. From impala shell `secimp`(as `temip` in `un2`) or `Hue`(as `temip`):\n\t1. Check missing partitions in `temip.temip_kudu_terminated_alarms` and `temip.temip_impala_terminated_alarm` by running  \n  `select count(*), par_dt from <database>.<table> where par_dt='<partition>' group by par_dt;` on both tables.  \n\t\tWe receive TeMIP alarms every day. So if there are general ERRORS(logs) or we have partitions containing less alarms than usual(eg. count), it suggests that there might be problems with the TeMIP server or our application and in need of investigating.\n\t1. Delete existing wrong partitions that overlap with the required interval, either from kudu table `temip.temip_kudu_terminated_alarms` or from impala table `temip.temip_impala_terminated_alarms`.\n\t\t- If wrong partitions are contained in kudu table (only 10 most recent days are in kudu), do:  \n`ALTER table temip.temip_kudu_terminated_alarms DROP IF EXISTS RANGE PARTITION 'v1'<= values < 'v2';`,   \nwhere v1 and v2 the required interval.\n\t\t- If wrong partitions are contained in impala table (10 days past the current date), do:  \n`ALTER table temip.temip_impala_terminated_alarms DROP IF EXISTS PARTITION (par_dt='v');`,   \nwhere v is the wrong partition.\n\t1. In order to not transfer again old data that have remained, run `truncate table temip.temipdb_term_alarms_load_par;`.\n1. As `temip` in `un2` Run the script with arguments\n    ``` bash\n    sh /usr/icom/scripts/Sqoop_Oracle_HDFS_Impala_Load_TeMIP_v832.sh \"temipaharchi.alarmobject0\" identifier 30 <current-pardt> \"terminationtimestamp>='v1' and terminationtimestamp<'v2'\"\n    ```\n    - **current-pardt:** is the `today` par_dt, the day the script is run. Format `YYYYMMDD`. It has no significant value to the internal process.\n    - **v1, v2:** Use values for `terminationtimestamp` that are between the start and end of the interval you want to load from Oracle. Format `01-MAY-22`.\n    Example for day 20220501:\n    ``` bash\n    sh /usr/icom/scripts/Sqoop_Oracle_HDFS_Impala_Load_TeMIP_v832.sh \"temipaharchi.alarmobject0\" identifier 30 20230104 \"terminationtimestamp>='01-MAY-22' and terminationtimestamp<'02-MAY-22'\"\n    ```\n    The data will be **loaded** into table `temip.temipdb_term_alarms_load_par`.\n1. From impala shell `secimp`(as `temip` in `un2`) or `Hue`(as `temip`):\n    1. Refresh the staging table in the impala shell:  \n      `refresh temip.temipdb_term_alarms_load_par;`\n    1. Run the following sql command, which transfers automatically all data to the right partition (par_dt) of temip.temip_impala_terminated_alarms:\n        ``` sql\n        insert overwrite temip.temip_impala_terminated_alarms partition (par_dt)\n        select concat('OPERATION_CONTEXT ',split_part(upper(ocname),':',2),' ALARM OBJECT ', identifier) outentityspec,\n        null last_Modification_Timestamp,\n        split_part(upper(ocname),':',2) operation_context,\n        cast(identifier as bigint) identifier,\n        \"Terminated-Oracle\" state,\n        \"Closed\" problem_status,\n        case when clearancereportflag = \"1\" then true else false end clearance_report_flag,\n        acknowledgementuseride as acknowledgement_user_identifier,\n        handledby as handled_by,\n        closedby as closed_by,\n        handleduseridentifier as handled_user_identifier,\n        releaseuseridentifier as release_user_identifier,\n        closeuseridentifier as close_user_identifier,\n        terminationuseridentif as termination_user_identifier,\n        acknowledgementtimesta as acknowledgement_time_stamp,\n        handletimestamp as handle_time_stamp,\n        closetimestamp as close_time_stamp,\n        terminationtimestamp as termination_time_stamp,\n        releasetimestamp as release_time_stamp,\n        null automatic_terminate_on_close,\n        creationtimestamp as creation_timestamp,\n        archivetimestamp as archive_time_stamp,\n        clearancetimestamp as clearance_time_stamp,\n        null previous_state,\n        managedobject as managed_object,\n        targetentities as target_entities,\n        --targetentities60512 as target_entities,\n        alarmtype as alarm_type,\n        eventtime as event_time,\n        probablecause as probable_cause,\n        securityalarmcause as security_alarm_cause,\n        specificproblems as specific_problems,\n        --specificproblems (id)-8eloume to join kai edw,\n        null backed_up_status,\n        backupobject as backup_object,\n        trendindication as trend_indication,\n        thresholdinfo as threshold_info,\n        cast(notificationidentifier as bigint) notification_identifier,\n        correlnotifinfo as correl_notif_info,\n        monitoredattributes as monitored_attributes,\n        proposedrepairactions as proposed_repair_actions,\n        null additional_information,\n        domain as domain,\n        securityalarmdetector as security_Alarm_Detector,\n        null service_User,\n        null service_Provider,\n        ocname as oc_Name,\n        cast(parentalarmobject as bigint) parent_alarm_object,\n        null severity_changed_time_stamp,\n        alarmcomment as alarm_comment,\n        agentalarmidentifier as agent_alarm_identifier,\n        agententity as agent_entity,\n        perceivedseverity as perceived_Severity,\n        additionaltext as additional_Text,\n        alarmobjectoperatorno as alarm_Object_Operator_Ndef,\n        originalseverity as original_Severity,\n        originaleventtime as original_Event_Time,\n        0 useridentifier,\n        usertext as user_Text,\n        cast(satotal as bigint) sa_total,\n        null deleted,\n        from_timestamp(to_timestamp(terminationtimestamp,'yyyy-MM-dd HH:mm:ss'),'yyyyMMdd') as par_dt\n        --,*\n        from temip.temipdb_term_alarms_load_par a;\n        ```\n        **Ndef:** There are comments that might affect the query if not handled carefully. \n    1. Check if data transferred successfully by running the command below and comparing the result with the number of retrieved records in the logs produced by the script.  \n      ``` sql\n      SELECT par_dt, count(*) FROM temip.temip_impala_terminated_alarms where par_dt > 'v';\n      Eg. Logs: INFO mapreduce.ImportJobBase: Retrieved 1113488 records.  \n      +----------+  \n      | count(*) |  \n      +----------+  \n      | 1113488  |  \n      +----------+  \n      ```\n## TeMIP Wildfly Server\nIn order to change the `logging level` of any of the `categories` of the `TeMIP Wildfly Server` access the `WildFly Management`.\n### Logging\nIn order to change the `logging level` of any of the categories of the `TeMIP Wildfly Server` access the `WildFly Management`.\n- [Home URL](https://999.999.999.999:8888/)\n- **User:** `admin`\n- **Password:** [abc-syspasswd.kdbx](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/abc-syspasswd.kdbx)\n#### Change logging level\n1. Login to the `WildFly Management` by following the [home url](https://999.999.999.999:10213/console/App.html#home), select `Configuration` ~> `Subsystems` ~> `Logging` and select `View`.\n1. Select `LOG CATEGORIES`\n1. From the table select the `category` you desire to change its **logging level** e.g. com.jkl.bigstreamer.abc.temip is our main TeMIP App.\n1. Press the `Edit` option below the table\n1. Select the desired `Level` between the given options. \n1. Click `Save`\n1. Ssh from `un2` with `temip` to `temip1` or `temip2` with `temip` user and check that in the file `/opt/wf_cdef_temip/standalone/configuration/standalone-full.xml` the level of the previously configured logger has changed successfully. It should be configured **automatically**. \n## Useful Links\n- [TeMIP Dir](https://metis.ghi.com/obss/bigdata/abc/temip)\n- [TeMIP Application Deployment](https://metis.ghi.com/obss/bigdata/abc/temip/temip-devops/-/wikis/Application-Deployment)\n- [TeMIP Wiki](https://metis.ghi.com/obss/bigdata/abc/temip/temip-devops/-/wikis/home)\n#tags:\n# - temip\n# - kudu\n# - impala\n# - alarms\n# - wildfly\n# - oozie\n# - synchronization\n# - alerting\n# - oracle\n# - sqoop\n# - impala tables\n# - kudu tables\n# - alarm ingestion\n# - terminated alarms\n# - temip recovery\n# - active alarms\n# - logging level\n# - coordinator workflows\n# - manual actions"
        },
        {
            "question": "How can I manually re-run the Reference Data Load for 'DEVICES' on a specific day?",
            "answer": "Run the script with the desired type and date:\n```bash\n/shared/abc/refdata/bin/210_refData_Load.sh DEVICES 20230530\n```",
            "category": "Data Management & Query Execution",
            "files": "Reference_Data_Flow.md",
            "context": "---\\ntitle: Reference Data ETL Flow\\nsystem: BigStreamer\\ncomponent: Reference Data\\njob_name: refdata_etl\\nsource_system: Local Filesystem (vantagerd)\\nsource_tables:\\n  - cells_YYYYMMDD.csv.gz\\n  - crm_YYYYMMDD.csv.gz\\n  - devices_YYYYMMDD.csv.gz\\n  - services_YYYYMMDD.csv.gz\\ndestination_system: Hive\\ndestination_tables:\\n  - refdata.rd_cells_load\\n  - refdata.rd_services_load\\n  - refdata.rd_crm_load\\n  - refdata.rf_devices_load\\nschedule: daily at 00:05\\ncoordinator: none (cron-based)\\nworkflow: 210_refData_Load.sh / 220_refData_Daily_Snapshot.sh\\nscript_path: /shared/abc/refdata/bin\\nmonitoring_table: none\\nowner: intra\\ntags:\\n  - Reference Data\\n  - refdata\\n  - rd_cells_load\\n  - rd_crm_load\\n  - rd_services_load\\n  - rf_devices_load\\n  - Hive\\n  - Snapshot\\n  - Crontab\\n  - Bash Scripts\\n  - Troubleshooting\\n  - Data Validation\\n  - Vantagerd\\n---\\n# Reference Data Flow\\nThis document describes the reference data ingestion process for cells, CRM, devices, and services. It includes file locations, loading scripts, cron scheduling, Hive targets, and troubleshooting steps.\\n## Installation info\\nDescribes the input/output directories, involved nodes and users, script and log locations, and crontab setup for automatic file loading.\\n### Data Source File\\n- Local FileSystem Directories\\n  - host :`un2.bigdata.abc.gr (999.999.999.999)`\\n  - user : `vantagerd`\\n  - spool area : `/data/1/vantage_ref-data/REF-DATA/` link points to `/shared/vantage_ref-data/REF-DATA`\\n  - file_types : `<refType>_<refDate>.csv.gz`  \\n*\\\\<refType\\\\>: `cells, crm, devices, services` (i.e. cells_20230530.csv.gz)*\\n  - load_suffix : `<YYYYMMDD>.LOADED` *(i.e. cells_20230530.csv.cells_20230531.LOADED)*\\n- HDFS Directories\\n\\t- hdfs landingzone : `/ez/landingzone/REFDATA`\\n### Scripts-Logs Locations\\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\\n- user : `intra`\\n- script path : `/shared/abc/refdata/bin`\\n- script files: \\n\\t- `210_refData_Load.sh`\\n\\t- `220_refData_Daily_Snapshot.sh`\\n- log path : `/shared/abc/refdata/log`\\n- log files: \\n\\t- `210_refData_Load.<YYYYMM>.log`\\n\\t- `220_refData_Daily_Snapshot.<YYYYMM>.log`\\n### Crontab Scheduling\\n- node : `un2.bigdata.abc.gr (999.999.999.999)`\\n- user : `intra`  \\n\\truns at : Daily at 00:05\\n\\t`5 0 * * * /shared/abc/refdata/bin/210_refData_Load.sh CELLS $(date '+\\\\%Y\\\\%m\\\\%d' -d \\\"yesterday\\\")`\\nNdef1: The entry above loads reference data for CELLS.  \\nNdef2: For each reference type a new entry of the `210_refData_Load.sh` is required passing the following parameters  \\n\\t\\\\<reference Type\\\\> : `cells, crm, devices, services`  \\n\\t\\\\<reference Date\\\\> : `yesterday` is the default value  \\n### Hive Tables\\n- Target Database: `refdata`\\n- Target Tables: \\n\\t1. `rd_cells_load`\\n\\t1. `rd_services_load`\\n\\t1. `rd_crm_load`\\n\\t1. `rf_devices_load`\\n| Table Name         | Description                      |\\n|--------------------|----------------------------------|\\n| rd_cells_load      | Historical data of cell metadata |\\n| rd_services_load   | Historical data of services      |\\n| rd_crm_load        | Historical data of CRM records   |\\n| rf_devices_load    | Historical data of device info   |\\n## Data process\\nHigh-level overview of how reference files are prepared, parsed, and loaded into historical and snapshot Hive tables.\\n### High Level Overview\\n![High_Level_Overview](https://metis.ghi.com/obss/bigdata/abc/alarm-archiving/refdata/-/raw/main/docs/ReferenceData.High_Level_Overview.png)\\n##### Steps 1-3: \\nabc is responsible for the preparation/creation of the Reference Data flat files.  \\nThese files are stored into a specific directory in `UN2` node using the SFTP-PUT method as user `vantagerd`  \\n##### Steps 4-5:\\nScript `210_refData_Load.sh` is responsible to read, parse and load the contents of reference files into HIVE tables (aka LOAD tables).  \\nThese tables keep the data of all completed loads. That is, they contain all the historicity of the reference data.  \\nThe data of each load is stored in a separate partition identified by the date of the loading (i.e. par_dt=20230530)  \\n##### Steps 6-7:\\nScript `220_refData_Daily_Snapshot.sh` reads the most recently added data from the LOAD table and store them as a snapshot into a separate table (aka snapshot tables).  \\nThe data of these tables are used for the enrichment of various fact data (i.e. Traffica (SAI))\\n## Manually Run\\nInstructions for manually triggering the data load process for a specific file and date using the load script.\\n`210_refData_Load.sh` script is responsible for the loading of a reference file.\\nTo run the script two arguments are required  \\n`/shared/abc/refdata/bin/210_refData_Load.sh <refType>  <refDate>`  \\n1st: **\\\\<refType\\\\>**, the Reference Type\\n```\\n- CELLS\\n- CRM\\n- DEVICES\\n- SERVICES\\n```\\n2nd: **\\\\<refDate\\\\>**, the date that the flat file contains in its filename  \\n\\ti.e.\\n```\\ncells_20220207.csv.gz\\ncells_20220208.csv.gz\\ncells_20220209.csv.gz\\nservices_20220207.csv.gz\\ndevices_20220208.csv.gz\\ncrm_20220209.csv.gz\\n```\\nIn case of loading the files above we should execute the following commands\\n```bash\\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20220207\\n/shared/abc/refdata/bin/210_refData_Load.sh cells 20220208\\n/shared/abc/refdata/bin/210_refData_Load.sh CELLS 20220209\\n/shared/abc/refdata/bin/210_refData_Load.sh services 20220207\\n/shared/abc/refdata/bin/210_refData_Load.sh DEVICES 20220208\\n/shared/abc/refdata/bin/210_refData_Load.sh crm 20220209\\n```\\nOnce the loads completed, the flat files renamed to <CURRENT_YYYYMMDD>.LOADED\\n```\\ncells_20220207.csv.20230531.LOADED\\ncells_20220208.csv.20230531.LOADED\\ncells_20220209.csv.20230531.LOADED\\nservices_20220207.csv.20230531.LOADED\\ndevices_20220208.csv.20230531.LOADED\\ncrm_20220209.csv.20230531.LOADED\\n```\\n## Troubleshooting\\nExplains how to identify and resolve issues with reference data loads using error logs and manual script execution.\\n**Currently, the Reference Data flow does not support the `Monitoring` services.**  \\n- An email will be sent by the system with the point of failure.\\ni.e.\\n```\\nSubject: ALERT: Reference data Loading, Type:CELL,  File:cells_20220207.csv\\nBody: \\n\\tReference Type  : CELL\\n\\tReference File  : cells_20220207.csv\\n\\tReference Scirpt: 210_refData_Load.sh\\n\\t------------------------------------------\\n\\tERROR:$(date '+%F %T'), ALTER TABLE or LOAD DATA command failed.\\n```\\n- Check the log files for errors/exceptions  \\n```bash\\negrep -i 'error|fail|exception|problem' /shared/abc/refdata/log/210_refData_Load.YYYYMM.log\\negrep -i 'error|fail|exception|problem' /shared/abc/refdata/log/220_refData_Daily_Snapshot.YYYYMM.log\\n```\\nIn case of failure follow the instructions described in **`Manually Run`**\\n### Common errors  \\n- Reference data file is empty or the contents of the file is not the expected.  \\nIf this is the case, update abc that the file is invalid and ask them to send a new.  \\n- Other factors not related to the specific flow\\n\\t- impala/hive availability\\n\\t- Kerberos authentication\\n\\t*Ndef: The flow checks if the ticket is still active before any HDFS action.  \\n\\tIn case of expiration the flow performs a `kinit` command*\\n## Data Check\\nProvides queries and instructions to verify data load completeness and partition existence in Hive tables.\\n- **Check final tables for new partitions**:\\n\\t```\\n\\tsu - intra\\n\\t/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k -q \\\"refresh refdata.rd_cells_load; show partitions refdata.rd_cells_load;\\\"\\n\\t\\n\\t+----------+-----------+--------+---------+\\n\\t| par_dt   | #Rows     | #Files | Size    |\\n\\t+----------+-----------+--------+---------+\\n\\t| 20220227 | 98090     | 1      | 41.88MB |\\n\\t| 20220228 | 98021     | 1      | 41.84MB |\\n\\t| 20220301 | 97353     | 1      | 41.76MB |\\n\\t| Total    | 142404322 | 1500   | 59.63GB |\\n\\t+----------+-----------+--------+---------+\\n\\t```\\n- **Check the amount of data in final tables**:\\n```bash\\nsu - intra\\n/usr/bin/impala-shell -i un-vip.bigdata.abc.gr --ssl -k -q \\\"select par_dt, count(*) as cnt from refdata.rd_cells_load group by par_dt order by 1;\\\"\\n\\tpar_dt   | cnt    \\n\\t---------+--------\\n\\t20221130 | 2784494\\n```"
        },
        {
            "question": "Where can I find and check the reconciliation log for the Location Mobility voiceOut export?",
            "answer": "The reconciliation log is stored at:\n```bash\n/shared/abc/location_mobility/logging/LM_07_voiceOut_reconciliation.log\n```\nIt lists export timestamps, filenames, dates, and record counts for verification.",
            "category": "Data Management & Query Execution",
            "files": "trustcenter_flows.md",
            "context": "---\\ntitle: TrustCenter Data Export Flows\\ndescription: Overview and support guide for TrustCenter-related export workflows including Location Mobility, Router Analytics, Application Usage Insights (AUI), and Customer Satisfaction Index (CSI). Describes scheduling, file formats, SFTP transfers, Impala sources, Oozie jobs, and troubleshooting procedures.\\nauthor: mtuser / intra / ABC BigStreamer Team\\nupdated: 2025-05-01\\ntags:\\n  - trustcenter\\n  - location mobility\\n  - lm\\n  - router analytics\\n  - ra\\n  - application usage insights\\n  - aui\\n  - customer satisfaction index\\n  - csi\\n  - oozie\\n  - sftp\\n  - export flows\\n  - bigstreamer\\n  - impala\\n  - reconciliation logs\\n---\\n# TrustCenter Flows\\nThis is a document that will assist on support. Business documents can be found [here](https://metis.ghi.com/obss/bigdata/abc/reporting/reporting/-/tree/master/mini%20projects/location_mobility/docs).\\n## Location Mobility\\nLocation Mobility (LM) reffers to extraction of data from BigStreamer into files.  \\nThe output files are transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them.  \\nThese files are:\\n- `LM_02_lte_yyyyMMdd_xxx.txt`\\n- `LM_03_smsIn_yyyyMMdd_xxx.txt`\\n- `LM_04_smsOut_yyyyMMdd_xxx.txt`\\n- `LM_05_voiceInOut_yyyyMMdd_xxx.txt`\\n- `LM_06_voiceIn_yyyyMMdd_xxx.txt`\\n- `LM_07_voiceOut_yyyyMMdd_xxx.txt`\\n- `LM_08_cellHist_yyyyMMdd_xxx.txt`\\nAlong with those, the reconciliation files are produced and sent for each one.  \\nThey give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\\n``` bash\\ncat /shared/abc/location_mobility/logging/LM_05_voiceInOut_reconciliation.log\\n#e.g for LM_05_voiceInOut and 1st of February 2022\\n2022-02-01 08:06:33 LM_05_voiceInOut_20220201_00001.txt 20220201 20906\\n2022-02-01 10:02:36 LM_05_voiceInOut_20220201_00002.txt 20220201 23810\\n2022-02-01 12:02:45 LM_05_voiceInOut_20220201_00003.txt 20220201 179719\\n2022-02-01 14:03:45 LM_05_voiceInOut_20220201_00004.txt 20220201 876051\\n2022-02-01 16:05:13 LM_05_voiceInOut_20220201_00005.txt 20220201 1581201\\n2022-02-01 20:02:00 LM_05_voiceInOut_20220201_00006.txt 20220201 1606966\\n```\\n**Reconcilication Files**:  \\n`/shared/abc/location_mobility/logging/LM_*` on `un2.bigdata.abc.gr`\\n**Troubleshooting Steps**:\\n- Check to see if the file was produced at the right time and contained the expected number of rows.\\n### LM_02_lte\\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala table `eea.eea_hour`.  \\nThe filename format is `LM_02_lte_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_02_lte_20220301_00002.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n  B --> C[ Remdef Script ]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `mtuser`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every 2 hours`  \\n**Coordinator**: `Location_Mobility_2Hour_CO`\\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\n  graph TD \\n  A[Impala: eea.eea_hour] -->| Impala Query | B1[location_mobility.lm_lte_exp]\\n  B1 --> |Merge HDFS files to a single file | B2[File: LM_02_lte_yyyyMMdd_000xx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B2 --> |SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/lm`\\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log```\\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` on `un2.bigdata.abc.gr`\\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/eea_hour.lock`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Find in the failed execution's log the message:  \\n\\t- login on `un2.bigdata.abc.gr` with personal account  \\n\\t- `su - mtuser`\\n    ``` logs\\n    # e.g for 2021-02-22\\n    [...] - INFO: end_date=2021-02-22 09:00:00\\n    [...] - INFO: max_date=2021-02-22 09:00:00\\n    ```\\n    If end_date is newer or equal to max_date, it means that table `eea.eea_hour` does not contain new data and therefore there is nothing to be done during this execution.  \\n\\t\\tabc should load data in `eea.eea_hour` table first and then execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \\nFor example if 6 files were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_lte_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_lte_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n### LM_03_smsIn\\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.sms_raw_v, osix.osix_sms_raw` that fulfill some conditions.  \\nThe filename format is `LM_03_smsIn_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_03_smsIn_20220301_00002.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n  B --> C[ Remdef Script ]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `mtuser`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every 2 hours`  \\n**Coordinator**: `Location_Mobility_2Hour_CO`\\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD\\n  A[Impala: sai.sms_raw_v] --> |union all | D[Impala: osix.osix_sms_raw ] --> | Impala Query | B[File: LM_03_smsIn_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/lm`\\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log```\\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh` on `un2.bigdata.abc.gr`\\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_in_v2.sql` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/sms_in.lock`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Check if this message exists in the failed execution's log:  \\n\\t- login on `un2.bigdata.abc.gr` with personal account  \\n\\t- `su - mtuser`\\n    ``` logs\\n    [...] - INFO: Nothing to export.\\n    ```\\n    This means that tables `sai.sms_raw_v` or `osix.osix_sms_raw` do not contain new data and therefore there is nothing to be done during this execution.  \\n    New data should be loaded in the following tables and then execute the script.  \\n\\t\\t- `sai.sms_raw`, updated by TRAFFICA flow (`sai.sms_raw_v` is a view on `sai.sms_raw` table).  \\n\\t\\t- `osix.osix_sms_raw`, responsible abc\\n- If failed execution's log contains the message:\\n    ``` logs\\n    [...] - ERROR: Script is being executed by another process. Exiting..\\n    ```\\n    and `ps -ef | grep export_lm_sms_in_v2_mon.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/sms_in.lock` and execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \\nFor example if 6 files were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag.  \\nFor example if the first 6 files for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_sms_in_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n### LM_04_smsOut\\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.sms_raw_v, osix.osix_sms_raw` that fulfill some conditions.  \\nThe filename format is `LM_04_smsOut_yyyyMMdd_xxx.txt` where `xx` is a serial number between `01` and `12`.  \\nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_04_smsOut_20220301_00002.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n  B --> C[ Remdef Script ]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `mtuser`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every 2 hours`  \\n**Coordinator**: `Location_Mobility_2Hour_CO`\\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: sai.sms_raw_v] --> |union all | D[Impala: osix.osix_sms_raw ] -->| Impala Query | B[File: LM_04_smsOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/lm`\\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d').log```\\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh` on `un2.bigdata.abc.gr`\\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_sms_out_v2.sql` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/sms_out.lock`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Check if this message exists in the failed execution's log:  \\n\\t- login on `un2.bigdata.abc.gr` with personal account  \\n\\t- `su - mtuser`\\n    ``` logs\\n    [...] - INFO: Nothing to export.\\n    ```\\n    This means that table `sai.sms_raw_v` or `osix.osix_sms_raw` do not contain new data and therefore there is nothing to be done during this execution.  \\n    New data should be loaded in the following tables and then execute the script.  \\n\\t\\t- `sai.sms_raw`, updated by TRAFFICA flow (`sai.sms_raw_v` is a view on `sai.sms_raw` table).  \\n\\t\\t- `osix.osix_sms_raw`, responsible abc\\n- If failed execution's log contains the message\\n    ``` logs\\n    [...] - ERROR: Script is being executed by another process. Exiting..\\n    ```\\n    and `ps -ef | grep export_lm_sms_out_v2_mon.sh` return no process means the previous execution was forcefully stopped.  \\n\\t\\tDelete the lock file `/shared/abc/location_mobility/run/sms_out.lock` and execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \\nFor example if 6 files were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d') 2>&1\\n    ```\\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_sms_out_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_sms_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n### LM_05_voiceInOut\\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions. \\nThe filename format is `LM_05_voiceInOut_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_05_voiceInOut_20220301_00002.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n  B --> C[ Remdef Script ]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `mtuser`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every 2 hours`  \\n**Coordinator**: `Location_Mobility_2Hour_CO`\\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] --> | Impala Query | B[File: LM_05_voiceInOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/lm`\\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log```\\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh` on `un2.bigdata.abc.gr`\\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2.sql` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/voice_inout.lock`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Check if this message exists in the failed execution's log:  \\n\\t- login on `un2.bigdata.abc.gr` with personal account  \\n\\t- `su - mtuser`\\n    ``` logs\\n    [...] - INFO: Nothing to export.\\n    ```\\n    This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution.\\n    New data should be loaded in the following tables and then execute the script.  \\n\\t\\t- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \\n\\t\\t- `osix.osix_voice_raw`, responsible abc\\n- If failed execution's log contains the message:\\n    ``` logs\\n    [...] - ERROR: Script is being executed by another process. Exiting..\\n    ```\\n    and `ps -ef | grep export_lm_voice_inout_v2_mon.sh` return no process means the previous execution was forcefully stopped.  \\n\\t\\tDelete the lock file `/shared/abc/location_mobility/run/voice_inout.lock` and execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up.  \\nFor example if 6 files were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_voice_inout_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_inout_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n### LM_06_voiceIn\\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions.  \\nThe filename format is `LM_06_voiceIn_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_06_voiceIn_20220301_00002.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n  B --> C[ Remdef Script ]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `mtuser`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every 2 hours`  \\n**Coordinator**: `Location_Mobility_2Hour_CO`\\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] -->| Impala Query | B[File: LM_06_voiceIn_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/lm`\\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log```\\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh` on `un2.bigdata.abc.gr`\\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_in_v2.sql` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/voice_in.lock`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Check if this message exists in the failed execution's log:  \\n\\t- login on `un2.bigdata.abc.gr` with personal account  \\n\\t- `su - mtuser`\\n    ``` logs\\n    [...] - INFO: Nothing to export.\\n    ```\\n    This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution. \\n    New data should be loaded in the following tables and then execute the script.  \\n\\t\\t- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \\n\\t\\t- `osix.osix_voice_raw`, responsible abc\\n- If failed execution's log contains the message:\\n    ``` logs\\n    [...] - ERROR: Script is being executed by another process. Exiting..\\n    ```\\n    and `ps -ef | grep export_lm_voice_in.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/voice_in.lock` and execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag.  \\nThis will instruct the script to catch-up meaning to export files for N 2-hour intervals.  \\nThis is not needed if 4 or less files were missed in which case the procedure will automatically catch up. For example if 6 files were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_voice_in_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_in_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n### LM_07_voiceOut\\nUnder normal circumstances this file is produced every 2 hours and contains data for 2 hours from the Impala tables `sai.voice_raw_v, osix.osix_voice_raw` that fulfill some conditions.  \\nThe filename format is `LM_07_voiceOut_yyyyMMdd_xxx.txt` where `xxx` is a serial number between `01` and `12`.  \\nFor example, if the file contains data for the 1st of March 2022 from 02:00 to 04:00 the filename will be `LM_07_voiceOut_20220301_00002.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie Coord: Location_Mobility_2Hour_CO] -->|SHELL| B[Master Script ]\\n  B --> C[ Remdef Script ]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `mtuser`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every 2 hours`  \\n**Coordinator**: `Location_Mobility_2Hour_CO`\\n**Master Script**: `000.Location_Mobility_Hourly_Oozie_Main.sh`\\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_hourly.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: sai.voice_raw_v] --> |union all | D[Impala: osix.osix_voice_raw ] -->| Impala Query | B[File: LM_07_voiceOut_yyyyMMdd_xxx.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/lm`\\n**Logs**: ```/shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log```\\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh` on `un2.bigdata.abc.gr`\\n**SQL Script**: `/shared/abc/location_mobility/run/renew/export_lm_voice_out_v2.sql` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/voice_out.lock`\\n**Troubleshooting Steps**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Check if this message exists in the failed execution's log:  \\n\\t- login on `un2.bigdata.abc.gr` with personal account  \\n\\t- `su - mtuser`\\n    ``` logs\\n    [...] - INFO: Nothing to export.\\n    ```\\n    This means that table `sai.voice_raw_v` or `osix.osix_voice_raw` do not contain new data and therefore there is nothing to be done during this execution. \\n    New data should be loaded in the following tables and then execute the script.  \\n\\t\\t- `sai.voice_raw`, updated by TRAFFICA flow (`sai.voice_raw_v` is a view on `sai.voice_raw` table).  \\n\\t\\t- `osix.osix_voice_raw`, responsible abc\\n- If failed execution's log contains the message:\\n    ``` logs\\n    [...] - ERROR: Script is being executed by another process. Exiting..\\n    ```\\n    and `ps -ef | grep export_lm_voice_out_v2_mon.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/voice_out.lock` and execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N 2-hour intervals. This is not needed if 4 or less files were missed in which case the procedure will automatically catch up. For example if 6 files were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n- If you need to export files for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if the first 6 files for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_voice_out_v2_mon.sh -t 20220313 --max-files 6 >> /shared/abc/location_mobility/log/lm_export_voice_out_v2_mon.cron.$(date '+%Y%m%d').log 2>&1\\n    ```\\n### LM_08_cellHist\\nUnder normal circumstances this file is produced every day and contains yesterday's data from the Impala table `refdata.rd_cells_v`.  \\nThe filename format is `LM_08_cellHist_yyyyMMdd_00001.txt`.  \\nFor example, if the file contains data for the 1st of March 2022 the filename will be `LM_08_cellHist_20220301_00001.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie Coord: Location_Mobility_Daily_CO] -->|SHELL| B[Master Script ]\\n  B --> C[ Remdef Script ]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `mtuser`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every day at 07:00`  \\n**Coordinator**: `Location_Mobility_Daily_CO`\\n**Master Script**: `000.Location_Mobility_Daily_Oozie_Main.sh`\\n**Remdef Script**: `mtuser@un2:/shared/abc/location_mobility/run/run_lm_exports_daily.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: refdata.rd_cells_v] -->| Impala Query | B[File: LM_08_cellHist_yyyyMMdd_00001.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /lm]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/lm`\\n**Logs**: ```/shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log```\\n**Script**: `/shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/rd_cells.lock`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Find in the failed execution's log the message:  \\n\\t- login on `un2.bigdata.abc.gr` with personal account  \\n\\t- `su - mtuser`\\n    ``` logs\\n    [...] - INFO: max_date=yyyyMMdd and export_date=yyyyMMdd\\n    ```\\n    If the desired export_date is newer than max_date, it means that table `refdata.rd_cells_v` does not contain new data and therefore there is nothing to be done during this execution.  \\n\\t\\tLoad table `refdata.rd_cells` first and then execute the script.\\n- If failed execution's log contains the message:\\n    ``` logs\\n    [...] - ERROR: Script is being executed by another process. Exiting..\\n    ```\\n    and `ps -ef | grep export_lm_rd_cells.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/rd_cells.lock` and execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded.  \\nBefore manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 5 or more dates weren't exported execute the script with the `--max-files <N>` flag.  \\nThis will instruct the script to catch-up meaning to export files for N dates.  \\nThis is not needed if 4 or less dates were missed in which case the procedure will automatically catch up.  \\nFor example if 6 dates were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh --max-files 6 >> /shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log 2>&1\\n    ```\\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/renew/export_lm_rd_cells_v2_mon.sh -t 20220313 >> /shared/abc/location_mobility/log/export_lm_rd_cells_v2_mon.oozie.$(date '+%Y%m%d').log 2>&1\\n    ```\\n## Router Analytics\\nRouter Analytics (RA) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\\n- `RA_01_yyyymmdd_00001_x.gz` \\n- `RA_02_yyyymmdd_00001_x.gz`\\n- `RA_03_yyyymmdd.gz`\\nAlong with those, the reconciliation files are produced and sent for each one. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\\n``` bash\\ncat /shared/abc/location_mobility/logging/RA_BS_01_reconciliation.log\\n#e.g for LM_05_voiceInOut and 31st of January 2022\\n2022-02-01 09:06:39 RA_01_20220131_00001_[0-5] 20220131 68579162\\n```\\n**Reconcilication Files**: `/shared/abc/location_mobility/logging/RA_BS_*` on `un2.bigdata.abc.gr`\\n**Troubleshooting Steps**:\\n- Check to see if the file was produced at the right time and contained the expected number of rows.\\n### RA_01\\nUnder normal circumstances this file is produced every day and contains yesterday's data from the Impala table `npce.device_session`. The filename format is `RA_01_yyyymmdd_00001_x.gz` where `x` is a serial number between `1` and `5` as due to size the file is split into subfiles. For example, the files containing data for the 1st of March 2022 will be `RA_01_20220301_00001_[0-5].gz`.\\n``` mermaid\\n  graph TD\\n  A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n  B -->|sudo to mtuser| C[Master Script]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `intra`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every day at 07:00`  \\n**Coordinator**: `export_Router_Analytics_files_daily`\\n**Master Script**: `/shared/abc/location_mobility/run/run_ra_exports.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: npce.device_session] -->| Impala Query | B[File: RA_01_yyyymmdd_00001_x.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/ra`\\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log```\\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_01.sh` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/ra_01.lock`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Find in the failed execution's log the message:\\n    ``` logs\\n    # e.g for 2021-02-01\\n    [...] - INFO: max_date=20220131 and export_date=20220131\\n    ```\\n    If the desired export_date is newer than max_date, it means that table `npce.device_session` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\\n- If failed execution's log contains the message:\\n    ``` logs\\n    [...] - ERROR: Script is being executed by another process. Exiting..\\n    ```\\n    and `ps -ef | grep export_ra_bs_01.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_01.lock` and execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 4 or more dates weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N dates. This is not needed if 3 or less dates were missed in which case the procedure will automatically catch up. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 5 dates were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/export_ra_bs_01.sh --max-files 6 >> /shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\\n    ```\\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/export_ra_bs_01.sh -t 20220313 >> /shared/abc/location_mobility/log/ra_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\\n    ```\\n### RA_02\\nUnder normal circumstances this file is produced every day and contains yesterday's data from the Impala table `npce.device_traffic`. The filename format is `RA_02_yyyymmdd_00001_x.gz` where `x` is a serial number between `1` and `5` as due to size the file is split into subfiles. For example, the files containing data for the 1st of March 2022 will be `RA_02_20220301_00001_[0-5].gz`.\\n``` mermaid\\n  graph TD\\n  A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n  B -->|sudo to mtuser| C[Master Script]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `intra`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every day at 07:00`  \\n**Coordinator**: `export_Router_Analytics_files_daily`\\n**Master Script**: `/shared/abc/location_mobility/run/run_ra_exports.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: npce.device_traffic] -->| Impala Query | B[File: RA_02_yyyymmdd_00001_x.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/ra`\\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log```\\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_02.sh` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/ra_02.lock`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Find in the failed execution's log the message:\\n    ``` logs\\n    # e.g for 2021-02-01\\n    [...] - INFO: max_date=20220131 and export_date=20220131\\n    ```\\n    If the desired export_date is newer than max_date, it means that table `npce.device_traffic` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\\n- If failed execution's log contains the message:\\n    ``` logs\\n    [...] - ERROR: Script is being executed by another process. Exiting..\\n    ```\\n    and `ps -ef | grep export_ra_bs_02.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_02.lock` and execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 4 or more dates weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N dates. This is not needed if 3 or less dates were missed in which case the procedure will automatically catch up. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 5 dates were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/export_ra_bs_02.sh --max-files 6 >> /shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log 2>&1\\n    ```\\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/export_ra_bs_02.sh -t 20220313 >> /shared/abc/location_mobility/log/ra_export_bs_02.oozie.`date +%Y%m%d`.log 2>&1\\n    ```\\n### RA_03\\nUnder normal circumstances this file is produced every Wednesday and contains past week's data from the Impala table `npce.device_dms`. The filename format is `RA_03_yyyymmdd.gz`. For example, the files containing data up to the 2nd of March 2022 will be `RA_03_20220302.gz`.\\n``` mermaid\\n  graph TD\\n  A[Oozie: export_Router_Analytics_files_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n  B -->|sudo to mtuser| C[Master Script]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `intra`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every Wednesday at 16:00`  \\n**Coordinator**: `export_Router_Analytics_files_to_mediation_ra_03_weekly`\\n**Master Script**: `/shared/abc/location_mobility/run/run_ra_03_export.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: npce.device_dms] -->| Impala Query | B[File: RA_03_yyyymmdd.gz <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /ra]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/ra`\\n**Logs**: ```/shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log```\\n**Script**: `/shared/abc/location_mobility/run/export_ra_bs_03.sh` on `un2.bigdata.abc.gr`\\n**Lock file**: `/shared/abc/location_mobility/run/ra_03.lock`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Find in the failed execution's log the message:\\n    ``` logs\\n    # e.g for 2021-01-26\\n    [...] - INFO: max_date=20220126 and export_date=20220202\\n    ```\\n    If the desired export_date is newer than max_date, it means that table `npce.device_dms` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\\n- If failed execution's log contains the message:\\n    ``` logs\\n    [...] - ERROR: Script is being executed by another process. Exiting..\\n    ```\\n    and `ps -ef | grep export_ra_bs_03.sh` return no process means the previous execution was forcefully stopped. Delete the lock file `/shared/abc/location_mobility/run/ra_03.lock` and execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 2 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N executions. **Make sure there is sufficient space both in the local path and the sftp path**. For example if 2 files were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/export_ra_bs_03.sh --max-files 2 >> /shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log 2>&1\\n    ```\\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. **Make sure there is sufficient space both in the local path and the sftp path**. For example if file for 16th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/export_ra_bs_03.sh -t 20220316 >> /shared/abc/location_mobility/log/ra_export_bs_03.oozie.`date +%Y%m%d`.log 2>&1\\n    ```\\n## Application Data Usage Insights\\nApplication Data Usage Insights (AUI) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\\n- `AUI_01_yyyymmdd_0000x.txt`\\nAlong with those, a reconciliation file are produced and sent. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\\n``` bash\\ncat /shared/abc/location_mobility/logging/AUI_BS_01_reconciliation.log\\n#e.g for AUI_01 and 21st of February 2022\\n2021-02-22 06:00:09 AUI_01_20210221_00005.txt 20210221 15\\n```\\n**Reconcilication File**: `/shared/abc/location_mobility/logging/AUI_BS_01_reconciliation.log` on `un2.bigdata.abc.gr`\\n**Troubleshooting Steps**:\\n- Check to see if the file was produced at the right time and contained the expected number of rows.\\n### AUI_01\\nUnder normal circumstances this file is produced every 4 hours and contains data from 6 to 2 hours ago of the Impala table `npce.abc_apps_raw_events`. The filename format is `AUI_01_yyyymmdd_0000x.txt` where `x` is a serial number between `1` and `6`. For example, the files containing data for the 1st of March 2022 from 00:00 to 04:00 will be `AUI_01_20220301_00001.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie: export_Application_Data_Usage_Insights_files_4_hours] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n  B -->|sudo to mtuser| C[Master Script]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `intra`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every 4 hours`  \\n**Coordinator**: `export_Application_Data_Usage_Insights_files_4_hours`\\n**Master Script**: `/shared/abc/location_mobility/run/run_aui_exports.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: npce.abc_apps_raw_events] -->| Impala Query | B[File: AUI_01_yyyymmdd_0000x.txt <br> Server: un2.bigdata.abc.gr <br> Path: /data/location_mobility/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /aui]\\n```\\n**User**: `mtuser`\\n**Local path**: `/data/location_mobility/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/aui`\\n**Logs**: ```/shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log```\\n**Script**: `/shared/abc/location_mobility/run/export_aui.sh` on `un2.bigdata.abc.gr`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Find in the failed execution's log the message:\\n    ``` logs\\n    date: invalid date \\u2018NULL 6 hours ago\\u2019\\n    ```\\n    This means that table `npce.abc_apps_raw_events` does not contain new data and therefore there is nothing to be done during this execution. Communicate with def to load the table first and then execute the script.\\n**Ndefs**:\\n- If files were missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If 5 or more files weren't exported execute the script with the `--max-files <N>` flag. This will instruct the script to catch-up meaning to export files for N 4-hour intervals. This is not needed if 4 or less dates were missed in which case the procedure will automatically catch up. For example if 6 dates were not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/export_aui.sh --max-files 6 >> /shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\\n    ```\\n- If you need to export file for a specific date execute the script with the `-t <yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/location_mobility/run/export_aui.sh -t 20220313 >> /shared/abc/location_mobility/log/aui_export_bs_01.oozie.`date +%Y%m%d`.log 2>&1\\n    ```\\n## Customer Satisfaction Index\\nCustomer Satisfaction Index (CSI) reffers to extraction of data from BigStreamer into files. The output files are compressed and transferred to an exchange directory so that a service, TrustCenter which is managed by def, reads and deletes them. These files are:\\n- `CSI_fix_mmddyyyy_wXX.txt`\\n- `CSI_mob_mmddyyyy_mmddyyyy.txt`\\nAlong with those, a reconciliation file are produced and sent. They give information on the date of the execution, the name of the file, the export date and the number of lines it contains.\\n``` bash\\ncat /shared/abc/export_sai_csi/logging/CSI_mob_reconciliation.log\\n#e.g for CSI_mob and 30th of January 2022\\n2022-01-30 09:02:42  CSI_mob_01242022_01302022.txt  20220124  4223904\\n```\\n**Reconcilication File**: `/shared/abc/export_sai_csi/logging/CSI_*` on `un2.bigdata.abc.gr`\\n**Troubleshooting Steps**:\\n- Check to see if the file was produced at the right time and contained the expected number of rows.\\n### CSI_fix\\nUnder normal circumstances this file is produced every 4 hours and contains data from 2 days ago ago of the Impala table `sai.cube_indicators_it`. The filename format is `CSI_fix_mmddyyyy_wXX.txt` where `XX` is a serial number between `1` and `52` for the week of the year. For example, the file containing data for the 2nd of February 2022 which belongs to the 5th week of the year, will be `CSI_fix_02042022_w05.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie: export_CSI_fix_and_mobile_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n  B -->|sudo to mtuser| C[Master Script]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `intra`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every day at 7:00`  \\n**Coordinator**: `export_CSI_fix_and_mobile_daily`\\n**Master Script**: `/shared/abc/location_mobility/run/run_csi_exports_daily.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: sai.cube_indicators_it] -->| Impala Query | B[File: CSI_fix_mmddyyyy_wXX.txt <br> Server: un2.bigdata.abc.gr <br> Path: /shared/abc/export_sai_csi/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /csi]\\n```\\n**User**: `mtuser`\\n**Local path**: `/shared/abc/export_sai_csi/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/csi`\\n**Logs**: ```/shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log```\\n**Script**: `/shared/abc/export_sai_csi/run/export_csi_fix.sh` on `un2.bigdata.abc.gr`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Find in the failed execution's log the message:\\n    ``` logs\\n    # e.g for 2022-01-10\\n    Problem with 20220108.\\n    ```\\n    This means that table `sai.cube_indicators_it` does not contain new data and therefore there is nothing to be done during this execution. Load table `brond.cube_indicators` first and then execute the script.\\n**Ndefs**:\\n- If one date was missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If you need to export file for a specific date execute the script with argument `<yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/export_sai_csi/run/export_csi_fix.sh 20220313 >> /shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log 2>&1\\n    ```\\n### CSI_mob\\nUnder normal circumstances this file is produced every day and contains data for the current week of the Impala table `sai.sub_aggr_csi_it`. The filename format is `CSI_mob_mmddyyyy_mmddyyyy.txt` where the first date is the last loaded Monday and the second the current date. For example, the file containing data for the 2nd of February 2022 will be `CSI_mob_01312022_02022022.txt`.\\n``` mermaid\\n  graph TD\\n  A[Oozie: export_CSI_fix_and_mobile_daily] -->|SSH| B[Host: un2.bigdata.abc.gr <br> User: intra2]\\n  B -->|sudo to mtuser| C[Master Script]\\n```\\nThe workflow triggers a master script which in turn executes the substeps\\n**User**: `intra`\\n**Scheduler**: `Oozie`\\n**Schedule**: `Every day at 7:00`  \\n**Coordinator**: `export_CSI_fix_and_mobile_daily`\\n**Master Script**: `/shared/abc/location_mobility/run/run_csi_exports_daily.sh`\\nThe master script triggers the export procedure.\\n``` mermaid\\ngraph TD \\n  A[Impala: sai.sub_aggr_csi_it] -->| Impala Query | B[File: CSI_mob_mmddyyyy_mmddyyyy.txt <br> Server: un2.bigdata.abc.gr <br> Path: /shared/abc/export_sai_csi/out]\\n  B -->|SFTP| C[User: trustcenterftp <br> Server: cne.def.gr <br> SFTP Path: /csi]\\n```\\n**User**: `mtuser`\\n**Local path**: `/shared/abc/export_sai_csi/out`\\n**SFTP user**: `trustcenterftp`\\n**SFTP path**: `/csi`\\n**Logs**: ```/shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log```\\n**Script**: `/shared/abc/export_sai_csi/run/export_csi_mob_daily.sh` on `un2.bigdata.abc.gr`\\n**Troubleshooting Steps**:\\n- Identify system or service errors in the log file e.g failed Impala query.\\n- Find in the failed execution's log the message:\\n    ``` logs\\n    # e.g for 2022-01-10\\n    Problem with 20220108.\\n    ```\\n    This means that table `sai.sub_aggr_csi_it` does not contain new data and therefore there is nothing to be done during this execution. Load table `sai.sub_aggr_csi_it` first and then execute the script.\\n**Ndefs**:\\n- If one date was missing the script will catch up at the next execution, assuming the table has been loaded. Before manually executing the script in this case, check if the missing file has been automatically exported in the reconciliation log.\\n- If you need to export file for a specific date execute the script with argument `<yyyymmdd>` flag. For example if file for 13th of March 2022 was not exported run:\\n    ``` bash\\n    /shared/abc/export_sai_csi/run/export_csi_mob_daily.sh 20220313 >> /shared/abc/export_sai_csi/log/sai_csi.cron.`date +%Y%m%d`.log 2>&1\\n    ```"
        },
        {
            "question": "How do I manually update the BIOS and iDRAC firmware on a Dell PowerEdge C6320 server via CLI?",
            "answer": "To update the BIOS and iDRAC firmware via CLI on a PowerEdge C6320:\n1. Download the `.BIN` update packages:\n   - BIOS: `BIOS_CCTDP_LN64_2.13.0.BIN`\n   - iDRAC: `iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN`\n2. Transfer them to `/tmp/` on the server.\n3. Run:\n```bash\nchmod +x ./iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\n./iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\n```\nRepeat similarly for the BIOS file.\n4. Follow prompts to complete installation. Ensure the process completes without interruption.",
            "category": "Infrastructure & Deployment",
            "files": "X20201125-IM1363402.md",
            "context": "---\\ntitle: PowerEdge C6320 BIOS and iDRAC Update After Hardware Failure on sn87\\ndescription: Step-by-step hardware remediation for sn87 node removal from BigStreamer cluster due to CPU issue, including opening Dell case, collecting lifecycle logs, updating iDRAC and BIOS firmware via OS CLI on PowerEdge C6320 servers.\\ntags:\\n  - bigstreamer\\n  - abc\\n  - sn87\\n  - hardware-failure\\n  - dell\\n  - poweredge-c6320\\n  - idrac\\n  - bios\\n  - firmware-upgrade\\n  - cli-update\\n  - lifecycle-controller\\n  - ipmitool\\n  - tsr\\n  - support-assist\\n  - dell-case\\n  - server-out-of-cluster\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  cluster: abc\\n  node: sn87\\n  server_model: PowerEdge C6320\\n  issue_id: IM1363402\\n  vendor: Dell\\n  dell_case_id: 2108129800\\n  troubleshooting_interface: iDRAC\\n  update_tools:\\n    - ipmitool\\n    - Support Assist\\n    - OS shell CLI\\n  firmware_components_updated:\\n    - BIOS: 2.13.0\\n    - iDRAC: 2.81.81.81\\n  update_files:\\n    - BIOS_CCTDP_LN64_2.13.0.BIN\\n    - iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\\n  reference_links:\\n    - https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Updating%20BIOS%20on%20Dell%2013G%20PowerEdge%20Servers.pdf\\n    - https://dl.dell.com/topicspdf/idrac_2-75_rn_en-us.pdf\\n---\\n# abc - BigStreamer - IM1363402 - abc BigStreamer - HW\\n## Description\\nWe see that sn87 has a problem with the CPU (attached). It has gone out of cluster.\\n## Root Cause\\nsn87 was removed from the cluster due to a CPU-related hardware fault, verified through iDRAC logs. BIOS and iDRAC versions were outdated.\\n## Actions Taken\\n1. Check Idrac logs for the description error `Overview-->Server-->Logs`\\n2. Export the lifecycle logs `Overview-->Server-->Troubleshooting-->Support Assist-->Export Collection` and save the TSR*.zip\\n3. Open a case on DELL SUPPORT(2108129800). Dell need the service tag from `Overview` of Idrac\\n4. Send them the TSR*.zip\\n5. In this case necessary was the update of BIOS & Lifecycle Controller of Idrac\\n6. Dell send us the right update files based on our servers `PowerEdge C6320`\\n7. Updated the BIOS base on the link `https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Updating%20BIOS%20on%20Dell%2013G%20PowerEdge%20Servers.pdf`\\n(to update the BIOS via OS-CLI, see APPENDIX below )\\n8. Updated the Lifecycle Controller base on the link `https://dl.dell.com/topicspdf/idrac_2-75_rn_en-us.pdf`\\n(to update the Lifecycle Controller via OS-CLI, see APPENDIX below )\\n9. After the update of both versions the host was up with the roles stopped for 1 day.\\n10. After 1 day send the lifecycle logs like `Step 2` and forward the zip file to Dell.\\n11. If any error exist start the roles.\\n## Affected Systems\\nabc Bigstreamer HW\\n## References\\nThe following appendix describes the full CLI-based firmware upgrade process for Dell PowerEdge C6320 servers.\\n### Appendix: BIOS and iDRAC Firmware Upgrade via OS Shell on PowerEdge C6320\\n-------------------------------------------------------------------------------\\n- Download new iDRAC FW from link below (Nfgh: download the \\\".bin\\\" format, not the \\\".exe\\\" format): https://www.dell.com/support/home/en-us/drivers/driversdetails?driverid=5hn4r&oscode=naa&productcode=poweredge-c6320\\neg: iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \\n- Download new BIOS from (Nfgh: download the \\\".bin\\\" format, not the \\\".efi\\\" format): https://www.dell.com/support/home/en-us/drivers/driversdetails?driverid=cctdp&oscode=naa&productcode=poweredge-c6320\\neg : BIOS_CCTDP_LN64_2.13.0.BIN\\nProcedure :\\n---------------\\nLogin to C6320 eg sn75 as root\\nStore the downloaded files under /tmp/\\nProcedure executed via OS shell\\nGet current BIOS version \\n---------------------------\\n[root@sn75 /]# ipmitool  mc getsysinfo system_fw_version 2.3.4\\nGet current iDRAC version\\n---------------------------\\n[root@sn75 /]# ipmitool   mc info | grep Firmware\\nFirmware Revision         : 2.40\\nUPDATE iDRAC PROCEDURE (mc cold restart is preformed automatically) :\\n------------------------------------------------------------------------\\n```\\n[root@sn75 /]# \\n[root@sn75 tmp]# ll iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \\n\\n  -rw-r--r-- 1 root root 111350247 Dec  6 14:17 iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\\n[root@sn75 tmp]# chmod +x iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \\n[root@sn75 tmp]# ./iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \\n  Update Package 21.04.200 (BLD_1123)\\n  Copyright (C) 2003-2021 Dell Inc. All Rights Reserved.\\n  Release Title:\\n  iDRAC 2.81.81.81, A00\\n  Release Date:\\n  July 02, 2021\\n  Default Log File Name:\\n  5HN4R_A00\\n  Reboot Required:\\n  No\\n  Running validation...\\n  iDRAC\\n  The version of this Update Package is newer than the currently installed version.\\n  Software application name: iDRAC\\n  Package version: 2.81.81.81\\n  Installed version: 2.40.40.40\\n  Continue? Y/N:Y\\n  Executing update...\\n  WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER PRODUCTS WHILE UPDATE IS IN PROGRESS.\\n  THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE!\\n  ...............................................................   USB Device is not found\\n  ..............................................................   USB Device is not found\\n  ...............................................................   USB Device is not found\\n  Device: iDRAC\\n    Application: iDRAC\\n    Failed to reach virtual device. This could be caused by BitLocker or other security software being enabled. For more information, see the\\n    Update Package User\\u00e2\\u20ac\\u2122s Guide.\\n  The update completed successfully.\\n```\\n## Nfgh\\n------\\nIF THE ABOVE ERROR IS SHOWN, THEN REBOOT THE iDRAC (\\\"#ipmitool  -U root -P c0sm0t31 mc reset cold\\\")   and REPEAT to get the below correct output, without the \\\"Failed to reach virtual device.\\\" message:!!!\\n```  \\nDevice: iDRAC\\nApplication: iDRAC\\nUpdate Successful.\\nThe update completed successfully.\\n```\\nUpdate BIOS PROCEDURE (REBOOT REQUIRED !!!)\\n----------------------------------------------\\n```\\n[root@sn75 /]# cd /tmp\\n[root@sn75 /]#  ll BIOS_CCTDP_LN64_2.13.0.BIN \\n[root@sn75 /]#  chmod +x BIOS_CCTDP_LN64_2.13.0.BIN \\n[root@sn75 /]#  ./BIOS_CCTDP_LN64_2.13.0.BIN \\n  Running validation...\\n  \\n  PowerEdge C6320 BIOS\\n  \\n  The version of this Update Package is newer than the currently installed version.\\n  Software application name: BIOS\\n  Package version: 2.13.0\\n  Installed version: 2.3.4\\n  Continue? Y/N:Y\\n  Executing update...\\n  WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER PRODUCTS WHILE UPDATE IS IN PROGRESS.\\n  THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE!\\n  ................................................................................................................\\n  Device: PowerEdge C6320 BIOS\\n    Application: BIOS\\n    The BIOS image file is successfully loaded. Do not shut down, cold reboot, power cycle, or turn off the system, till the BIOS update is complete otherwise the\\n    system will be corrupted or damaged. Bios update takes several minutes and it may be unresponsive during that time. Nfgh: If OMSA is installed on the system,\\n    the OMSA data manager service stops if it is already running.\\n  \\n  Would you like to reboot your system now?\\n  \\n  Continue? Y/N:Y \\n``` \\nGet NEW BIOS version \\n------------------------\\n[root@sn75 /]# ipmitool  mc getsysinfo system_fw_version 2.3.4\\nGet NEW iDRAC version\\n------------------------\\n[root@sn75 /]# ipmitool   mc info | grep Firmware\\nFirmware Revision         : 2.40"
        },
        {
            "question": "What steps are needed to recover from a BigStreamer cluster-wide Namenode failover situation?",
            "answer": "If both Namenodes are in standby mode:\n1. Access Cloudera Manager: `https://172.25.37.232:7183`\n2. Restart `nn1`; this promotes `nn2` to active and `nn1` to standby.\n3. Validate services in Cloudera Manager and HUE (`https://172.25.37.236:8888/oozie/list_oozie_workflows/`).\n4. Use Kibana (`http://10.20.9.82:5601/app/kibana`) to review logs for anomalies.\n5. If needed, open a Cloudera support case with diagnostics.",
            "category": "Infrastructure & Deployment",
            "files": "X20201220-IM1391585.md",
            "context": "---\\ntitle: BigStreamer Namenode Failover and Service Instability Recovery\\ndescription: Incident response to cluster-wide failures in BigStreamer due to both Namenodes entering standby mode, affecting HDFS, HBase, Oozie, and geolocation streams; includes manual failover, service validation, and Cloudera case escalation.\\ntags:\\n  - bigstreamer\\n  - abc\\n  - hdfs\\n  - namenode\\n  - failover\\n  - hbase\\n  - oozie\\n  - impala\\n  - yarn\\n  - cloudera\\n  - hue\\n  - cluster-health\\n  - service-recovery\\n  - geolocation\\n  - locmob\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  issue_id: IM1391585\\n  cluster: abc\\n  interfaces:\\n    - Cloudera Manager: https://172.25.37.232:7183\\n    - HUE: https://172.25.37.236:8888/oozie/list_oozie_workflows/\\n    - Kibana: http://10.20.9.82:5601/app/kibana\\n  symptoms:\\n    - Both Namenodes in standby\\n    - HDFS/HBase/Oozie/Impala service alerts\\n    - Geolocation & Location Mobility stream failures\\n  resolution:\\n    - Restarted nn1 \\u2192 nn2 became active\\n    - Manually stabilized HDFS services\\n    - Opened Cloudera support case with diagnostics\\n---\\n# abc - IM1391585 - issue BigStreamer\\n## Description\\nPlease check immediately if BigStreamer is working properly.\\nWe have received many alerts over the weekend and today regarding HDFS, for various nodes as well as for services (eg oozie). We also noticed problems with geolocation streams and loc mob files.\\n## Actions Taken\\n1. Connect with personal creds `https://172.25.37.232:7183` Cloudera Manager\\n2. Both Namenodes entered standby mode simultaneously which caused bad health on HDFS,HBASE,OOZIE,IMPALA. After nn1 restarted nn2 became the Active and nn1 the Standby namenode. All the other services was stable after this manual action except HBASE which restarted.\\n> The dual-standby state of both Namenodes caused a loss of HDFS coordination, which cascaded failures to other dependent services like HBase, Oozie, and Impala. After restarting nn1, high availability was restored with nn2 becoming active.\\n3. Since all services were stable check HUE `https://172.25.37.236:8888/oozie/list_oozie_workflows/` to ensure that all workflows running.\\n4. The specific timeline which namenodes crashed the load,cpu,network,hdfs_read/write,nodes health,,namenodes health,impala queries if something heavy executed,yarn applications if something heavy executed `http://10.20.9.82:5601/app/kibana`\\n5. Opened a case on Cloudera with namenodes diagnostics.\\n## Affected Systems\\nabc Bigstreamer\\n## Action Points\\nMonitor the status/health of services and inform with mail/alert when a service/role is down."
        },
        {
            "question": "How can I restore a corrupted partition in the `refdata.rd_cells_load` table?",
            "answer": "To recover partition `20201110`:\n1. Copy the file from a good partition:\n```bash\nhdfs dfs -cp /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/\n```\n2. Hide the corrupt file:\n```bash\nhdfs dfs -mv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/.cells_20201110.csv\n```\n3. Refresh metadata:\n```bash\nimpala-shell -q 'refresh refdata.rd_cells_load;'\n```",
            "category": "Data Management & Query Execution",
            "files": "X20201202-IM1353607.md",
            "context": "---\\ntitle: Refdata.rd_cells_load Partition Recovery for 10/11 and 11/11\\ndescription: Data correction steps for low row count in partition 20201110 of refdata.rd_cells_load table in abc BigStreamer, by copying and renaming Parquet files in HDFS.\\ntags:\\n  - bigstreamer\\n  - abc\\n  - impala\\n  - hdfs\\n  - refdata\\n  - rd_cells\\n  - data-recovery\\n  - partition\\n  - data-load\\n  - partition-correction\\n  - hdfs-copy\\n  - csv\\n  - row-mismatch\\n  - impala-refresh\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  issue_id: IM1353607\\n  cluster: abc\\n  table: refdata.rd_cells_load\\n  corrected_partitions:\\n    - 20201110\\n  source_partition: 20201111\\n  impala_queries:\\n    - show partitions refdata.rd_cells_load\\n    - show files in refdata.rd_cells_load partition (par_dt>='20201110')\\n    - select par_dt, count(*) from refdata.rd_cells_load where par_dt>='20201109' group by par_dt\\n  hdfs_operations:\\n    - hdfs dfs -cp ...\\n    - hdfs dfs -mv ...\\n    - impala refresh refdata.rd_cells_load\\n---\\n# abc - BigStreamer - IM1353607  - abc BigStreamer (refdata.rd_cells)\\n## Description\\nPlease load the data for 11/11 and for 10/11\\n## Actions Taken\\n1. Check the size of current partition from Impala-Shell\\n``` bash\\nshow partitions refdata.rd_cells_load;\\n```\\nexample output\\n``` bash\\npar_dt   | #Rows     | #Files | Size    | Bytes Cached | Cache Replication | Format | Incremental stats | Location                                                                 \\n---------+-----------+--------+---------+--------------+-------------------+--------+-------------------+--------------------------------------------------------------------------\\n20201109 |    105576 |      1 | 44.82MB | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201109\\n20201110 |         6 |      1 | 191B    | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110\\n20201111 |    105325 |      1 | 45.63MB | NOT CACHED   | NOT CACHED        | TEXT   | true              | hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111\\n```\\nWe notisted significant less Rows for par_dt \\\"20201110\\\" .\\n2. We check \\\"20201110\\\" & \\\"20201111\\\" partition files from HDFS.\\n``` bash\\nimpala> refresh refdata.rd_cells_load;\\nimpala> show files in refdata.rd_cells_load partition (par_dt>='20201110');\\n```\\nexample output:\\n```\\nPath                                                                                         | Size    | Partition      \\n---------------------------------------------------------------------------------------------+---------+----------------\\nhdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv | 191B    | par_dt=20201110\\nhdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv | 45.63MB | par_dt=20201111\\n```\\n3. We copy partition \\\"20201111\\\" file to \\\"20201110\\\".\\n``` bash\\nhdfs dfs -cp /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201111/cells_20201111.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/\\n```\\n> The 20201110 partition was missing valid data, so we copied the 20201111 file as a substitute.\\n4. We rename to hide the file for \\\"20201110\\\".\\n``` bash\\nhdfs dfs -mv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/cells_20201110.csv /ez/warehouse/refdata.db/rd_cells_load/par_dt=20201110/.cells_20201110.csv\\n```\\n> This preserves the original but hides it from Impala visibility by prefixing with a dot.\\n5. Repeat Step 2.\\n6. We execute the query bellow to check if the partitions \\\"20201111\\\" & \\\"20201110\\\" have the same number of Rows.\\n``` bash\\nimpala> select par_dt, count(*) cnt from refdata.rd_cells_load where par_dt>='20201109' group by par_dt order by 1;\\n```\\nexample output\\n```\\npar_dt   | cnt   \\n---------+-------\\n20201109 | 105576\\n20201110 | 105325\\n20201111 | 105325\\n```\\n> The row count for 20201110 is slightly lower than 20201109 due to missing historical data before correction. Partition 20201111 was reused as a fallback."
        },
        {
            "question": "Why did the osix.sip ingestion stop on 25/11/2020, and how was it resolved?",
            "answer": "The OSIX-SIP-NORM topology had stopped and the monitoring script failed to resubmit it. Resolution involved:\n1. SSH into `unosix1` and switch to `osix` user.\n2. Run `./submit_sip_norm.sh` in the topology path.\n3. Use `yarn application -list | grep OSIX-SIP-NORM` to verify.\n4. Validate data with:\n```sql\nSELECT count(*), par_dt FROM osix.sip WHERE par_dt>'20201124' GROUP BY par_dt;\n```",
            "category": "Troubleshooting & Issue Resolution",
            "files": "X20201126-IM1367129.md",
            "context": "---\\ntitle: osix.sip Ingestion Stopped on 25/11/2020 - Topology and Listener Investigation\\ndescription: Resolution steps for halted data ingestion in osix.sip table starting 25/11/2020 07:00, including OSIX-SIP-NORM topology checks, listener health validation, and manual topology resubmission.\\ntags:\\n  - bigstreamer\\n  - abc\\n  - osix\\n  - osix.sip\\n  - data-ingestion\\n  - listener\\n  - topology\\n  - resubmit\\n  - kudu\\n  - impala\\n  - yarn\\n  - coord_OsixStreaming_SIP_MonitorResubmit\\n  - monitoring\\n  - streaming\\n  - sip_norm\\n  - log-analysis\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  issue_id: IM1367129\\n  cluster: abc\\n  component: osix.sip\\n  affected_node: unosix1\\n  ingestion_stopped_at: 2020-11-25 07:00\\n  scripts_used:\\n    - submit_sip_norm.sh\\n  monitoring_tools:\\n    - coord_OsixStreaming_SIP_MonitorResubmit\\n    - http://172.25.37.251/dashboard/#osix_listeners\\n  logs_checked:\\n    - /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM/date_monitor_sip_norm.log\\n  commands_executed:\\n    - yarn application -list\\n    - impala-shell SELECT par_dt\\n  reference_docs:\\n    - 18316_abc_Generic_MOP_CDH_5_16_2_Upgrade\\n---\\n# abc - BigStreamer - IM1367129 - osix.sip 25/11/2020\\n## Description\\nNo data is being loaded from 25/11/2020 at 07:00 on osix.sip.\\n## Actions Taken\\n1. ssh unosix1 with your personal account\\n2. sudo -iu osix\\n3. kinit -kt osix.keytab osix\\n4. yarn application -list | grep OSIX-SIP-NORM\\n5. In our case the topology was down and the kudu script didn't resubmit it.\\n> Root cause: The OSIX-SIP-NORM topology was not running and the automated monitor did not restart it.\\n6. Check if `coord_OsixStreaming_SIP_MonitorResubmit` is running.\\n7. listener is healthy and receiving data `http://172.25.37.251/dashboard/#osix_listeners`\\n8. The rate for `listen_sip_core` should be between 12K and 22K messages.If there is an extreme problem e.g. the rate is 0, consider restarting the problematic listener.\\n> If monitor script failed to detect the downed topology, inspect failure reasons in its log.\\n9. Check the logs of monitor script `hdfs dfs -ls /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM` and `hdfs dfs -cat /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM/date_monitor_sip_norm.log`\\n10. Start again the topology `unosix1:/home/users/osix/topologies/binary-input-impala-output/sip_norm/` and execute `./submit_sip_norm.sh` until the state appeared `RUNNING`\\n11. yarn application -list | grep OSIX-SIP-NORM\\n12. Connect to impala-shell or Hue and execute `SELECT count(*), par_dt FROM osix.sip WHERE par_dt>'20201124' group by par_dt;` to check if the data inserted on the table.\\n> Ensure new partitions are created by verifying that `par_dt='20201125'` is present.\\n## Affected Systems\\nabc Bigstreamer\\n## Nfgh\\nRecommended Mop for help `18316_abc_Generic_MOP_CDH_5_16_2_Upgrade`"
        },
        {
            "question": "How do you verify if an ingestion script for the `pollaploi` table ran successfully?",
            "answer": "To validate the workflow:\n1. Check for new files via SFTP: `sftp bigd@172.16.166.30`\n2. On the server (`un2`), verify file count:\n```bash\nwc -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/<filename>\n```\n3. Run:\n```sql\nSELECT count(*) FROM energy_efficiency.pollaploi;\n```\n4. The row count from the file should match the table.\n5. Also verify workflow status in HUE under `energy_efficiency_load_pollaploi`.",
            "category": "Troubleshooting & Issue Resolution",
            "files": "X20201211-IM1382364.md",
            "context": "---\\ntitle: Energy Efficiency - pollaploi Table Update Monitoring and Validation\\ndescription: Troubleshooting steps and validation procedure for verifying updates in the energy_efficiency.pollaploi table in abc BigStreamer, including workflow status, file comparison, and row count consistency.\\ntags:\\n  - bigstreamer\\n  - abc\\n  - energy_efficiency\\n  - pollaploi\\n  - workflow\\n  - hue\\n  - sftp\\n  - impala\\n  - data-validation\\n  - table-update\\n  - data-ingestion\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  issue_id: IM1382364\\n  schema: energy_efficiency\\n  table: pollaploi\\n  source_server: 172.16.166.30\\n  source_directory: energypm\\n  workflow: energy_efficiency_load_pollaploi\\n  nodes:\\n    - un2.bigdata.abc.gr\\n  user: intra\\n  log_files:\\n    - /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log\\n    - /shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.next_date.log\\n  source_data_path: /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/\\n  validation_steps:\\n    - file row count vs table count match\\n    - workflow success\\n    - Impala query execution check\\n---\\n# abc - IM1382364 - Energy efficiency info update\\n## Description\\nThis task involves monitoring the pollaploi table to confirm it is updated whenever a new file arrives and diagnosing why recent data may not have been ingested.\\nPlease let us know whenever the pollaploi table in schema energy efficiency is updated. \\nAlso to investigate why an update has not been made based on the latest file.\\n## Actions Taken\\n1. ssh un2 with your personal account\\n2. sudo -iu intra\\n3. sftp `bigd@172.16.166.30`\\n4. cd energypm\\n5. ls -ltr\\n6. Open HUE dashboard and search for `energy_efficiency_load_pollaploi` Workflow\\n7. Check if workflow failed.\\n8. ssh `un2` with your personal account.\\n9. sudo -i\\n10. less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log` and less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.next_date.log`. The next date should return no changes.\\n> Confirm that the log mentions \\\"no new data\\\" or similar, indicating no update was needed.\\n11. At un2 `wc -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/2020_10_pollaploi.txt`\\n12. Connect toImpala using impala-shell and execute `select count(*) from energy_efficiency.pollaploi`\\n> The number of rows in the 2020_10_pollaploi.txt file (from `wc -l`) should match the row count in the pollaploi table.\\n13. The row counts from step 11 and step 12 must match\\n14. Check on Impala Queries UI if the queries ran without exception `STATEMENT RLIKE '.*energy_efficiency_load_pollaploi.*'`\\n## Affected Systems\\nabc Bigstreamer"
        },
        {
            "question": "What should be done when RStudio Connect reports a license expiration error due to proxy or time sync issues?",
            "answer": "First, SSH into the server and ensure system time and timezone are correct using:\n```bash\nsudo timedatectl\nsudo hwclock -w\n```\nThen export required proxy settings:\n```bash\nexport http_proxy=<proxy>\nexport https_proxy=<proxy>\n```\nFinally, run:\n```bash\n/opt/rstudio-connect/bin/license-manager deactivate\n/opt/rstudio-connect/bin/license-manager activate <product-key>\n/opt/rstudio-connect/bin/license-manager verify\nsudo systemctl restart rstudio-connect\n```",
            "category": "Application Functionality & Flow",
            "files": "X20211005-IM1663315.md",
            "context": "---\\ntitle: RStudio Connect License Expired - Activation Recovery Procedure\\ndescription: Step-by-step procedure for resolving expired license issues in RStudio Connect, including system time sync, proxy setup, license activation/deactivation with `license-manager`, and service restarts. Covers common errors and recovery instructions for failed activation due to internet or certificate issues.\\ntags:\\n  - bigstreamer\\n  - rstudio\\n  - rstudio-connect\\n  - license expired\\n  - license-manager\\n  - activation error\\n  - proxy\\n  - time sync\\n  - reboot\\n  - unrstudio1\\n  - rstudio-connect\\n  - ssl\\n  - product key\\n  - verify license\\n  - timezone\\n  - systemctl\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: login failure 04/10\\n  system: abc BigStreamer RStudio Connect\\n  error_message: License Expired\\n  root_cause: license-manager could not reach license server due to proxy/SSL issues\\n  server: unrstudio1\\n  recovery_commands:\\n    - license-manager deactivate\\n    - license-manager activate\\n    - license-manager verify\\n    - systemctl restart rstudio-connect\\n  special_case: activation error (13) caused by incorrect system time or timezone\\n---\\n# abc - RStudio - login failure 04/10\\n## Description\\nRStudio Connect returned a license expiration message on 04/10. Attempts to connect failed due to inability to verify or reactivate the license. This was likely caused by system time issues or inability to reach the license server over HTTPS.\\nThe message was the following:\\n```\\nAn error has occurred\\nLicense Expired\\nYour RStudio Connect license has expired. Please contact your Customer Success representative or email sales@rstudio.com to obtain a current license.\\n```\\n## Actions Taken\\n### Initial Troubleshooting and Environment Preparation\\n1. ssh unrstudio1\\n2. Make sure the time zone is correct for the machine. (sudo timedatectl)\\n3. Resync the date and time of the machine. (sudo hwclock -w)\\n### Deactivation and Activation Attempt\\n4. /opt/rstudio-connect/bin/license-manager deactivate\\n```\\nError deactivating product key: (19): Connection to the server failed. Ensure that you have a working internet connection, you've configured any required proxies, and your system's root CA certificate store is up to date; see https://rstudio.org/links/licensing_ssl for more information.\\n```\\n5. /opt/rstudio-connect/bin/license-manager activate <product-key>\\n```\\nError verify: (19): The product is activated however the license manager is currently unable to connect to the license server to verify the activation.\\nPlease ensure that you can make a connection to the activation server and then re-activate the product.\\n```\\n### Proxy Configuration for License Server Access\\n6. To pass the error from the step 4&5 then export `export http_proxy=<ip:port>` & 'export https_proxy=<ip:port>' 'export http_proxy=<ip:port>'\\n7. Try again to deactivate like step 4\\n### License Status and Verification\\n8. Try again to activate like step 5 and then run the below commands:\\n```bash\\nsudo /opt/rstudio-connect/bin/license-manager status\\nsudo /opt/rstudio-connect/bin/license-manager verify\\n/opt/rstudio-connect/bin/license-manager verify #run without sudo\\n```\\n### Service Restart\\n9. systemctl restart rstudio-connect # ONLY IF the Activaton Status on step 8 was `Activated`\\n10. systemctl status rstudio-connect\\n## Affected Systems\\nabc Bigstreamer Rstudio-Connect\\n### Troubleshooting Activation Error Code (13)\\n**In case you  receive the following error while executing step 5:**\\n```\\nError activating product key: (13): The activation has expired or the system time has been tampered with. Ensure your time, timezone, and date settings are correct. If you're sure the license is not expired, try performing the following steps, in order: \\n1. Fix the timezone on your system.\\n2. Fix the date on your system.\\n3. Fix the time on your system.\\n4. Perform a system restart (important!)\\n```\\nYou must **reboot** your node and then repeat 1-10 steps"
        },
        {
            "question": "How can you manually rerun a failed Prometheus `dwh22_last` load due to a cron failure?",
            "answer": "If the cron job failed, SSH into the node and manually override the `yesterday_dt` in:\n```bash\n/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh\n```\nThen execute:\n```bash\n/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.<date>.log\n```\nVerify the table via:\n```sql\nselect count(*), par_dt from prometheus.dwh22_last group by par_dt;\n```",
            "category": "Application Functionality & Flow",
            "files": "X20211215-IM1742741.md",
            "context": "---\\ntitle: Prometheus Table `dwh22_last` Empty \\u2013 Cron Job Debug & Reload\\ndescription: Investigation and resolution steps for the empty `prometheus.dwh22_last` table and downstream view `prometheus.prom_total_subscrs`. Includes cron validation, log inspection, manual script rerun with parameter substitution, and data verification queries.\\ntags:\\n  - bigstreamer\\n  - prometheus\\n  - dwh22_last\\n  - prom_total_subscrs\\n  - empty table\\n  - missing data\\n  - cronjob\\n  - hive\\n  - impala\\n  - data pipeline\\n  - historical table\\n  - last table\\n  - reload\\n  - manual rerun\\n  - script override\\n  - log inspection\\n  - daily partition\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: N/A\\n  system: abc BigStreamer Prometheus Load\\n  failure_target: prometheus.dwh22_last (and prom_total_subscrs view)\\n  trigger: empty partition on `dwh22_last`\\n  script: /shared/abc/prometheus/bin/Cron_Prometheus_Load.sh\\n  log_dir: /shared/abc/prometheus/log/\\n  root_cause: cron failed or did not run for target date\\n  recovery_method: manual override of yesterday_dt and re-run of load script\\n  verification_query: select count(*), par_dt from prometheus.dwh22_last group by par_dt;\\n---\\n# abc - BigStreamer/BackEnd  - prometheus.dwh22_last empty \\nThis document outlines how to resolve an empty prometheus.dwh22_last table due to a missed or failed cron execution, including log investigation, manual script rerun with date override, and post-load verification.\\n## Actions Taken\\n### Step 1 \\u2013 Identify and Validate Cron Schedule\\n1. ssh un2 with your personal account; sudo -iu intra\\n### Step 2 \\u2013 Check Script Execution Log\\n2. crontab -l | grep prometheus\\n```bash\\n0 6 * * * /shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.`date '+\\\\%Y\\\\%m\\\\%d'`.log 2>&1\\n```\\n### Step 3 \\u2013 Rerun Cron Manually with Date Override (If Needed)\\n3. Check the latest log file to find the root cause `/shared/abc/prometheus/log/Cron_Prometheus_Load.date_of_issue.log`\\n### Step 4 \\u2013 Revert Temporary `yesterday_dt` Override\\n4. If the issue date is today (i.e., partition not yet generated), simply re-run the script as is: `/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.issue_date.log`\\nIf the issue date passed then comment the `yesterday_dt=` and replace it with `yesterday_dt=<issue date -1>`. Then run the script ``/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.issue_date.log``\\n### Step 5 \\u2013 Validate Load via SQL\\n5. When the script finished replace the old value of `yestarday_dt` on script.\\n6. Checks:\\n```sql\\nselect count(*),par_dt from prometheus.table where par_dt >= 'issue_date -1' group by 2;\\n```\\n## Affected Systems\\nabc Bigstreamer Prometheus Tables\\n```\\nprometheus.DWH22_hist - IMPALA\\nprometheus.DWH22_last - IMPALA\\nrometheus.dwh3_hist - HIVE\\nprometheus.DWH3_hist - IMPALA\\nprometheus.DWH3_last - IMPALA\\nprometheus.dwh4_hist - HIVE\\nprometheus.DWH4_hist  - IMPALA\\nprometheus.DWH4_last - IMPALA\\nprometheus.dwh9_hist - HIVE\\nprometheus.DWH9_hist - IMPALA\\nprometheus.DWH9_last - IMPALA\\nprometheus.dwh11_hist - HIVE\\nprometheus.DWH11_hist - IMPALA\\nprometheus.DWH11_last - IMPALA\\nprometheus.dwh14_hist - HIVE\\nprometheus.DWH14_hist - IMPALA\\nprometheus.DWH14_last - IMPALA\\nprometheus.dwh17_hist - HIVE\\nprometheus.DWH17_hist - IMPALA\\nprometheus.DWH17_last - IMPALA\\nprometheus.dwh2_hist - HIVE\\nprometheus.DWH2_hist - IMPALA\\nprometheus.DWH2_last - IMPALA\\nprometheus.dwh43_hist - HIVE\\nprometheus.DWH43_hist - IMPALA\\nprometheus.DWH43_last - IMPALA\\n```"
        },
        {
            "question": "How do you recover missing partitions in the `aums.archive_data` table using SFTP and Streamsets?",
            "answer": "1. SSH into the SFTP server:\n```bash\nsftp bigd@172.16.166.30\n```\n2. Get and re-upload the missing zip files.\n3. Trigger reprocessing by refreshing metadata in Impala:\n```sql\nrefresh aums.archive_data;\nshow files in aums.archive_data partition (par_dt>='20220330');\n```\n4. Monitor logs at `/shared/sdc/log` to ensure Streamsets picks up the files.",
            "category": "Troubleshooting & Issue Resolution",
            "files": "X20220331-IM1829518.md",
            "context": "---\\ntitle: Missing Partitions in aums.archive_data Table (March 30\\u201331, 2022)\\ndescription: Manual diagnosis and re-upload procedure for missing partitions in the `aums.archive_data` table for 2022-03-30 and 2022-03-31. Details SFTP file verification, local backup and re-upload, Streamsets automated reprocessing, and verification via HDFS and Impala.\\ntags:\\n  - bigstreamer\\n  - aums\\n  - archive_data\\n  - streamsets\\n  - missing data\\n  - sftp\\n  - partition load\\n  - hdfs\\n  - impala\\n  - data pipeline\\n  - data ingestion\\n  - intra\\n  - zip files\\n  - reload\\n  - streamsets logs\\n  - manual reingestion\\n  - aems_data\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM1829518\\n  system: abc BigStreamer AUMS ingestion\\n  affected_table: aums.archive_data\\n  missing_dates:\\n    - 2022-03-30\\n    - 2022-03-31\\n  data_source: SFTP server at 172.16.166.30\\n  file_pattern: aems_data_YYYYMMDDHHMMSS.zip\\n  ingestion_tool: Streamsets\\n  root_cause: Files existed on SFTP but ingestion was skipped\\n  recovery_steps:\\n    - Pull, delete, and re-upload missed zip files\\n    - Trigger Streamsets to re-ingest\\n    - Validate new partitions in Impala\\n    - Monitor /shared/sdc/log\\n---\\n# abc - BigStreamer - IM1829518 -  missing data  aums.archive_data\\n## Description\\naums schema archive_data table has not loaded data for 03/30/2022\\n## Actions Taken\\n### 1. Check Partition in HDFS\\n1.  Login to un2 and change to intra user with `sudo su - intra ` command\\n2.  Give the following command in order to check the wanted partition\\n```bash\\n[intra@un2 ~]$ sudo -u hdfs hdfs dfs -ls /ez/warehouse/aums.db/archive_data/par_dt=20220330`\\n```\\nYou must be able to see the following ouput \\n```bash\\nls: /ez/warehouse/aums.db/archive_data/par_dt=20220330': No such file or directory\\n```\\n### 2. Refresh and Verify Table in Impala\\n3.  Connect to impala with `intra` user in order to refresh the table\\n` > refresh aums.archive_data;`\\n4.  Check if you can see the missing data with the following command from impala using `intra` user:\\n```bash\\n> show files in aums.archive_data partition (par_dt>='20220329');\\n```\\nIf not then let's check the sftp server. You will notice that files for 31/03/2022 also missing.\\n### 3. Validate Files on SFTP Server\\n5.  From un2: `ssh bigd@172.16.166.30/;`\\n```bash\\nsftp> ls aums\\n```\\nYou must be able to see the zip files : aems_data_20220329233417.zip and aems_data_20220330233347.zip for 30/03/2022 and 31/03/2022.\\n### 4. Re-download and Re-upload Missing Files\\n6. Lets try to put those file to a local directory, remove them and upload them with the following commands: \\n```bash\\n[intra@un2 data_aums]$ sftp bigd@172.16.166.30\\n```\\nConnected to 172.16.166.30.\\nLocally transfer the file for 30/03/2022:\\n```bash\\nsftp> get aems_data_20220330233347.zip\\n```\\nRemove file:\\n```bash\\nsftp> rm  aems_data_20220330233347.zip\\n```\\nLocally transfer the file for 31/03/2022:\\n```bash\\nsftp> get aems_data_20220331233417.zip\\n```\\nRemove file:\\n```bash\\nsftp> rm  aems_data_20220331233417.zip\\n```\\nNow, let's upload those files again:\\n```bash\\nsftp> put aems_data_20220330233347.zip`\\nsftp> put aems_data_20220331233417.zip\\n```\\n### 5. Verify Partition Reload via Impala\\n7. Streamsets won't upload those files simultaneously. You will be able to see first the partition for 30/03/2022 and secondly partition for 31/03/2022.\\nFrom impala shell with `intra` user run the following command and make sure you will be able to see the missing partitions\\n```bash\\n> show files in aums.archive_data partition (par_dt>='20220330');\\n```\\n### 6. Monitor Streamsets Logs\\n8. Check logs at un2:/shared/sdc/log"
        },
        {
            "question": "What causes recurring CDSW downtime every Sunday morning, and how can it be diagnosed?",
            "answer": "High disk I/O on `/var/lib/cdsw` leads to Kubernetes control plane (etcd) failures. Diagnose by checking logs:\n```bash\nless /var/log/cdsw/cdsw_health.log\nless /var/log/warn | grep grpc\ncdsw status\n```",
            "category": "Infrastructure & Deployment",
            "files": "X20230130-IM2073052.md",
            "context": "---\\ntitle: CDSW Outage Due to Disk I/O Saturation on mncdsw1 Node\\ndescription: Investigation into recurring CDSW service failures caused by high disk I/O on `/var/lib/cdsw` (dm-7) resulting in etcd timeouts and failure of Kubernetes control plane pods. Logs show loss of apiserver connectivity and persistent Sunday morning crashes.\\ntags:\\n  - bigstreamer\\n  - cdsw\\n  - mncdsw1\\n  - etcd\\n  - disk io\\n  - kubelet\\n  - control plane\\n  - kubernetes\\n  - cdsw pods\\n  - grpc errors\\n  - liveness probe\\n  - unexpected error\\n  - /var/lib/cdsw\\n  - pv nfs\\n  - project storage\\n  - sunday crash\\n  - root cause analysis\\n  - cluster instability\\n  - application unresponsive\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM2073052\\n  system: abc BigStreamer CDSW\\n  root_cause: High disk I/O on /var/lib/cdsw (dm-7) leading to etcd timeout and Kubernetes control plane failure\\n  node_affected: mncdsw1.bigdata.abc.gr\\n  storage_path: /var/lib/cdsw\\n  symptom: \\\"Unexpected error\\\" in UI and CDSW pods check failure\\n  recurrence: Every Sunday ~06:00 based on `/var/log/warn` and `stderr.log`\\n  affected_component: Kubernetes etcd / apiserver / control plane pods\\n  action_taken: Restart CDSW, check logs, raise architecture issue\\n---\\n# abc - BigStreamer - IM2073052 - CDSW Crash Due to Disk I/O on mncdsw1\\n## Description\\nThe error \\\"Unexpected Error\\\" presented to users was traced to an etcd failure caused by high disk I/O on the CDSW storage volume. Despite the node mncdsw1 restarting and appearing healthy, CDSW services were unable to synchronize due to control plane issues.\\nIn CDSW we get error \\\"Unexpected Error. An unexpected error occurred\\\" when connecting. We saw that the node mncdsw1.bigdata.abc.gr was down. We did a restrart, just that, and it now appears to be in good health status.\\nHowever, we still get the same error.\\nIn CDSW status it has the following message:\\n```\\nFailed to run CDSW Nodes Check. * Failed to run CDSW system pods check. * Failed to run CDSW application pods check. * Failed to run CDSW services check. * Failed to run CDSW secrets check. * Failed to run CDSW persistent volumes check. * Failed to run...\\n```\\n## Actions Taken\\n### 1. Restart CDSW via Cloudera Manager\\n1. Restart CDSW\\nThe customer had already restarted CDSW, so we tried it once more in order to live monitor it.\\n```bash\\nCloudera Manager -> CDSW -> Restart\\n```\\n### 2. Monitor Startup Logs with cdsw status\\n2. Check status\\nWe followed the logs until CDSW was available again.\\n```bash\\n#from mncdsw1\\ncdsw status\\n...\\nCloudera Data Science Workbench is ready!\\n```\\nSince CDSW was up and running, we continued with root cause analysis.\\n### 3. Inspect /var/log/cdsw/cdsw_health.log for Errors\\n3. Check logs\\n```bash\\nless /var/log/cdsw/cdsw_health.log\\n```\\nFirstly, we noticed an abnormal behavior with some of the control plane pods:\\n```bash\\n2023-01-29 05:50:53,868 ERROR cdsw.status:Pods not ready in cluster kube-system ['component/kube-controller-manager', 'component/kube-scheduler'].\\n```\\nAnd after that, CDSW lost connection with apiserver pod completely:\\n```bash\\n2023-01-29 05:51:42,392 WARNING urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549bb50>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\\n2023-01-29 05:51:42,735 WARNING urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b710>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\\n2023-01-29 05:51:43,065 WARNING urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b050>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\\n2023-01-29 05:51:43,371 ERROR cdsw.status:Failed to run CDSW Nodes Check.\\n```\\n### 4. Investigate Disk I/O Saturation on /var/lib/cdsw\\n4. Check node resources\\nFrom Cloudera Manager we saw that CPU and Memory were not increased but Disk I/O reached 100%.\\n![IM2073052_diskio](.media/IM2073052_diskio.png)\\nFrom the image above we noticed that the issue occured on dm-7.\\n```bash\\n[root@mncdsw1 ~]# ll /dev/mapper/cdsw-var_lib_cdsw\\nlrwxrwxrwx 1 root root 7 Dec 16  2021 /dev/mapper/cdsw-var_lib_cdsw -> ../dm-7\\n```\\n```bash\\n[root@mncdsw1 ~]# lsblk | grep cdsw-var\\n\\u2514\\u2500cdsw-var_lib_cdsw                                                                         253:7    0   931G  0 lvm  /var/lib/cdsw\\n```\\n```bash\\n[root@mncdsw1 ~]# kubectl get pv\\nNAME\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 CAPACITY\\u00a0\\u00a0 ACCESS MODES\\u00a0\\u00a0 RECLAIM POLICY\\u00a0\\u00a0 STATUS\\u00a0\\u00a0 CLAIM\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 STORAGECLASS\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 REASON\\u00a0\\u00a0 AGE\\n0c9df8bb\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 1Ti\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 RWX\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Retain\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Bound\\u00a0\\u00a0\\u00a0 default-user-120/b128af5f\\u00a0\\u00a0 cdsw-storageclass-whiteout\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 83m\\n1214923b\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 1Ti\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 RWX\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Retain\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Bound\\u00a0\\u00a0\\u00a0 default-user-98/1ec1e99a\\u00a0\\u00a0\\u00a0 cdsw-storageclass-whiteout\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 11m\\n1297834a\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 1Ti\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 RWX\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Retain\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Bound\\u00a0\\u00a0\\u00a0 default-user-9/740094c3\\u00a0\\u00a0\\u00a0\\u00a0 cdsw-storageclass-whiteout\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 54s\\n1a2f7a8a\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 1Ti\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 RWX\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Retain\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Bound\\u00a0\\u00a0\\u00a0 default-user-9/92acb87f\\u00a0\\u00a0\\u00a0\\u00a0 cdsw-storageclass-whiteout\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 55s\\n1f498fe8\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 1Ti\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 RWX\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Retain\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 Bound\\u00a0\\u00a0\\u00a0 default-user-120/588500de\\u00a0\\u00a0 cdsw-storageclass-whiteout\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 106s \\n```\\n```bash\\n[root@mncdsw1 ~]# kubectl get pv 0c9df8bb -o yaml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n   name: 0c9df8bb\\nspec:\\n   accessModes:\\n   - ReadWriteMany\\n   capacity:\\n     storage: 1Ti\\n   mountOptions:\\n   - nfsvers=4.1\\n   nfs:\\n      path: /var/lib/cdsw/current/projects/cdn/4xsyzsv0lnij00ob\\n      server: 10.255.241.130\\n   persistentVolumeReclaimPolicy: Retain\\n     storageClassName: cdsw-storageclass-whiteout\\n     volumeMode: Filesystem \\n```\\nIt seems that every CDSW project uses mncdsw1:/var/lib/cdsw for storage.\\n5. Check kubelet logs\\n```bash\\nll /run/cloudera-scm-agent/process/ | grep -i master\\n```\\n```bash\\n[root@mncdsw1 ~]# ll /run/cloudera-scm-agent/process/145081-cdsw-CDSW_MASTER/logs/\\ntotal 111880\\n-rw-r--r-- 1 root root  9658036 Jan 30 10:24 stderr.log\\n-rw-r--r-- 1 root root 10485841 Jan 30 05:42 stderr.log.1\\n-rw-r--r-- 1 root root 10485989 Jan  4 19:40 stderr.log.10\\n-rw-r--r-- 1 root root 10485928 Jan 30 00:20 stderr.log.2\\n-rw-r--r-- 1 root root 10486166 Jan 29 18:58 stderr.log.3\\n-rw-r--r-- 1 root root 10485841 Jan 29 13:36 stderr.log.4\\n-rw-r--r-- 1 root root 10485790 Jan 29 08:06 stderr.log.5\\n-rw-r--r-- 1 root root 10485858 Jan 25 16:41 stderr.log.6\\n-rw-r--r-- 1 root root 10485835 Jan 21 08:56 stderr.log.7\\n-rw-r--r-- 1 root root 10485760 Jan 15 14:47 stderr.log.8\\n-rw-r--r-- 1 root root 10485805 Jan 10 11:57 stderr.log.9\\n-rw-r--r-- 1 root root    12055 Nov 21 14:58 stdout.log\\n```\\n### 5. Analyze kubelet Logs and etcd Failures\\nIn stderr.log.5 file there were many log entries indicating a problem with etcd.\\n```bash\\nI0129 05:50:11.022246   89953 prober.go:117] Liveness probe for \\\"etcd-mncdsw1.bigdata.abc.gr_kube-system(ef618d8c591c98ed7bd7d66b177d34f7):etcd\\\" failed (failure): HTTP probe failed with statuscode: 503\\n```\\n```bash\\nE0129 05:51:22.881553   89953 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\\\"\\\", APIVersion:\\\"\\\"}, ObjectMeta:v1.ObjectMeta{Name:\\\"etcd-mncdsw1.bigdata.abc.gr.17299b09446544c4\\\", GenerateName:\\\"\\\", Namespace:\\\"kube-system\\\", SelfLink:\\\"\\\", UID:\\\"\\\", ResourceVersion:\\\"27938507\\\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:\\\"\\\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\\\"Pod\\\", Namespace:\\\"kube-system\\\", Name:\\\"etcd-mncdsw1.bigdata.abc.gr\\\", UID:\\\"ef618d8c591c98ed7bd7d66b177d34f7\\\", APIVersion:\\\"v1\\\", ResourceVersion:\\\"\\\", FieldPath:\\\"spec.containers{etcd}\\\"}, Reason:\\\"Unhealthy\\\", Message:\\\"Liveness probe failed: HTTP probe failed with statuscode: 503\\\", Source:v1.EventSource{Component:\\\"kubelet\\\", Host:\\\"mncdsw1.bigdata.abc.gr\\\"}, FirstTimestamp:time.Date(2022, time.November, 21, 15, 0, 1, 0, time.Local), LastTimestamp:time.Date(2023, time.January, 29, 5, 50, 41, 21788692, time.Local), Count:700, Type:\\\"Warning\\\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\\\"\\\", Related:(*v1.ObjectReference)(nil), ReportingController:\\\"\\\", ReportingInstance:\\\"\\\"}': 'rpc error: code = Unknown desc = OK: HTTP status code 200; transport: missing content-type field' (will not retry!)\\n```\\nApparently the high DiskI/O was affecting the etcd server.\\n6. Check warn file logs\\n```bash\\nless /var/log/warn\\n...\\nJan 29 05:51:22 mncdsw1.bigdata.abc.gr dockerd[86247]: W0129 03:51:22.867126\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0 1 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {https://127.0.0.1:2379\\u00a0 <nil> 0 <nil>}. E\\nrr :connection error: desc = \\\"transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused\\\". Reconnecting..\\n```\\n### 6. Review /var/log/warn for Recurring Errors\\nRun the following to check if this error occured in the past.\\n```bash\\ncat /var/log/warn | grep\\u00a0 -e \\\"Jan.*29.*grpc: addrConn.createTransport failed to connect to\\\" | less\\n```\\n![IM2073052_warn_logs1](.media/IM2073052_warn_logs1.png)\\n![IM2073052_warn_logs2](.media/IM2073052_warn_logs2.png)\\nThe errors appear every Sunday morning.\\n## Our Ticket Response\\nfrom the analysis we saw that CDSW crashed as there was a problem in the Control Plane pods of the Kubernetes Cluster in which CDSW is deployed.  We notice in the logs that the problem started with timeouts in requests to the etcd of the cluster, which seem to be due to a high Disk I/O of the sdb disk of mncdsw1 at that moment. As a result we have the inability to synchronize the control plane pods with the base of the cluster, which led to their termination and by extension the entire service. Attached you will also find the screenshot that describes the high Disk I/O at that time.\\nContinuing the analysis we noticed that this behavior is periodic, and more specifically it happens every Sunday at 6 am. Attached you will also find the screenshots that show that there was the same problem on January 15, 22 and 29. Before January the logs are clean. Could you tell us if you have any job set up every Sunday morning that needs a lot of Disk I/O?\\n## Action Points\\nWe opened [this](https://metis.xyztel.com/obss/oss/sysadmin-group/abc/BigStreamer/bigstreamer/-/issues/25) issue to re-evaluate CDSW disk architecture."
        },
        {
            "question": "How can SparkPortForwarder issues in CDSW that cause engine exit status 33 be resolved?",
            "answer": "Restart the SparkPortForwarder environment:\n1. From Cloudera Manager, restart Docker Daemon on `wrkcdsw1` and the Application role on `mncdsw1`.\n2. Use CLI to validate:\n```bash\ncdsw status | grep wrkcdsw1\nkubectl logs <spark-forwarder-pod> -n <namespace>\n```",
            "category": "Infrastructure & Deployment",
            "files": "X20240923-IM2379531.md",
            "context": "---\\ntitle: CDSW SparkPortForwarder Failures Causing Engine Exit Status 33\\ndescription: CDSW jobs failing with engine exit status 33 due to SparkPortForwarder connection errors on wrkcdsw1. Includes log traces, restart of Docker Daemon Worker, and validation through CDSW UI and Kubernetes logs.\\ntags:\\n  - bigstreamer\\n  - cdsw\\n  - spark\\n  - engine status 33\\n  - sparkportforwarder\\n  - wrkcdsw1\\n  - job failure\\n  - pod termination\\n  - kubernetes\\n  - docker\\n  - kubelet\\n  - port forwarding\\n  - cloudera manager\\n  - cdsw ui\\n  - kubectl logs\\n  - spark forwarder restart\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM2379531\\n  system: abc BigStreamer CDSW\\n  root_cause: SparkPortForwarder pod on wrkcdsw1 stuck in terminating state, causing port-forwarder.sock connection refused errors and engine failures\\n  affected_nodes:\\n    - wrkcdsw1.bigdata.abc.gr\\n    - mncdsw1.bigdata.abc.gr\\n  user_visible_error: Engine exited with status 33\\n  error_trace: \\\"dial unix /run/cloudera/data-science-workbench/port-forwarder/port-forwarder.sock: connect: connection refused\\\"\\n  resolution:\\n    - Restarted Docker Daemon Worker on wrkcdsw1 via Cloudera Manager\\n    - Restarted CDSW Application role on mncdsw1\\n    - Verified healthy SparkPortForwarder logs and job success via CDSW UI\\n  outcome: Spark jobs resumed execution without failure; port mapping verified\\n---\\n# abc - IM2379531 - CDSW failed jobs\\n## Description\\nFailed CDSW jobs with a common error have been observed since yesterday (and today).\\nFailed setting up spark (node: wrkcdsw1.bigdata.abc.gr) (error: dial unix /run/cloudera/data-science-workbench/port-forwarder/port-forwarder.sock: connect: connection refused)\\nxEngine exited with status 33.\\n________________________________________\\nCDSW status\\n|             spark-port-forwarder-w9zjv            |    1/1    |    Running    |      1       |   2024-09-19 09:06:51+00:00   |   10.255.241.133   |   10.255.241.133   |       spark-port-forwarder       |\\n|             spark-port-forwarder-z7cdt            |    1/1    |    Running    |      1       |   2024-09-19 09:07:00+00:00   |   10.255.241.132   |   10.255.241.132   |       spark-port-forwarder       |\\n|      tcp-ingress-controller-5b46dd4877-qm77x      |    1/1    |    Running    |      0       |   2024-09-19 09:21:22+00:00   |    100.66.0.22     |   10.255.241.130   |      tcp-ingress-controller      |\\n|          usage-reporter-55b457bccd-nbt7q          |    1/1    |    Running    |      0       |   2024-09-19 09:06:41+00:00   |    100.66.0.37     |   10.255.241.130   |          usage-reporter          |\\n|                web-7db65dccd9-g49qt               |    1/1    |    Running    |      0       |   2024-09-19 09:19:18+00:00   |    100.66.0.10     |   10.255.241.130   |               web                |\\n|                web-7db65dccd9-ksff4               |    1/1    |    Running    |      0       |   2024-09-19 09:20:15+00:00   |    100.66.0.21     |   10.255.241.130   |               web                |\\n|                web-7db65dccd9-xcxs2               |    1/1    |    Running    |      0       |   2024-09-19 09:20:15+00:00   |    100.66.0.11     |   10.255.241.130   |               web                |\\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\nAll required pods are ready in cluster default.\\nAll required Application services are configured.\\nAll required secrets are available.\\nPersistent volumes are ready.\\nPersistent volume claims are ready.\\nIngresses are ready.\\nChecking web at url: https://mncdsw1.bigdata.abc.gr\\nOK: HTTP port check\\nCloudera Data Science Workbench is ready!\\n## Actions Taken\\n1. After checking logs of `wrkcdsw1.bigdata.abc.gr` `SparkPortForwarder` we saw that the latest request that handled was:\\n```bash\\ncdsw status | grep wrkcdsw1.bigdata.abc.gr\\nkubectl logs spark-port-forwarder-thrr9 -n <namespace>\\n```\\nThe output of the logs:\\n```bash\\n2024-09-21 22:35:16.863 11 INFO SparkPortForwarder Failed to dial onward connection data = {\\\"err\\\":\\\"dial tcp 100.66.1.227:30742: connect: connection refused\\\",\\\"name\\\":\\\"spark-driver\\\",\\\"podId\\\":\\\"2liofp42ubkcj7yc\\\",\\\"port\\\":30742,\\\"target\\\":\\\"100.66.1.227:30742\\\"}\\n2024-09-22 02:25:23.457 11 INFO SparkPortForwarder Returning port mappping data = {\\\"mapping\\\":{\\\"spark-blockmanager\\\":26577,\\\"spark-driver\\\":22768}}\\n2024-09-22 02:26:29.689 11 INFO SparkPortForwarder Garbage collecting forwarders for pod data = {\\\"podId\\\":\\\"z48obsz9bocvu2wz\\\"}\\n```\\n2. We tried to delete the pod of `wrkcdsw1.bigdata.abc.gr` `SparkPortForwarder` but it stucked on `Terminating` status.\\n```bash\\nkubectl delete pod <pod_name> -n <namespace>\\n```\\n3. Thus, CDSW Application(mncdsw1) from Cloudera UI was down.\\n4. From [Cloudera Manager UI](https://172.25.37.232:7183/cmf/home) we have restarted the `Docker Deamon Worker` of `wrkcdsw1.bigdata.abc.gr` and `Application` role of `mncdsw1`. After that we have checked the logs and `wrkcdsw1.bigdata.abc.gr` `SparkPortForwarder` handled succefully all the requests.\\nActions:\\n```bash\\nCloudera Manager -> CDSW -> `Docker Deamon Worker` Role of `wrkcdsw1 -> Restart\\nCloudera Manager -> CDSW -> `Application` role of `mncdsw1` -> Restart\\n```\\nLogs:\\n1. Checks:\\n```bash\\ncdsw status | grep wrkcdsw1.bigdata.abc.gr\\nkubectl logs <pod-name-spark-forwarder> -n <namespace>\\n```\\n2. Output:\\n```\\n2024-09-23 09:20:40.579 11 INFO SparkPortForwarder Start mapping ports data = {\\\"podId\\\":\\\"1t47ok1jnxqb1pi9\\\"}\\n2024-09-23 09:20:40.579 11 INFO SparkPortForwarder Start trying to forward port data = {\\\"name\\\":\\\"spark-driver\\\",\\\"podId\\\":\\\"1t47ok1jnxqb1pi9\\\",\\\"port\\\":26404}\\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Finish trying to forward port, success data = {\\\"name\\\":\\\"spark-driver\\\",\\\"podId\\\":\\\"1t47ok1jnxqb1pi9\\\",\\\"port\\\":26404}\\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Start trying to forward port data = {\\\"name\\\":\\\"spark-blockmanager\\\",\\\"podId\\\":\\\"1t47ok1jnxqb1pi9\\\",\\\"port\\\":30123}\\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Finish trying to forward port, success data = {\\\"name\\\":\\\"spark-blockmanager\\\",\\\"podId\\\":\\\"1t47ok1jnxqb1pi9\\\",\\\"port\\\":30123}\\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Finish mapping ports data = {\\\"podId\\\":\\\"1t47ok1jnxqb1pi9\\\"}\\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Returning port mappping data = {\\\"mapping\\\":{\\\"spark-blockmanager\\\":30123,\\\"spark-driver\\\":26404}}\\n2024-09-23 09:21:29.302 11 INFO SparkPortForwarder Garbage collecting forwarders for pod data = {\\\"podId\\\":\\\"1t47ok1jnxqb1pi9\\\"}\\n```\\n5. Additional checks made from the [CDSW UI](https://mncdsw1.bigdata.abc.gr). We reviewed the status of running jobs and examined the logs of them.\\n```bash\\nSite Administration -> Usage -> Select job Name -> Logs Tab\\n```\\nIn the logs of an example job we searched for `SparkPortForwarder` entries for `wrkcdsw1` in order to evaluate that no errors appeared.\\n## Affected Systems\\nCDSW"
        },
        {
            "question": "What caused the LM_02_LTE export to fail and how was it resolved without restarting the flow?",
            "answer": "The export failed due to an out-of-memory error caused by a heavy query on `sn102` and missing HDFS files. It was resolved via:\n- Configuration cleanup in `refdata.mediation_loc_mobility_load_info`\n- `refresh` command on `npce.eea_hour` table before querying:\n```sql\nrefresh npce.eea_hour;\n```",
            "category": "Data Management & Query Execution",
            "files": "X20230420-IM2131290.md",
            "context": "---\\ntitle: Location Mobility Export Failure Due to Memory Exhaustion and Missing HDFS File\\ndescription: The Location Mobility LM_02_LTE file failed to export due to memory exhaustion on sn102 caused by a heavy user query, followed by missing HDFS files triggering I/O errors on other nodes. Issue was resolved via retention cleanup and table refresh, without flow restarts.\\ntags:\\n  - bigstreamer\\n  - location mobility\\n  - lm_02_lte\\n  - sn102\\n  - impala\\n  - memory exhaustion\\n  - out of memory\\n  - disk io error\\n  - missing hdfs file\\n  - ranai-geo\\n  - npce.eea_hour\\n  - impala daemon\\n  - export failure\\n  - trustcenter\\n  - hdfs\\n  - retention\\n  - refresh table\\n  - root cause analysis\\n  - lte\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM2131290\\n  system: abc BigStreamer TrustCenter LTE\\n  failed_flow: Location Mobility LM_02_LTE export\\n  node_with_issue: sn102.bigdata.abc.gr\\n  primary_error: Out-of-memory error caused by impala query from ranai-geo\\n  secondary_error: Disk I/O error due to missing HDFS file on sn111\\n  recovery_method: Automatic retention and table refresh; dev config correction\\n  impala_query_user: ranai-geo\\n  tables_involved:\\n    - npce.eea_hour\\n    - refdata.mediation_loc_mobility_load_info\\n---\\n# abc - BigStreamer - IM2131290 - Location Mobility LM_02_LTE Export Failure\\n## Description\\nThe LM_02_LTE export from the Location Mobility workflow failed starting April 19 at 15:00. Below is the initial user-reported issue:\\nAs of yesterday noon at 15:00 the creation of the LM_02_LTE file of the location mobility stream fails. From the HUE jobs you can see that the workflow (export_Location_Mobility_files_to_mediat...) is killed.\\nWe also saw from the logs (lm_export_lte_v2_mon.cron.20230419.log) that at 15:00 when the problem starts we have the following error:\\nQuery submitted at: 2023-04-19 15:00:31 (Coordinator: http://sn72.bigdata.abc.gr:25000)\\nQuery progress can be monitored at: http://sn72.bigdata.abc.gr:25000/query_plan?query_id=c74df6d614d535ea:4de432ac00000000\\nERROR: Failed due to unreachable impalad(s): sn102.bigdata.abc.gr:22000\\nCould not execute command: SELECT\\nachievable_thr_bytes_down_1,\\nachievable_thr_bytes_up_1,\\nachievable_thr_time_down_1,\\n..................................................................\\n[2023/04/19 15:19:13] - ERROR: Impala shell command for par_msisdn= failed.\\n[2023/04/19 15:19:13] - ERROR: Clean up and exit.\\n% Total % Received % Xferd Average Speed Time Time Time Current\\nFrom there onwards we observe errors of the form:\\nQuery submitted at: 2023-04-19 17:00:31 (Coordinator: http://sn64.bigdata.abc.gr:25000)\\nQuery progress can be monitored at: http://sn64.bigdata.abc.gr:25000/query_plan?query_id=094fdeda997b8d44:5826172200000000\\nERROR: Disk I/O error on sn62.bigdata.abc.gr:22000: Failed to open HDFS file hdfs://nameservice1/ez/warehouse/npce.db/eea_hour/pardt=2023041910/ef4d08b3073e8531-d909e37b0000010\\ne_1525512597_data.0.txt\\nError(2): No such file or directory\\nRoot cause: RemfghException: File does not exist: /ez/warehouse/npce.db/eea_hour/pardt=2023041910/ef4d08b3073e8531-d909e37b0000010e_1525512597_data.0.txt\\nat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:85)\\nat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)\\nat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)\\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1909)\\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:736)\\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:415)\\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)\\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)\\nat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:869)\\nWe have similar errors in today's log file lm_export_lte_v2_mon.cron.20230420.log\\n## Root Cause Analysis\\nFirst thing that we have checked were the comments on the [md](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/trustcenter_flows.md) that exists for this flow.\\nAfter checking the logs of the flow we saw that the flow was running successfully after it had failed for half day. The reason that it was able to run  was the retention that had already taken place the day that we received the ticket.\\nWe were able to identify the issue that caused the problem. After checking the logs of the host we saw that the host didn't have enough memory at the time.\\n![sn102_memory](.media/sn102_memory.JPG) because of the user `ranai-geo` that had run an impala query \\n![query](.media/query.JPG) that took all the resources of the impala deamon on sn102.\\n![query_details](.media/q_details.JPG) \\n## Actions Taken\\nNo restart needed of the flow. There were some adjustments from dev team that took place.\\n### Development Intervention\\n```\\nIt was necessary to make some corrections in the configuration table of the refdata.mediation_loc_mobility_load_info flow to synchronize the data sets that will be exported.\\nAdditionally, due to the following exception a refresh table was added before the select in npce.eea_hour.\\nERROR: Disk I/O error on sn111.bigdata.abc.gr:22000: Failed to open HDFS file hdfs://nameservice1/ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\\nError(2): No such file or directory\\nRoot cause: RemfghException: File does not exist: /ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\\n```"
        },
        {
            "question": "How can I install a root certificate authority on a node using SaltStack and ensure Java applications trust it?",
            "answer": "1. Move the certificate to the correct path and rename to `.crt` if needed:\n```bash\nmv /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.crt /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.cer\n```\n2. Apply the SaltStack state to install the certificate:\n```bash\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os\n```\n3. To install it for Java:\n```bash\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_jssecacerts\n```",
            "category": "Infrastructure & Deployment",
            "files": "Certificate_authority_installation.md",
            "context": "---\\ntitle: \\\"Certificate Authority Installation\\\"\\ndescription: \\\"Step-by-step procedure for installing a root certificate authority using SaltStack, including OS trust and Java keystore integration.\\\"\\ntags:\\n  - certificate\\n  - saltstack\\n  - tls\\n  - root ca\\n  - jssecacerts\\n  - java keystore\\n  - tls internal certificate\\n  - infrastructure\\n  - security\\n  - ssl\\n---\\n### Certificate Authority installation\\nThis guide explains how to install a root certificate authority in a system using SaltStack automation, covering both OS-level trust and Java keystore (`jssecacerts`) integration.\\nThe following procedure describes how to install a root certificate authority using SaltStack:\\n 1.  Move given certificate under `admin:/etc/salt/salt/tls/internal_certificate/root_certificate/`\\n 2.  Rename `.cer` file to `.crt`.\\n- If certificate has `.crt` suffix you should first verify that it base64 encoded by opening file and make sure it starts with `-----BEGIN CERTIFICATE----`\\n ```bash\\nmv /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.crt /etc/salt/salt/tls/internal_certificate/root_certificate/certificate.cer\\n```\\n3. Install certificate by using saltStack formula:\\n```bash\\n###Test what actions will take affect before actually run the installation formula\\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os test=True\\n### Install the certificate\\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_os\\n```\\n4. Install jssecacerts by using saltStack formula:\\n```bash\\nsalt 'node_name' state.apply tls.internal_certificate.install_root_certificate_jssecacerts\\n```\\n> Ndef: Keep in mind that above command will fail if there is no java installed at the specified node\\n**Congratulations!** Certificate installation is complete!"
        },
        {
            "question": "What steps are required to configure a Kubernetes user environment for RAN.AI access, including service account and kubeconfig setup?",
            "answer": "1. Create `service_account.yml` and `role_binding.yml`, then apply:\n```bash\nkubectl apply -f service_account.yml\nkubectl apply -f role_binding.yml\n```\n2. For Kubernetes ≥1.24, also create a service-account-token `Secret`.\n3. Generate kubeconfig using the plugin:\n```bash\nkubectl view-serviceaccount-kubeconfig -n <namespace> <account> > ~/.kube/config\n```\nIf no plugin is available, manually extract token and certificate using `kubectl get secret` with jsonpath.",
            "category": "Infrastructure & Deployment",
            "files": "create_ranai_kubernetes_user.md",
            "context": "---\\ntitle: \\\"Kubernetes User Environment Setup\\\"\\ndescription: \\\"Step-by-step guide to install kubectl and helm, create Kubernetes service accounts, and generate user-specific kubeconfig for RAN.AI environments.\\\"\\ntags:\\n  - kubernetes\\n  - kubectl\\n  - service account\\n  - kubeconfig\\n  - user setup\\n  - helm\\n  - role binding\\n  - authentication\\n  - secret\\n  - RAN.AI\\n---\\n# Kubernetes User Environment Setup\\nThis guide explains how to set up a user environment for Kubernetes access in a RAN.AI cluster. It covers kubectl and helm installation, service account creation, role bindings, secrets, and kubeconfig generation\\u2014manually or using a plugin.\\n## Tools\\nThe main tool that needs to be installed is **kubectl**, instructions for which can be found\\n[here](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/).\\n### Install bash completion for kubectl:\\nAdditionally after\\ninstallation, completion can be enabled by executing:\\n```bash\\nmkdir -p /etc/bash_completion.d\\nkubectl completion bash | sudo tee /etc/bash_completion.d/kubectl\\n```\\nIf possible a nice to have plugin for setting up kubeconfigs can be found at\\n[kubectl-view-serviceaccount-kubeconfig-plugin](https://github.com/superbrothers/kubectl-view-serviceaccount-kubeconfig-plugin/releases). Simply place the binary under `/usr/local/bin/`.\\nAdditionally in order to install **helm**, follow the [instructions](https://helm.sh/docs/intro/install/)\\nand set up completion by executing the following:\\n```bash\\nhelm completion bash | sudo tee /etc/bash_completion.d/helm\\n```\\n## Service Account\\nCreate the following YAML files, that contain the definition for the service account and its\\nrole binding:\\n- **`service_account.yml`**\\n```yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: <Account Name>\\n  namespace: <RAN.AI Namespace>\\n```\\n- **`role_binding.yml`**\\n```yaml\\nkind: RoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: <Binding Name>\\n  namespace: <RAN.AI Namespace>\\nsubjects:\\n- kind: ServiceAccount\\n  name: <Account Name>\\n  namespace: <RAN.AI Namespace>\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: edit\\n```\\n### Apply service account and role binding\\n```bash\\nkubectl apply -f service_account.yml\\nkubectl apply -f role_binding.yml\\n```\\n### User Secret\\nFor Kubernetes versions over 1.24 when creating a service account it's secret is not automatically created\\nand mounted, so in that case create the following secret:\\n```yaml\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: <Account Secret Name>\\n  namespace: <RAN.AI Namespace>\\n  annotations:\\n    kubernetes.io/service-account.name: <Account Name>\\ntype: kubernetes.io/service-account-token\\n```\\n### Generate user kubeconfig using plugin:\\nExecute the following to get the new users config. If the plugin is not available the config must be constructed manually:\\n```\\nkubectl view-serviceaccount-kubeconfig -n <RAN.AI Namespace> <Account Name> > <User's Home>/.kube/config\\n```\\nFor reference the config looks like this:\\n```yaml\\napiVersion: v1\\nclusters:\\n- cluster:\\n    certificate-authority-data: ...\\n    server: https://<Server>:<Port>\\n  name: <Cluster Name>\\ncontexts:\\n- context:\\n    cluster: <Cluster Name>\\n    namespace: <RAN.AI Namespace>\\n    user: <Account Name>\\n  name: kubernetes-admin@kubernetes\\ncurrent-context: kubernetes-admin@kubernetes\\nkind: Config\\npreferences: {}\\nusers:\\n- name: <Account Name>\\n  user:\\n    token: ...\\n```\\nIn the above segment `certificate-authority-data` and `token` can be obtained by executing:\\n```bash\\n# Find the Account's Secret Token name\\nkubectl get secrets -n <RAN.AI Namespace>\\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.ca\\\\.crt}'\\nkubectl get -n <RAN.AI Namespace> secret <Account Secret Token> -o jsonpath='{.data.token}' | base64 --decode\\n```"
        },
        {
            "question": "How can I check if the Retention or Anonymization job failed on abc BigStreamer, and where do I find error logs?",
            "answer": "1. Run the status check:\n```bash\ngrep \"Script Status\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.202012.log | tail -n1\n```\nIf `Status != 0`, extract Snapshot ID and run:\n```bash\negrep -i '(error|problem|except|fail)' /shared/abc/cdo/log/Retention/*<snapshot_id>*.log\n```\nRepeat similar steps for anonymization using `RunID` and `Anonymize` logs.",
            "category": "Troubleshooting & Issue Resolution",
            "files": "check_anonymization.md",
            "context": "---\\ntitle: \\\"abc - Retention and Anonymization Job Status Checks\\\"\\ndescription: \\\"Instructions for checking the status and logs of Retention_Dynamic_Drop_DDL and Anonymize_Data_Main shell scripts on abc using Snapshot ID and RunID from log files.\\\"\\ntags:\\n  - abc\\n  - BigStreamer\\n  - retention\\n  - anonymization\\n  - job monitoring\\n  - script status\\n  - log analysis\\n  - shell commands\\n  - troubleshooting\\n  - snapshot\\n  - runid\\n---\\n## Retention Check\\n### Step 1 \\u2013 Initial Status Check\\nLogin to `un2` as `intra` and run the following command:\\n```bash\\ngrep \\\"Script Status\\\" /shared/abc/cdo/log/203.Retention_Dynamic_Drop_DDL.202012.log | tail -n1\\n```\\nExample output:\\nScript Status ==> Scr:203.Retention_Dynamic_Drop_DDL.sh, Dt:2020-12-18 08:13:12, Status:0, Snapshot:1608267602, RunID:1608271202, ExpRows:3327, Secs:790, 00:13:10\\nIf Status != 0, the script has failed.\\n---\\n### Step 2 \\u2013 Deeper Investigation\\nExtract the Snapshot value from the above output (e.g. 1608267602) and check for any logged problems:\\n```bash\\negrep -i '(error|problem|except|fail)' /shared/abc/cdo/log/Retention/*1608267602*.log\\n```\\nIf the result has fewer than 10 lines, it\\u2019s usually not concerning. A large number of matches indicates an issue.\\n## Anonymization Check\\n### Step 1 \\u2013 Initial Status Check\\n``` bash\\ngrep \\\"Script Status\\\" /shared/abc/cdo/log/100.Anonymize_Data_Main.202012.log | tail -n1\\n```\\nExample output:\\nScript Status ==> Scr:100.Anonymize_Data_Main.sh, Dt:2020-12-17 21:01:03, Status:, RunID:1608228002, Secs:3661, 01:01:01\\n### Step 2 \\u2013 Check Detailed Errors\\nExtract the RunID (e.g. 1608228002) and inspect logs:\\n```bash\\negrep '(:ERROR|with errors)' /shared/abc/cdo/log/Anonymize/*1608228002*.log | less\\n```\\nIf the command returns output, there is a problem.\\n## Notes\\n- All paths refer to shared log directories on abc BigStreamer.\\n- Retention jobs refer to dropping old data.\\n- Anonymization jobs refer to data privacy transformations.\\n---"
        },
        {
            "question": "How was the UI issue for Permanent Anonymization & Retention resolved when users couldn't access it over VPN?",
            "answer": "The UI was attempting to load external CDN resources which VPN blocked. Resolution involved replacing the WAR file on Wildfly:\n```bash\nchown trustuser:trustcenter <new_war_file>\nchmod 644 <new_war_file>\nmv <new_war_file> /opt/trustcenter/wf_cdef_trc/standalone/deployments/\n```\nNo restart required—Wildfly creates a `.deployed` marker automatically.",
            "category": "Troubleshooting & Issue Resolution",
            "files": "Change_war_for_Anonymization&Retention_UI.md",
            "context": "---\\ntitle: \\\"Permanent Anonymization & Retention UI Access Issue and WAR Replacement\\\"\\ndescription: \\\"Troubleshooting and resolution steps for the abc BigStreamer Permanent Anonymization & Retention UI access issue due to CDN resource blocking over VPN. Involves replacing the Wildfly WAR deployment.\\\"\\ntags:\\n  - abc\\n  - bigstreamer\\n  - wildfly\\n  - anonymization\\n  - retention\\n  - ui issue\\n  - vpn restriction\\n  - deployment\\n  - war replacement\\n  - haproxy\\n  - trustcenter\\n---\\n# abc - Permanent Anonymization & Retention UI issue\\nThis guide documents the root cause and resolution steps for the abc Permanent Anonymization & Retention UI failing to load over VPN due to external CDN dependency, and the WAR file replacement procedure on Wildfly.\\n## Description\\n```\\nRegarding the problem with the Permanent Anonymization & Retention UI (https://cne.def.gr:8643/customapps)\\nLet me remind you that access to this particular UI is not possible via VPN.\\nThe reason is that it is trying to load a library from the internet (cdn.jsdelivr.net).\\nA new war has been added which should replace the old one in wildfly.\\n```\\n## Actions Taken\\n## Actions Taken\\n\\n1. SSH into `unc2` using your personal account.\\n2. Switch to root and check HAProxy backend configuration:\\n   sudo -i\\n   less /etc/haproxy/haproxy.cfg\\n   (Search for the `tru-backend` section.)\\n3. Connect to `unekl1` and back up the existing WAR file:\\n   ssh unekl1\\n   cp -rp /opt/trustcenter/wf_cdef_trc/standalone/deployments/wftrust-landing-web.war /opt/trustcenter/wf_cdef_trc/standalone/deployments/wftrust-landing-web.war.bkp\\n4. Set correct permissions on the new WAR file:\\n   chown trustuser:trustcenter <new_war_file>\\n   chmod 644 <new_war_file>\\n5. Move the new WAR file to the deployments directory:\\n   mv <new_war_file> /opt/trustcenter/wf_cdef_trc/standalone/deployments/\\n6. No Wildfly restart is required. A .deployed marker file (wftrust-landing-web.war.deployed) will be created automatically.\\n7. Verify Wildfly is running:\\n   su - trustuser\\n   bash\\n   trust-status\\n8. Repeat steps 3\\u20137 on `unekl2`.\\n9. Clear your browser cache and access the UI at:\\n   https://cne.def.gr:8643/customapps\\n## Affected Systems\\nabc Bigstreamer"
        },
        {
            "question": "How do you increase Java Heap Memory for Streamsets and verify it using system tools?",
            "answer": "1. Set Java options in Cloudera Manager:\n```bash\n-Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow\n```\n2. Remove any override from the `sdc-env.sh` safety valve.\n3. Restart Streamsets.\n4. Confirm JVM heap with:\n```bash\nps -ef | grep -i streamsets | grep -i xmx\njmap -heap <PID>\n```",
            "category": "Application Functionality & Flow",
            "files": "configure_streamsets_java_heap_space.md",
            "context": "---\\ntitle: \\\"Streamsets - Java Heap Space Configuration and Monitoring\\\"\\ndescription: \\\"Steps to increase Java Heap Memory for Streamsets via Cloudera Manager, clean up redundant configs, restart services, and verify memory settings using process inspection tools (ps, jmap, jconsole).\\\"\\ntags:\\n  - streamsets\\n  - java heap\\n  - memory configuration\\n  - cloudera manager\\n  - jmap\\n  - jconsole\\n  - troubleshooting\\n  - performance tuning\\n  - gc logs\\n  - bigstreamer\\n  - heap dump\\n  - xmx\\n  - xms\\n---\\n# Streamsets - Java Heap Space\\nStreamsets Java Heap Memory was increased due to [this](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/102#ndef_95081) issue.\\nThis guide documents the resolution of a Java Heap Space issue on the Streamsets Data Collector. It includes steps to increase heap size using Cloudera Manager, remove deprecated safety valve overrides, verify JVM options with `ps`, and inspect memory usage via `jmap` and `jconsole`. Applicable for performance tuning and troubleshooting OOM errors on Streamsets pipelines.\\n## Actions Taken\\nThis procedure outlines how to address Streamsets memory issues by increasing the Java heap size and verifying runtime memory settings.\\n1. Configure Java Options from CLoudera Manager\\n   ```bash\\n   cluster -> Streamsets -> Configuration -> Java options: `-Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow`\\n   ```\\n2. Remove old configuration\\n   ```bash\\n   cluster-> Streamsets -> Configuration -> Data Collector Advanced Configuration Snippet (Safety Valve) for sdc-env.sh\\n   ```\\n   ```bash\\n   #Remove the following line, if exists\\n   export SDC_JAVA_OPTS=\\\"-Xmx16384m -Xms16384m -server -XX:-OmitStackTraceInFastThrow ${SDC_JAVA_OPTS}\\\"\\n   ```\\n3. Restart Streamsets\\n   ```bash\\n   cluster -> Streamsets -> Restart\\n   ```\\n4. Check Streamsets Process Options\\n   ```bash\\n   [root@un2 ~]# ps -ef | grep -i streamsets | grep -i xmx\\n\\n   sdc      24898 24873 45 12:45 ?        00:40:11 /usr/java/default/bin/java -classpath /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs/main/streamsets-datacollector-bootstrap-3.21.0.jar:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/root-lib/* -Djava.security.manager -Djava.security.policy=file:///var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR/sdc-security.policy -Xmx1024m -Xms1024m -server -XX:-OmitStackTraceInFastThrow -Xmx32768m -Xms32768m -server -XX:-OmitStackTraceInFastThrow -Dsdc.dist.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0 -Dsdc.resources.dir=/var/lib/sdc/resources -Dsdc.hostname=un2.bigdata.abc.gr -Dsdc.conf.dir=/var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR -Dsdc.data.dir=/shared/sdc/data -Dsdc.log.dir=/shared/sdc/log/ -javaagent:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs/main/streamsets-datacollector-bootstrap-3.21.0.jar -Dsdc.libraries.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs -Dsdc.librariesExtras.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs-extras -Dsdc.rootLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/root-lib -Dsdc.bootstrapLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libexec/bootstrap-libs -Dsdc.apiLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/api-lib -Dsdc.asterClientLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/aster-client-lib -Dsdc.containerLib.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/container-lib -Dsdc.libsCommon.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libs-common-lib -Dsdc.userLibs.dir=/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/user-libs -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -Djdk.nio.maxCachedBufferSize=262144 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/shared/sdc/log//gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/shared/sdc/log//sdc_heapdump_1675334705.hprof -XX:ErrorFile=/shared/sdc/log//hs_err_1675334705.log com.streamsets.pipeline.BootstrapMain -mainClass com.streamsets.datacollector.main.DataCollectorMain -apiClasspath /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/api-lib/*.jar -containerClasspath /var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR:/opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/container-lib/*.jar -streamsetsLibrariesDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs -userLibrariesDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/user-libs -configDir /var/run/cloudera-scm-agent/process/147717-streamsets-DATACOLLECTOR -libsCommonLibDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/libs-common-lib -streamsetsLibrariesExtraDir /opt/cloudera/parcels/STREAMSETS_DATACOLLECTOR-3.21.0/streamsets-libs-extras\\n   ```\\n   > Tip: When defining Java configuration options, avoid defining duplicate options. If you do define duplicates, the last option passed to the JVM usually takes precedence.\\n5. Check Max Java Heap Space for Streamsets via jconsole or jmap\\n### jconsole\\n   > Needs additional Java Options: -Dcom.sun.management.jmxremdef -Dcom.sun.management.jmxremdef.port=3333 -Dcom.sun.management.jmxremdef.local.only=false -Dcom.sun.management.jmxremdef.authenticate=false -Dcom.sun.management.jmxremdef.ssl=false\\\"\\n   ```bash\\n   [root@un2 ~]# /usr/java/latest/bin/jconsole\\n   ```\\n   Select `com.streamsets.pipeline.BootstrapMain`,  `Connect` and check the metrics.\\n### jmap\\n   ```bash\\n   jmap -heap <pid>\\n   #output example\\n   [root@un2 ~]# jmap -heap 24898\\n   Attaching to process ID 24898, please wait...\\n   Debugger attached successfully.\\n   Server compiler detected.\\n   JVM version is 25.181-b13\\n   using parallel threads in the new generation.\\n   using thread-local object allocation.\\n   Concurrent Mark-Sweep GC\\n   Heap Configuration:\\n      MinHeapFreeRatio         = 40\\n      MaxHeapFreeRatio         = 70\\n      MaxHeapSize              = 34359738368 (32768.0MB)\\n      NewSize                  = 2442723328 (2329.5625MB)\\n      MaxNewSize               = 2442723328 (2329.5625MB)\\n      OldSize                  = 31917015040 (30438.4375MB)\\n      NewRatio                 = 2\\n      SurvivorRatio            = 8\\n      MetaspaceSize            = 21807104 (20.796875MB)\\n      CompressedClassSpaceSize = 1073741824 (1024.0MB)\\n      MaxMetaspaceSize         = 17592186044415 MB\\n      G1HeapRegionSize         = 0 (0.0MB)\\n   Heap Usage:\\n   New Generation (Eden + 1 Survivor Space):\\n      capacity = 2198470656 (2096.625MB)\\n      used     = 1493838840 (1424.6357345581055MB)\\n      free     = 704631816 (671.9892654418945MB)\\n      67.94900063473942% used\\n   Eden Space:\\n      capacity = 1954217984 (1863.6875MB)\\n      used     = 1433160568 (1366.768424987793MB)\\n      free     = 521057416 (496.91907501220703MB)\\n      73.33678124620104% used\\n   From Space:\\n      capacity = 244252672 (232.9375MB)\\n      used     = 60678272 (57.8673095703125MB)\\n      free     = 183574400 (175.0701904296875MB)\\n      24.84241891937215% used\\n   To Space:\\n      capacity = 244252672 (232.9375MB)\\n      used     = 0 (0.0MB)\\n      free     = 244252672 (232.9375MB)\\n      0.0% used\\n   concurrent mark-sweep generation:\\n      capacity = 31917015040 (30438.4375MB)\\n      used     = 12194092928 (11629.193237304688MB)\\n      free     = 19722922112 (18809.244262695312MB)\\n      38.20561826573617% used\\n   57229 interned Strings occupying 8110512 bytes.\\n   ```\\nReference: https://docs.streamsets.com/platform-datacollector/latest/datacollector/UserGuide/Configuration/DCEnvironmentConfig.html"
        },
        {
            "question": "How can the Cube Indicators Spark job be executed manually via terminal for a specific date?",
            "answer": "1. SSH into `un1.bigdata.abc.gr` as user `intra`:\n```bash\nsudo -i -u intra\ncd projects/cube_ind\n```\n2. Refresh script and edit submit file:\n```bash\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\nvim run_cube.sh  # set <date> 2 days after the missing date\n```\n3. Run the job:\n```bash\n./run_cube.sh\n```",
            "category": "Application Functionality & Flow",
            "files": "execute_indicators_terminal.md",
            "context": "---\\ntitle: \\\"Execute Cube Indicators via Terminal\\\"\\ndescription: \\\"Instructions for manually executing the Cube Indicators Spark job from terminal on un1.bigdata.abc.gr, including how to pull the latest script, modify execution date, and run the submit script.\\\"\\ntags:\\n  - cube indicators\\n  - spark job\\n  - pyspark\\n  - hdfs\\n  - brond\\n  - manual execution\\n  - terminal\\n  - big data\\n  - intra\\n---\\n# Execute Cube Indicators via Terminal\\nThis guide explains how to manually run the Cube Indicators Spark job for missing dates from the terminal. It includes pulling the latest script from HDFS, updating the execution date, and submitting the job.\\n1. SSH into `un1.bigdata.abc.gr` and switch to the `intra` user:\\n```bash\\nssh un1.bigdata.abc.gr\\nsudo -i -u intra\\n```\\n2. Navigate to the working directory:\\n```bash\\ncd projects/cube_ind\\n```\\n3. Remove the old PySpark script:\\n```bash\\nrm Indicators_Spark_Job.py\\n```\\n4. Authenticate with Kerberos and fetch the updated script from HDFS:\\n```bash\\nkinit -kt /home/intra/intra.keytab intra\\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\\n```\\n6. Edit submit script to change execution date. The execution date should be 2 days after the missing data date.\\nFor example, to load data for 2021-01-01, set the execution date to 2021-01-03.\\n```bash\\nvim run_cube.sh\\n```\\nUpdate the relevant line:\\n```bash\\n...verhead=4096 Indicators_Spark_Job.py <date> brond.dsl_s...\\n```\\n7. Run the Spark job:\\n```bash\\n./run_cube.sh\\n```\\n---\\ntags:\\n  - cube indicators\\n  - pyspark\\n  - spark job\\n  - brond\\n  - manual data load\\n  - hdfs\\n  - intra\\n---"
        },
        {
            "question": "What dependencies are involved in populating the `brond.cube_indicators` table and what scripts populate them?",
            "answer": "The Oozie coordinator `Coord_Cube_Spark_Indicators` populates `brond.cube_indicators`. Its dependencies include:\n- `brond.fixed_radio_matches_unq_inp` → populated by `101_fixed_radius.sh`\n- `radius.radacct_hist` → `radius.pl`\n- `brond.brond_retrains_hist` → `brond_retrains.pl`\n- `brond.dsl_stats_week_xdsl_hist` → from `coord_brond_load_dsl_daily_stats`",
            "category": "Data Management & Query Execution",
            "files": "cube_indicators_pipeline.md",
            "context": "---\\ntitle: \\\"Cube Indicators Pipeline\\\"\\ndescription: \\\"Overview of the Brond Cube Indicators data pipeline, including Oozie coordinators, dependency tables, and the scripts responsible for generating input data.\\\"\\ntags:\\n  - brond\\n  - cube indicators\\n  - oozie\\n  - hadoop\\n  - coordinator\\n  - radius\\n  - retrains\\n  - fixed customers\\n  - data pipeline\\n  - xdsl\\n---\\n# Cube Indicators Pipeline\\nThis document summarizes the data flow and dependencies of the `brond.cube_indicators` pipeline. It includes the Oozie coordinators involved, input tables, and the scripts or jobs that populate each dependency. The main output is the `brond.cube_indicators` table, populated for `par_date = today - 2 days`.\\n* Coordinator `1011_Fixed_brond_customers`  populates table `brond.fixed_brond_customers_daily`\\n* Coordinator `Coord_post_BROND_FIXED_CUSTOMERS` populates tables `brond.fixed_brond_customers_daily_unq` & `brond.fixed_customers_brond_latest`\\n* Coordinator `Coord_Cube_Spark_Indicators` for `par_date=date -2 days` populates table `brond.cube_indicators`  and its dependencies are:\\n  * `brond.fixed_radio_matches_unq_inp` populated by script `un2:/shared/abc/brond/bin/101_fixed_radius.sh`\\n  * `brond.fixed_brond_customers_daily_unq`\\n  * `radius.radacct_hist` populated by script `un2:/shared/abc/radius/DataParser/scripts/radius.pl`\\n  * `brond.brond_retrains_hist` populated by script `un2:/shared/abc/brond/DataParser/scripts/brond_retrains.pl`\\n  * `brond.dsl_stats_week_xdsl_hist` populated by Coordinator `coord_brond_load_dsl_daily_stats`\\n---\\ntags:\\n  - cube_indicators\\n  - brond\\n  - coordinator\\n  - radius\\n  - retrains\\n  - fixed_customers\\n  - xdsl\\n  - spark\\n  - hadoop\\n  - data_dependencies\\n---"
        },
        {
            "question": "What is the recommended process for loading missing cube indicators data for a past date?",
            "answer": "1. Identify the missing data date (e.g., 2021-01-01).\n2. Set the execution date to 2 days after (2021-01-03) in `run_cube.sh`.\n3. Pull the latest script:\n```bash\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\n```\n4. Execute:\n```bash\n./run_cube.sh\n```",
            "category": "Data Management & Query Execution",
            "files": "execute_indicators_terminal.md, cube_indicators_pipeline.md",
            "context": "---\\ntitle: \\\"Execute Cube Indicators via Terminal\\\"\\ndescription: \\\"Instructions for manually executing the Cube Indicators Spark job from terminal on un1.bigdata.abc.gr, including how to pull the latest script, modify execution date, and run the submit script.\\\"\\ntags:\\n  - cube indicators\\n  - spark job\\n  - pyspark\\n  - hdfs\\n  - brond\\n  - manual execution\\n  - terminal\\n  - big data\\n  - intra\\n---\\n# Execute Cube Indicators via Terminal\\nThis guide explains how to manually run the Cube Indicators Spark job for missing dates from the terminal. It includes pulling the latest script from HDFS, updating the execution date, and submitting the job.\\n1. SSH into `un1.bigdata.abc.gr` and switch to the `intra` user:\\n```bash\\nssh un1.bigdata.abc.gr\\nsudo -i -u intra\\n```\\n2. Navigate to the working directory:\\n```bash\\ncd projects/cube_ind\\n```\\n3. Remove the old PySpark script:\\n```bash\\nrm Indicators_Spark_Job.py\\n```\\n4. Authenticate with Kerberos and fetch the updated script from HDFS:\\n```bash\\nkinit -kt /home/intra/intra.keytab intra\\nhdfs dfs -get /user/intra/cube/Indicators_Spark_Job.py\\n```\\n6. Edit submit script to change execution date. The execution date should be 2 days after the missing data date.\\nFor example, to load data for 2021-01-01, set the execution date to 2021-01-03.\\n```bash\\nvim run_cube.sh\\n```\\nUpdate the relevant line:\\n```bash\\n...verhead=4096 Indicators_Spark_Job.py <date> brond.dsl_s...\\n```\\n7. Run the Spark job:\\n```bash\\n./run_cube.sh\\n```\\n---\\ntags:\\n  - cube indicators\\n  - pyspark\\n  - spark job\\n  - brond\\n  - manual data load\\n  - hdfs\\n  - intra\\n---\n\n---\n\n---\\ntitle: \\\"Cube Indicators Pipeline\\\"\\ndescription: \\\"Overview of the Brond Cube Indicators data pipeline, including Oozie coordinators, dependency tables, and the scripts responsible for generating input data.\\\"\\ntags:\\n  - brond\\n  - cube indicators\\n  - oozie\\n  - hadoop\\n  - coordinator\\n  - radius\\n  - retrains\\n  - fixed customers\\n  - data pipeline\\n  - xdsl\\n---\\n# Cube Indicators Pipeline\\nThis document summarizes the data flow and dependencies of the `brond.cube_indicators` pipeline. It includes the Oozie coordinators involved, input tables, and the scripts or jobs that populate each dependency. The main output is the `brond.cube_indicators` table, populated for `par_date = today - 2 days`.\\n* Coordinator `1011_Fixed_brond_customers`  populates table `brond.fixed_brond_customers_daily`\\n* Coordinator `Coord_post_BROND_FIXED_CUSTOMERS` populates tables `brond.fixed_brond_customers_daily_unq` & `brond.fixed_customers_brond_latest`\\n* Coordinator `Coord_Cube_Spark_Indicators` for `par_date=date -2 days` populates table `brond.cube_indicators`  and its dependencies are:\\n  * `brond.fixed_radio_matches_unq_inp` populated by script `un2:/shared/abc/brond/bin/101_fixed_radius.sh`\\n  * `brond.fixed_brond_customers_daily_unq`\\n  * `radius.radacct_hist` populated by script `un2:/shared/abc/radius/DataParser/scripts/radius.pl`\\n  * `brond.brond_retrains_hist` populated by script `un2:/shared/abc/brond/DataParser/scripts/brond_retrains.pl`\\n  * `brond.dsl_stats_week_xdsl_hist` populated by Coordinator `coord_brond_load_dsl_daily_stats`\\n---\\ntags:\\n  - cube_indicators\\n  - brond\\n  - coordinator\\n  - radius\\n  - retrains\\n  - fixed_customers\\n  - xdsl\\n  - spark\\n  - hadoop\\n  - data_dependencies\\n---"
        },
        {
            "question": "How can I resolve OpenLDAP replication issues caused by a changed Manager password?",
            "answer": "To fix replication:\n1. Create a new `replication_config.ldif` with updated credentials.\n2. Apply it on both kerb1 and kerb2:\n```bash\nldapmodify -H ldaps://kerb1.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\nldapmodify -H ldaps://kerb2.bigdata.abc.gr -D \"cn=config\" -W -f replication_config.ldif\n```\n3. Verify replication by creating `testuser` on kerb1 and checking its presence on kerb2 with:\n```bash\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \"cn=Manager,dc=bigdata,dc=abc,dc=gr\" -W -b \"ou=People,dc=bigdata,dc=abc,dc=gr\"  'uid=testuser'\n```",
            "category": "Troubleshooting & Issue Resolution",
            "files": "fix_openldap_replication_procedure.md",
            "context": "---\\ntitle: \\\"Fixing OpenLDAP Replication Issues\\\"\\ndescription: \\\"Step-by-step instructions for resolving OpenLDAP replication failures between kerb1 and kerb2, including password updates, slapcat/slapadd procedures, and verification via ldapsearch.\\\"\\ntags:\\n  - openldap\\n  - replication\\n  - ldap\\n  - slapcat\\n  - slapadd\\n  - kerb1\\n  - kerb2\\n  - phpldapadmin\\n  - ldapsearch\\n  - slapd\\n  - sync\\n  - mirror mode\\n  - user creation\\n  - credentials\\n  - config.ldif\\n  - data.ldif\\n  - restore ldap\\n  - slapd.d\\n---\\n# How to fix openldap replication\\nThis guide documents how to fix broken OpenLDAP replication between kerb1 and kerb2, addressing two scenarios: a Manager password change or corruption due to events like power outages. It includes configuration updates, slapcat/slapadd restore steps, verification procedures, and UI-based user creation checks via phpLDAPadmin.\\nThe broken replication between `kerb1`/`kerb2` could happened in case any of the below cases appeared/happened.\\n- Case 1: You changed the `Manager` password of openldap instance\\n- Case 2: Replication broken for any other reason (power outage etc) between `kerb1` and `kerb2`\\nFor every ldasearch the password of `Manager` is [here](KnowledgeBase/prodsyspasswd.kdbx)\\n## For Case 1 follow the below steps:\\nLogin into kerb1 node as root\\n```bash\\nssh kerb1\\nsudo -i\\n```\\nBackup:\\n```bash\\nslapcat -n 0 -l config.ldif\\nslapcat -n 2 -l data.ldif\\n```\\nCreate ldif file replication fix\\n```bash\\nvi replication_config.ldif\\ndn: olcDatabase={0}config,cn=config\\nchangetype:modify\\nreplace: olcSyncrepl\\nolcSyncrepl: rid=001\\n  provider=ldaps://kerb1.bigdata.abc.gr/\\n  binddn=\\\"cn=config\\\"\\n  bindmethod=simple\\n  credentials=\\\"new password\\\"\\n  searchbase=\\\"cn=config\\\"\\n  type=refreshAndPersist\\n  retry=\\\"5 5 300 +\\\"\\n  timeout=1\\nolcSyncrepl: rid=002\\n  provider=ldaps://kerb2.bigdata.abc.gr/\\n  binddn=\\\"cn=config\\\"\\n  bindmethod=simple\\n  credentials=\\\"new password\\\"\\n  searchbase=\\\"cn=config\\\"\\n  type=refreshAndPersist\\n  retry=\\\"5 5 300 +\\\"\\n  timeout=1\\nadd: olcMirrorMode\\nolcMirrorMode: TRUE\\ndn: olcDatabase={2}bdb,cn=config\\nchangetype:modify\\nreplace: olcSyncrepl\\nolcSyncrepl: rid=003\\n  provider=ldaps://kerb1.bigdata.abc.gr/\\n  binddn=\\\"cn=Manager,dc=bigdata,dc=abc,dc=gr\\\"\\n  bindmethod=simple\\n  credentials=`new password`\\n  searchbase=\\\"dc=bigdata,dc=abc,dc=gr\\\"\\n  type=refreshAndPersist\\n  retry=\\\"5 5 300 +\\\"\\n  timeout=1\\nolcSyncrepl: rid=004\\n  provider=ldaps://kerb2.bigdata.abc.gr/\\n  binddn=\\\"cn=Manager,dc=bigdata,dc=abc,dc=gr\\\"\\n  bindmethod=simple\\n  credentials=`new password`\\n  searchbase=\\\"dc=bigdata,dc=abc,dc=gr\\\"\\n  type=refreshAndPersist\\n  retry=\\\"5 5 300 +\\\"\\n  timeout=1\\nadd: olcMirrorMode\\nolcMirrorMode: TRUE\\n```\\nFix the replication:\\n```bash\\nldapmodify  -H ldaps://kerb1.bigdata.abc.gr -D \\\"cn=config\\\" -W -f replication_config.ldif\\nldapmodify  -H ldaps://kerb2.bigdata.abc.gr -D \\\"cn=config\\\" -W -f replication_config.ldif\\n```\\nChecks:\\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:\\nLogin into admin node as root:\\nOpen firefox\\n```bash\\nfirefox\\n```\\nphpldapadmin link: (https://kerb1.bigdata.abc.gr/phpldapadmin/)\\n## Steps to create an ldap user\\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\\n2. Expand tree `people`\\n3. Click `create a new entry here`\\n4. The type of account will be `User account`\\n5. Fill all the empty boxes. \\n6. The user will be `no login` and the group/gid `disabled`\\n7. Create object\\nAfter succesfully creation of `testuser` check if exist on `kerb2` from the dropdown menu or via ldapsearch\\n```bash\\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \\\"cn=Manager,dc=bigdata,dc=abc,dc=gr\\\" -W -b \\\"ou=People,dc=bigdata,dc=abc,dc=gr\\\"  'uid=testuser'\\n```\\nIf user exist then replication fixed. Just delete the `testuser`.\\n## Steps to delete an ldap user\\n1. Connect with username `cn=Manager,dc=bigdata,dc=abc,dc=gr` and password from `kdbx` file.\\n2. Expand tree `people`\\n3. Check the new user and from the right bar click `delete this entry`\\n## For Case 2 follow the below steps:\\nIdentify which `kerb` ldap instance has the issue. For example check if they had the same amount of `users` and `groups` with `ldapsearch` commands from checks\\nFrom the `kerb` ldap instance without corruption :\\n```bash\\nslapcat -n 0  config.ldif\\nslapcat -n 1 data.ldif\\nscp *.ldif `ldap_instance_with_corruption`:/tmp\\n```\\nGo to corrupted `kerb` instance:\\nBackup:\\n```bash\\ncp -rp /etc/openldap/ /tmp/openldap.bak\\ncp -rp /var/lib/ldap/ /tmp\\n```\\nClear:\\n```bash\\nsystemctl stop slapd\\nsystemctl status slapd\\ncd /etc/openldap/\\nrm \\u2013Rf slapd.d\\ncd /etc/openldap\\nmkdir slapd.d\\ncd /var/lib/ldap/\\nrm \\u2013Rf *\\n```\\nRestore:\\n```bash\\nslapadd -n 0 -F /etc/openldap/slapd.d -l /tmp/config.ldif\\nslapadd -n 2 -F /etc/openldap/slapd.d -l /tmp/data.ldif -w\\n```\\nOwnership:\\n```bash\\nchown -R ldap:ldap /var/lib/ldap\\n```\\nStart `slapd` service:\\n```bash\\nsystemctl start slapd\\nsystemctl status slapd\\n```\\nChecks:\\nCreate a new user on ldap kerb1 via `UI` with name `testuser`:\\nThe procedure to create a new user described [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/openldap_change_manager_password.md)\\nLogin into admin node as root:\\nOpen firefox\\n```bash\\nfirefox\\n```\\nphpldapadmin link: (https://kerb1.bigdata.abc.gr/phpldapadmin/)\\nAfter successfully creation of `testuser` check if exist on `kerb2` from the dropdown menu or via ldapsearch\\n```bash\\nldapsearch -H ldaps://kerb2.bigdata.abc.gr -D \\\"cn=Manager,dc=bigdata,dc=abc,dc=gr\\\" -W -b \\\"ou=People,dc=bigdata,dc=abc,dc=gr\\\"  'uid=testuser'\\n```\\nIf user exist then replication fixed. Just delete the `testuser`.\\nThe procedure to delete a new user described [here](/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/openldap_change_manager_password.md)\\n---\\ntags:\\n  - ldap\\n  - openldap\\n  - kerberos\\n  - slapcat\\n  - slapadd\\n  - phpldapadmin\\n  - ldap replication\\n  - directory service\\n  - slapd\\n  - user management\\n  - config.ldif\\n  - data.ldif\\n---"
        },
        {
            "question": "How do I change the LDAP bind password for R-Studio Connect and verify the update?",
            "answer": "1. Edit the bind credentials:\n```bash\nvi /etc/rstudio-connect/rstudio-connect.gcfg\n```\n2. Restart RStudio Connect:\n```bash\nsystemctl restart rstudio-connect\n```\n3. If license issues occur, reset time and activate license:\n```bash\nhwclock -w\nexport http_proxy=...; /opt/rstudio-connect/bin/license-manager activate <key>\n```",
            "category": "Application Functionality & Flow",
            "files": "groupnet_change_bind_users_passwords.md",
            "context": "---\\ntitle: \\\"GROUPNET - Change Bind Users' Passwords\\\"\\ndescription: \\\"Step-by-step guide for updating LDAP bind user passwords for RAN.AI Geolocation and R-Studio Connect services in the GROUPNET domain, including license reactivation if needed.\\\"\\ntags:\\n  - ldap\\n  - bind user\\n  - password rotation\\n  - rstudio connect\\n  - ran.ai\\n  - groupnet\\n  - license activation\\n  - cyberark\\n  - keycloak\\n  - ldap integration\\n  - rstudio-connect.gcfg\\n  - authentication\\n---\\n# GROUPNET - Change bind users' passwords\\nThis document outlines the procedure for rotating LDAP bind user passwords for the GROUPNET domain, covering both RAN.AI Geolocation (`t1-svc-cneranaibind`) and R-Studio Connect (`t1-svc-cnebind`). It also includes instructions for resolving expired license issues following a restart.\\n- [GROUPNET - Change bind users' passwords](#groupnet---change-bind-users-passwords)\\n  - [RAN.AI Geolocation - t1-svc-cneranaibind](#ranai-geolocation---t1-svc-cneranaibind)\\n  - [R-Studio Connect - t1-svc-cnebind](#r-studio-connect---t1-svc-cnebind)\\n    - [Re-activate License for R-Studio Connect](#re-activate-license-for-r-studio-connect)\\nThe purpose of this document is to describe the procedure on how to change the passwords for the bind users used for querying the LDAP of GROUPNET domain.\\n## RAN.AI Geolocation - t1-svc-cneranaibind\\n1. Inform users that the authentication with GROUPNET accounts may encounter errors during the procedure (approximate 1 hour). No pod will be restarted.\\n2. Go to [https://cne.def.gr/auth/admin](https://cne.def.gr/auth/admin)\\n3. Login with an administrative account\\n4. Navigate to User Federation > GROUPNET\\n5. Request password update `t1-svc-cneranaibind` via Remedy and obtain the new password\\n6. Update `Bind Credential` field and press `Save`\\n7. Press `Test authentication`\\n## R-Studio Connect - t1-svc-cnebind\\n1. Inform users for downtime of approximate 1 hour\\n2. Login to `unrstudio1.bigdata.abc.gr` as root via Cyberark\\n3. Request password update `t1-svc-cnebind` via Remedy and obtain the new password\\n4. Edit `/etc/rstudio-connect/rstudio-connect.gcfg`\\n``` bash\\nvi  /etc/rstudio-connect/rstudio-connect.gcfg\\n# Update **BindPassword** with the password obtained in step 3 and save\\n```\\n5. Restart R-Studio Connect\\n``` bash\\nsystemctl restart rstudio-connect\\n```\\n6. Check R-Studio Connect status\\n``` bash\\nsystemctl status rstudio-connect\\n```\\n7. Verify LDAP authentication by logging in to [https://unrstudio1.bigdata.abc.gr/connect](https://unrstudio1.bigdata.abc.gr/connect)\\n8. Due to the fact that the server is not directly connected to the Internet, R-Studio Connect might display an error about expired license after the reboot. In this case follow the steps listed [below](#re-activate-license-for-r-studio-connect).\\n9. Inform users that the application is available.\\n### Re-activate License for R-Studio Connect\\n1. Login to `unrstudio1.bigdata.abc.gr` as root via Cyberark\\n2. Ensure that time is accurate and the time zone is correct for the machine.\\n```bash\\ntimedatectl\\n```\\n3. Sync date and time to hardware clock of the machine.\\n``` bash\\nhwclock -w\\n```\\n4. Deactivate license\\n``` bash\\nexport http_proxy=http://un-vip.bigdata.abc.gr:5555\\nexport https_proxy=http://un-vip.bigdata.abc.gr:5555\\n/opt/rstudio-connect/bin/license-manager deactivate\\n```\\n5. Activate license\\n``` bash\\nexport http_proxy=http://un-vip.bigdata.abc.gr:5555\\nexport https_proxy=http://un-vip.bigdata.abc.gr:5555\\n/opt/rstudio-connect/bin/license-manager activate <product-key>\\n# This should display Activation status as Activated \\n```\\n6. In case you  receive the following:\\n```text\\nError activating product key: (13): The activation has expired or the system time has been tampered with. Ensure your time, timezone, and date settings are correct. If you're sure the license is not expired, try performing the following steps, in order: \\n1. Fix the timezone on your system.\\n2. Fix the date on your system.\\n3. Fix the time on your system.\\n4. Perform a system restart (important!)\\n```\\nFix any time/date issues and **reboot the server**.\\n7. Verify license status\\n``` bash\\n/opt/rstudio-connect/bin/license-manager status\\n/opt/rstudio-connect/bin/license-manager verify\\n```\\n8. Restart R-Studio Connect\\n``` bash\\nsystemctl restart rstudio-connect\\n```\\n9. Check R-Studio Connect status\\n``` bash\\nsystemctl status rstudio-connect\\n```\\n10. Verify LDAP authentication by logging in to [https://unrstudio1.bigdata.abc.gr/connect](https://unrstudio1.bigdata.abc.gr/connect)"
        },
        {
            "question": "What steps are required to open a hardware support ticket to Dell for a BigStreamer node?",
            "answer": "1. Find the iDRAC IP:\n```bash\nipmitool lan print | grep -i 'IP Address'\n```\n2. Log in via iDRAC web interface and note the Service Tag.\n3. Call Dell support and follow instructions.\n4. Download TSR logs and send them:\n```bash\nLocation: /home/cloudera/Downloads/TSR<date>_<service_tag>.zip\n```",
            "category": "Infrastructure & Deployment",
            "files": "how_to_create_dell_ticket.md",
            "context": "---\\ntitle: \\\"abc - BigStreamer - How to Open a Ticket to DELL\\\"\\ndescription: \\\"Step-by-step instructions for opening a hardware support ticket with DELL for abc BigStreamer nodes, including gathering the service tag and exporting TSR logs from iDRAC.\\\"\\ntags:\\n  - dell support\\n  - hardware ticket\\n  - tsr logs\\n  - idrac\\n  - ipmitool\\n  - bigstreamer\\n  - abc\\n  - service tag\\n  - vnc\\n  - server diagnostics\\n  - supportassist\\n---\\nThis document describes how to open a hardware support ticket to DELL for an abc BigStreamer node, including instructions to retrieve the node's iDRAC IP, collect TSR logs via iDRAC, and deliver them to DELL support.\\n# abc - BigStreamer - How to open a ticket to DELL\\n## Description\\nBelow is a step-by-step description of the process from opening a ticket to collecting TSR logs from iDRAC.\\n## Actions Taken\\n1. ssh with your personal account on the issue node.\\n2. Switch to root and find the iDRAC management IP:\\n```bash\\nsudo -i\\nipmitool lan print | grep -i 'IP Address'\\n# If ipmitool is missing:\\nyum install ipmitool\\n```\\n3. Open Firefox on a VNC session and navigate to the iDRAC IP address found in Step 2.\\n4. From `Server-->Overview-->Server Information` copy the `Service Tag number`\\n5. Call Dell support `2108129800`. They need the `Service Tag number` from step 4\\n6. A DELL engineer will create a case and send you all the necessary instructions. If not the link to collect the TSR logs is `https://www.dell.com/support/kbdoc/el-gr/000126803/export-a-supportassist-collection-via-idrac7-and-idrac8`\\n7. Inform `abc` before any action on the IDRAC.\\n8. Download the TSR `.zip` file locally from the iDRAC interface. If using VNC on a node like `un4`, the downloaded files will be stored under: `/home/cloudera/Downloads/`. The filename format is: `TSR<date>_<service_tag>.zip`.\\n9. Send the zip file/files to DELL and wait for their response.\\n## Completion\\nYou have now completed the process. Await DELL\\u2019s response and proceed based on their instructions."
        },
        {
            "question": "How are Kubernetes certificates renewed across masters in the RAN.AI cluster?",
            "answer": "1. On each master node, run:\n```bash\nkubeadm certs renew all\nkubeadm certs check-expiration\n```\n2. Replace `/root/.kube/config` with updated content from `/etc/kubernetes/admin.conf`.\n3. Restart static pods by stopping container IDs:\n```bash\nctrctl stop <controller> <scheduler> <apiserver>\n```\n4. Delete CoreDNS pods:\n```bash\nkubectl delete pod <coredns-pods> -n kube-system\n```",
            "category": "Infrastructure & Deployment",
            "files": "ranai_kubernetes_renew_certificates.md",
            "context": "---\\ntitle: Kubernetes Certificate Renewal Procedure\\ndescription: Yearly renewal process for expiring Kubernetes certificates on kubemaster1, kubemaster2, and kubemaster3 including backup, kubeadm certs renewal, and container restarts.\\ntags: [kubernetes, certificates, renewal, kubeadm, kubemaster, static pods, downtime, cluster-admin, tls]\\ncategory: infrastructure\\nproduct: BigStreamer\\nplatform: kubernetes\\nconfidentiality: internal\\n---\\n# Scope\\nOnce a year the kubernetes certificates are expiring. In order to prevent that, we have a procedure below in which we are describing the steps that will help you to renew them.\\n## Setup\\n**Masters**: `kubemaster1`, `kubemaster2`, `kubemaster3`\\n**Important ndef:** This procedure requires downtime.\\n## Procedure\\nLogin to kubemaster1.bigdata.abc.gr with your personal account and become root:\\n- Check the certificates expiration date:\\n```bash\\nsudo su -write the metadata block, tags at the end, tell me what do to clean it and tell me what descriptions to write in order for the document to be properly retrieved in my RAG chatbot\\nkubeadm certs check-expiration\\n```\\n- Keep a backup of kubernetes configuration to tmp \\n```bash\\ncp -ar /etc/kubernetes /tmp/\\n```\\n- Keep a backup of incelligent service account\\n```bash\\ncp -ar /home/users/incelligent/.kube/config /tmp/bckup_renew_certificates/\\n```\\n- Renew the certificates\\n```bash\\nkubeadm  certs renew all\\nkubeadm certs check-expiration\\n```\\n- Run the following\\n```bash\\ncp -p /root/.kube/config /root/.kube/config_old\\ncp /etc/kubernetes/admin.conf  /root/.kube/config\\n```\\n- From the newly create`/etc/admin/conf` make sure to copy the `client-certificate-data` and `client-key-data` data content. Edit the file /home/users/incelligent/.kube/config and replace the values of client-certificate-data and client-key-data with the ones copied from /etc/kubernetes/admin.conf.\\nin order to add the new certificates.\\n- Check again the certificates expiration date\\n```bash\\nkubeadm certs check-expiration\\n```\\n- Check the kubectl functionality\\n```bash\\nkubectl get pods\\n```\\n- When the certificates of controller, apiserver and scheduler are renewed you must also stop containers of those three:\\n```bash\\nctrctl ps\\nCONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES\\n1350c48cbfb5        b3c57ca578fb           \\\"kube-controller-man\\u2026\\\"   11 minutes ago      Up 11 minutes                           k8s_kube-controller-manager_kube-controller-manager-cti-cx1_kube-system_9eb854fb973ddd6df55fb792a2fbf743_9\\n1bd22e95ef01        5a84bb672db8           \\\"kube-scheduler --au\\u2026\\\"   11 minutes ago      Up 11 minutes                           k8s_kube-scheduler_kube-scheduler-cti-cx1_kube-system_649aa160f1bd0840b2bb0f70b6493f99_9\\ncf43799ae77d0       b6e18ffb844e6          \\\"kube-apiserver --au\\u2026\\\"   11 minutes ago      Up 11 minutes                 \\n```\\nStop containers IDs:\\n```bash\\nctrctl stop 1350c48cbfb5 1bd22e95ef01 cf3cb7655b99d\\n```\\n> Ndef_1: Keep in mind that when the containers will be stopped, there will be downtime\\n> Ndef_2: Keep in mind that those pods are static so you don't need to kill them. They will be automatically restarted\\n- Also delete core-dns pod:\\n```bash\\nkubectl get pod -n kube-system -l k8s-app=kube-dns\\nNAME                      READY   STATUS    RESTARTS      AGE\\ncoredns-64897985d-7dzkl   1/1     Running   3 (84d ago)   644d\\ncoredns-64897985d-rw5kc   1/1     Running   0             83d\\n```\\n```bash\\nkubectl delete pod coredns-64897985d-7dzkl  coredns-64897985d-rw5kc  -n kube-system\\n```\\n- Repeat the same procedure for `kubemaster2` and `kubemaster3`\\n> Note: The user `incelligent` exists only on kubemaster1. You do not need to update or copy the user config on kubemaster2 and kubemaster3."
        },
        {
            "question": "What caused the AppEmptyQueryException alerts in IPVPN and how were they resolved?",
            "answer": "Alerts were caused by missing CSV files for interface and CPU/MEM metrics. Diagnosis involved checking logs:\n```bash\ntail -f /shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/log/nnm_interface_metrics.cron.20231012.log\nssh custompoller@nnmprd01\n```\nRoot cause: NNM did not generate files for the alert time windows.",
            "category": "Troubleshooting & Issue Resolution",
            "files": "ipvpn_sm_AppEmptyQuery_resolution_MoP.md",
            "context": "---\\ntitle: \\\"MoP - AppEmptyQueryException Alerts Resolution\\\"\\ndescription: \\\"Step-by-step resolution procedure for AppEmptyQueryException alerts caused by missing interface and CPU/MEM metrics in BigStreamer\\u2019s bigcust tables, including log inspection and verification on un2 and nnmprd01.\\\"\\ntags:\\n  - mop\\n  - AppEmptyQueryException\\n  - bigcust\\n  - ipvpn\\n  - interface metrics\\n  - cpu metrics\\n  - memory metrics\\n  - csv transfer\\n  - nnmprd01\\n  - un2\\n  - ip_vpn\\n  - dataparser\\n  - sftp\\n  - missing data\\n  - bigstreamer\\n  - alerts\\n---\\nThis MoP describes how to investigate and resolve `AppEmptyQueryException` alerts triggered due to missing data in the `bigcust` interface and CPU/MEM metrics tables. The procedure involves log review on the ingestion and source systems (un2 and nnmprd01) and CSV generation verification.\\n# Method of Procedure (MoP) - AppEmptyQueryException Alerts Resolution\\n## Description\\nThis MoP document outlines the resolution steps for addressing AppEmptyQueryException alerts related to missing data from bigcust tables. The alerts pertain to two categories: IF (Interface Metrics) and CPU/MEM (CPU and Memory Metrics).\\n## Affected Alerts\\n1. IF Alerts:\\n   - Occurred for 5-minute intervals starting from `2023-10-12 14:30:00` until `2023-10-12 14:50:00`.\\n2. CPU/MEM Alerts:\\n   - Occurred for 5-minute intervals starting from `2023-10-12 14:30:00` until `2023-10-12 14:45:00` and then for `2023-10-12 14:55:00` & `2023-10-12 15:00:00`.\\n## Resolution Steps\\n### Resolution for IF Alerts:\\n1. Check `bigcust.perf_interfacemetrics_ipvpn_hist` table to confirm the missing data for the mentioned 5-minute intervals.\\n2. Review the CSV file loading process into HDFS for errors:\\n- Connect to `un2` as `ipvpn` and inspect the log file:\\n```bash\\nssh root@un2\\nsu ipvpn\\ntail -f /shared/abc/ip_vpn/interfaces_flow/Dataparser/scripts/log/nnm_interface_metrics.cron.20231012.log\\n```\\n- The last loaded CSV file was `InterfaceMetrics_20231012142500238.csv.gz` at `2023-10-12 14:28:00`. No files were loaded from that point until 14:50, with the message: \\\"File '/home/custompoller/nnm_interface_metrics/InterfaceMetrics*.csv.gz' not found.\\\"\\n3. Verify the transfer process on `nnmprd01` for errors:\\n- Connect to `nnmprd01` via passwordless SSH from `un2`:\\n```bash\\nssh custompoller@nnmprd01\\n```\\n- Review the transfer log:\\n```ssh\\ntail -f /home/custompoller/export_metrics/log/transfer-new-files-cut.20231012.log\\n```\\nThe last file moved from the source directory to the destination directory on `nnmprd01` was `InterfaceMetrics_20231012142500238.csv.gz` at `14:28:01`. This continues until `14:50`, aligning with the observations in the `bigcust.perf_interfacemetrics_ipvpn_hist` table.\\n#### Conclusion for IF Alerts\\nRoot Cause: No Interface Metrics files were generated by NNM between `14:30` and `14:50`, explaining the IF alerts.\\n### Resolution for CPU/MEM Alerts:\\n1. Check `bigcust.nnm_ipvpn_componentmetrics_hist` table to confirm the missing data for the mentioned 5-minute intervals.\\n2. Examine the SFTP transfer process from `nnprd01` for errors:\\n- Navigate to the log file on `ipvpn@un2`:\\n```bash\\ntail -f /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20231012.log\\n```\\nThe last SFTP transfer was for the file `BIG-CUSTOMERS-CPU-MEM-UTIL_20231012142323337.csv.gz` at `14:26`. Subsequently, the logs show the message \\\"No file found.. sleep and try again..\\\" until `14:58`, when a CSV file was found.\\n#### Conclusion for CPU/MEM Alerts\\nRoot Cause: No CPU/MEM files were generated by NNM between `14:30` and `15:00`, explaining the CPU/MEM alerts.\\n"
        },
        {
            "question": "How can you monitor and force IDM (FreeIPA) replication between idm1 and idm2?",
            "answer": "To check replication:\n```bash\nipa-replica-manage list -v\n```\nTo force sync from idm2 to idm1:\n```bash\nipa-replica-manage force-sync --from idm2.bigdata.abc.gr\n```",
            "category": "Data Management & Query Execution",
            "files": "manage_idm_replication.md",
            "context": "---\\ntitle: \\\"Manage IDM Replication\\\"\\ndescription: \\\"Step-by-step guide for monitoring, forcing, and troubleshooting FreeIPA (IDM) LDAP replication across idm1 and idm2 nodes, including preauthentication issues and resolution procedures related to SPNs and Kerberos.\\\"\\ntags:\\n  - idm\\n  - ldap\\n  - freeipa\\n  - kerberos\\n  - spn\\n  - preauthentication\\n  - kdc\\n  - replication\\n  - ipa-replica-manage\\n  - ipa\\n  - gssapi\\n  - ipa config\\n  - krbTicketFlags\\n  - kadmin\\n  - hue\\n  - authentication\\n  - troubleshooting\\n  - directory\\n  - bigstreamer\\n  - abc\\n---\\nThis document outlines the setup and operational procedures for managing FreeIPA (IDM) replication between two nodes. It also includes detailed troubleshooting steps for resolving SPN-related Kerberos preauthentication errors and replication failures across KDCs.\\n# Manage IDM Replication\\n## Setup\\nIDM (FreeIPA) has been installed on two nodes for High Availability. Replication between the two nodes is performed on the LDAP service using GSSAPI authentication (Kerberos) using `ldap/_HOST` Service Principal Names (SPNs). It is a \\\"push\\\" replication, so each change is propagated to the other instance from the instance that it was performed.\\nEach KDC uses the LDAP on the same host as backend, so since both LDAP Servers are replicated between the two instances (active-active) KDCs are up to date with between the two hosts.\\n```mermaid\\n  graph LR\\n  A[idm1.bigdata.abc.gr<br>SPN: ldap/idm1.bigdata.abc.gr]\\n  B[idm2.bigdata.abc.gr<br>SPN: ldap/idm2.bigdata.abc.gr]\\n  A-->B\\n  B-->A\\n```\\n## Procedure\\n### Check replication\\n``` bash\\n# Assuming you are on idm1\\nkinit <admin user>\\nipa-replica-manage list -v # List replication targets of idm1\\nipa-replica-manage list -v idm2.bigdata.abc.gr # This will connect to idm2 and show if it managed to push it's changes to the local instance (idm1)\\n```\\n```log\\nidm1.bigdata.abc.gr: replica\\n  last update status: Error (0) Replica acquired successfully: Incremental update succeeded\\n  last update ended: 2023-12-21 12:41:17+00:00\\n```\\n### Force replication\\n``` bash\\n# Assuming you are on idm1\\nkinit <admin user>\\nipa-replica-manage force-sync  --from idm2.bigdata.abc.gr # This will connect to idm2 and schedule an immediate sync (push) to the local instance (idm1)\\n```\\n```log\\nipa: INFO: Setting agreement cn=meToidm1.bigdata.abc.gr,cn=replica,cn=dc\\\\=bigdata\\\\,dc\\\\=abc\\\\,dc\\\\=gr,cn=mapping tree,cn=config schedule to 2358-2359 0 to force synch\\nipa: INFO: Deleting schedule 2358-2359 0 from agreement cn=meToidm1.bigdata.abc.gr,cn=replica,cn=dc\\\\=bigdata\\\\,dc\\\\=abc\\\\,dc\\\\=gr,cn=mapping tree,cn=config\\nipa: INFO: Replication Update in progress: FALSE: status: Error (0) Replica acquired successfully: Incremental update succeeded: start: 0: end: 0\\n```\\n## Troubleshooting\\n:warning: All problems in this section have been solved and all actions mentioned below have been reverted as part of obss/oss/sysadmin-group/abc/BigStreamer/bigstreamer#53 \\n### A brief history of preauthentication\\nPreauthentication is authentication of the client to the KDC when requesting a Kerberos ticket. This means that instead of issuing an `AS-REQ` the client issues a `TGS-REQ` which is logged in KDCs' logs.\\nWe will not include specific preauthentication protocols in this presentation. Feel free to research them.\\n### Troubleshooting Preauthentication Issues\\n_At the time of writing we cannot replicate the issue to get accurate log entries, but we can give you is accurate commands for the investigation_ :smile:\\nWhen we failed over the Cloudera Services from `CNE.abc.GR` to `BIGDATA.abc.GR` we faced a problem with users coming from `CNE.abc.GR` not being able to login to Cloudera Services.\\nTo resolve the issue we issued the following command, that disables preauthentication for SPNs:\\n```bash\\n# Assuming you are on idm1 and have valid admin Kerberos ticket\\nipa config-mod --ipaconfigstring=\\\"KDC:Disable Default Preauth for SPNs\\\"\\n```\\nThis resolved our issue, but created two new problems:\\n1. SPNs do not require preauthentication, but `krbtgt/BIGDATA.abc.GR` requires preauthentication. This means that if you `kinit` with an SPN `kinit -R` will refuse to run with the dreaded `NO_PREAUTH`. This is problematic for the Hue Kerberos Renewer. As a workaround we are renewing the Kerberos ticket cache of Hue via `cron`.\\n2. Replication from `idm2.bigdata.abc.gr` to `idm1.bigdata.abc.gr`. The error is the error is also `NO_PREAUTH`. This means that `ldap/idm2.bigdata.abc.gr` was rejected by `ldap/idm1.bigdata.abc.gr` due to the lack of preauthentication. Let's inspect the service principals:\\n```bash\\nipa service-find ldap/idm1.bigdata.abc.gr --all --raw\\n```\\n```log\\n    -----------------\\n    1 service matched\\n    -----------------\\n      dn: krbprincipalname=ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      krbcanonicalname: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\\n      krbprincipalname: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\\n      ...\\n      managedby: fqdn=idm1.bigdata.abc.gr,cn=computers,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      ipaKrbPrincipalAlias: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\\n      ...\\n      krbPwdPolicyReference: cn=Default Service Password Policy,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      krbTicketFlags: 128\\n      memberof: cn=replication managers,cn=sysaccounts,cn=etc,dc=bigdata,dc=abc,dc=gr\\n      objectClass: ipaobject\\n      objectClass: top\\n      objectClass: ipaservice\\n      objectClass: pkiuser\\n      objectClass: krbprincipal\\n      objectClass: krbprincipalaux\\n      objectClass: krbTicketPolicyAux\\n      objectClass: ipakrbprincipal\\n    ----------------------------\\n    Number of entries returned 1\\n    ----------------------------\\n```\\n```bash\\nipa service-find ldap/idm2.bigdata.abc.gr --all --raw\\n```\\n```log\\n    -----------------\\n    1 service matched\\n    -----------------\\n      dn: krbprincipalname=ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      krbcanonicalname: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\\n      krbprincipalname: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\\n      ...\\n      managedby: fqdn=idm2.bigdata.abc.gr,cn=computers,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      ipaKrbPrincipalAlias: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\\n      ...\\n      krbPwdPolicyReference: cn=Default Service Password Policy,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      krbTicketFlags: 0\\n      memberof: cn=replication managers,cn=sysaccounts,cn=etc,dc=bigdata,dc=abc,dc=gr\\n      objectClass: ipaobject\\n      objectClass: top\\n      objectClass: ipaservice\\n      objectClass: pkiuser\\n      objectClass: krbprincipal\\n      objectClass: krbprincipalaux\\n      objectClass: krbTicketPolicyAux\\n      objectClass: ipakrbprincipal\\n    ----------------------------\\n    Number of entries returned 1\\n    ----------------------------\\n```\\n`krbTicketFlags` seems to be different. Checking the same principals from `kadmin` interface:\\n```bash\\nkadmin.local -q \\\"get_principal ldap/idm1.bigdata.abc.gr\\\"\\n```\\n```log\\n    Principal: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\\n    Expiration date: [never]\\n    ...\\n    Attributes: REQUIRES_PRE_AUTH\\n    Policy: [none]\\n```\\n```bash\\nkadmin.local -q \\\"get_principal ldap/idm2.bigdata.abc.gr\\\"\\n```\\n```log\\n    Principal: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\\n    Expiration date: [never]\\n    ...\\n    Attributes:\\n    Policy: [none]\\n```\\nSeems that preauthentication is enforced via `krbTicketFlags`. We can remove it by issuing the following command:\\n```bash\\nkadmin.local -q \\\"modify_principal -requires_preauth ldap/idm1.bigdata.abc.gr\\\" # This is also replicated to idm2, since pushing from idm1 to idm2 worked\\n```\\nNow replication works.\\n### Re-enable SPN Preauthentication (Post-Migration Cleanup)\\nWhen `CNE.abc.GR` is removed we will re-enable preauthentication with the following command to resolve the issue with Hue:\\n```bash\\nkadmin.local -q \\\"modify_principal +requires_preauth ldap/idm1.bigdata.abc.gr\\\"\\nipa config-mod --ipaconfigstring=\\\"\\\"\\nipactl restart\\n```"
        },
        {
            "question": "How can you disable and later re-enable Kerberos preauthentication for service principals in FreeIPA?",
            "answer": "To disable preauthentication:\n```bash\nipa config-mod --ipaconfigstring=\"KDC:Disable Default Preauth for SPNs\"\n```\nTo re-enable it later:\n```bash\nkadmin.local -q \"modify_principal +requires_preauth ldap/idm1.bigdata.abc.gr\"\nipa config-mod --ipaconfigstring=\"\"\nipactl restart\n```",
            "category": "Data Management & Query Execution",
            "files": "manage_idm_replication.md",
            "context": "---\\ntitle: \\\"Manage IDM Replication\\\"\\ndescription: \\\"Step-by-step guide for monitoring, forcing, and troubleshooting FreeIPA (IDM) LDAP replication across idm1 and idm2 nodes, including preauthentication issues and resolution procedures related to SPNs and Kerberos.\\\"\\ntags:\\n  - idm\\n  - ldap\\n  - freeipa\\n  - kerberos\\n  - spn\\n  - preauthentication\\n  - kdc\\n  - replication\\n  - ipa-replica-manage\\n  - ipa\\n  - gssapi\\n  - ipa config\\n  - krbTicketFlags\\n  - kadmin\\n  - hue\\n  - authentication\\n  - troubleshooting\\n  - directory\\n  - bigstreamer\\n  - abc\\n---\\nThis document outlines the setup and operational procedures for managing FreeIPA (IDM) replication between two nodes. It also includes detailed troubleshooting steps for resolving SPN-related Kerberos preauthentication errors and replication failures across KDCs.\\n# Manage IDM Replication\\n## Setup\\nIDM (FreeIPA) has been installed on two nodes for High Availability. Replication between the two nodes is performed on the LDAP service using GSSAPI authentication (Kerberos) using `ldap/_HOST` Service Principal Names (SPNs). It is a \\\"push\\\" replication, so each change is propagated to the other instance from the instance that it was performed.\\nEach KDC uses the LDAP on the same host as backend, so since both LDAP Servers are replicated between the two instances (active-active) KDCs are up to date with between the two hosts.\\n```mermaid\\n  graph LR\\n  A[idm1.bigdata.abc.gr<br>SPN: ldap/idm1.bigdata.abc.gr]\\n  B[idm2.bigdata.abc.gr<br>SPN: ldap/idm2.bigdata.abc.gr]\\n  A-->B\\n  B-->A\\n```\\n## Procedure\\n### Check replication\\n``` bash\\n# Assuming you are on idm1\\nkinit <admin user>\\nipa-replica-manage list -v # List replication targets of idm1\\nipa-replica-manage list -v idm2.bigdata.abc.gr # This will connect to idm2 and show if it managed to push it's changes to the local instance (idm1)\\n```\\n```log\\nidm1.bigdata.abc.gr: replica\\n  last update status: Error (0) Replica acquired successfully: Incremental update succeeded\\n  last update ended: 2023-12-21 12:41:17+00:00\\n```\\n### Force replication\\n``` bash\\n# Assuming you are on idm1\\nkinit <admin user>\\nipa-replica-manage force-sync  --from idm2.bigdata.abc.gr # This will connect to idm2 and schedule an immediate sync (push) to the local instance (idm1)\\n```\\n```log\\nipa: INFO: Setting agreement cn=meToidm1.bigdata.abc.gr,cn=replica,cn=dc\\\\=bigdata\\\\,dc\\\\=abc\\\\,dc\\\\=gr,cn=mapping tree,cn=config schedule to 2358-2359 0 to force synch\\nipa: INFO: Deleting schedule 2358-2359 0 from agreement cn=meToidm1.bigdata.abc.gr,cn=replica,cn=dc\\\\=bigdata\\\\,dc\\\\=abc\\\\,dc\\\\=gr,cn=mapping tree,cn=config\\nipa: INFO: Replication Update in progress: FALSE: status: Error (0) Replica acquired successfully: Incremental update succeeded: start: 0: end: 0\\n```\\n## Troubleshooting\\n:warning: All problems in this section have been solved and all actions mentioned below have been reverted as part of obss/oss/sysadmin-group/abc/BigStreamer/bigstreamer#53 \\n### A brief history of preauthentication\\nPreauthentication is authentication of the client to the KDC when requesting a Kerberos ticket. This means that instead of issuing an `AS-REQ` the client issues a `TGS-REQ` which is logged in KDCs' logs.\\nWe will not include specific preauthentication protocols in this presentation. Feel free to research them.\\n### Troubleshooting Preauthentication Issues\\n_At the time of writing we cannot replicate the issue to get accurate log entries, but we can give you is accurate commands for the investigation_ :smile:\\nWhen we failed over the Cloudera Services from `CNE.abc.GR` to `BIGDATA.abc.GR` we faced a problem with users coming from `CNE.abc.GR` not being able to login to Cloudera Services.\\nTo resolve the issue we issued the following command, that disables preauthentication for SPNs:\\n```bash\\n# Assuming you are on idm1 and have valid admin Kerberos ticket\\nipa config-mod --ipaconfigstring=\\\"KDC:Disable Default Preauth for SPNs\\\"\\n```\\nThis resolved our issue, but created two new problems:\\n1. SPNs do not require preauthentication, but `krbtgt/BIGDATA.abc.GR` requires preauthentication. This means that if you `kinit` with an SPN `kinit -R` will refuse to run with the dreaded `NO_PREAUTH`. This is problematic for the Hue Kerberos Renewer. As a workaround we are renewing the Kerberos ticket cache of Hue via `cron`.\\n2. Replication from `idm2.bigdata.abc.gr` to `idm1.bigdata.abc.gr`. The error is the error is also `NO_PREAUTH`. This means that `ldap/idm2.bigdata.abc.gr` was rejected by `ldap/idm1.bigdata.abc.gr` due to the lack of preauthentication. Let's inspect the service principals:\\n```bash\\nipa service-find ldap/idm1.bigdata.abc.gr --all --raw\\n```\\n```log\\n    -----------------\\n    1 service matched\\n    -----------------\\n      dn: krbprincipalname=ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      krbcanonicalname: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\\n      krbprincipalname: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\\n      ...\\n      managedby: fqdn=idm1.bigdata.abc.gr,cn=computers,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      ipaKrbPrincipalAlias: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\\n      ...\\n      krbPwdPolicyReference: cn=Default Service Password Policy,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      krbTicketFlags: 128\\n      memberof: cn=replication managers,cn=sysaccounts,cn=etc,dc=bigdata,dc=abc,dc=gr\\n      objectClass: ipaobject\\n      objectClass: top\\n      objectClass: ipaservice\\n      objectClass: pkiuser\\n      objectClass: krbprincipal\\n      objectClass: krbprincipalaux\\n      objectClass: krbTicketPolicyAux\\n      objectClass: ipakrbprincipal\\n    ----------------------------\\n    Number of entries returned 1\\n    ----------------------------\\n```\\n```bash\\nipa service-find ldap/idm2.bigdata.abc.gr --all --raw\\n```\\n```log\\n    -----------------\\n    1 service matched\\n    -----------------\\n      dn: krbprincipalname=ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      krbcanonicalname: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\\n      krbprincipalname: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\\n      ...\\n      managedby: fqdn=idm2.bigdata.abc.gr,cn=computers,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      ipaKrbPrincipalAlias: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\\n      ...\\n      krbPwdPolicyReference: cn=Default Service Password Policy,cn=services,cn=accounts,dc=bigdata,dc=abc,dc=gr\\n      krbTicketFlags: 0\\n      memberof: cn=replication managers,cn=sysaccounts,cn=etc,dc=bigdata,dc=abc,dc=gr\\n      objectClass: ipaobject\\n      objectClass: top\\n      objectClass: ipaservice\\n      objectClass: pkiuser\\n      objectClass: krbprincipal\\n      objectClass: krbprincipalaux\\n      objectClass: krbTicketPolicyAux\\n      objectClass: ipakrbprincipal\\n    ----------------------------\\n    Number of entries returned 1\\n    ----------------------------\\n```\\n`krbTicketFlags` seems to be different. Checking the same principals from `kadmin` interface:\\n```bash\\nkadmin.local -q \\\"get_principal ldap/idm1.bigdata.abc.gr\\\"\\n```\\n```log\\n    Principal: ldap/idm1.bigdata.abc.gr@BIGDATA.abc.GR\\n    Expiration date: [never]\\n    ...\\n    Attributes: REQUIRES_PRE_AUTH\\n    Policy: [none]\\n```\\n```bash\\nkadmin.local -q \\\"get_principal ldap/idm2.bigdata.abc.gr\\\"\\n```\\n```log\\n    Principal: ldap/idm2.bigdata.abc.gr@BIGDATA.abc.GR\\n    Expiration date: [never]\\n    ...\\n    Attributes:\\n    Policy: [none]\\n```\\nSeems that preauthentication is enforced via `krbTicketFlags`. We can remove it by issuing the following command:\\n```bash\\nkadmin.local -q \\\"modify_principal -requires_preauth ldap/idm1.bigdata.abc.gr\\\" # This is also replicated to idm2, since pushing from idm1 to idm2 worked\\n```\\nNow replication works.\\n### Re-enable SPN Preauthentication (Post-Migration Cleanup)\\nWhen `CNE.abc.GR` is removed we will re-enable preauthentication with the following command to resolve the issue with Hue:\\n```bash\\nkadmin.local -q \\\"modify_principal +requires_preauth ldap/idm1.bigdata.abc.gr\\\"\\nipa config-mod --ipaconfigstring=\\\"\\\"\\nipactl restart\\n```"
        },
        {
            "question": "How do you resolve Nagios fork, SSH, or return code 255 errors?",
            "answer": "1. To fix fork errors, increase limits in `.bashrc`:\n```bash\nulimit -u 8888\nulimit -n 2222\n```\n2. For SSH issues, edit `commands.cfg`:\n```bash\ncheck_by_ssh -E 8 -o StrictHostKeyChecking=no ...\n```\n3. For return code 255 errors, increase concurrent checks in `nagios.cfg`:\n```bash\nmax_concurrent_checks=50\nservice nagios restart\n```",
            "category": "Application Functionality & Flow",
            "files": "nagios-errors.md",
            "context": "---\\ntitle: \\\"Nagios Alarms & Errors - Fork, SSH, Return Code 255\\\"\\ndescription: \\\"Steps to resolve common Nagios issues on BigStreamer admin nodes including 'fork: retry', 'ssh_exchange_identification: Connection closed', and 'Return code 255 is out of bounds' by adjusting ulimits, SSH command options, and Nagios configuration settings.\\\"\\ntags:\\n  - nagios\\n  - bigstreamer\\n  - monitoring\\n  - fork error\\n  - ssh_exchange_identification\\n  - return code 255\\n  - max_concurrent_checks\\n  - bashrc\\n  - nagios.cfg\\n  - commands.cfg\\n  - ssh\\n  - alerts\\n  - admin\\n  - abc\\n---\\n# Nagios Alarms & Errors\\n**Component**: Nagios  \\n**Environment**: BigStreamer  \\n**Owner**: kpar  \\n**Status**: Closed  \\n**Date**: 2021-05-12  \\n**Issue Number**: -  \\n## Description\\nThis document describes how to resolve the following Nagios errors:\\n- `/etc/bashrc: fork: retry: Resource temporarily unavailable`\\n- `ssh_exchange_identification: Connection closed by remote host`\\n- `Return code of 255 is out of bounds`\\n## Resolution Steps\\n### 1. Fix \\\"fork: retry\\\" Error\\nAs root or as the `nagios` user, edit the `.bashrc` file:\\n```bash\\nvi /home/nagios/.bashrc\\n```\\nAdd the following lines:\\n```bash\\nulimit -u 8888\\nulimit -n 2222\\n```\\n---\\n### 2. Fix SSH \\\"Connection closed by remote host\\\" Error\\nAs root, edit the following Nagios command file:\\n```bash\\nvi /usr/local/nagios/etc/objects/commands.cfg\\n```\\nReplace this line:\\n```bash\\n$USER1$/check_by_ssh  -H $HOSTADDRESS$ -t 30 -C \\\"/usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$ -p $ARG3$\\\"\\n```\\nWith this:\\n```bash\\n$USER1$/check_by_ssh -E 8 -o StrictHostKeyChecking=no -H $HOSTADDRESS$ -t 30 -C \\\"/usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$ -p $ARG3$\\\"\\n```\\n---\\n### 3. Fix \\\"Return code of 255 is out of bounds\\\" Error\\nAs root, edit the Nagios configuration file:\\n```bash\\nvi /usr/local/nagios/etc/nagios.cfg\\n```\\nFind and change the following setting:\\n```bash\\nmax_concurrent_checks=50\\n```\\nThen restart the Nagios service:\\n\\n```bash\\nservice nagios restart\\n```\\n---\\n## Keywords\\nlogs, fork, bounds, connection closed, ssh, ulimit, max_concurrent_checks, Nagios admin"
        },
        {
            "question": "How are Internet Banking service audit records ingested, processed, and written to storage in real-time?",
            "answer": "Events are received by Wildfly servers via HTTP POST from backend servers, passed through a Netscaler Load Balancer to Kafka topics (e.g., `prod-trlog-ibank-ingest-stream-mir`), then mirrored via Kafka MirrorMaker. Spark Streaming jobs consume these messages and write to both Kudu and HBase:\n```mermaid\ngraph TD\nKafka -->|Spark: Prod_IBANK_IngestStream| Kudu\nKafka -->|Spark: Prod_IBANK_IngestStream_Visible| HBase\n```\nConfiguration paths include:\n```bash\n/opt/wildfly/default/prodrestib/standalone/configuration/standalone.xml\n```",
            "category": "Application Functionality & Flow",
            "files": "ibank.md",
            "context": "---\\ntitle: Internet Banking - Data Ingestion and Processing\\ndescription: Complete documentation of the Internet Banking data pipeline: streaming ingestion, batch migration, Spark processing, Kafka, Wildfly, retention policies, Oozie jobs, and operational troubleshooting across DR/PR clusters.\\ntags:\\n  - ibank\\n  - internet banking\\n  - spark streaming\\n  - kafka\\n  - kudu\\n  - hbase\\n  - wildfly\\n  - retention\\n  - migration\\n  - sqoop\\n  - batch processing\\n  - cron jobs\\n  - troubleshooting\\n  - monitoring\\n  - cloudera manager\\n  - impala\\n  - data pipeline\\n  - RAG\\n  - BigStreamer\\n---\\n# Internet Banking\\n## Stream\\nProcesses user transaction events in real-time from Wildfly servers to Kafka, then through Spark Streaming to Kudu and HBase for persistent storage.\\n### Wilfly Transaction Receiver\\nWildfly applications receive HTTP POST requests with transaction events from Internet Banking backend servers and forward them to Kafka topics.\\nUser generated events are sent to Wildfly instances installed at edge servers as HTTP POST requests. The requests originate from the backend servers of Internet Banking. Requests are loadbalanced by NetScaler (Loadbalancer) that is managed by mno's networking department. The events are then sent to a Kafka topic. Usually only one site is active (Primary Site).\\n```mermaid\\n  graph TD\\n  A[Internet Banking Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\\n  B -->|Active| C[pr1edge01.mno.gr]\\n  B -->|Active| D[pr1edge02.mno.gr]\\n  B -.->|Stopped| E[dr1edge01.mno.gr]\\n  B -.->|Stopped| F[dr1edge02.mno.gr]\\n  C -->|Kafka Producer| G[Primary Site's Kafka: prod-trlog-ibank-ingest-stream-mir]\\n  D -->|Kafka Producer| G\\n  E -.->|Stopped| H[Disaster Site's Kafka: prod-trlog-ibank-ingest-stream-mir]\\n  F -.->|Stopped| H\\n```\\n**User**: `PRODREST`\\n**Installation Path**: `/opt/wildfly/default/prodrestib`\\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`\\n**General Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/standalone.xml`\\n**Application Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml`\\n**Application Logs**: `/var/log/wildfly/prodrestib/server.log`\\n**Access Log**: `/var/log/wildfly/prodrestib/access.log`\\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\\n**Alerts**:\\n- [PR][IBANK] REST Endpoints Average Response Time\\n- [DR][IBANK] REST Endpoints Average Response Time\\n- [PR][IBANK] Ingestion Average Error rate\\n- [DR][IBANK] Ingestion Average Error rate\\n**Troubleshooting Steps**:\\n- Check application logs for error messages.\\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\\n- If the logs do not indicate a specific problem, check for any problems regarding Kafka performance.\\n### Kafka Topic Mirroring via MirrorMaker\\nDescribes the Kafka MirrorMaker setup that replicates `-mir` topics from the active site to final shared topics used by both clusters.\\nAfter being published to Kafka, the traffic is mirrored to the opposite site via Kafka MirrorMaker. In order to be able to have both Sites active at any given time and simplify the failover procedure the each Site produce messages to a topic with the suffix `-mir`. The topics with the `-mir` suffix contain events published to the Site. The `-mir` topics are then copied via Kafka MirrorMakers to the final topics (no `-mir` suffix), that has all the messages from both Sites.\\n#### PR replication\\n```mermaid\\n  graph TD\\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\\n  E[dr1edge02.mno.gr] -.->|Stopped| F\\n  C -->|Kafka MirrorMakers <br> pr1node01.mno.gr/pr1node04.mno.gr| G[PR Site Kafka: prod-trlog-ibank-ingest-stream]\\n  F -->|Kafka MirrorMakers <br> pr1node05.mno.gr/pr1node06.mno.gr| G[PR Site Kafka: prod-trlog-ibank-ingest-stream]\\n\\n```\\n#### DR replication\\n``` mermaid\\n  graph TD\\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-ibank-ingest-stream-mir]\\n  E[dr1edge02.mno.gr] -.->|Stopped| F\\n  F -->|Kafka MirrorMakers <br> dr1node01.mno.gr/dr1node04.mno.gr| H[DR Site Kafka: prod-trlog-ibank-ingest-stream]\\n  C -->|Kafka MirrorMakers <br> dr1node05.mno.gr/dr1node06.mno.gr| H[DR Site Kafka: prod-trlog-ibank-ingest-stream]\\n```\\n**MirrorMaker User**: `kafka`\\n**Configuration**: Cloudera Manager\\n**Logs**: Cloudera Manager\\n**Instructions for start/stop**: [Here](../procedures/manage_mirrormaker.md)\\n**Alerts**:\\n- Cloudera Manager alerts regarding Kafka\\n### Spark Streaming Pipelines\\nSpark topologies consume Kafka events and write them to Kudu and HBase, separating full stream and visible-only transactions.\\nAfter being sent to the final Kafka topic, the records are consumed by two separate Spark Streaming topologies\\n#### Prod_IBANK_IngestStream\\nThis topology takes all records from the final Kafka topics, extracts some values and writes them to a Kudu table. It also inserts some columns that are too big for Kudu to an HBase table. This topology runs independently **on both clusters**.\\n``` mermaid\\n  graph TD\\n  A[Kafka: prod-trlog-ibank-ingest-stream] --> B[Spark: Prod_IBANK_IngestStream]\\n  B --> C[Kudu: prod_trlog_ibank.service_audit_stream]\\n  B --> D[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM]\\n```\\n**User**: `PRODREST`\\n**Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**Submit Script**: `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- [PR][IBANK] Spark Waiting Batches\\n- [DR][IBANK] Spark Waiting Batches\\n**Troubleshooting Steps**:\\n- If the alert chart has **no line**, ensure that the application is not running (Cloudera Manager > YARN > Applications) and then submit the topology using the **Submit Script**. After resubmitting the application check the old application logs for the root cause that it failed\\n- If the alert has exceeded the threshold, investigate HBase/Kudu for delays and check the application logs for any errors. You can also try to kill (use Cloudera Manager or yarn command) and then re-submit the topology.\\n#### Prod_IBANK_IngestStream_Visible\\n**Visible transaction**: Visible are considered transactions from applications that have show_customer marked as true in the Impala table `prod_trlog_ibank.service_name`.\\nThis topology takes **visible** records from the final Kafka topics and writes them to HBase tables. This topology runs independently **on both clusters**.\\n``` mermaid\\n  graph TD\\n  A[Kafka: prod-trlog-ibank-ingest-stream] --> C[Spark: Prod_IBANK_IngestStream_Visible]\\n  B[Impala: prod_trlog_ibank.service_name] -->|Queried every 6 hours| C[Spark: Prod_IBANK_IngestStream_Visible]\\n  C --> D[HBase: PROD_IBANK:SERVICE_AUDIT]\\n  C --> E[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_APP]\\n  C --> F[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_ID]\\n  C --> G[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_SNTRNCAT]\\n  C --> H[HBase: PROD_IBANK:SERVICE_AUDIT_OBSCURE]\\n```\\n**User**: `PRODREST`\\n**Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**Submit Script**: `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_VISIBLE_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- [PR][IBANK Visible] Spark Waiting Batches\\n- [DR][IBANK Visible] Spark Waiting Batches\\n**Troubleshooting Steps**:\\n- If the alert chart has **no line**, ensure that the application is not running (Cloudera Manager > YARN > Applications) and then submit the topology using the **Submit Script**.\\n- If the alert has exceeded the threshold, investigate HBase for delays and check the application logs for any errors. You can also try to kill (use Cloudera Manager or yarn command) and then re-submit the topology.\\n## Batch\\nDescribes the daily and hourly batch flows that process and enrich ingested data, merge and de-duplicate them, and make them available to external systems.\\n### Main Batch Job Entry Point\\nMain batch entry point that coordinates migration, merge, enrichment, and final insertions to operational tables.\\nAs mentioned before, the information processed by the [Prod_IBANK_IngestStream](#prod_ibank_ingeststream) topology is stored in Kudu/HBase tables. At the end of each day (at **1:00 am in PR site** and **2:00 am in DR site** by **Cron**) the records from HBase/Kudu is enriched with additional information and the transfered to a parquet table. After the data are inserted to the parquet table, sequent jobs are triggered to produce aggregations and data for external systems. This procedure runs **independantly on both sites**.\\n**User**: `PRODREST`\\n**Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log`\\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- _See below_\\n**Troubleshooting Steps**:\\nThis procedure runs multiple steps which we will describe below with seperate troubleshooting steps for each step. Use the following steps for all alarms as a general guide:\\n- Identify the failed step using the alarm name\\n- Identify the root cause for the failed job based on the logs of the submit script/cluster application/query\\n- If the problem is with an external system, ask the customer to inform the owners of the external system\\n- if the problem is temporary or if you cannot determine the root cause of the problem, try to re-run the failed application\\n### Sub-steps\\nThe following steps run **on both clusters independently**, unless specified otherwise.\\n#### MSSQL Sqoop Import (Migration)\\nThis step transfers transactions from the legacy MSSQL server, which is managed by mno, to the cluster as part of the daily data migration from the legacy system.\\n``` mermaid\\n  graph TD\\n  A[MSSQL] -->|Sqoop Import| B[Impala: prod_trlog_ibank.historical_service_audit_raw_v2]\\n  B -->|Impala Insert| C[Impala: prod_trlog_ibank.historical_service_audit_v1]\\n```\\n**User**: `PRODREST`\\n**Script Logs**: ```/opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log```\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_migration/sqoop_import_ibank_hist_recursive_prod_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- IBank_Migration Historical JOB\\n- IBank_Migration Historical Sqoop_Import\\n- IBank_Migration Historical Impala_Insert\\n**Troubleshooting Steps**:\\n- Use the script/sqoop logs to identify the cause of the failure\\n- If the alert is Sqoop_Import, you can safely execute the script again\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODREST/historical/ibank_migration/sqoop_import_ibank_hist_recursive_prod_STABLE.sh /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2 FandFUser prod_trlog_ibank.historical_service_audit_raw_v2 prod_trlog_ibank.historical_service_audit_v1 `date +%Y-%m-%d` 1 1 4 jceks://hdfs/user/PRODREST/migration/credentials/mssql_ibank.jceks mssqlprod.password >>  /opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log 2>&1\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 10-11-2019\\n    nohup /opt/ingestion/PRODREST/historical/ibank_migration/sqoop_import_ibank_hist_recursive_prod_STABLE.sh /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2 FandFUser prod_trlog_ibank.historical_service_audit_raw_v2 prod_trlog_ibank.historical_service_audit_v1 2019-11-10 1 1 4 jceks://hdfs/user/PRODREST/migration/credentials/mssql_ibank.jceks mssqlprod.password &>>  /opt/icomdev/ibank_hist/prod_migrate/sqoop_20191110_1_1_20191112_0900.log &\\n    ```\\n- If the alert is Impala_Insert, check the that the records from the MSSQL server and the `prod_trlog_ibank.historical_service_audit_raw_v2` have the same count\\n  - MSSQL server\\n    ``` bash\\n    # Replace the sample date 16/11/2019-17/11/2019\\n\\tsource /opt/ingestion/PRODREST/historical/ibank_migration/config\\n    sqoop-eval  -Dhadoop.security.credential.provider.path=jceks://hdfs/user/PRODREST/migration/credentials/mssql_ibank.jceks --connect 'jdbc:jtds:sqlserver://999.999.999.999:2544;useNTLMv2=true;databaseName=InternetBankingAudit' --username FandFUser --password-alias mssqlprod.password --query \\\"select count() from ServiceAudit WITH (nolock, INDEX(IX_ServiceAuditNew_Timestamp)) where Timestamp >= '2019-11-16 00:00:00' and Timestamp < '2019-11-17 00:00:00'\\\"\\n    ```\\n  - prod_trlog_ibank.historical_service_audit_raw_v2\\n    ``` bash\\n    impala-shell -k --ssl -i ${HOSTNAME/01/} -q \\\"select  count(*) from prod_trlog_ibank.historical_service_audit_raw_v2;\\\"\\n    ```\\n- In case the counts are different between the two try again with Hive:\\n  - prod_trlog_ibank.historical_service_audit_raw_v2\\n    ``` bash\\n    # For Primary Site\\n    beeline -u \\\"jdbc:hive://pr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAP.mno.GR;ssl=true\\\" -e \\\"select  count(*) from prod_trlog_ibank.historical_service_audit_raw_v2;\\\"\\n    # For Disaster Site\\n    beeline -u \\\"jdbc:hive://dr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAD.mno.GR;ssl=true\\\" -e \\\"select  count(*) from prod_trlog_ibank.historical_service_audit_raw_v2;\\\"\\n    ```\\n- If the counts are the same with Hive:\\n  ``` bash\\n  # For Primary Site\\n  beeline -u \\\"jdbc:hive://pr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAP.mno.GR;ssl=true\\\"\\n  # For Disaster Site\\n  beeline -u \\\"jdbc:hive://dr1edge.mno.gr:10000/default;principal=hive/_HOST@BDAD.mno.GR;ssl=true\\\"\\n  ```\\n  And run the insert:\\n  ``` SQL\\n  INSERT INTO prod_trlog_ibank.historical_service_audit_v1 partition (par_dt) SELECT id, to_timestamp(trtimestamp, 'yyyy-MM-dd HH:mm:ss.SSSSSSS'), trim(host), trim(servicename), trim(type), trim(application), cast(substr(executiontime,1,2)as BIGINT)36001000000000 + castsubstr(executiontime,4,2)as BIGINT)601000000000+ cast(substr(executiontime,7,2)as BIGINT)*1000000000 + cast(substr(executiontime,10,7)as BIGINT)*100, clientsession, trim(clientusername), cast(clientcustomer AS bigint), clientipadress, clientdetails, trim(clientrequestpath), cast(clientlongitude AS FLOAT), cast(clientlatitude AS FLOAT), cast(substr(clientexecutiontime,1,2)as BIGINT)36001000000000 + cast(substr(clientexecutiontime,4,2)as BIGINT)601000000000+ cast(substr(clientexecutiontime,7,2)as BIGINT)*1000000000 + cast(substr(clientexecutiontime,10,7)as BIGINT)*100, webhost, responsetextdata, requesttextdata, errordata, replace(to_date(trtimestamp),'-','') AS par_dt FROM prod_trlog_ibank.historical_service_audit_raw_v2;\\n  ```\\n  And then refresh the table\\n  ``` bash\\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \\\"SET DDL_SYNC=true; refresh prod_trlog_ibank.historical_service_audit_v1;\\\"\\n  ```\\n- Run the rest of the steps\\n#### Insert to Service Audit\\nThis step transfers transactions migrated from the legacy MSSQL server to the final table.\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.historical_service_audit_v1] -->|Impala Insert| B[Impala: prod_trlog_ibank.service_audit]\\n  ```\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_migrate_hist_to_service_audit.log`\\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_migrate_hist_to_service_audit_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- IBank_Migration Historical to SA JOB\\n- IBank_Migration Historical to SA Impala_Insert\\n**Troubleshooting Steps**:\\n- Use the script logs to identify the cause of the failure\\n- The script cleans up after failure, so if the problem was temporary run the script again\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODREST/historical/ibank_migrate_hist_to_service_audit_STABLE.sh prod_trlog_ibank.historical_service_audit_v1 prod_trlog_ibank.service_audit `date +%Y%m%d` 1 1 >> /var/log/ingestion/PRODREST/ibank/log/ibank_migrate_hist_to_service_audit.log  2>&1\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODREST/historical/ibank_migrate_hist_to_service_audit_STABLE.sh prod_trlog_ibank.historical_service_audit_v1 prod_trlog_ibank.service_audit 20191110 1 1 >> /var/log/ingestion/PRODREST/ibank/log/ibank_migrate_hist_to_service_audit.log  2>&1 &\\n    ```\\n- Run the rest of the steps\\n#### Merge Batch\\nThis step transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and deletes the data from the original tables.\\n``` mermaid\\n  graph TD\\n  A[Kudu: prod_trlog_ibank.service_audit_stream] --> B[Spark: PROD_IBank_MergeBatch]\\n  C[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM] --> B\\n  B --> D[Impala: prod_trlog_ibank.service_audit_old]\\n  ```\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- IBank_Ingestion MergeBatch JOB\\n**Troubleshooting Steps**:\\n- Use the script/spark logs to identify the cause of the failure\\n- Ensure that no records are present in prod_trlog_ibank.service_audit_old\\n  ``` bash\\n  # eg. 09-11-2019\\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \\\"select  count(*) from prod_trlog_ibank.service_audit_old where par_dt='20191109';\\\"\\n  ```\\n- If no records exist and no other process is up, you can ran the script again.\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_IBank_MergeBatch LOCK_IBANK_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_IBank_MergeBatch  /opt/ingestion/PRODREST/ibank/lock/ FULL 1800 \\\"`date --date='-1 day' '+%Y-%m-%d 00:00:00'`\\\" \\\"`date '+%Y-%m-%d 00:00:00'`\\\"   >> /var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log 2>&1\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019\\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_IBank_MergeBatch LOCK_IBANK_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_IBank_MergeBatch  /opt/ingestion/PRODREST/ibank/lock/ FULL 1800 \\\"2019-11-09 00:00:00\\\" \\\"2019-11-10 00:00:00\\\"   >> /var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log 2>&1 &\\n    ```\\n- The process runs for well over an hour under normal circumstances or even longer for heavy load. Use of `screen` command advised\\n- If the problem is with resources (out-of-memory errors):\\n  - You can adjust the values at `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh`. At the moment these values are as high as they can go.\\n  - You can run the MergeBatch for parts of the day\\n    ``` bash\\n    # eg. 09-11-2019\\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2019-11-09 00:00:00\\\" \\\"2019-11-09 12:00:00\\\"\\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2019-11-09 12:00:00\\\" \\\"2019-11-09 18:00:00\\\"\\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2019-11-09 18:00:00\\\" \\\"2019-11-10 00:00:00\\\"\\n    ```\\n- Run the rest of the steps\\n#### Distinct join to Service Audit\\nSome records that are ingested by the [Stream](#stream) can also be present in the MSSQL server. In this step we insert to the final table the transactions that are unique to the [Stream](#stream), excluding the ones that are already present in the final table due to the data migration by MSSQL.\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit_old] -->|Impala Insert| B[Impala: prod_trlog_ibank.service_audit]\\n```\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`\\n**Script**: `/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- IBank_Migration Enrich SA from SA_old JOB\\n**Troubleshooting Steps**:\\n- Use the script logs to identify the cause of the failure\\n- Ensure that only records coming from prod_trlog_ibank.historical_service_audit_v1 are present in prod_trlog_ibank.service_audit. These records come from Insert to Service Audit [sub-step](#insert-to-service-audit) and their number should match.\\n  ``` bash\\n  # eg. 09-11-2019\\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \\\"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\\\"\\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \\\"select  count(*) from prod_trlog_ibank.historical_service_audit_v1 where par_dt like '20191109';\\\"\\n  ```\\n- If these records match and no other process is up, you can run the script again.\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # e.g. 09-11-2019\\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\\n    ```\\n- Run the rest of the steps\\n#### Report stats to Graphite\\nReports statistics about the ingestion process.\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/cron_report_stats.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- This process is not monitored\\n- You can safely skip this step\\n- Sample execution:\\n  ``` bash\\n  nohup  /opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh prod_trlog_ibank.service_audit ibank >> /var/log/ingestion/PRODREST/ibank/log/cron_report_stats.log\\n  ```\\n- Run the rest of the steps\\n#### Trigger external flows\\nNdef: **ONLY DR SITE**\\nCreates a trigger file for external flows. Related to [Datawerehouse](./ibank_dwh.md)\\n**User**: `PRODREST`\\n**Script Logs**: `-`\\n**Script**: `-`\\n**Alerts**:\\n- IBank_Migration Create UC4 file Create UC4 file\\n**Troubleshooting Steps**:\\n- Ensure that you are running steps for the DR site\\n- If you are running the steps for the Primary skip this step\\n- Execution:\\n  ``` bash\\n  touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY\\n  touch /opt/applications/landing_zone/PRODUSER/triggers/IBANK_SA_`date '+%Y%m%d'`.READY\\n  ```\\n- Run the rest of the steps\\n#### Drop Hourly Batch Partitions (DR Site)\\nNdef: **ONLY DR SITE**\\nDrop partitions from the [Hourly Merge Batch](#hourly-merge-batch) tables\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/drop_hourly_partitions.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh`\\n**Alerts**:\\n- IBank_Migration Drop hourly partitions JOB\\n**Troubleshooting Steps**:\\n- Ensure that you are running steps for the DR site\\n- If you are running the steps for the Primary skip this step\\n- Use the script logs to identify the cause of the failure\\n- For the previous day:\\n  ``` bash\\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \\\"prod_trlog_ibank.service_audit_hourly\\\" \\\"`date -d '-1 day' '+%Y%m%d'`\\\" >> /var/log/ingestion/PRODREST/ibank/log/drop_hourly_partitions.log 2>&1 &\\n  ```\\n- For a specified date:\\n  ``` bash\\n  # e.g. 09-11-2019\\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \\\"prod_trlog_ibank.service_audit_hourly\\\" \\\"20191109\\\" >> /var/log/ingestion/PRODREST/ibank/log/drop_hourly_partitions.log 2>&1 &\\n  ```\\n- Run the rest of the steps\\n#### Execute aggregations\\nNdef: **This flow is supspended. DO NOT EXECUTE**. Information listed here are for completeness.\\nThis flow computes aggregations for use with the [Queries](#queries).\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/update_ibank_aggr_tables_STABLE.sh`\\n**Alerts**:\\n- IBank_Migration Aggregations JOB\\n- IBank_Migration Aggregations HBase\\n- IBank_Migration Aggregations Kudu\\n**Troubleshooting Steps**:\\n- **DO NOT RUN THIS STEP**\\n- For the previous day:\\n  ``` bash\\n  /opt/ingestion/PRODREST/common/scripts/update_ibank_aggr_tables_STABLE.sh prod_trlog_ibank.service_audit prod_trlog_ibank.aggr_service_audit_clun_app prod_trlog_ibank.aggr_service_audit_clun >> /var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log 2>&1\\n  ```\\n- For a specified date:\\n  ``` bash\\n  # e.g. 09-11-2019\\n  /opt/ingestion/PRODREST/common/scripts/update_ibank_aggr_tables_STABLE.sh prod_trlog_ibank.service_audit prod_trlog_ibank.aggr_service_audit_clun_app prod_trlog_ibank.aggr_service_audit_clun 20191109 >> /var/log/ingestion/PRODREST/ibank/log/update_ibank_aggr_tables.log 2>&1\\n  ```\\n- Run the rest of the steps\\n#### Upsert to HBase (Migration)\\nThis step enriches the HBase visible tables with the transactions that are unique to MSSQL server.\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> C[Impala Insert/Spark]\\n  B[Impala: prod_trlog_ibank.service_audit_old] --> C\\n  C --> D[HBase: PROD_IBANK:SERVICE_AUDIT]\\n  C --> E[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_APP]\\n  C --> F[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_ID]\\n  C --> G[HBase: PROD_IBANK:SERVICE_AUDIT_IDX_CLUN_SNTRNCAT]\\n  C --> H[HBase: PROD_IBANK:SERVICE_AUDIT_OBSCURE]\\n```\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\\n**Alerts**:\\n- IBank_Migration Enrich hbase tables JOB\\n- IBank_Migration Enrich hbase tables Impala_insert\\n- IBank_Migration Enrich hbase tables Spark\\n**Troubleshooting Steps**:\\n- Use the script logs to identify the cause of the failure\\n  Ndef: If job failed and the following error appears :`ERROR: RetriesExhaustedWithDetailsException: Failed <num> actions: CallTimeoutException: <num> times, servers with issues: [dr/pr]1node02.mno.gr`,  execute script again. The error has to do with HBase merging/spliting on a region server, but a detailed reason is unknown.\\n- The script uses upsert and can be safely run many times.\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # e.g. 09-11-2019\\n    /opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20191109  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\\n    ```\\n- Run the rest of the steps\\n#### Send reports to business users\\nSends email reports to users for duplicate transaction IDS and leftover transactions in `prod_trlog_ibank.service_audit_stream`.\\n**User**: `PRODREST`\\n**Script Logs**: `-`\\n**Script**: `-`\\n**Alerts**:\\n- IBank_Migration GUID_Report JOB\\n- IBank_Migration GUID_Report Impala\\n**Troubleshooting Steps**:\\n- Check `/var/log/ingestion/PRODREST/ibank/log/ibank_report_duplicate_identical.log` for errors\\n- You can safely skip this step if not running for the previous day\\n- Sample execution:\\n   ``` bash\\n  /opt/ingestion/PRODREST/common/scripts/ibank_report_duplicate_identical_STABLE.sh  prod_trlog_ibank service_audit_old service_audit_duplicates >> /var/log/ingestion/PRODREST/ibank/log/ibank_report_duplicate_identical.log 2>&1 &\\n   ```\\n#### Duplicates between Impala and Kudu/HBase\\nDelete data which exist both Impala and Kudu. Sometimes MergeBatch fails because YARN kills executors for exceeding memory limits. AS a result, transactions are inserted to impala and job fails before drop them from kudu and HBase\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- Check `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log` for errors\\n- You can safely skip this step if not running for the previous day\\n- Sample execution:\\n  ``` bash\\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_ibank.service_audit_stream prod_trlog_ibank.service_audit_old ibank >> /var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log 2>&1\\n  ```\\n- Run the rest of the steps\\n#### Update monitoring postgres database\\nNdef: **IF AND ONLY IF**  all steps performed succesfully and grafana monitoring isn't updated, proceed with the following:\\nUpdated the monitoring postgres database to appeared green/success in Grafana.\\n- For a specified date:\\n```bash\\n# e.g 2023-03-30\\nssh Exxxx@pr1edge01.mno.gr\\nsudo -i -u postgres\\npsql -d monitoring\\nselect * from prod.monitoring where par_dt = 20230330;\\nINSERT INTO prod.monitoring (application, job_name,component,status,par_dt,start_time,end_time,description,params,host) VALUES ('IBank_Migration','Enrich SA from SA_old','JOB',0,20230330,'2023-03-31 03:18:30.000','2023-03-31 05:00:42.000','','','pr1edge01.mno.gr') ON CONFLICT (application, job_name,component,par_dt) DO UPDATE SET status=0, start_time='2023-03-31 03:18:30.000', end_time='2023-03-31 05:00:42.000',description='';\\n```\\n- Check from Grafana that the failed job is now succeded\\n### Hourly Merge Batch\\nEvery hour a  transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and without deleting the data from the original tables. This table is used by mno to query the latest transactions.\\n``` mermaid\\n  graph TD\\n  A[Kudu: prod_trlog_ibank.service_audit_stream] --> B[Spark: PROD_IBank_MergeBatch_Hourly]\\n  C[HBase: PROD_IBANK:SERVICE_AUDIT_STREAM] --> B\\n  B --> D[Impala: prod_trlog_ibank.service_audit_hourly]\\n  ```\\n**User**: `PRODREST`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- Use the spark logs to identify the cause of the failure\\n- The data in the Hourly tables is only for the current day. No need to fill missing hours, as the next MergeBatch will insert the data to the final table.\\n## Queries\\nExplains how Internet Banking queries are handled using REST endpoints and backend access to Impala and HBase.\\nThe ingested data are queried in order to be displayed by the Internet Banking application (under the Calendar/\\u0397\\u03bc\\u03b5\\u03c1\\u03bf\\u03bb\\u03cc\\u03b3\\u03b9\\u03bf application). The application displays to the user only **Visible** transactions. The queries are submitted to the cluster as HTTP requests using the same Wildfly instances as the [Stream](#stream) flow.\\n```mermaid\\n  graph TD\\n  A[Internet Banking Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\\n  B -->|Active| C[pr1edge01.mno.gr]\\n  B -->|Active| D[pr1edge02.mno.gr]\\n  B -.->|Stopped| E[dr1edge01.mno.gr]\\n  B -.->|Stopped| F[dr1edge02.mno.gr]\\n  C -->|Impala JDBC / HBase client| G[Primary Site]\\n  D -->|Impala JDBC / HBase client| G\\n  E -.->|Stopped| H[Disaster Site]\\n  F -.->|Stopped| H\\n```\\n**User**: `PRODREST`\\n**Installation Path**: `/opt/wildfly/default/prodrestib`\\n**Deployments Path**: `/opt/wildfly/default/prodrestib/standalone/deployments`\\n**General Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/standalone.xml`\\n**Application Configuration Path**: `/opt/wildfly/default/prodrestib/standalone/configuration/BigStreamer/beanconfig/translogApiIBankJmxConfig.xml`\\n**Application Logs**: `/var/log/wildfly/prodrestib/server.log`\\n**Access Log**: `/var/log/wildfly/prodrestib/access.log`\\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\\n**Alerts**:\\n- [PR][IBANK] Query Average Response Time\\n- [DR][IBANK] Query Average Response Time\\n- [PR][IBANK] Query Average Error rate\\n- [DR][IBANK] Query Average Error rate\\n**Troubleshooting Steps**:\\n- Check in Grafana dashboard `MONITORING/Monitoring Alerts` the charts that are in alerting state\\n- If the response time is for _Old implementation_ (see below) queries check if the response time is constant, a single query can increase the mean value of the response time. Check the access log for entries with large response time for the specific endpoint. If it is an isolated query the customer is not impacted.\\n- Check application logs for error messages.\\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\\n- If the logs do not indicate a specific problem, check for any problems regarding Impala/Kudu/HBase.\\n### Old implementation\\nThere are two versions for each query. The old implementention queries Impala tables stored in both HDFS and Kudu/HBase. This implementation had performance problems for many concurrent users.\\n**Endpoints**:\\n- auditCount **NOT USED BY mno**\\n- auditSearch **NOT USED BY mno**\\n- selectById **USED BY mno**\\n### New implementation\\nThe new implementation uses a subset of the data (only visible transactions) stored in HBase. Queries required to access **non-Visible** transactions have to rely on the old implementation.\\n**Endpoints**:\\n- auditCountVisible **NOT USED BY mno**\\n- auditSearchVisible **USED BY mno**\\n- selectByIdVisible **NOT USED BY mno**\\n## Retention Mechanism\\nDescribes automated retention flows that drop old partitions from Impala, HDFS, and HBase to manage storage over time.\\n### Impala Retention\\nEvery day (at **3:00 pm in both sites** by **Cron**) This script drops partitions from impala tables `prod_trlog_ibank.service_audit_old` and `prod_trlog_ibank.historical_service_audit_v1` older than 10 days. It also removes a HDFS directory under `/mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2_vault/` that correspond to 30 days before.\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/common/log/retention_mechanism_daily.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/retention_mechanism_daily_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr`\\n**Alerts**:\\n- Retention prod_trlog_ibank.service_audit_old JOB\\n- Retention prod_trlog_ibank.historical_service_audit_v1 JOB\\n- Retention /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2_vault/`date -d \\\"-30 day\\\" +%Y-%m-%d`_`date -d \\\"-29 day\\\" +%Y-%m-%d` JOB\\n**Troubleshooting Steps**:\\n- Use the script logs to identify the cause of the failure\\n- After the root cause for the failure is resolved, run manually the following commands\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019\\n    impala-shell -k -i ${HOSTNAME/01/} --ssl --query \\\"set SYNC_DDL=true;alter table prod_trlog_ibank.service_audit_old drop partition ( par_dt <= 20191109 ) purge;\\\"\\n    impala-shell -k -i ${HOSTNAME/01/} --ssl --query \\\"set SYNC_DDL=true;alter table prod_trlog_ibank.historical_service_audit_v1 drop partition ( par_dt <= '20191109' ) purge;\\\"\\n    hdfs dfs -rm -R -skipTrash /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2_vault/2019-11-09_2019-11-10\\n    ```\\n#### Additional Tables\\nKudu table's `prod_trlog_ibank.service_audit_stream` partitions are maintained by an Oozie jobs every day. See section [Create next week kudu partitions](#create-next-week-kudu-partitions).\\n\\nParquet table's `prod_trlog_ibank.service_audit_hourly` partitions are dropped after each successful Merge run. See section [Drop hourly partitions](#drop-hourly-partitions).\\n#### DEV\\nEvery day (at **16:05 on DR site only** by **Cron**). This script drops partitions from impala table `dev_trlog_ibank.service_audit` older than 60 days.\\n**User**: `DEVREST`\\n**Script Logs**: `/var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log`\\n**Script**: `/opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh`\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- Use the script logs to identify the cause of the failure\\n- After the root cause for the failure is resolved, run manually the following command\\n  ``` bash  \\n  /opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh >> /var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log 2>&1\\n  ```\\n### HBase retention\\nEvery day (at **16:00 in both sites** by **Cron**) This script deletes rows from hbase `PROD_IBANK:SERVICE_AUDIT_STREAM` which refere to transactions inserted 7 days ago. Those rows may have been not deleted properly after merge batch job. To do so the topology queries rows from impala table `prod_trlog_ibank.service_audit` inside partition with par_dt refering 7 days ago.\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/daily_tools_cleanupHBaseSAS.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/daily_tools_cleanupHBaseSAS.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr`\\n**Alerts**:\\n- Retention IbankCleanupHbaseSAS JOB\\n**Troubleshooting Steps**:\\n- Use the script logs to identify the cause of the failure\\n- After the root cause for the failure is resolved, check on `/opt/ingestion/PRODREST/no_bkp/duplicate_cid_hbase` if a manual re-run must be done for a specific date\\n  - For a specified date eg 2022-06-30:\\n    ``` bash\\n    /opt/ingestion/PRODREST/common/spark/submit/submitmnoSparkTopology_tools_cleanupHbaseSAS prod_trlog_ibank.service_audit PROD_IBANK:SERVICE_AUDIT_STREAM LIST 20220630\\n    ```\\n##  Oozie Jobs\\nOozie workflows that import lookup tables and manage weekly Kudu partitioning.\\n###  Lookup tables\\nEvery day (at 07:15 by Oozie on DR & PR site ), we transfers 3 tables with reference data from the legacy MSSQL server, which is managed by mno, to the cluster. We keep only latest version to BigData (no partition).\\n**User**: `PRODREST`\\n**Coordinator**: `Coord_IbankLookupTables_PROD`\\n**Workflow**: `ImportLookupTables`\\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/lookupTables/import_lookup_tables.sh`\\n**Logs**: from HUE\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- Open Hue to find coordinator status and logs\\n###  Create next week kudu partitions\\nEvery day (at 10:00 by Oozie on DR site and 10:40 by Oozie on PR site ), we delete old Kudu partitions and add new Kudu partitions on `prod_trlog_ibank.service_audit_stream`.\\n**User**: `PRODREST`\\n**Coordinator**: `Coord_IBankCreateKuduPartitionsPROD`\\n**Workflow**: `CreateKuduPartitionsPROD`\\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\\n**Logs**: from HUE\\n**Alerts**:\\n- **Not Monitored**\\n**Troubleshooting Steps**:\\n- Open Hue to find coordinator status and logs\\nThere is a case that merge batch has not properly run for a specific date, **even though** the transactions for that specific date are present on impala `service_audit`. For example by moving a service_audit partiton from DR to PR or vice versa, or when merge batch failed **after** deleting transactions from kudu.\\nIn that case we can manually mark merge batch as complete for that date, and therefore next oozie job created by `Coord_IBankCreateKuduPartitionsPROD` will delete that partition from kudu, by manipulating HBase table `PROD_IBANK:MERGE_BATCH_STATE_INFO`.\\nExample for a specific date (10/10/2022):\\n - Run HBase shell\\n ```\\n   hbase shell\\n   ```\\n - And then inside HBase shell:\\n ```\\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase', 'true'\\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase_time','0'\\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu', 'true'\\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu_time', '0'\\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running', 'false'\\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running+time', '0'\\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala', 'true'\\n   put 'PROD_IBANK:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala_time', '0'\\n   ```\\n#### DEV\\nEvery day (at **11:10 by Oozie** on **DR site only** ), we delete old Kudu partitions and add new Kudu partitions on `dev_trlog_ibank.service_audit_stream`.\\n**User**: `DEVREST`\\n**Coordinator**: `Coord_IBankCreateKuduPartitionsDEV`\\n**Workflow**: `CreateKuduPartitionsDEV`\\n**Local path**: `/opt/ingestion/DEVREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\\n**Logs**: from HUE\\n**Alerts**:\\n- Not Monitored"
        },
        {
            "question": "What steps should be taken if the IBank transfer detail extract job fails unexpectedly?",
            "answer": "1. Check logs:\n```bash\ncat /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\n```\n2. Use the Spark UI on `dr1edge01.mno.gr` or `pr1edge01.mno.gr` to check YARN logs.\n3. If the issue was temporary and resolved, rerun manually:\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t transfer\n```\nOr for a specific date (e.g., Nov 9, 2019):\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t transfer -p 20191109\n```",
            "category": "Troubleshooting & Issue Resolution",
            "files": "datawarehouse-ibank.md",
            "context": "---\\ntitle: Datawarehouse IBank Extract and Export Processes\\ndescription: Comprehensive operational guide for extracting and exporting Internet Banking service data from `prod_trlog_ibank.service_audit` to detail tables and then to MSSQL via Sqoop. Includes scheduler setup (UC4), script paths, Spark and Sqoop jobs, troubleshooting, and historical retention notes.\\nauthor: produser / mno big data engineering\\nupdated: 2025-05-01\\ntags:\\n  - datawarehouse\\n  - ibank\\n  - internet banking\\n  - spark\\n  - sqoop\\n  - uc4\\n  - dwh\\n  - produser\\n  - impala\\n  - extract\\n  - export\\n  - transfer\\n  - payment\\n  - card\\n  - loan payment\\n  - cancel payment\\n  - time deposit\\n  - mass debit\\n  - man date\\n  - my bank\\n  - service audit\\n  - yarn\\n  - staging\\n  - reconciliation\\n  - retention\\n  - monitoring\\n  - logs\\n---\\n# Datawarehouse ibank\\n## Extract\\n**Extraction of detail tables**\\nOur spark application will extract information from prod_trlog_ibank.service_audit table to different detail tables based on different service names with the help of lookup table service_name where needed. The columns of the detail tables are produced either from a non json column of the input table (eg. client_username) or from parsing the json fields request_text_data and response_text_data and extracting specific fields based on their name. When two fields have the same name then we will keep the greater value either for a number field or for a string based on lexicographic order.\\nThese jobs are executed by the bank's scheduler. There is an agent installed on the BDA edge node which is called UC4. The UC4 agent executes the below script for every job:\\n**UC4 Agent Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh` on `dr1edge01.mno.gr` **OR** `pr1edge01.mno.gr` (according to which site is the active for the UC4 agent)\\n**User**: `PRODUSER`\\n**Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\nThe above script executes the below scripts for every job and the below scripts and each of these scripts executes this generic spark-submit script:\\n**Generic Spark Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submitter.sh`\\nThe jobs which perform the extraction of the details from service_audit are:\\n### Transfer Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_transfer_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_transfer]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_transfer.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT TRANSFER\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t transfer\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t transfer -p 20191109\\n    ```\\n### Payment Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_payment_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_payment]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_payment.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT PAYMENT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t payment\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t payment -p 20191109\\n    ```\\n### Loan Payment Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_loanPayment_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_loan_payment]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_loan_payment.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT LOAN_PAYMENT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t loanPayment\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t loanPayment -p 20191109\\n    ```\\n### Cancel Payment Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_cancelPayment_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_cancel_payment]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_cancel_payment.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT CANCEL_PAYMENT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t cancelPayment\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t cancelPayment -p 20191109\\n    ```\\n### Card Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_card_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_card]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_card.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT CARD\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t card\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t card -p 20191109\\n    ```\\n### Stock Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_stock_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_stock]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_stock.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT STOCK\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t stock\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t stock -p 20191109\\n    ```\\n### Time Deposit Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_timeDeposit_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_time_deposit]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_time_deposit.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT TIME_DEPOSIT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t timeDeposit\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t timeDeposit -p 20191109\\n    ```\\n### Mass Debit Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_massDebit_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_mass_debit]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_mass_debit.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT MASS_DEBIT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t massDebit\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t massDebit -p 20191109\\n    ```\\n### Man Date Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_manDate_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_man_date]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_man_date.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT MAN_DATE\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -p 20191109\\n    ```\\n### My Bank Extract\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] --> B[Spark: PROD_IBank_DWH_myBank_YYYYMMDD-YYYYMMDD]\\n  B --> D[Impala: prod_trlog_ibank_analytical.dwh_details_my_bank]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/submit_my_bank.sh`\\n**User**: `PRODUSER`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_extract.log`\\n**Alert**:\\n- DWH_IBank EXTRACT MY_BANK\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and spark logs to identify the cause of the failure \\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t myBank\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t myBank -p 20191109\\n    ```\\n---\\n## Export\\n**Export of details tables and part of service_audit columns to mno datawarehouse**\\nThe data are copied temporary to an internal staging table with an impala query and then we use sqoop-export to export the data to Datawarehouse.\\nThese jobs are executed by the bank's scheduler. There is an agent installed on the BDA edge node which is called UC4. The UC4 agent executes the below script for every job:\\n**UC4 Agent Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh` on `dr1edge01.mno.gr` **OR** `pr1edge01.mno.gr` (according to which site is the active for the UC4 agent)\\n**User**: `PRODUSER`\\n**Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\nThe above script executes the below scripts for every job and the below scripts and each of these scripts executes this generic sqoop script:\\n**Generic Sqoop Job Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_to_dwh.sh`\\nThe jobs which perform the export of the details to the MSSQL Server are:\\n### Transfer Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_transfer] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_transfer_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_TransferDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.TransferDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_transfer.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT TRANSFER\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t transfer\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t transfer -p 20191109\\n    ```\\n### Payment Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_payment] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_payment_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_PaymentDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.PaymentDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_payment.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT TRANSFER\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t payment\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t payment -p 20191109\\n    ```\\n### Loan Payment Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_loan_payment] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_loan_payment_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_LoanPaymentDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.LoanPaymentDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_loan_payment.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT LOAN_PAYMENT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment -p 20191109\\n    ```\\n### Cancel Payment Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_cancel_payment] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_cancel_payment_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_CancelPaymentDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.CancelPaymentDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_cancel_payment.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT CANCEL_PAYMENT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t cancelPayment\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t cancelPayment -p 20191109\\n    ```\\n### Card Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_card] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_card_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_CardDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.CardDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_card.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT CARD\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t card\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t card -p 20191109\\n    ```\\n### Stock Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_stock] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_stock_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_StockDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.StockDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_stock.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT STOCK\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t stock\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t stock -p 20191109\\n    ```\\n### Time Deposit Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_time_deposit] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_time_deposit_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_TimeDepositDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.TimeDepositDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_time_deposit.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT TIME_DEPOSIT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t timeDeposit\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t timeDeposit -p 20191109\\n    ```\\n### Mass Debit Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_mass_debit] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_mass_debit_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_MassDebitDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.MassDebitDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_mass_debit.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT MASS_DEBIT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t massDebit\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t massDebit -p 20191109\\n    ```\\n### Man Date Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_man_date] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_man_date_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_ManDateDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.TransferDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_man_date.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT MAN_DATE\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t manDate\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t manDate -p 20191109\\n    ```\\n### My Bank Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank_analytical.dwh_details_my_bank] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_my_bank_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_MyBankDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.MyBankDetails]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_my_bank.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT MY_BANK\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t myBank\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t myBank -p 20191109\\n    ```\\n### Service Audit Export\\n``` mermaid\\n  graph TD\\n  A[Impala: prod_trlog_ibank.service_audit] -->|Impala Insert| B[Impala: prod_trlog_ibank_analytical.dwh_details_service_audit_stg]\\n  B --> C[Sqoop: PROD_IBank_DWH_EXPORT_ServiceAuditDetails_YYYYMMDD-YYYYMMDD]\\n  C -->|Sqoop Export| D[MSSQL Server: InternetBankingDW.ServiceAudit]\\n  ```\\n**Submit Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_service_audit.sh`\\n**User**: `PRODUSER`\\n**Sqoop Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**UC4 Agent Script Logs**: `/var/log/datawarehouse-ibank/PRODUSER/sched_export_to_dwh.log`\\n**Alert**:\\n- DWH_IBank EXPORT SERVICE_AUDIT\\n**Troubleshooting Steps**:\\n- Use the UC4 agent script logs and sqoop logs to identify the cause of the failure \\n- The export job will not be executed if the previous day is not a business day\\n- The script cleans up after failure, so if the problem was temporary **communicate with mno UC4 administrators to rerun the job if you fixed the problem. If they ask us to rerun the job, then:**\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t serviceAudit\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019 (Ndef that the argument is one day after the desired date)\\n    /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t serviceAudit -p 20191109\\n    ```\\n## Retention Mechanism (Suspended)\\nNdef: **This mechanism is suspended. DO NOT Troubleshoot**. As of 22/09/2023 all tables listed bellow have been dropped by mno (**Mail Subject: Incident IM2220978 receipt confirmation. Related Interaction: SD2285018**).\\nInformation shown here is for completeness.\\n**Execution**: Every day (at **3:30 pm in DR site** by **Cron**) \\n**Description**: This script drops partitions from impala tables `prod_trlog_ibank_analytical.dwh_details*` older than 10 days.\\n**User**: `PRODUSER`\\n**Script Logs**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily.log`\\n**Script**: `/opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily_STABLE.sh` on `dr1edge01.mno.gr`\\n**Alerts**:\\n- Retention DWH_retention {$table}\\nWhere $table can be\\n- prod_trlog_ibank_analytical.dwh_details_cancel_payment\\n- prod_trlog_ibank_analytical.dwh_details_card\\n- prod_trlog_ibank_analytical.dwh_details_loan_payment\\n- prod_trlog_ibank_analytical.dwh_details_man_date\\n- prod_trlog_ibank_analytical.dwh_details_mass_debit\\n- prod_trlog_ibank_analytical.dwh_details_my_bank\\n- prod_trlog_ibank_analytical.dwh_details_payment\\n- prod_trlog_ibank_analytical.dwh_details_stock\\n- prod_trlog_ibank_analytical.dwh_details_time_deposit\\n- prod_trlog_ibank_analytical.dwh_details_cancel_transfer\\n**Troubleshooting Steps**:\\n- Use the script logs to identify the cause of the failure\\n- After the root cause for the failure is resolved, run manually the following command\\n  - To keep the only last 10 days:\\n    ``` bash\\n        /opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily_STABLE.sh >> /opt/ingestion/PRODUSER/datawarehouse-ibank/retention_mechanism_daily.log 2>&1\\n    ```"
        },
        {
            "question": "How do you verify that the Impala export of Online transaction data to analytical tables completed successfully?",
            "answer": "1. Check the export script logs:\n```bash\ncat /var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log\n```\n2. Use the script to manually export data:\n```bash\n/opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh prod_trlog_online.service_audit prod_trlog_online.aggr_service_audit prod_trlog_online.aggr_service_audit_idx 20191109\n```\n3. Validate row counts in Impala to ensure the export inserted the expected number of rows.",
            "category": "Data Management & Query Execution",
            "files": "online.md",
            "context": "---\\ntitle: \\\"Online\\\"\\ndescription: \\\"Real-time and batch ingestion pipeline for Online transactions. Covers Wildfly, Kafka, Spark Streaming, daily batch jobs, hourly merging, and query handling for downstream applications and reporting.\\\"\\ntags:\\n  - online\\n  - ingestion\\n  - kafka\\n  - spark\\n  - kudu\\n  - hbase\\n  - wildfly\\n  - streaming\\n  - batch\\n  - parquet\\n  - queries\\n  - impala\\n  - retention\\n  - oozie\\n  - prodrest\\n---\\n# Online\\n## Stream\\nProcesses Online transaction events in real-time from backend servers via Wildfly and Kafka, storing them through Spark into Kudu and HBase.\\n### Wildfly Transaction Ingestion\\nWildfly instances receive HTTP POST events from backend servers, loadbalanced through NetScaler, and forward them to Kafka topics at the active site.\\nUser generated events are sent to Wildfly instances installed at edge servers as HTTP POST requests. The requests originate from the backend servers of Internet Banking. Requests are loadbalanced by NetScaler (Loadbalancer) that is managed by mno's networking department. The events are then sent to a Kafka topic. Usually only one site is active (Primary Site).\\n```mermaid\\n  graph TD\\n  A[Online Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\\n  B -->|Active| C[pr1edge01.mno.gr]\\n  B -->|Active| D[pr1edge02.mno.gr]\\n  B -.->|Stopped| E[dr1edge01.mno.gr]\\n  B -.->|Stopped| F[dr1edge02.mno.gr]\\n  C -->|Kafka Producer| G[Primary Site's Kafka: prod-trlog-online-ingest-stream-mir]\\n  D -->|Kafka Producer| G\\n  E -.->|Stopped| H[Disaster Site's Kafka: prod-trlog-online-ingest-stream-mir]\\n  F -.->|Stopped| H\\n```\\n**User**: `PRODREST`\\n**Installation Path**: `/opt/wildfly/default/prodreston`\\n**Deployments Path**: `/opt/wildfly/default/prodreston/standalone/deployments`\\n**General Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/standalone.xml`\\n**Application Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml`\\n**Application Logs**: `/var/log/wildfly/prodreston/server.log`\\n**Access Log**: `/var/log/wildfly/prodreston/access.log`\\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\\n**Alerts**:\\n- [PR][ONLINE] REST Endpoints Average Response Time\\n- [DR][ONLINE] REST Endpoints Average Response Time\\n- [PR][ONLINE] Ingestion Average Error rate\\n- [DR][ONLINE] Ingestion Average Error rate\\n**Troubleshooting Steps**:\\n- Check application logs for error messages.\\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\\n- If the logs do not indicate a specific problem, check for any problems regarding Kafka performance.\\n### Kafka Mirroring via MirrorMaker\\nExplains how Kafka MirrorMaker replicates `-mir` topics between sites to ensure high availability and simplify failover.\\nAfter being published to Kafka, the traffic is mirrored to the opposite site via Kafka MirrorMaker. In order to be able to have both Sites active at any given time and simplify the failover procedure the each Site produce messages to a topic with the suffix `-mir`. The topics with the `-mir` suffix contain events published to the Site. The `-mir` topics are then copied via Kafka MirrorMakers to the final topics (no `-mir` suffix), that has all the messages from both Sites.\\n#### PR replication\\n```mermaid\\n  graph TD\\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-online-ingest-stream-mir]\\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-online-ingest-stream-mir]\\n  E[dr1edge02.mno.gr] -.->|Stopped| F\\n  C -->|Kafka MirrorMakers <br> pr1node01.mno.gr/pr1node04.mno.gr| G[PR Site Kafka: prod-trlog-online-ingest-stream]\\n  F -->|Kafka MirrorMakers <br> pr1node05.mno.gr/pr1node06.mno.gr| G[PR Site Kafka: prod-trlog-online-ingest-stream]\\n```\\n#### DR replication\\n``` mermaid\\n  graph TD\\n  A[pr1edge01.mno.gr] -->|Kafka Producer| C[PR Site Kafka: prod-trlog-online-ingest-stream-mir]\\n  B[pr1edge02.mno.gr] -->|Kafka Producer| C\\n  D[dr1edge01.mno.gr] -.->|Stopped| F[DR Site Kafka: prod-trlog-online-ingest-stream-mir]\\n  E[dr1edge02.mno.gr] -.->|Stopped| F\\n  F -->|Kafka MirrorMakers <br> dr1node01.mno.gr/dr1node04.mno.gr| H[DR Site Kafka: prod-trlog-online-ingest-stream]\\n  C -->|Kafka MirrorMakers <br> dr1node05.mno.gr/dr1node06.mno.gr| H[DR Site Kafka: prod-trlog-online-ingest-stream]\\n```\\n**MirrorMaker User**: `kafka`\\n**Configuration**: Cloudera Manager\\n**Logs**: Cloudera Manager\\n**Instructions for start/stop**: [Here](../procedures/manage_mirrormaker.md)\\n**Alerts**:\\n- Cloudera Manager alerts regarding Kafka\\n### Spark Streaming Topologies\\nDescribes the Spark Streaming topologies that consume Kafka data and write them to Kudu and HBase, including support for oversized columns.\\nAfter being sent to the final Kafka topic, the records are consumed by two separate Spark Streaming topologies\\n#### Prod_Online_IngestStream\\nThis topology takes all records from the final Kafka topics, extracts some values and writes them to a Kudu table. It also inserts some columns that are too big for Kudu to an HBase table. This topology runs independently **on both clusters**.\\n``` mermaid\\n  graph TD\\n  A[Kafka: prod-trlog-online-ingest-stream] --> B[Spark: Prod_Online_IngestStream]\\n  B --> C[Kudu: prod_trlog_online.service_audit_stream]\\n  B --> D[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM]\\n```\\n**User**: `PRODREST`\\n**Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**Submit Script**: `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- [PR][ONLINE] Spark Waiting Batches\\n- [DR][ONLINE] Spark Waiting Batches\\n**Troubleshooting Steps**:\\n- If the alert chart has **no line**, ensure that the application is not running (Cloudera Manager > YARN > Applications) and then submit the topology using the **Submit Script**. After resubmitting the application check the old application logs for the root cause that it failed\\n- If the alert has exceeded the threshold, investigate HBase/Kudu for delays and check the application logs for any errors. You can also try to kill (use Cloudera Manager or yarn command) and then re-submit the topology.\\n## Batch\\nOutlines the batch workflows that enrich and aggregate daily Online transactions, merge data across systems, and prepare output for consumption.\\n### Daily Batch Controller Script\\nMain daily batch coordinator that orchestrates MergeBatch, aggregation, retention cleanup, and reporting steps across both clusters.\\nAs mentioned before, the information processed by the [Prod_Online_IngestStream](#prod_online_ingeststream) topology is stored in Kudu/HBase tables. At the end of each day (at **4:15 am in PR & DR site** by **Cron**) the records from HBase/Kudu is enriched with additional information and the transfered to a parquet table. After the data are inserted to the parquet table, sequent jobs are triggered to produce aggregations and data for external systems. This procedure runs **independantly on both sites**.\\n**User**: `PRODREST`\\n**Logs**: `/var/log/ingestion/PRODREST/online/log/online_daily_batch_jobs.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/online_daily_batch_jobs_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- _See below_\\n**Troubleshooting Steps**:\\nThis procedure runs multiple steps which we will describe below with seperate troubleshooting steps for each step. Use the following steps for all alarms as a general guide:\\n- Identify the failed step using the alarm name\\n- Identify the root cause for the failed job based on the logs of the submit script/cluster application/query\\n- If the problem is with an external system, ask the customer to inform the owners of the external system\\n- if the problem is temporary or if you cannot determine the root cause of the problem, try to re-run the failed application\\n### Sub-steps\\nThe following steps run **on both clusters independently**, unless specified otherwise.\\n#### Merge Batch to Final Table\\nThis step transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and deletes the data from the original tables.\\n``` mermaid\\n  graph TD\\n  A[Kudu: prod_trlog_online.service_audit_stream] --> B[Spark: PROD_Online_MergeBatch]\\n  C[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM] --> B\\n  B --> D[Impala: prod_trlog_online.service_audit]\\n  ```\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/cronExecutor_OnlineBatch_full.log`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- Online_Ingestion MergeBatch JOB\\n**Troubleshooting Steps**:\\n- Use the script/spark logs to identify the cause of the failure\\n- Ensure that no records are present in prod_trlog_online.service_audit\\n  ``` bash\\n  # eg. 09-11-2019\\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \\\"select  count(*) from prod_trlog_online.service_audit where par_dt='20191109';\\\"\\n  ```\\n- If no records exist and no other process is up, you can ran the script again.\\n  - For the previous day:\\n    ``` bash\\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_Online_MergeBatch LOCK_ONLINE_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_Online_MergeBatch  /opt/ingestion/PRODREST/online/lock/ FULL 1800 \\\"`date --date='-1 day' '+%Y-%m-%d 00:00:00'`\\\" \\\"`date '+%Y-%m-%d 00:00:00'`\\\"  >> /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log 2>&1 &\\n    ```\\n  - For a specified date:\\n    ``` bash\\n    # eg. 09-11-2019\\n    /opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_Online_MergeBatch LOCK_ONLINE_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_Online_MergeBatch  /opt/ingestion/PRODREST/online/lock/ FULL 1800 \\\"2019-11-09 00:00:00\\\" \\\"2019-11-10 00:00:00\\\"   >> /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log 2>&1 &\\n    ```\\n- The process runs for well 30 minutes under normal circumstances or even longer for heavy load. Use of `screen` command advised\\n- If the problem is with resources (out-of-memory errors):\\n  - You can adjust the values at `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh`. At the moment these values are as high as they can go.\\n  - You can run the MergeBatch for parts of the day\\n    ``` bash\\n    # eg. 09-11-2019\\n    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2019-11-09 00:00:00\\\" \\\"2019-11-09 12:00:00\\\"\\n    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2019-11-09 12:00:00\\\" \\\"2019-11-09 18:00:00\\\"\\n    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \\\"2019-11-09 18:00:00\\\" \\\"2019-11-10 00:00:00\\\"\\n    ```\\n- Run the rest of the steps\\n#### Report stats to Graphite\\nReports statistics about the ingestion process.\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/cron_report_stats.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- This process is not monitored\\n- You can safely skip this step\\n- Sample execution:\\n  ``` bash\\n  nohup  /opt/ingestion/PRODREST/common/scripts/report_stats_impala_STABLE.sh prod_trlog_online.service_audit online >> /var/log/ingestion/PRODREST/online/log/cron_report_stats.log\\n  ```\\n- Run the rest of the steps\\n#### Drop Hourly MergeBatch Partitions (DR)\\nNdef: **ONLY DR SITE**\\nDrop partitions from the [Hourly Merge Batch](#hourly-merge-batch) tables\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/drop_hourly_partitions.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh`\\n**Alerts**:\\n- Online_Migration Drop hourly partitions JOB\\n**Troubleshooting Steps**:\\n- Ensure that you are running steps for the DR site\\n- If you are running the steps for the Primary skip this step\\n- Use the script logs to identify the cause of the failure\\n- For the previous day:\\n  ``` bash\\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \\\"prod_trlog_online.service_audit_hourly\\\" \\\"`date -d '-1 day' '+%Y%m%d'`\\\" >> /var/log/ingestion/PRODREST/online/log/drop_hourly_partitions.log 2>&1 &\\n  ```\\n- For a specified date:\\n  ``` bash\\n  # e.g. 09-11-2019\\n  nohup /opt/ingestion/PRODREST/common/scripts/drop_hourly_partitions_STABLE.sh \\\"prod_trlog_online.service_audit_hourly\\\" \\\"20191109\\\" >> /var/log/ingestion/PRODREST/online/log/drop_hourly_partitions.log 2>&1 &\\n  ```\\n- Run the rest of the steps\\n#### Aggregation to Analytical Tables\\nThis flow computes aggregations for use with the [Queries](#queries).\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh`\\n**Alerts**:\\n- Online_Migration Aggregations JOB\\n- Online_Migration Aggregation_SA Impala_Insert\\n- Online_Migration Aggregation_SA_Index Kudu_Insert\\n**Troubleshooting Steps**:\\n- For the previous day:\\n  ``` bash\\n  /opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh prod_trlog_online.service_audit prod_trlog_online.aggr_service_audit prod_trlog_online.aggr_service_audit_idx  >> /var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log 2>&1 &\\n  ```\\n- For a specified date:\\n  ``` bash\\n  # e.g. 09-11-2019\\n  /opt/ingestion/PRODREST/common/scripts/update_online_aggr_tables_STABLE.sh prod_trlog_online.service_audit prod_trlog_online.aggr_service_audit prod_trlog_online.aggr_service_audit_idx 20191109 >> /var/log/ingestion/PRODREST/online/log/update_online_aggr_tables.log 2>&1\\n  ```\\n- Run the rest of the steps\\n#### Send Daily Duplicate Reports\\nSends email reports to users for duplicate transaction IDS and leftover transactions in `prod_trlog_online.service_audit_stream`.\\n**User**: `PRODREST`\\n**Script Logs**: `-`\\n**Script**: `-`\\n**Alerts**:\\n- Online_Ingestion GUID_Report Impala\\n- Online_Ingestion GUID_Report JOB\\n**Troubleshooting Steps**:\\n- Check `/var/log/ingestion/PRODREST/online/log/online_report_duplicate_identical.log` for errors\\n- You can safely skip this step if not running for the previous day\\n- Sample execution:\\n  ``` bash\\n  /opt/ingestion/PRODREST/common/scripts/online_report_duplicate_identical_STABLE.sh  `date -d '-1 day' '+%Y%m%d'`  prod_trlog_online service_audit service_audit_duplicates >> /var/log/ingestion/PRODREST/online/log/online_report_duplicate_identical.log 2>&1 &\\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_online.service_audit_stream prod_trlog_online.service_audit_old online >> /var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log 2>&1 &\\n  ```\\n#### Duplicates between Impala and Kudu/HBase\\nDelete data which exist both Impala and Kudu. Sometimes MergeBatch fails because YARN kills executors for exceeding memory limits. AS a result, transactions are inserted to impala and job fails before drop them from kudu and HBase\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- Check `/var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log` for errors\\n- You can safely skip this step if not running for the previous day\\n- Sample execution:\\n  ``` bash\\n  /opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_online.service_audit_stream prod_trlog_online.service_audit_old online >> /var/log/ingestion/PRODREST/online/log/report_duplicates_kudu_hbase_impala.log 2>&1\\n  ```\\n### Hourly Merge to Intermediate Table\\nEvery hour a  transfers transactions ingested by the [Stream](#stream) flow to an intermediate table and without deleting the data from the original tables. This table is used by mno to query the latest transactions.\\n``` mermaid\\n  graph TD\\n  A[Kudu: prod_trlog_online.service_audit_stream] --> B[Spark: PROD_Online_MergeBatch_Hourly]\\n  C[HBase: PROD_ONLINE:SERVICE_AUDIT_STREAM] --> B\\n  B --> D[Impala: prod_trlog_online.service_audit_hourly]\\n  ```\\n**User**: `PRODREST`\\n**Spark Logs**: Use Firefox on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` to access the logs via YARN Resource Manager UI\\n**Alerts**:\\n- Not Monitored*\\n**Troubleshooting Steps**:\\n- Use the spark logs to identify the cause of the failure\\n- The data in the Hourly tables is only for the current day. No need to fill missing hours, as the next MergeBatch will insert the data to the final table.\\n## Queries\\nDetails the query layer used by Online applications to fetch transaction records through Impala via REST APIs hosted in Wildfly.\\nThe ingested data are queried in order to be displayed by the Online application (used by branches). The queries are submitted to the cluster as HTTP requests using the same Wildfly instances as the [Stream](#stream) flow.\\n```mermaid\\n  graph TD\\n  A[Online Backend Servers] -->|HTTP Requests| B[Load Balancer - Netscaler]\\n  B -->|Active| C[pr1edge01.mno.gr]\\n  B -->|Active| D[pr1edge02.mno.gr]\\n  B -.->|Stopped| E[dr1edge01.mno.gr]\\n  B -.->|Stopped| F[dr1edge02.mno.gr]\\n  C -->|Impala JDBC | G[Primary Site]\\n  D -->|Impala JDBC | G\\n  E -.->|Stopped| H[Disaster Site]\\n  F -.->|Stopped| H\\n```\\n**User**: `PRODREST`\\n**Installation Path**: `/opt/wildfly/default/prodreston`\\n**Deployments Path**: `/opt/wildfly/default/prodreston/standalone/deployments`\\n**General Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/standalone.xml`\\n**Application Configuration Path**: `/opt/wildfly/default/prodreston/standalone/configuration/BigStreamer/beanconfig/translogApiOnlineJmxConfig.xml`\\n**Application Logs**: `/var/log/wildfly/prodreston/server.log`\\n**Access Log**: `/var/log/wildfly/prodreston/access.log`\\n**Instructions for start/stop**: [Here](../procedures/manage_wildfly.md)\\n**Instructions for migrating traffic to DR site**: [Here](../procedures/failover.md)\\n**Alerts**:\\n- [PR][ONLINE] Query Average Response Time\\n- [DR][ONLINE] Query Average Response Time\\n- [PR][ONLINE] Query Average Error rate\\n- [DR][ONLINE] Query Average Error rate\\n- [PR][ONLINE] REST Endpoints Average Response Time\\n- [DR][ONLINE] REST Endpoints Average Response Time\\n**Troubleshooting Steps**:\\n- Check in Grafana dashboard `MONITORING/Monitoring Alerts` the charts that are in alerting state\\n- Check if the response time is constant, a single query can increase the mean value of the response time. Check the access log for entries with large response time for the specific endpoint. If it is an isolated query the customer is not impacted.\\n- Check application logs for error messages.\\n- In case there are many errors in the application logs, try to restart the problematic Wildfly instance with the provided instructions.\\n- If the logs do not indicate a specific problem, check for any problems regarding Impala/Kudu\\n### Query Endpoints\\nQueries regarding Online query Impala tables stored in both HDFS and Kudu.\\n**Endpoints**:\\n- dynamic search\\n- by-id\\n- by-core-fields\\n- by-application\\n- top-by-branchcode-clientusername\\n- first-by-branchcode-computername\\n- aggr-computernum-usernum-transnum-groupby-branchcode\\n- aggr-transnum-groupby-branchcode-clientusername-by-branchcode\\n- aggr-opcodenum-transnum-groupby-opclass\\n- aggr-transnum-groupby-opclass-opcode-by-opclass\\n## Retention Mechanism\\nRetention scripts and cron jobs that purge old transaction data from Impala, HDFS, and HBase based on date thresholds to manage system size.\\n### Impala Retention\\n#### DEV\\nEvery day (at **16:05 on DR site only** by **Cron**). This script drops partitions from impala table `dev_trlog_online.service_audit` older than 60 days.\\n**User**: `DEVREST`\\n**Script Logs**: `/var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log`\\n**Script**: `/opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh`\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- Use the script logs to identify the cause of the failure\\n- After the root cause for the failure is resolved, run manually the following command\\n  ``` bash  \\n  /opt/ingestion/DEVREST/common/scripts/retention_mechanism_daily_STABLE.sh >> /var/log/ingestion/DEVREST/common/log/retention_mechanism_daily.log 2>&1\\n  ```\\n### Additional Tables\\nKudu table's `prod_trlog_online.service_audit_stream` partitions are maintained by an Oozie jobs every day. See section [Create next week kudu partitions](#create-next-week-kudu-partitions).\\nParquet table's `prod_trlog_online.service_audit_hourly` partitions are dropped after each successful Merge run. See section [Drop hourly partitions](#drop-hourly-partitions).\\n### HBase retention\\nEvery day (at **16:15 in both sites** by **Cron**) This script deletes rows from hbase `PROD_ONLINE:SERVICE_AUDIT_STREAM` which refere to transactions inserted 7 days ago. Those rows may have been not deleted properly after merge batch job. To do so the topology queries rows from impala table `prod_trlog_online.service_audit` inside partition with par_dt refering 7 days ago.\\n**User**: `PRODREST`\\n**Script Logs**: `/var/log/ingestion/PRODREST/online/log/daily_tools_cleanupHBaseSAS.log`\\n**Script**: `/opt/ingestion/PRODREST/common/scripts/daily_tools_cleanupHBaseSAS.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr`\\n**Alerts**:\\n- Retention OnlineCleanupHbaseSAS JOB\\n**Troubleshooting Steps**:\\n- Use the script logs to identify the cause of the failure\\n- After the root cause for the failure is resolved, check on `/opt/ingestion/PRODREST/no_bkp/duplicate_cid_hbase` if a manual re-run must be done for a specific date\\n  - For a specified date eg 2022-06-30:\\n    ``` bash\\n    /opt/ingestion/PRODREST/common/spark/submit/submitmnoSparkTopology_tools_cleanupHbaseSAS prod_trlog_online.service_audit PROD_ONLINE:SERVICE_AUDIT_STREAM LIST 20220630\\n    ```\\n##  Oozie Jobs\\nScheduled Oozie jobs for importing lookup tables and managing future Kudu partitions for Online service audit tables.\\n###  Lookup tables\\nEvery day (at 07:00 by Oozie on DR & PR site ), we transfers 3 tables with reference data from the legacy MSSQL server, which is managed by mno, to the cluster. We keep only latest version to BigData (no partition).\\n**User**: `PRODREST`\\n**Coordinator**: `Coord_OnlineLookupTables_PROD`\\n**Workflow**: `ImportLookupTables`\\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/lookupTables/import_lookup_tables.sh`\\n**Logs**: from HUE\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- Open Hue to find coordinator status and logs\\n###  Create next week kudu partitions\\nEvery day (at 10:10 by Oozie on DR site and 10:30 by Oozie on PR site ), we delete old Kudu partitions and add new Kudu partitions on `prod_trlog_online.service_audit_stream`.\\n**User**: `PRODREST`\\n**Coordinator**: `Coord_OnlineCreateKuduPartitionsPROD`\\n**Workflow**: `CreateKuduPartitionsPROD`\\n**Local path**: `/opt/ingestion/PRODREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\\n**Logs**: from HUE\\n**Alerts**:\\n- Not Monitored\\n**Troubleshooting Steps**:\\n- Open Hue to find coordinator status and logs\\nThere is a case that merge batch has not properly run for a specific date, **even though** the transactions for that specific date are present on impala `service_audit`. For example by moving a service_audit partiton from DR to PR or vice versa, or when merge batch failed **after** deleting transactions from kudu.\\nIn that case we can manually mark merge batch as complete for that date, and therefore next oozie job created by `Coord_OnlineCreateKuduPartitionsPROD` will delete that partition from kudu, by manipulating HBase table `PROD_ONLINE:MERGE_BATCH_STATE_INFO`.\\nExample for a specific date (10/10/2022):\\n - Run HBase shell\\n ```\\n   hbase shell\\n   ```\\n - And then inside HBase shell:\\n ```\\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase', 'true'\\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_hbase_time','0'\\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu', 'true'\\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:drop_kudu_time', '0'\\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running', 'false'\\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:running+time', '0'\\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala', 'true'\\n   put 'PROD_ONLINE:MERGE_BATCH_STATE_INFO', 'MergeBatchInfoKey_20221010', 'MergaBatchState:write_impala_time', '0'\\n   ```\\n#### DEV\\nEvery day (at **11:00 by Oozie** on **DR site only** ), we delete old Kudu partitions and add new Kudu partitions on `dev_trlog_online.service_audit_stream`.\\n**User**: `DEVREST`\\n**Coordinator**: `Coord_OnlineCreateKuduPartitionsDEV`\\n**Workflow**: `CreateKuduPartitionsDEV`\\n**Local path**: `/opt/ingestion/DEVREST/hdfs_mirror/create_kudu_partition/create_next_weeks_partitions.sh`\\n**Logs**: from HUE\\n**Alerts**:\\n- Not Monitored"
        },
        {
            "question": "What caused the Spark History Server on dr1node03 to crash and how was it resolved?",
            "answer": "The Spark on YARN History Server crashed due to a Java heap `OutOfMemoryError` caused by a low heap size of 512MB. It was resolved by increasing the heap size to 2GB via Cloudera Manager, aligning it with the PR site configuration, and restarting the service role.",
            "category": "Infrastructure & Deployment",
            "files": "20230305-IM2098517.md",
            "context": "---\\ntitle: Spark History Server on DR Site Crashed Due to OutOfMemoryError\\ndescription: The Spark on YARN History Server on dr1node03 (DR site) exited unexpectedly due to a Java heap OutOfMemoryError; resolved by increasing heap size from 512MB to 2GB to match PR Site configuration and restarting the role.\\ntags:\\n  - mno\\n  - bigstreamer\\n  - spark\\n  - yarn\\n  - history server\\n  - java heap size\\n  - outofmemory\\n  - dr1node03\\n  - dr site\\n  - cloudera\\n  - service restart\\n  - role config\\n  - unexpected exit\\n  - IM2098517\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM2098517\\n  system: mno BigStreamer DR Site\\n  root_cause: Spark History Server on DR crashed due to insufficient Java heap size (512MB), resulting in OutOfMemoryError\\n  resolution_summary: Increased heap size to 2GB to align with PR configuration and restarted the role successfully\\n  affected_host: dr1node03\\n  cloudera_service: Spark on YARN\\n---\\n# mno - BigStreamer - IM2098517 - Health issue on dr1edge01\\n## Description\\nOn 07/03/2023, Cloudera Manager reported an unexpected exit for the Spark on YARN History Server running on `dr1node03` in the Disaster Recovery (DR) site. The crash was caused by an `OutOfMemoryError`, due to the default Java heap size being set too low (512MB).\\nThe following health issue has occurred in the cloudera manager system:\\n```\\nSpark on yarn - History Server (dr1node03) - Unexpected Exits\\n```\\n## Actions Taken\\n### Investigation\\n1. Login to Cloudera for DR Site\\n2. We inspected logs for this role for the time that the problem arose: `Cloudera > Diagnostics > Logs` and chose `Service: Spark on Yarn` and `Role: History Server`. We could not identify the root cause by these logs\\n3. ssh to dr1node03 as root, went to `/var/run/process` , and inspected logs from the process that ran at the time of the problem and found out that the process with pid 51291 was killed while a `OutOfMemoryError` occured\\n![terminal_screenshot](.media/IM2098517/spark_on_yarn.png)\\n### Resolution\\n4. We checked the  `java heap size` of the History Server through Cloudera UI configuration tab. It was set to 512M.\\n5. We checked the respective option for the PR Site and it was set to 2G\\n6. We set the `java heap size` of the History Server to 2G at the DR Site\\n7. We proceeded to restart of the role after communication with the customer\\n## Our Ticket Response\\n```\\n07/03/23 16:28:55 Europe/Eastern (MASTROKOSTA MARIA):\\nWe have restarted the History Server after a phone call. There was no problem during the restart.\\nThank you\\n07/03/23 16:21:21 Europe/Eastern (MASTROKOSTA MARIA):\\nThe exit occurred due to an out of memory error. We have changed the java heap size of the History Server from 512MB to 2GB as in the PR. We will need to restart the role. There will be no outage.\\n```"
        },
        {
            "question": "What adjustment was made to fix the IBANK MergeBatch failure due to memory exhaustion on the DR site?",
            "answer": "The Spark job failed due to memory overload. It was fixed by editing the Spark submit script to increase `coalesce` and `shuffle.partitions`:\n```bash\n-coalesce=96 \n--spark.sql.shuffle.partitions=96 \n```\nThen, the job was rerun with:\n```bash\n/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh\n```",
            "category": "Application Functionality & Flow",
            "files": "20210430-IM1317401.md",
            "context": "---\\ntitle: IBANK MergeBatch Failure on DR Site Due to Memory Exhaustion - Manual Recovery\\ndescription: The IBANK_Migration MergeBatch job failed on 2021-04-30 due to excessive data volume and memory exhaustion. The Spark job was reconfigured with increased coalesce and shuffle partitions, and rerun from the merge section of the ingestion script to complete the Data Warehouse pipeline.\\ntags:\\n  - bigstreamer\\n  - mno\\n  - ibank\\n  - mergebatch\\n  - yarn\\n  - spark\\n  - coalesce\\n  - shuffle.partitions\\n  - memory error\\n  - ingestion pipeline\\n  - spark tuning\\n  - merge failed\\n  - dr site\\n  - historical migration\\n  - prodrest\\n  - parquet ingestion\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM1317401\\n  system: MNO BigStreamer - IBANK DR Site\\n  root_cause: Spark MergeBatch job failed due to memory issues caused by large data volume; default configuration insufficient\\n  user_visible_error: Job failed in Yarn and did not appear in active applications list\\n  action_taken:\\n    - Inspected job status in Grafana\\n    - Verified failure in `monitor_sched_jobs` and via `yarn`\\n    - Increased `coalesce` and `shuffle.partitions` values in Spark submit script\\n    - Restarted ingestion script from merge section\\n  outcome: MergeBatch job completed successfully after reconfiguration\\n---\\n# mno - BigStreamer - IM1317401 - [PR][IBANK] Data warehouse flows\\n## Description\\nThe IBank_Migration job pipeline appeared successful for all historical stages on `20210429`, except for the `MergeBatch` job, which failed. This job is critical for aggregating and inserting the final data into the Data Warehouse. Investigation showed that it crashed due to memory issues when processing large data volumes, and a reconfiguration of the Spark parameters was required to complete execution.\\nData Warehouse jobs have not run:\\n```sql\\nselect * from prod_trlog_ibank.monitor_sched_jobs where par_dt=20210429\\n\\nIBank_Migration Historical JOB 20210429 SUCCESS 2021-04-30 02:00:01.000 2021-04-30 02:03:50.000 dr1edge01.mno.gr\\n2 IBank_Migration Historical Sqoop_Import 20210429 SUCCESS 2021-04-30 02:00:01.000 2021-04-30 02:01:51.000 dr1edge01.mno.gr\\n3 IBank_Migration Historical Impala_Insert 20210429 SUCCESS 2021-04-30 02:03:07.000 2021-04-30 02:03:50.000 dr1edge01.mno.gr\\n4 IBank_Migration Historical to SA Impala_Insert 20210429 SUCCESS 2021-04-30 02:04:23.000 2021-04-30 02:06:21.000 dr1edge01.mno.gr\\n5 IBank_Migration Historical to SA JOB 20210429 SUCCESS 2021-04-30 02:04:23.000 2021-04-30 02:06:21.000 dr1edge01.mno.gr\\n6 IBank_Ingestion MergeBatch JOB 20210429 FAILED 2021-04-30 09:37:35.000 2021-04-30 09:37:35.000 dr1edge.mno.gr\\n```\\nThe merge batch has also crashed.\\n## Actions Taken\\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account\\n2. Inspected `[PR][IBANK] Overview` graph\\n3. Merge Batch job has FAILED\\n4. MergeBatch job was not running : `yarn application -list | grep -i merge | grep -v Hourly`\\n5. Found Spark job failure due to OOM (Out Of Memory) error in Yarn logs.\\n6. Failure was reproducible; retry without changes failed again.\\n7. `Vi /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh`\\n8. Search for \\u201ccoalesce\\u201d , Change/replace `-coalesce=$NUMBER_OF_EXECUTORS \\\\ ` , To : `-coalesce=96 \\\\ `\\n9. Search for `--spark.sql.shuffle.partitions=16  \\\\`  to : `--spark.sql.shuffle.partitions=96  \\\\`\\n10. As user PRODREST, rerun ingestion script from merge step: `/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh`\\n## Affected Systems\\nDR Site IBANK"
        },
        {
            "question": "How do you investigate a Grafana alert for IBANK query latency, and why might it not need intervention?",
            "answer": "1. Login to Grafana (`https://dr1edge01.mno.gr:3000`) and inspect the alert panel.\n2. Switch graph to display `max` instead of `mean` query time.\n3. Check Wildfly access logs:\n```bash\n/var/log/wildfly/prodrestib/access.log\n```\n4. Validate cluster health in Cloudera Manager.\nThe alert may be caused by a few user-triggered long queries skewing the mean, and typically clears on its own.",
            "category": "Troubleshooting & Issue Resolution",
            "files": "20201014-IM1317401.md",
            "context": "---\\ntitle: [PR][IBANK] Query Average Response Time Alert on Grafana\\ndescription: Investigation of Grafana alert \\\"[PR][IBANK] Query Average Response Time\\\" caused by a few high-latency queries due to user activity. The alert was caused by bias in the mean response time and resolved without intervention.\\ntags:\\n  - bigstreamer\\n  - mno\\n  - ibank\\n  - grafana\\n  - pr site\\n  - response time\\n  - latency\\n  - average query time\\n  - mean vs max\\n  - cloudera\\n  - wildfly\\n  - edge nodes\\n  - monitoring\\n  - grafana alerts\\n  - query metrics\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM1317401\\n  system: MNO BigStreamer IBANK PR Site\\n  root_cause: Few user-triggered long queries skewed the mean query response time, triggering the Grafana alert\\n  user_visible_alert: \\\"[PR][IBANK] Query Average Response Time alert\\\" on Grafana dashboard\\n  action_taken:\\n    - Edited the Grafana graph to temporarily display max instead of mean query time\\n    - Checked access logs on PR edge nodes\\n    - Verified Cloudera cluster health on PR site\\n    - Coordinated with MNO to confirm user-triggered activity\\n  outcome: Alert cleared on its own; no intervention required\\n---\\n# mno - BigStreamer - IM1317401 - [PR][IBANK] Query Average Response Time alert\\n## Description\\nAlert message on Grafana:\\n[PR][IBANK] Query Average Response Time alert\\n## Actions Taken\\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account\\n2. Inspected `[PR][IBANK] Query Average Response Time alert` graph\\n```\\nI edited the graph temporarily to display the max response time instead of the mean value. Usually the problem affects 3-4 queries and is due to user actions in the PR site.\\nThese problematic queries add bias to the mean time and create the alarm. By checking the max values, I saw that that was the case.\\n```\\n3. Login to pr1edge01.mno.gr/pr1edge02.mno.gr with personal account and check access logs under `/var/log/wildfly/prodrestib/access.log`\\n4. Login to Primary Site Cloudera Manager `https://pr1edge01.mno.mno.gr:7183` and check that the cluster is in healthy status\\n5. No action taken. The alarm will clear without the need for manual action.\\n6. Since no actions had been initiated from our team on the PR site at the time of the incident, we requested that MNO verify whether internal user activity may have caused the query load.\\n## Affected Systems\\nPrimary Site IBANK query"
        },
        {
            "question": "What caused the EXPORT job failure for DWH_IBANK CARD component and how can it be avoided?",
            "answer": "The failure (Code 6) was due to an Impala `COMPUTE STATS` query hogging memory on `prod_trlog_ibank.service_audit`, blocking metadata operations. The issue resolved after query completion. It can be avoided by disabling stats computation for that table during critical extract windows.",
            "category": "Data Management & Query Execution",
            "files": "20221027-IM2006951.md",
            "context": "---\\ntitle: DWH_IBank EXPORT Job Failure Due to Compute Stats Resource Contention\\ndescription: EXPORT job for component CARD failed with Code 6 due to Impala resource saturation caused by COMPUTE STATS on large table `prod_trlog_ibank.service_audit`, which blocked metadata operations and caused extract script timeouts.\\ntags:\\n  - mno\\n  - bigstreamer\\n  - dwh_ibank\\n  - card export\\n  - compute stats\\n  - impala\\n  - query timeout\\n  - code 6\\n  - resource pool\\n  - disaster recovery\\n  - service_audit\\n  - export failure\\n  - grafana alert\\n  - cloudera manager\\n  - metadata refresh\\n  - monitoring dashboard\\n  - impala insert\\n  - sqoop export\\n  - job recovery\\n  - im2024442\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM2024442\\n  system: mno BigStreamer DWH Disaster Recovery Site\\n  root_cause: EXPORT job timed out because Impala was saturated by a COMPUTE STATS operation on the service_audit table, which blocked subsequent metadata and extract queries\\n  resolution_summary: The compute stats query completed and resources were released; workaround was to rerun job and disable stats generation for the problematic table\\n  blocking_query: COMPUTE STATS prod_trlog_ibank.service_audit\\n  affected_job: DWH_IBank EXPORT CARD\\n  job_status_code: 6\\n---\\n# mno - BigStreamer - IM2024442 - Failed job at Grafana\\n## Description\\nOn 26/10/2022, the DWH_IBank EXPORT job for component `CARD` failed with status code 6, indicating a timeout in the control script while waiting for the `EXTRACT` phase to finish.\\n```bash\\nApplication: DWH_IBank\\nJob Name: EXPORT\\nComponent: CARD\\nDate: 26/10/2022\\nStatus: FAILED\\nDescription: Code:6\\n```\\n## Actions Taken\\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account and confirm that Datawarehouse Flows have failed from `Monitoring/Monitoring PR/DR` dashboard.\\n2. All flows have failed with `Code: 6` which means that the control script has timed-out while monitoring the `EXTRACT` script. The `EXTRACT` step has 2 sub-steps: `Impala Insert` and `Sqoop Export`.\\n3. Check logs:\\nFrom `dr1edge01.mno.gr` with personal account:\\n``` bash\\nless /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\\n```\\nIn this file for all flows that failed we see that the last log entry is the submission of the `Impala Insert` part of the `EXTRACT`, which was still running. This means that another query is hogging all resources for Impala and our flows are waiting to be executed.\\n4. Login to Disaster Site Cloudera Manager `https://dr1edge01.mno.mno.gr:7183` and check for resource intensive Impala queries `Clusters > Impala > Queries`. The key resource here is memory as this is the only metric that can be defined in Resource Pools.\\nThe query that created the problem was `COMPUTE STATS prod_trlog_ibank.service_audit`. Pictures are not included because Impala does not report statistics for the `COMPUTE STATS` queries, but given the size of the table and the time of execution it matched. This hypothesis was later confirmed when the same problem appeared on a later `COMPUTE STATS` execution.\\n5. We informed the customer to re-run the failed jobs and proposed to stop computing statistics for that table as they did not impact our application.\\n``` text\\n27/10/22 13:16:01 Europe/Eastern (POULAS GIORGOS):\\nPlease rerun the flow steps that encountered a problem.\\nWe are continuing to investigate the root cause of the problem.\\n**Workaround**\\n27/10/22 13:32:44 Europe/Eastern (POULAS GIORGOS):\\nFollowing the previous answer, the compute statistics on the prod_trlog_ibank.service_audit table committed many resources on the Calatog Server and the query Coordinator (dr1node02), resulting in REFRESH/INVALIDATE METADATA operations experiencing long execution times and causing jobs to time out. After the compute statistics were completed, Impala released the resources and resumed.\\nAs we have communicated in the past, from our perspective the statistics of the table are not necessary. Please let us know if we can disable the production of statistics for this particular table.\\n**Resolved**\\n```\\n6. The changes for the statistics were implemented as part of [this ticket](obss/oss/sysadmin-group/mno/cloudera-cluster#180).\\nDisaster Recovery Site Datawarehouse"
        },
        {
            "question": "How can overlapping executions cause failures in DWH_IBANK EXPORT jobs, and how should they be managed?",
            "answer": "If a manual EXPORT job is triggered before the previous scheduled job completes, resource contention occurs. YARN logs showed overlapping apps:\n```text\napplication_1651064786946_8294 (manual)\napplication_1651064786946_8190 (scheduled)\n```\nAlways ensure the scheduler flow has completed before manual job execution.",
            "category": "Application Functionality & Flow",
            "files": "20220504-IM1851937.md",
            "context": "---\\ntitle: DWH_IBANK EXPORT Batch Job Failed Due to Overlapping Execution\\ndescription: The DWH_IBANK EXPORT batch job failed because it was manually started before the previous scheduled job completed, causing a conflict in execution on the DR site. The issue was identified via Grafana and YARN logs and resolved by advising proper job sequencing.\\ntags:\\n  - mno\\n  - bigstreamer\\n  - ibank\\n  - dwh_ibank\\n  - batch job\\n  - export job\\n  - yarn\\n  - application_1651064786946_8190\\n  - application_1651064786946_8294\\n  - grafana\\n  - job conflict\\n  - job failure\\n  - dr site\\n  - produser\\n  - spark\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM1851937\\n  system: mno BigStreamer - Disaster Site\\n  root_cause: Manual rerun of DWH_IBANK EXPORT job started before the scheduled job completed, causing failure\\n  user_visible_error: Batch job failed in Grafana for application DWH_IBANK, component SERVICE_AUDIT\\n  detection_method:\\n    - Grafana monitoring alert\\n    - YARN job history UI\\n  action_taken:\\n    - Confirmed job failure through YARN UI\\n    - Identified overlapping executions between job application_1651064786946_8294 and application_1651064786946_8190\\n    - Advised customer to rerun job only after scheduled flow completes\\n  outcome: No system-wide impact; job was to be rescheduled manually\\n---\\n# mno - BigStreamer - IM1851937 - DWH_IBANK batch job failed\\n## Description\\nOn 5/5/2022, the EXPORT batch job for the DWH_IBANK application failed in the Disaster Recovery (DR) site. The failure was detected through a Grafana alert and was attributed to conflicting executions: a manual run was triggered before the scheduled job had completed.\\napplication: DWH_IBANK\\njob_name: EXPORT\\ncomponent: SERVICE_AUDIT\\ndescription: Code 1\\n## Actions Taken\\n1. Login to grafana to make sure that the alert is about DR SITE\\n2. Login to `dr1edge01` and open firefox\\n3. At the YARN UI search for `PRODUSER`, sort by End date and search with \\\"PROD_IBANK_DWH_EXPORT_ServiceAudit. You will find the failed application.\\n4. From the UI we noticed that the job with id application_1651064786946_8294 started manually before the completion of the automated job with id application_1651064786946_8190, which led to the failure of the second job.\\n5. We informed the client that they should rerun the failed job manually from the scheduler after the completion of the manual step. Also, we pointed out that before proceding with manual actions they should make sure beforehand that all scheduled flows have completed.\\nA review of the application_ids on YARN revealed that application_1651064786946_8294 was launched manually before the completion of the automated scheduled job application_1651064786946_8190, resulting in a resource conflict and batch job failure.\\nWe advised the client to avoid overlapping manual executions with scheduler runs to prevent such conflicts in the future.\\n## Affected Systems\\nDisaster Site IBank Batch"
        },
        {
            "question": "How should you interpret and resolve a 'Code 6' failure for an EXTRACT job like DWH_IBANK MY BANK?",
            "answer": "The monitoring script timed out before the Spark application started. Check logs:\n```bash\nless /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\n```\nThen confirm Spark success via YARN UI. If the job succeeded, simply rerun the EXTRACT job from the scheduler.",
            "category": "Data Management & Query Execution",
            "files": "20230127-IM2072206.md",
            "context": "---\\ntitle: DWH_IBank EXTRACT Job for MY BANK Failed Due to Timeout in Monitoring Script\\ndescription: The EXTRACT batch job for the MY BANK component failed with code 6 because the monitoring script timed out before the Spark application started; the Spark job succeeded and re-execution was successful.\\ntags:\\n  - mno\\n  - bigstreamer\\n  - dwh_ibank\\n  - my_bank\\n  - extract job\\n  - spark\\n  - yarn\\n  - grafana\\n  - timeout\\n  - monitoring\\n  - code 6\\n  - batch failure\\n  - impala\\n  - job rerun\\n  - sched_extract\\n  - im2072206\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: IM2072206\\n  system: mno BigStreamer DWH\\n  root_cause: Monitoring script timed out after 1.5 minutes while waiting for Spark application to start, though job itself eventually succeeded\\n  resolution_summary: Customer reran the job, which succeeded; no abnormal delay was detected in Spark startup\\n  affected_component: MY BANK\\n  failure_code: 6\\n  monitoring_script: sched_extract.sh\\n  spark_status: SUCCEEDED\\n  solution_reference: https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/101#ndef_94836\\n---\\n# mno - BigStreamer - IM2072206 - Batch Job Failed\\n## Description\\nOn 27/01/2023, the `EXTRACT` batch job for the `MY BANK` component of the `DWH_IBank` application failed with **Code 6** in Grafana.\\n```\\nApplication: DWH_IBank\\nJob Name: EXTRACT\\nComponent: MY BANK\\nStatus: Failed\\nDescription: Code 6\\n```\\n## Root Cause Analysis\\n1. Login to `https://dr1edge01.mno.gr:3000` with personal account and confirm that Datawarehouse Flow failed from `Monitoring/Monitoring PR/DR` dashboard.\\nThe flow failed with `Code: 6` which means that the control script has timed-out while monitoring the `EXTRACT` script.\\n2. Check logs:\\nFrom `dr1edge01.mno.gr` with personal account:\\n``` bash\\nless /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\\n```\\n![IM2072206_extract_logs](.media/IM2072206_extract_logs.png)\\nThe monitoring database was updated with status FAILED due to `Check if app is running` timeout.\\n3. Check the Spark application status from YARN UI\\n![IM2072206_yarn_app](.media/IM2072206_yarn_app.png)\\nSpark App Status: SUCCEEDED.\\nThe script waited for only 1,5min and updated the monitoring database with Failed Status. Spark app began its execution after almost 2,5 minutes.\\n> 2,5min is not considered as a noticeable or abnormal delay time, so we did not investigate further.\\n4. Customer reran the job\\n5. Check logs and YARN UI of second application\\n![IM2072206_yarn_rerun](.media/IM2072206_yarn_rerun.png)\\n![IM2072206_rerun_logs](.media/IM2072206_rerun_logs.png)\\nIn this case the Spark app started immediately and the script updated the monitoring app with Running Status.\\n## Action Points\\nSolution has been given with [this](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/101#ndef_94836) issue."
        },
        {
            "question": "How can a complex Impala query lead to a cluster-wide service outage on DR?",
            "answer": "A high-complexity Impala query with regex and conversions used too many threads (~10.7h CPU time), causing Hive, Kudu, and Sentry to timeout. This led to unhealthy statuses in Cloudera Manager. No action was needed—services recovered once the query completed.",
            "category": "Infrastructure & Deployment",
            "files": "20220617-SD1949713.md",
            "context": "---\\ntitle: DR Cluster Service Disruptions Due to High-Complexity Impala Query\\ndescription: DR Cloudera services appeared unhealthy, causing mass alerts and crashing some applications. Root cause was a high-complexity Impala query that overloaded threads and caused timeouts in Hive, Kudu, and Sentry.\\ntags:\\n  - mno\\n  - bigstreamer\\n  - dr cluster\\n  - cloudera\\n  - impala\\n  - high complexity query\\n  - threads cpu time\\n  - sentry\\n  - hive timeout\\n  - kudu timeout\\n  - grafana\\n  - service disruption\\n  - resource spike\\n  - SD1949713\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  issue_id: SD1949713\\n  system: mno BigStreamer - DR Cluster\\n  root_cause: A complex Impala query with excessive regex and conversions spiked resource consumption and triggered cascading timeouts across Hive, Kudu, and Sentry\\n  user_visible_error: All Cloudera services reported unhealthy (red) status, with ~120 alert emails received\\n  detection_method:\\n    - Grafana alert\\n    - Cloudera Manager Impala queries tab\\n    - Service logs showing timeouts and resource exhaustion\\n  action_taken:\\n    - Validated alerts in Grafana\\n    - Identified problem query in Cloudera Manager\\n    - Investigated thread and resource usage\\n    - No manual action taken; services recovered after query completed\\n  outcome: Root cause identified and customer informed; system stabilized automatically after resource usage dropped\\n---\\n# mno - BigStreamer - SD1949713 - DR Cluster Service Disruptions\\n## Description\\nThe DR cluster triggered ~120 alert emails reporting unhealthy service status across the board. On logging into Cloudera Manager, all services were marked red. Some dependent applications had crashed due to this. Some of the applications crashed as well.\\n## Actions Taken\\n1. Login to grafana to make sure that the alert is about DR SITE. We noticed that there were alerts for IBANK Spark Waiting Batches but not for Visible which predisposes us for an issue with Kudu.\\n![ibank_kudu_problem](.media/SD1949713/ibank_kudu_problem.PNG)\\n2. Login to Cloudera UI for the DR Site.\\n3. From `Charts>Impala Perf` we noticed increased resource commitment through Impala Pool Reserved and Threads charts.\\n![Impala Pool Reserved](.media/Impala_pool_reserved.PNG)\\n![Threads](.media/threads.PNG)\\n4. From `Cloudera Manager>Impala>Queries` we searched for queries that took place at the time the problem raised. We found that the query with ID 6d44d9525a681fb8:5e536ffc00000000 had Threads:CPU Time 10.7h. Upon inspection through `Query Details` we saw that the query was of high complexity with conversions and comparisons with regex.\\n![Query](.media/query.PNG)\\n5. Through Cloudera logs, we noticed that the query impacted the services in the form of timeouts for Kudu and Hive due to slow communication with Sentry Service.\\n![hive_problem](.media/SD1949713/hive_problem.PNG)\\n![timeouts_kudu](.media/SD1949713/timeouts_kudu.PNG)\\n![sentry_problem](.media/SD1949713/sentry_problem.PNG)\\n6. The issue resolved itself once the query completed execution and the resource usage dropped. No manual recovery steps were required. We informed the client that it was due to a high complexity query ran by a normal user that resulted in an increased undertaking of resources.\\n## Affected Systems\\nDR Site\\n"
        },
        {
            "question": "What are the key steps to safely renew certificates for Cloudera and HAProxy services across PR and DR edge nodes?",
            "answer": "1. Backup existing certificates:\n```bash\ndcli -C \"cp -r /opt/cloudera/security /opt/cloudera/security.BAK_$(date +%d%b%Y%H%M%S)\"\ncp -r /opt/haproxy/security /opt/haproxy/security.BAK_$(date +%d%b%Y%H%M%S)\n```\n2. Validate and replace `.pem` files using signed `.cer` files.\n3. Distribute node certs to `/opt/cloudera/security/x509/` and import them into keystores:\n```bash\nkeytool -import -file node.cert -alias $HOSTNAME -keystore /opt/cloudera/security/jks/node.jks -storepass <password>\n```\n4. Restart HAProxy and relevant services:\n```bash\nsystemctl reload haproxy\npcs resource cleanup haproxy-clone\n```",
            "category": "Infrastructure & Deployment",
            "files": "certificate_renewal_procedure.md",
            "context": "---\\ntitle: Certificate Renewal Procedure for BigStreamer\\ndescription: Step-by-step guide to renew and validate Cloudera and HAProxy certificates across PR and DR environments, including certificate checks, backups, distribution, HAProxy replacement, and application restarts.\\ntags:\\n  - certificates\\n  - cloudera\\n  - haproxy\\n  - ssl\\n  - openssl\\n  - pem\\n  - jks\\n  - kudu\\n  - flows\\n  - cluster-maintenance\\n  - bigstreamer\\n  - edge-nodes\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  sites:\\n    - PR\\n    - DR\\n  systems:\\n    - node01\\n    - dr1edge01\\n    - pr1edge01\\n    - Xr1node03\\n    - un1\\n    - edge nodes\\n  backup_paths:\\n    - /backup/new_certs/\\n    - /backup/haproxy_certs/\\n    - /opt/cloudera/security/\\n    - /opt/haproxy/security/\\n  services:\\n    - haproxy\\n    - kudu\\n    - spark flows\\n    - cloudera-scm-agent\\n    - cloudera-scm-server\\n    - bigdatamanager\\n---\\n# Certificate Renewal Procedure\\nBack up every certificate before doing any action\\n### Backup Procedure\\n- From node1 as root:\\n``` bash\\ndcli -C \\\"cp -r /opt/cloudera/security /opt/cloudera/security.BAK_`date +%d%b%Y%H%M%S`\\\" \\n```\\n- From edge nodes as root:\\n```bash\\ncp -r /opt/haproxy/security /opt/haproxy/security.BAK_`date +%d%b%Y%H%M%S`\\n```\\n## Node and VIP Certificates check\\nThis section explains how to verify unsigned and signed certificates for Cloudera and edge nodes using OpenSSL. Ensures certificate integrity before replacement.\\n### Check unsigned certificates\\n- In this step we checked the presigned certificates in `backup/new_certs/cert_requests` if they have correct subject with the following command: \\n`openssl req -in new_certs/cert_requests/dr1edge.mno.gr-ert-file -noout -text`\\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/certreq.JPG)\\nand also we check the modulus if it is the same. Basically we check the output of the following commands:\\n`openssl rsa -in /opt/cloudera/security/x509/node.key -noout -text modulus`\\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/modulus1.JPG)\\n`openssl req -in new_certs/cert_requests/dr1edge.mno.gr-ert-file -noout -text -modulus`\\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/modulus2.JPG)\\nThen we provide to the customer the certificates located in `backup/new_certs/cert_requests` in order to be signed\\n### Check signed certificates from mno\\nIn the following folder are located the signed certificates\\nBackup NFS Folder: `/backup/new_certs/certificates`\\nCheck the certificates in the above mentioned folder for issuer, subject, TLS Web, date.\\nThe `'ln -1'` feature prints all files in the for loop per line\\n- Check the issuer\\n`for i in 'ln -1'; do echo $i; openssl x509 -noout -text -in $i | grep -i issuer ; done` \\nIn the above command we wait a return such as this: \\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/issuer.JPG)\\n- Check the subject\\n`for i in 'ln -1'; do echo $i; openssl x509 -noout -text -in $i | grep -i subject ; done`\\nIn the above command we wait a return such as this:\\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/subject.JPG)\\n- Check the TLS Web\\n`for i in 'ln -1'; do echo $i; openssl x509 -noout -text -in $i | grep -i 'TLS Web' ; done` \\nIn the above command we wait a return such as this: \\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/tls.JPG)\\n- Check the dates\\n`openssl x509 -noout -text -in 'cert_file' - dates`\\nIn the above command we wait a return such as this: \\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/dates.JPG)\\n- Or with a for loop for all the files\\n`for i in 'ln -1'; do openssl x509 -noout -text -in $i | grep -i 'ndef after'; done`\\nIn the above command we wait a return such as this: \\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/notafter.JPG)\\n### Haproxy certificates check and replacement\\nBackup NFS Folder: `/backup/haproxy_certs`\\n`ssh root@pr1edge01`\\nIn order to set the new haproxy certificates we need to have 9 certificates\\nCheck the haproxy security folder: `/opt/haproxy/security/x509/`\\n```\\ndevsqla_mno_gr.haproxy.pem\\npr1edge_mno_gr.haproxy.pem\\ndr1edge_mno_gr.haproxy.pem\\nqasqla_mno_gr.haproxy.pem\\nprodsqla_mno_gr.haproxy.pem\\n```\\nand the node certifate for PR and DR in the following format \\n`node.haproxy.pem`\\n- Now in the NFS mentioned files we need to replace the second certificate with the one that is located in the signed cert files that the customer has send to us\\n`vi /backup/haproxy_certs/devsqla_mno_gr.haproxy.pem` and replace the \\n```\\n--- BEGIN CERTIFICATE ---\\n... \\n--- END CERTIFICATE ---\\n```\\nwith the one located in `/backup/new_certs/certificates/devsqla_mno_gr-cert-file.cer`\\n- Moreover, as root replace the CERTIFICATE to the\\n`vi /backup/haproxy_certs/devsqla_mno_gr.haproxy.pem`\\nwith the certificate from \\n`cat /backup/new_certs/certificates/devsql_mno_gr-cert-file.cer` \\nand copy the section:\\n```\\n\\u00a0\\u00a0\\u00a0 ---BEGIN CERTIFICATE---\\n\\n\\u00a0\\u00a0\\u00a0 .....\\n\\n\\u00a0\\u00a0\\u00a0 ---END CERTIFICATE---\\n```\\nand replace it with the certificate on the pem file `/backup/haproxy_certs/devsqla_mno_gr.haproxy.pem`\\nFor all the other `pem` files we need to do the same procedure accordingly. **EVERY PEM HAS A UNIQUE CER FILE**\\nWe need to specify in more detail the above steps especially for the 4 edge nodes we have on PR & DR sites.\\n- Firstly, under `/opt/haproxy/security/x509/` folder there is the `node.haproxy.pem` certificate as mentioned before. We must copy this file under the NFS mentioned folder `/backup/haproxy_certs/`.\\nFor example:\\n```bash\\ncp /opt/haproxy/security/x509/node.haproxy.pem /backup/haproxy_certs/Xredge0X.node.haproxy.pem\\n```\\n- Then, from `/backup/haproxy_certs/Xredge0X.node.haproxy.pem` we must keep only the private key certificate section and replace the rest of the certificates with the ones that are located in the signed cert file `/backup/new_certs/certificates/Xr1edge0X_mno_gr-cert-file.cer` that the customer has sent to us.\\n- We must follow the same procedure for all edge nodes certificates.\\n#### Checks\\n- Check the issuer on previous certificates located in `/etc/pki/ca-trust/source/anchors/`\\n```\\nca1.crt\\nca.crt\\nca3.crt\\n```\\n- Check the issuer in the above mentioned crt\\n`for i in ca1.crt ca3.crt ca.crt; do echo $i; openssl x509 -noout -text -in $i | grep -i issuer; done`\\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/pki.JPG)\\nFrom the above image we excepted to see the issuer as `mnoInternalRoot` which is correct in `ca3.crt`.\\n- In order to check if the certificates has been signed from the same issuer. Run the following commands and check the output. It should be the same. If not, the certificate is wrong\\n```\\nopenssl x509 -noout -modulus -in 'cert_file'\\nopenssl rsa -noout -modulus -in 'cert_file'\\n```\\n![alt text](KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/media/x509.JPG)\\n### Actions Before Distributing the certificates\\nExplains how to safely stop Spark flows and prepare systems for certificate changes.\\nmno is obliged to move the traffic from PR site to DR site.\\nStop the flows, as user PRODREST:\\n```\\n# Signal Spark flows to shut down safely before cert replacement\\n[PRODREST@Xr1edge01]# touch SHUTDOWN\\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown_marker/\\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\\n```\\nCheck that flows stopped.\\n```\\n[PRODREST@Xr1edge01]# yarn application -list | grep -i PROD_\\n```\\nWhen executing the same procedure on the DR site, we should **additionally** stop the following flows as user DEVREST:\\n```\\n[DEVREST@dr1edge01]# touch SHUTDOWN\\n[DEVREST@dr1edge01]# hdfs dfs -put SHUTDOWN /user/DEVREST/service/DEV_IBank_Ingest/topology_shutdown_marker/\\n[DEVREST@dr1edge01]# hdfs dfs -put SHUTDOWN /user/DEVREST/service/DEV_Online_Ingest/topology_shutdown_marker/\\n```\\nCheck that flows stopped.\\n```\\n[DEVREST@dr1edge01]# yarn application -list | grep DEVREST\\n```\\n## Distribute the certificates\\nCovers how to copy, import, and activate the new signed certificates across all cluster nodes.\\n### Generate the keystore password (It's not the same for both sites)\\n`bdacli getinfo cluster_https_keystore_password`\\nFrom node01:\\n#### Node certificates\\nFor internal nodes:\\n```\\ndcli -C cp /backup/new_certs/certificates/\\\\$HOSTNAME-cert-file.cer /opt/cloudera/security/x509/node.cert\\n```\\nFor edge nodes:\\n```\\ncp /backup/new_certs/cert_2024/$HOSTNAME-cert-file.cer /opt/cloudera/security/x509/node.cert\\n```\\n#### JKS certificates\\nFor internal nodes:\\n```\\n# Import signed certificate into Cloudera's Java Keystore (JKS) on internal nodes\\ndcli -C keytool -import -file /opt/cloudera/security/x509/node.cert -alias \\\\$HOSTNAME -keystore /opt/cloudera/security/jks/node.jks -storepass KEYSTORE_PASS_FROM_ABOVE -keypass KEYSTORE_PASS_FROM_ABOVE -noprompt\\n```\\nFor edge nodes:\\n```\\nkeytool -import -file /opt/cloudera/security/x509/node.cert -alias $HOSTNAME -keystore /opt/cloudera/security/jks/node.jks -storepass KEYSTORE_PASS_FROM_ABOVE -keypass KEYSTORE_PASS_FROM_ABOVE -noprompt\\n```\\n#### Check new certificates\\nFor internal nodes:\\n\\n```\\ndcli -C \\\"keytool -list -v -keystore /opt/cloudera/security/jks/node.jks -alias \\\\$HOSTNAME\\\"\\n```\\nFor edge nodes:\\n```\\nkeytool -list -v -keystore /opt/cloudera/security/jks/node.jks -alias $HOSTNAME\\n```\\n#### Haproxy certificates\\nCopy the files from `/backup/haproxy_certs/` to `/opt/haproxy/security/x509/` and replace the existing ones.\\n**Special caution**:\\nMust copy `Xr1edge0X_mno_gr.node.haproxy.pem` that we created in the previous steps to the `node.haproxy.pem` certificate existing on the edge nodes\\n```\\n# Replace haproxy node certificate with newly signed one\\ncp /backup/haproxy_certs/Xr1edge0X_mno_gr.node.haproxy.pem /opt/haproxy/security/x509/node.haproxy.pem\\n```\\n**Do not copy root.inter.pem**\\nAfter copying the certificates, restart the haproxy service on both edge nodes\\n```\\nsystemctl reload haproxy \\nsystemctl status haproxy\\npcs resource cleanup haproxy-clone\\n```\\nIf after restarting HAProxy the service fails due to missing chain or improper concatenation, rebuild the node certificate manually like this:\\n```\\ncd /opt/cloudera/security/x509\\ncat node.hue.key node.cert > /opt/haproxy/security/x509/node.haproxy.pem\\n```\\n### Actions After Distributing the certificates\\nSteps to restart agents and verify successful service recovery after new certificates are in place.\\nWhen the new certificates replace the old ones, the services of the cluster will become healthy. Restart is required. [Official Guide](https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cm_mc_start_stop_service.html)\\nAll services except zookeeper need restart. The service `Bigdatamanager` should always remain stopped \\nWe prefer to start with Kudu because it takes longer to synchronize. Also, for KMS service perform restart 1 by 1. For KMS server perform restart on both passive services and then on both active.  \\nLastlty, after Kudu syncs start the flows.\\nWhen the cluster be stopped then:\\nFor edge nodes:\\n```\\nsystemctl status cloudera-scm-agent\\nsystemctl restart cloudera-scm-agent \\n```\\nFor internal nodes:\\n```\\ndcli -C \\\"systemctl status cloudera-scm-agent | grep -i Active\\\" \\n# Restart Cloudera agents across all nodes to load new certificates\\ndcli -C \\\"systemctl restart cloudera-scm-agent\\\" \\ndcli -C \\\"systemctl status cloudera-scm-agent | grep -i Active\\\"\\n```\\n```\\ndcli -c Xr1node03 \\\"systemctl restart cloudera-scm-server\\\" \\ndcli -c Xr1node03 \\\"systemctl status cloudera-scm-server\\\"\\n```\\n### Kudu Checks\\nOpen UIs from masters and tablets from internal firefox from edge nodes. Firefox launced with your personal Exxx account.\\nLogs from kudu logs on every node:\\n`tail -f /var/log/kudu/kudu-tserver.INFO` checks that the number will be equal to this number => `ls /u12/kudu/tablet/data/data/ | grep metadata | wc -l`, when the synchronization is successfully completed.\\n#### Start flows\\nStart ibank from edge Node as PRODREST\\n```\\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\\n```\\nStart ibank visible from edge Node as PRODREST\\n```\\n/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno_VISIBLE_STABLE.sh\\n```\\nStart online from edge Node as PRODREST\\n```\\n/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\\n```\\nSimilarly from a DR edge node as DEVREST:\\nStart ibank\\n```\\n/opt/ingestion/DEVREST/ibank/spark/submit/submitmnoSparkTopology_stream_cluster_mno.sh\\n```\\nStart online\\n```\\n/opt/ingestion/DEVREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno.sh\\n```\\n### Applications checks\\nWhen all Kudu Tablets are synchronized and **all flows are on RUNNING status** then:\\nas user PRODREST from an edge node:\\n```\\nimpala-shell xr1edge.mno.gr -k -ssl\\n```\\nExecute the following query:\\n```\\nselect max(timestamp) as time, 'ibank' as application from prod_trlog_ibank.service_audit_stream union select max(timestamp) as time, 'online' as application from prod_trlog_online.service_audit_stream;\\n```"
        },
        {
            "question": "How do you perform a controlled failover from the PR site to DR in the BigStreamer platform?",
            "answer": "1. Gracefully stop Spark streaming topologies by creating shutdown markers in HDFS:\n```bash\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\n```\n2. Stop batch jobs by commenting crontab entries and checking active processes.\n3. Start Wildfly instances on DR edge nodes.\n4. Ask networking to reroute Load Balancer traffic.\n5. Start UC4 agent on DR site:\n```bash\nsystemctl start uc4agent\n```",
            "category": "Application Functionality & Flow",
            "files": "failover.md",
            "context": "---\\ntitle: BigStreamer PR-DR Failover Procedure\\ndescription: Step-by-step failover procedure for BigStreamer cluster environments from production (PR) to disaster recovery (DR) site, including stopping streaming/batch jobs, migrating UC4 agents, switching Wildfly traffic, and updating external flows.\\ntags:\\n  - failover\\n  - dr\\n  - disaster-recovery\\n  - uc4\\n  - spark\\n  - streaming\\n  - batch\\n  - wildfly\\n  - bigstreamer\\n  - hdfs\\n  - yarn\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  clusters:\\n    - production\\n    - disaster recovery\\n  services:\\n    - Spark Streaming\\n    - Wildfly\\n    - UC4\\n    - HDFS\\n    - Yarn\\n  users:\\n    - PRODREST\\n    - DEVREST\\n    - PRODUSER\\n  systems:\\n    - dr1edge01\\n    - pr1edge01\\n    - edge nodes\\n    - cluster load balancer\\n---\\n# Failover\\n## Scope\\nIn case that the active site faces multiple issues that cannot be resolved in a small amount of time, we need to failover applications and procedures to the standby one. \\n## Setup\\nTwo symmetrical clusters have been setup named production (PR) and disaster (DR). Streaming and batch procedures are running in both sites. External traffic and UC4 flows however are only active in one of them. \\n## Procedure\\n### Stop Spark Streaming Topologies (PROD & DEV)\\nThis section describes how to gracefully shut down Spark Streaming topologies on the currently active site (PROD or DR), by disabling crontab restarts and creating shutdown markers in HDFS.\\n1. Stop production IBank, Online Spark topologies:\\n- Login with your personal account at `dr1edge01` or `pr1edge01`, based on the site that will be stopped.\\n- Switch user to `PRODREST`.\\n- Comment lines in crontab that run `/opt/ingestion/PRODREST/common/scripts/restart_topology_STABLE.sh` and `/opt/ingestion/PRODREST/common/scripts/restart_visible_topology.sh`.\\n- Create `SHUTDOWN` markers for the Spark topologies.\\n```bash\\n[PRODREST@Xr1edge01]# touch SHUTDOWN\\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown_marker/\\n[PRODREST@Xr1edge01]# hdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\\n```\\n- Wait for 5 minutes and check that the above applications are no longer running.\\n```bash\\n[PRODREST@Xr1edge01]# yarn application -list | grep PRODUSER\\n```\\n1. Stop development IBank, Online Spark topologies:\\n- Login with your personal account at `dr1edge01`. **This is done only on DR site**\\n- Switch user to `DEVREST`.\\n- Comment line in crontab that run `/opt/ingestion/DEVREST/common/scripts/restart_topology.sh`.\\n- Create `SHUTDOWN` markers for the Spark topologies. \\n```bash\\n[DEVREST@dr1edge01]# touch SHUTDOWN\\n[DEVREST@dr1edge01]# hdfs dfs -DEV_IBank_Ingest/topology_shutdown_marker/\\n[DEVREST@dr1edge01]# hdfs dfs -put SHUTDOWN /user/DEVREST/service/DEV_Online_Ingest/topology_shutdown_marker/\\n```\\n- Wait for 5 minutes and check that the above applications are no longer running.\\n``` bash\\n[DEVREST@dr1edge01]# yarn application -list | grep DEVREST\\n```\\n### Stop Batch Jobs (PROD & DEV)\\nThis section explains how to disable hourly and daily batch jobs for IBank and Online applications in both production and development environments, by commenting crontab lines and checking for active processes.\\n1. Disable daily and hourly IBank production batch jobs\\n- Login with your personal account at `dr1edge01` or `pr1edge01`, based on the site that will be stopped.\\n- Switch user to `PRODREST`.\\n- Comment lines in crontab that run `/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh` and `/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh`.\\n- Check that batch jobs are not already running.\\n```bash\\n[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh'\\n[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh'\\n```\\n- If they are already running wait for them to stop.\\n2. Disable daily and hourly Online production batch jobs:\\n- Login with your personal account at `dr1edge01` or `pr1edge01`, based on the site that will be stopped.\\n- Switch user to `PRODREST`.\\n- Comment lines in crontab that run `/opt/ingestion/PRODREST/common/scripts/online_daily_batch_jobs_STABLE.sh` and `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh`.\\n- Check that batch job is not already running.\\n```bash\\n[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/common/scripts/online_daily_batch_jobs_STABLE.sh'\\n[PRODREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_hourly_STABLE.sh'\\n```\\n- If they are already running wait for them to stop.\\n3. Disable daily IBank, Online development batch jobs:\\n- Login with your personal account at `dr1edge01`. **This is done only on DR site**\\n- Switch user to `DEVREST`.\\n- Comment lines that run `/opt/ingestion/DEVREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs.sh` in crontab.\\n- Check that batch jobs are not already running.\\n```bash\\n[DEVREST@Xr1edge01]# ps -ef | grep '/opt/ingestion/DEVREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs.sh'\\n```\\n- If they are already running wait for them to stop.\\n### Migrate Wildfly Traffic to DR Site\\nThis section covers how to shift Wildfly application traffic (IBank and Online) from the active to the standby site, by launching Wildfly on the DR edge nodes and coordinating with network administrators for load balancer changes.\\n1. Start `prodrestib` Wildfly instances at both edge nodes of the other site using this [procedure](manage_wildfly.md#start-a-wildfly-instance-prodrestib).\\n2. Start `prodreston` Wildfly instances at both edge nodes of the other site using this [procedure](manage_wildfly.md#start-a-wildfly-instance-prodreston).\\n3. Ask for a mno Network administrator to make a call.\\n4. Ask them to enable the new servers (mention the Loadbalancer IPs and the IP you want them to enable as explained [here](manage_wildfly.md#consolidated-network-information)).\\n5. Check logs for both Wildfly instances at both servers to ensure everything works.\\n6. When you are certain everything is OK, ask the mno Network administrators to disable the prexisting servers (mention the Loadbalancer IPs and the IP you want them to disable).\\n7. From the access logs of the prexisting Wildfly instances check that no traffic is received. \\n8. Stop these Wildfly instances as described in the procedures [here](manage_wildfly.md#stop-a-wildfly-instance-prodrestib) and [here](manage_wildfly.md#stop-a-wildfly-instance-prodreston).\\n### Migrate UC4 Agent and External Trigger Handling\\nThis section outlines the process of moving the UC4 job scheduler and external trigger file creation logic from the primary site to the DR site, including enabling UC4 agents, updating job scripts, and ensuring data warehouse monitoring continues without interruption.\\n1. Login to the edge servers of the active and passive site using your personal account and become `root`.\\n2. Stop UC4 agent at the edge nodes of the active site.\\n```bash\\nsystemctl stop uc4agent\\n```\\n3. Start service for UC4 agent at the edge servers of the passive site.\\n```bash\\nsystemctl start uc4agent\\n```\\n4. Add entries for last successful execution of IBank DataWarehouse at the edge servers of the passive site.\\n```bash\\nsudo -u PRODUSER /opt/ingestion/PRODUSER/datawarehouse-ibank/insert_rows_dwh_monitoring.sh <date> \\n# Use the previous day's date in YYYYMMdd format.\\n# If today is Sunday or Monday, use the date of the last Friday instead.\\n```\\n5. Migrate the creation of trigger files for external jobs\\n- On the active site:\\n```bash\\nvi /opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh\\n# Comment the followin lines along with the assosiated checks\\n# touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY\\n# touch /opt/applications/landing_zone/PRODUSER/triggers/IBANK_SA_`date '+%Y%m%d'`.READY\\n```  \\n- On the passive site:\\n```bash\\nvi /opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh\\n# Uncomment the followin lines along with the assosiated checks\\n# touch /home/bank_central_mno_gr/datawarehouse_status/IBANK_SA_`date '+%Y%m%d'`.READY\\n# touch /opt/applications/landing_zone/PRODUSER/triggers/IBANK_SA_`date '+%Y%m%d'`.READY\\n```\\n### Revert Failover to PR\\n>To revert the failover and restore traffic back to PR, repeat the above steps in reverse order, starting by stopping all workloads and UC4 agents in DR and reactivating them in PR."
        },
        {
            "question": "How do you manage Kafka MirrorMaker offsets to avoid replay in Spark streaming topologies?",
            "answer": "1. Stop MirrorMaker instances on PR and DR sites via Cloudera Manager.\n2. Commit offsets using Kafka CLI:\n```bash\nkinit kafka@BDAP.mno.GR\nkafka-consumer-groups --bootstrap-server <broker> --command-config group.properties --group <group> --all-topics --reset-offsets --to-datetime $DATETIME --execute\n```\n3. Restart MirrorMaker instances once offsets are committed.",
            "category": "Application Functionality & Flow",
            "files": "manage_mirrormaker.md",
            "context": "---\\ntitle: Kafka MirrorMaker Offset Management Procedure\\ndescription: Step-by-step instructions for stopping MirrorMakers, committing consumer group offsets, and restarting MirrorMakers on PR and DR Kafka clusters to avoid offset resets and message replay in Spark streaming topologies.\\ntags:\\n  - kafka\\n  - mirrormaker\\n  - consumer-groups\\n  - offsets\\n  - cloudera\\n  - spark-streaming\\n  - hdfs\\n  - kerberos\\n  - bigstreamer\\n  - kafka-admin\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  kafka_clusters:\\n    - PRBDA\\n    - DRBDA\\n  consumer_groups:\\n    - mir-trlog-ingest-stream-con-001\\n    - mir-trlog-ingest-stream-con-002\\n  kafka_nodes:\\n    - pr1node01\\n    - pr1node02\\n    - pr1node04\\n    - pr1node05\\n    - pr1node06\\n    - dr1node01\\n    - dr1node02\\n    - dr1node04\\n    - dr1node05\\n    - dr1node06\\n  kerberos_principals:\\n    - kafka@BDAP.mno.GR\\n    - kafka@BDAD.mno.GR\\n---\\n# Manage Kafka MirrorMaker\\nThis guide documents how to safely commit Kafka consumer group offsets in BigStreamer environments where Kafka MirrorMaker is used. It avoids offset resets by controlling the stop/commit/start sequence of MirrorMakers on PR and DR Kafka clusters using Cloudera Manager, Kerberos-authenticated CLI tools, and timestamp-based offset commits.\\n## Scope\\nKafka MirrorMaker has been configured with auto.offsets.reset=false. This means that the MirrorMaker consumers never commit the offsets.\\nIf the MirrorMakers' consumer group goes inactive(both MirrorMakers are offline at the same time), their consumer group will reset to the earliest offset available.\\nThis means that the last week will be mirrored to the final topic and will be replayed by the Spark streaming topology.\\nThis document describes how to commit the offsets for the MirrorMaker consumer groups, in order to avoid this issue.\\n## Setup\\n1. MirrorMakers on nodes pr1node01 and pr1node04:\\n- Replicate the traffic from the **Primary Site Mirror Topics** to the **Primary Site Final Topics**.\\n- Replicate Production Topics for both Internet Banking and Online Applications.\\n- Use **mir-trlog-ingest-stream-con-002** consumer group.\\n- Offsets are committed to the **Primary Site Kafka cluster**.\\n2. MirrorMakers on nodes pr1node05 and pr1node06:\\n- Replicate the traffic from the **Disaster Site Mirror Topics** to the **Primary Site Final Topics**.\\n- Replicate Production Topics for both Internet Banking and Online Applications.\\n- Use **mir-trlog-ingest-stream-con-001** consumer group.\\n- Offsets are committed to the **Disaster Site Kafka cluster**.\\n3. MirrorMakers on nodes dr1node01 and dr1node04:\\n- Replicate the traffic from the **Disaster Site Mirror Topics** to the **Disaster Site Final Topics**.\\n- Replicate Production Topics for both Internet Banking and Online Applications.\\n- Use **mir-trlog-ingest-stream-con-002** consumer group.\\n- Offsets are committed to the **Disaster Site Kafka cluster**.\\n4. MirrorMakers on nodes dr1node05 and dr1node06:\\n- Replicate the traffic from the **Primary Site Mirror Topics** to the **Disaster Site Final Topics**.\\n- Replicate Production Topics for both Internet Banking and Online Applications.\\n- Use **mir-trlog-ingest-stream-con-001** consumer group.\\n- Offsets are committed to the **Primary Site Kafka cluster**.\\n## Procedure\\n### Stop All Kafka MirrorMakers Affecting PR Site\\n1. Stop Primary Site MirrorMakers:\\nFrom the Primary Site Cloudera Manager with a user that has administrator privileges:\\n- PRBDA > Kafka > Instances\\n- Select the MirrorMakers running on nodes pr1node01,pr1node04,pr1node05 and pr1node06\\n- Stop\\n2. Stop Disaster Site MirrorMakers:\\nFrom the Disaster Site Cloudera Manager with a user that has administrator privileges:\\n- DRBDA > Kafka > Instances\\n- Select the MirrorMakers running on nodes dr1node05 and dr1node06\\n- Stop\\n### Stop All Kafka MirrorMakers Affecting DR Site\\n1. Stop Primary Site MirrorMakers:\\nFrom the Primary Site Cloudera Manager with a user that has administrator privileges:\\n- DRBDA > Kafka > Instances\\n- Select the MirrorMakers running on nodes dr1node01,dr1node04,dr1node05 and dr1node06\\n- Stop\\n2. Stop Disaster Site MirrorMakers:\\nFrom the Disaster Site Cloudera Manager with a user that has administrator privileges:\\n- PRBDA > Kafka > Instances\\n- Select the MirrorMakers running on nodes pr1node05 and pr1node06\\n- Stop\\n### Commit Consumer Group Offsets on PR Kafka Cluster\\nThe following steps can be performed at any node of the Primary Site cluster. The consumer groups need to be **inactive** for these action to be performed.\\n1. Create a file named group.properties:\\n```conf\\nsecurity.protocol=SASL_SSL\\nsasl.kerberos.service.name=kafka\\n```\\n2. Create a file named jaas.conf:\\n```conf\\nClient {\\n    com.sun.security.auth.module.Krb5LoginModule required\\n    useKeyTab=false\\n    useTicketCache=true\\n    doNotPrompt=true\\n    principal=\\\"kafka@BDAP.mno.GR\\\";\\n};\\nKafkaClient {\\n    com.sun.security.auth.module.Krb5LoginModule required\\n    useKeyTab=false\\n    useTicketCache=true\\n    doNotPrompt=true\\n    principal=\\\"kafka@BDAP.mno.GR\\\"\\n    service=\\\"kafka\\\";\\n};\\n```\\n3. Log in to kerberos as the **_kafka@BDAP.mno.GR_** principal and configure security:\\n```bash\\nkinit kafka@BDAP.mno.GR\\nexport KAFKA_JVM_PERFORMANCE_OPTS=\\\"-Djava.security.auth.login.config=./jaas.conf\\\"\\n```\\n4. Commit the offsets for all relevant consumer groups:\\n```bash\\nexport DATETIME=1970-01-01T00:00:00.000Z #UTC time. See ndefs\\nkafka-consumer-groups --bootstrap-server pr1node01.mno.gr:9093,pr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --reset-offsets --to-datetime $DATETIME --execute\\nkafka-consumer-groups --bootstrap-server pr1node01.mno.gr:9093,pr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\\n```\\n### Commit Consumer Group Offsets on DR Kafka Cluster\\nThe following steps can be performed at any node of the Disaster Site cluster. The consumer groups need to be **inactive** for these action to be performed.\\n1. Create a file named group.properties:\\n```conf\\nsecurity.protocol=SASL_SSL\\nsasl.kerberos.service.name=kafka\\n```\\n2. Create a file named jaas.conf:\\n```conf\\nClient {\\n    com.sun.security.auth.module.Krb5LoginModule required\\n    useKeyTab=false\\n    useTicketCache=true\\n    doNotPrompt=true\\n    principal=\\\"kafka@BDAD.mno.GR\\\";\\n};\\nKafkaClient {\\n    com.sun.security.auth.module.Krb5LoginModule required\\n    useKeyTab=false\\n    useTicketCache=true\\n    doNotPrompt=true\\n    principal=\\\"kafka@BDAD.mno.GR\\\"\\n    service=\\\"kafka\\\";\\n};\\n```\\n3. Log in to kerberos as the **_kafka@BDAD.mno.GR_** principal and configure security:\\n```bash\\nkinit kafka@BDAD.mno.GR\\nexport KAFKA_JVM_PERFORMANCE_OPTS=\\\"-Djava.security.auth.login.config=./jaas.conf\\\"\\n```\\n4. Commit the offsets for all relevant consumer groups:\\n```bash\\nexport DATETIME=1970-01-01T00:00:00.000Z #UTC time. See ndefs\\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --reset-offsets --to-datetime $DATETIME --execute\\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\\nkafka-consumer-groups --bootstrap-server pr1node01.mno.gr:9093,pr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-001 --all-topics --reset-offsets --to-datetime $DATETIME --execute\\n```\\n### Restart MirrorMakers Serving PR Site\\n1. Start Primary Site MirrorMakers:\\nFrom the Primary Site Cloudera Manager with a user that has administrator privileges:\\n- PRBDA > Kafka > Instances\\n- Select the MirrorMakers running on nodes pr1node01,pr1node04,pr1node05 and pr1node06\\n- Start\\nAll messages should be consumed in about one to two minutes.\\n2. Start Disaster Site MirrorMakers:\\nFrom the Disaster Site Cloudera Manager with a user that has administrator privileges:\\n- DRBDA > Kafka > Instances\\n- Select the MirrorMakers running on nodes dr1node05 and dr1node06\\n- Start\\nWait for traffic on all topics to get back to normal values before any changes.\\n### Restart MirrorMakers Serving DR Site\\n1. Start Primary Site MirrorMakers:\\nFrom the Disaster Site Cloudera Manager with a user that has administrator privileges:\\n- DRBDA > Kafka > Instances\\n- Select the MirrorMakers running on nodes dr1node01,dr1node04,dr1node05 and dr1node06\\n- Start\\nAll messages should be consumed in about one to two minutes.\\n2. Start Disaster Site MirrorMakers:\\nFrom the Primary Site Cloudera Manager with a user that has administrator privileges:\\n- PRBDA > Kafka > Instances\\n- Select the MirrorMakers running on nodes pr1node05 and pr1node06\\n- Start\\nWait for traffic on all topics to get back to normal values before any changes.\\n## Ndefs\\n- The result from the following queries can be useful during startup:\\n```sql\\nSELECT min(`timestamp`),max(`timestamp`) FROM prod_trlog_ibank.service_audit_stream\\nSELECT min(`timestamp`),max(`timestamp`) FROM prod_trlog_online.service_audit_stream\\n```\\n- Consider committing offsets at a time 5 minutes prior to max timestamp\\n- Time should be in UTC e.g. 2019-09-02T12:30:00.000Z = 2019-09-02T15:30:00.000 EEST\\n- The option _--all-topics_ commits the offsets **only** for the topics this consumer group has ever subscribed and not all the topics of the Kafka cluster\\n- These commands are only for consumers that use the new API (version 0.10 and later)\\n- The following commands can be useful:\\n```bash\\nexport DATETIME=1970-01-01T00:00:00.000Z\\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --describe # List offsets\\nkafka-consumer-groups --bootstrap-server dr1node01.mno.gr:9093,dr1node02.mno.gr:9093 --command-config group.properties --group mir-trlog-ingest-stream-con-002 --all-topics --reset-offsets --to-datetime $DATETIME # Dry run\\n```"
        },
        {
            "question": "How can you enable and apply HBase read and write quotas at the namespace level in Cloudera Manager?",
            "answer": "1. Enable quotas via safety valve in Cloudera Manager:\n```\nName: hbase.quota.enabled\nValue: true\n```\n2. SSH into a node and authenticate:\n```bash\nkinit -kt hbase.keytab `hostname`\n```\n3. Set read or write quotas:\n```bash\nset_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => 'namespace', LIMIT => '20req/sec'\n```",
            "category": "Data Management & Query Execution",
            "files": "manage_hbase_quotas.md",
            "context": "---\\ntitle: Manage HBase Quotas on BigStreamer\\ndescription: Procedure for enabling, setting, and removing HBase namespace-level read and write quotas in a Cloudera-managed environment on BigStreamer using Cloudera Manager and HBase shell.\\ntags:\\n  - hbase\\n  - quotas\\n  - cloudera\\n  - throttling\\n  - hbase-shell\\n  - namespace\\n  - read-quota\\n  - write-quota\\n  - bigstreamer\\n  - cm-safety-valve\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  components:\\n    - HBase\\n    - Cloudera Manager\\n  systems:\\n    - edge nodes\\n  commands:\\n    - hbase shell\\n    - kinit\\n    - Cloudera Safety Valve\\n---\\n# Manage HBase Quotas\\nThis document describes how to manage HBase quotas in the BigStreamer environment. It explains how to enable HBase throttling via Cloudera Manager, configure namespace-specific read and write request limits using the HBase shell, and cleanly remove quotas when no longer needed. Steps include using kinit, navigating HBase processes, and verifying changes through list_quotas.\\n## Step 1: Enable Global HBase Quotas via Cloudera Manager\\n1. Go to ```Cloudera Manager => HBase => Configuration => HBase Service Advanced configuration Snippet (Safety Valve) for hbase-site.xml```\\n2. Add the following configuration:\\n```\\nName: hbase.quota.enabled\\nValue: true\\nDescription: enable hbase quotas\\n```\\n3. Restart HBase service\\n## Step 2: Set Namespace-Level HBase Quotas\\n1. ssh to an edge node\\n2. kinit as hbase\\n```bash\\ncd /var/run/cloudera-scm-agent/processes\\nls \\u2013ltr HBASE\\ncd <latest hbase process folder>\\nkinit -kt hbase.keytab `hostname`\\n```\\n3. Get list of namespaces\\n```bash\\nhbase shell\\nlist_namespace\\n```\\n4. Set throttle READ quotas \\n```bash\\nhbase shell\\nset_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => 'namespace', LIMIT => 'Xreq/sec'\\n```\\n5. Set throttle WRITE quotas\\n```bash\\nhbase shell\\nset_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, NAMESPACE => 'namespace', LIMIT => 'Xreq/sec'\\n```\\n6. Show all quotas\\n```bash\\nhbase shell\\nlist_quotas\\n```\\n## Step 3: Remove Namespace-Level Quotas\\n1. ssh to an edge node\\n2. kinit as hbase\\n```bash\\ncd /var/run/cloudera-scm-agent/processes\\nls \\u2013ltr HBASE\\ncd <latest hbase process folder>\\nkinit -kt hbase.keytab `hostname`\\n```\\n3. Get list of namespaces and list of quotas already set\\n```bash\\nhbase shell\\nlist_namespace\\nlist_quotas\\n```\\n4. Delete throttle quotas\\n```bash\\nhbase shell\\nset_quota TYPE => THROTTLE, NAMESPACE => 'namespace', LIMIT => NONE\\n```\\n5. Verify that quotas have been removed\\n```bash\\nhbase shell\\nlist_quotas\\n```"
        },
        {
            "question": "What are the steps for upgrading Oracle Java 1.8 on BigStreamer edge nodes?",
            "answer": "1. Create a backup of the current Java installation:\n```bash\ncp -rap /usr/java/jdk1.8.0_<old> /usr/java/jdk1.8.0_<old>.bak\n```\n2. Use YUM to install updated version:\n```bash\nyum clean all\nyum update java-1.8\n```\n3. Copy certificates and configure alternatives:\n```bash\ncp jssecacerts to new path\nupdate-alternatives --config java\njava -version\n```",
            "category": "Infrastructure & Deployment",
            "files": "java_upgrade.md",
            "context": "---\\ntitle: Oracle Java 1.8 Minor Upgrade on Edge Nodes\\ndescription: Procedure for upgrading Oracle Java 1.8 to a newer minor version on BigStreamer edge nodes, including local RPM repository setup, edge node preparation, execution, certificate handling, update-alternatives configuration, and rollback instructions.\\ntags:\\n  - java\\n  - oracle-java\\n  - upgrade\\n  - edge-nodes\\n  - yum\\n  - rpm\\n  - certificates\\n  - update-alternatives\\n  - rollback\\n  - cloudera\\nlast_updated: 2025-05-01\\nauthor: ilpap\\ncontext:\\n  environment: BigStreamer\\n  systems:\\n    - pr1edge01\\n    - pr1edge02\\n    - dr1edge01\\n    - dr1edge02\\n    - pr1node01\\n  tools:\\n    - Oracle Java 8\\n    - YUM\\n    - update-alternatives\\n    - Wildfly\\n    - jssecacerts\\n  repositories:\\n    - /var/www/html/oracle_java/Packages\\n---\\n# Oracle Java 1.8 Upgrade Procedure on Edge Nodes\\nThis document describes the controlled upgrade of Oracle Java 1.8 minor versions on BigStreamer edge nodes. It covers the creation and maintenance of a local RPM repository on pr1node01, edge node backup and update procedures, handling of security certificates, switching Java versions using update-alternatives, and guidance for validating application behavior post-upgrade. Rollback steps are also provided.\\nThis document outlines the upgrade process of minor java version for Oracle Java 1.8\\non mno's edge nodes. All procedures pertain to PR and DR edge nodes, except the RPM repository\\ncreation which is performed on pr1node1:\\n- pr1edge01\\n- pr1edge02\\n- dr1edge01\\n- dr1edge02\\n## Step 1: Create Local RPM Repository\\nThis step only needs to be performed once as all subsequent RPM's will be placed inside this\\nrepository. SSH into **p1node01** and as root create the repository directories:\\n```bash\\n$ ssh Exxxx@pr1node01\\n$ sudo -i\\n# mkdir -p /var/www/html/oracle_java/Packages\\n```\\nDownload the desired RPMs from [Oracle Java SE Archive Downloads](https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html), place them inside\\n`/var/www/html/oracle_java/Packages` and create the repository:\\n```bash\\n# cd /var/www/html/oracle_java\\n# createrepo .\\n```\\nSSH into one of the edge nodes, create the corresponding yum repo file and **scp** it into\\nall other edge nodes:\\n```bash\\n$ ssh Exxx@pr1edge01\\n$ sudo -i\\n# vi /etc/yum.repos.d/oracle_java.repo\\n[oracle_java]\\nname = oracle_java\\nbaseurl =  http://p1node01.mno.gr/oracle_java\\nenabled = 1\\ngpgcheck = 0\\n# scp /etc/yum.repos.d/oracle_java.repo XXXedgeXX:/etc/yum.repos.d/\\n```\\nFinally on each edge node install the above packages:\\n```bash\\n# yum clean all\\n# yum install jdk-1.8\\n```\\n## Step 2: Update the Repository with New RPMs\\nDownload the desired version of Oracle Java 8 from\\n[Oracle Java SE Archive Downloads](https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html) or\\n[Oracle Java SE Downloads](https://www.oracle.com/java/technologies/downloads/) and\\nplace the RPMs inside `/var/www/html/oracle_java/Packages` of **pr1node01**. Login to\\n**pr1node01** and update the repository with the new packages:\\n```bash\\n$ ssh Exxxx@pr1node01\\n$ sudo -i\\n# cd /var/www/html/oracle_java\\n# createrepo --update .\\n```\\n## Step 3: Upgrade Java on Edge Hosts\\n### Preparation\\nBefore upgrading the edge nodes, their resources must be moved to other nodes and a backup\\nof the old java be made. Login to each edge node:\\n```bash\\n$ ssh Exxxx@XXXedgeXX\\n$ sudo -i\\n# cp -rap /usr/java/jdk1.8.0_<old version>-amd64/  /usr/java/jdk1.8.0_<old version>-amd64.bak/\\n```\\nAnd follow the procedures described in the **Switchover of Cluster Resources** chapter\\nof the **Security Vulnerabilities** MOP [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/21324_security_vulnerabilities_v3.docx).\\n### Execution\\nInside each edge node, update the java package using **YUM**:\\n```bash\\n# yum clean all\\n# yum update java-1.8\\n```\\nCopy the old certificates into the new installation directory and run the update alternatives\\ntool where you input the new version when prompted:\\n```bash\\n# cp -ap /usr/java/jdk1.8.0_<old version>-amd64.bak/jre/lib/security/jssecacerts \\\\\\n/usr/java/jdk1.8.0_<new version>-amd64/jre/lib/security/\\n# update alternatives --config java * javac\\n# java -version\\n```\\nIf everything is OK unstandby the node and check each wildfly instance's access and\\nserver logs for the following:\\n- `/var/log/wildfly/*/server.log`: There are no undeployed WARs\\n- `/var/log/wildfly/*/access.log`: Everything is reporting HTTP 200 status\\nDetailed wildfly information and management instructions can be found\\n[here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/procedures/manage_wildfly.md).\\n## Step 4: Rollback to Previous Java Version\\nLogin to each edge node and downgrade using the update-alternatives and inputting the previous version:\\n```bash\\n$ ssh Exxxx@XXXedgeXX\\n$ sudo -i\\n# update alternatives --config java * javac\\n# java -version\\n```"
        }
    ]
}