{
    "20230301-IM2095156.md": {
        "Description": "",
        "Actions Taken": "1. Login to Cloudera UI for the PR Site\n2. Cloudera > Yarn\n3. Upon inspection we noticed that the alert was about pr1node01 (Node Manager) and not pr1node03 (JobHistory Server)\n4. Ssh pr1node01 and inspect logs at /var/loh/hadoop-yarn. We could not find the root cause from logs\n5. Restart the Node Manager role for the specific node. After the restart the alert disappeared.\n6. During further investigation, from Cloudera UI we saw that prior to ```Process Status``` alert there was a ```NODE_MANAGER_LOG_FREE_SPACE``` alert\n7. From pr1node01 as root `df -h /`. The usage of `/` was at 98% at that time\n8. Upon inspection we noticed that the krb5kdc logs had increased over the last months peaking the monthly log file to ~80G.\n9. We proceeded to the removal of krb5kdc log files for 2022.\n10. As a permanent solution, we implemented changes to retention policy for krb5kdc logs. Specifically, we changed the rotation to weekly from monthly and the storage to 7 old logs from 12 logs files that it was prior the change.  This change was implemented at pr1node02, dr1node01 and dr1node02 as well.\n\n\n    ![logrotate_krb5kdc](.media/IM2095156/IM2095156_logrotate_krb5kdc.PNG)",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "```\n09/03/23 15:47:41 Europe/Eastern (MASTROKOSTA MARIA):\nGood evening,\n\nFollowing the investigation, we have changed the retention for krb5kdc logs. Specifically, we have set the rotation to be weekly instead of monthly and to keep 7 log files. Note that the monthly krb5kdc log file had reached 80G.\n\nPlease let us know if we can proceed with closing the ticket.\n\nThank you\n\n01/03/23 07:16:41 Europe/Eastern (MASTROKOSTA MARIA):\nGood morning,\n\nThere was a malfunction in the yarn node manager since 3.42, resulting in the ibank and online streaming topologies falling as recorded in ticket SD2157107.\n\nWe proceeded to restart at 4:53 to get it back up. During the restart, the online merge batch crashed, which was resubmitted (related ticket SD2157111).\n\nAt this time, yarn and the flows are running normally.\n\nFrom the investigation it appears that the root partition on pr1node01 had filled up, which was caused by the local kdc logs. We have proceeded to clean the corresponding log files and are investigating changes to the retention of the logs to avoid future problems.\n\nPlease let us know if you consider the workaround acceptable.\n\nThank you\n```",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20221027-IM2006951.md": {
        "Description": "```bash\nApplication: DWH_IBank\nJob Name: EXPORT\nComponent: CARD\nDate: 26/10/2022\nStatus: FAILED\nDescription: Code:6\n```",
        "Actions Taken": "1. Login to `https://dr1edge01.mno.gr:3000` with personal account and confirm that Datawarehouse Flows have failed from `Monitoring/Monitoring PR/DR` dashboard.\n2. All flows have failed with `Code: 6` which means that the control script has timed-out while monitoring the `EXTRACT` script. The `EXTRACT` step has 2 sub-steps: `Impala Insert` and `Sqoop Export`.\n3. Check logs\n\n    From `dr1edge01.mno.gr` with personal account:\n\n    ``` bash\n    less /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\n    ```\n\n    In this file for all flows that failed we see that the last log entry is the submission of the `Impala Insert` part of the `EXTRACT`, which was still running. This means that another query is hogging all resources for Impala and our flows are waiting to be executed.\n\n4. Login to Disaster Site Cloudera Manager `https://dr1edge01.mno.mno.gr:7183` and check for resource intensive Impala queries `Clusters > Impala > Queries`. The key resource here is memory as this is the only metric that can be defined in Resource Pools.\n\n   The query that created the problem was `COMPUTE STATS prod_trlog_ibank.service_audit`. Pictures are not included because Impala does not report statistics for the `COMPUTE STATS` queries, but given the size of the table and the time of execution it matched. This hypothesis was later confirmed when the same problem appeared on a later `COMPUTE STATS` execution.\n\n5. We informed the customer to re-run the failed jobs and proposed to stop computing statistics for that table as they did not impact our application.\n\n    ``` text\n    27/10/22 13:16:01 Europe/Eastern (POULAS GIORGOS):\n\tGood evening,\n\n\tPlease rerun the flow steps that encountered a problem.\n\n\tWe are continuing to investigate the root cause of the problem.\n\n    **Workaround**\n\n    27/10/22 13:32:44 Europe/Eastern (POULAS GIORGOS):\n    Following the previous answer, the compute statistics on the prod_trlog_ibank.service_audit table committed many resources on the Calatog Server and the query Coordinator (dr1node02), resulting in REFRESH/INVALIDATE METADATA operations experiencing long execution times and causing jobs to time out. After the compute statistics were completed, Impala released the resources and resumed.\n    \n    As we have communicated in the past, from our perspective the statistics of the table are not necessary. Please let us know if we can disable the production of statistics for this particular table.\n\n    **Resolved**\n    ```\n\n6. The changes for the statistics were implemented as part of [this ticket](obss/oss/sysadmin-group/mno/cloudera-cluster#180).",
        "Affected Systems": "Disaster Recovery Site Datawarehouse",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20220524-IM1868465.md": {
        "Description": "``` bash\nWe have the following failed messages on Grafana\n\napplication: iBank_Migration\njob_name: Historical\ncomponent: Impala_Insert\ndate: 23/05/2022\nstatus: Failed\ndescription:\nhost: pr1edge01.mno.gr\n\napplication: iBank_Migration\njob_name: Historical\ncomponent: JOB\ndate: 23/05/2022\nstatus: Failed\ndescription: Impala rcords are less than retrieved sqoop records\nhost: pr1edge01.mno.gr\n```",
        "Actions Taken": "Workaround Steps\n\n1. Check Grafana\n\n   Grafana -> LOCAL MONITOR/ Batch Jobs PR\n\n   ```bash\n   Historical | Sqoop_Import : SUCCESS \n   Historical | Impala_Insert : FAILED\n   ```\n\n2. Check Main script logs\n\n   ```bash\n   sudo su - PRODREST\n   crontab -l #find log file\n   less /var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log\n   ```\n\n   We saw the following:\n\n   ```bash\n   restval_sqoop_2=1\n   sqoop import failed again\n   ```\n\n   According to the log file, the first step of the Main Script failed with exit code 1.\n   More Info for the Steps here:[<https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#mssql-sqoop-import>]\n\n3. Check \"MSSQL Sqoop Import\" step logs:\n\n   ```bash\n   less /opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log\n   ```\n\n   We found out the the INSERT Impala query failed with error: Memory limit exceeded.\n\n4. Backup prod_trlog_ibank.historical_service_audit_raw_v2 files\n\n   ```bash\n   hdfs dfs -mkdir /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/20220524_import_historical_service_audit_raw_v2_dvoul\n\n   hdfs dfs -mv /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2/* /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/20220524_import_historical_service_audit_raw_v2_dvoul/\n   ```\n\n5. Run Main script with PRDREST user\n\n   ```bash\n   screen\n\n   /opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh >> /var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log\n   ```\n\nRoot Cause Analysis Steps\n\nNow that we are certain that we have an Impala OOM issue, we have to check Impala resources specifically.\n\n1. From Cloudera Manager select Impala Service and check the Out of Memory Impala Queries Chart\n\n   In our case we had a large number of OOM quries between 23:00 and 02:00\n\n2. Check for large queries in the corresponding time slot\n\n   Cloudera Manager -> Impala -> Queries\n\n   Select those that have a lot of Aggregate Peak Memory Usage from the filters on the left.",
        "Affected Systems": "Primary Site IBank Batch",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20201218-IM1389913.md": {
        "Description": "```\nRecords from older dates have been detected in prod_trlog_ibank.service_audit_stream.\n\ntime count(*)\n20201030 22.927.271\n20201110 5.947.186\n20201114 3.294.430\n20201116 3.090.276\n20201118 5.325.057\n20201124 3.465.507\n20201125 3.527.222\n20201201 2.405.322\n```",
        "Actions Taken": "1. The root cause was investigated by lmn\n```\nThe problem you are reporting only appears on the Disaster Site. It is related to the problems that the Kudu service was experiencing at that time.\nFrom the comparison of data with the Primary Site we see that the data has been passed correctly to service_audit.\nPlease confirm that you have the same picture of the problem and if we can proceed to delete the data from the service_audit_stream.\n```\nCustomer then wanted to transfer data from PR to DR.\n\n5. Connected  on MobaXterm via ssh to `dr1edge01` and changed user to `PRODREST` using sudo.\n7. Open `impala-shell` for DR site and check row count of `prod_trlog_ibank.service_audit`  for these specific dates.\n```\nimpala-shell -i dr1edge.mno,gr -k --ssl\nselect par_dt, count(*)  from prod_trlog_ibank.service_audit where par_dt in (20201105, 20201111, 20201120, 20201127, 20201202) group by 1 order by 1;\npar_dt count\n20201105 20189258\n20201111 18855105\n20201120 20212408\n20201127 36624142\n20201202 25042327\n```\nExecute the same query for PR site by opening the `impala-shell` for PR, to compare row counts.\n```\nimpala-shell -i pr1edge.mno,gr -k --ssl\n```\n\n8. Create temporary table at PR and insert the partitions we want to transfer to DR\n```\nimpala-shell -i pr1edge.mno,gr -k --ssl\n...\nCREATE TABLE prod_trlog_ibank.service_audit_temp LIKE prod_trlog_ibank.service_audit;\nset MAX_ROW_SIZE=100mb;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201105;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201111;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201120;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201127;\nINSERT INTO prod_trlog_ibank.service_audit_temp PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit WHERE par_dt = 20201202;\n```\n8. Create an empty temporary table at DR.\n```\nimpala-shell -i pr1edge.mno,gr -k --ssl\n...\nCREATE TABLE prod_trlog_ibank.service_audit_temp LIKE prod_trlog_ibank.service_audit;\n```\n\n9. Open Cloudera Manager `https://dr1edge01.mno.gr:7183` at DR and login with your `EXXXXX` account.\n10. Go to `Backup`>`Replication Schedules` from the top bar.\n11. Edit configuration of `one_off_replication` and change `Databases` field to point to the new temporary table at DR `Databases | prod_trlog_ibank | service_audit_temp `\n10. From `Actions` of `one_off_replication` execute `Dry Run`. Check logs of the execution. Sometimes there is an `Connection timed out` error. If that's the case, click on `Abort` and execute `Dry Run` once again.\n11. After `Dry Run` has finished, under `one_off_replication` there will be information on the number of databases, tables and partitions that are going to be transferred. In this case we had 1 database, 1 table and 400 partitions (each day has 40 subpartitions and we had 5 dates).\n11. From actions of `one_off_replication` click on `Run Now`.\n11. After it finishes, check that all rows have been transferred correctly. Open `impala-shell` for DR site and check row count of `prod_trlog_ibank.service_audit_temp`  for these specific dates.\n```\nimpala-shell -i dr1edge.mno,gr -k --ssl\nINVALIDATE METADATA prod_trlog_ibank.service_audit_temp;\nselect par_dt, count(*)  from prod_trlog_ibank.service_audit_temp where par_dt in (20201105, 20201111, 20201120, 20201127, 20201202) group by 1 order by 1;\npar_dt count\n20201105 20189258\n20201111 18855105\n20201120 20212408\n20201127 36624142\n20201202 25042327\n```\n\n12. Delete problematic partitions of `prod_trlog_ibank.service_audit` at DR.\n```\nimpala-shell -i dr1edge.mno,gr -k --ssl\n...\nALTER TABLE prod_trlog_ibank.service_audit DROP PARTITION (PAR_DT IN (20201105, 20201111, 20201120, 20201127, 20201202));\n```\n\n14. Insert data from temporary table.\n```\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201105;\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201111;\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201120;\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201127;\nINSERT INTO prod_trlog_ibank.service_audit PARTITION (par_dt, par_clun) SELECT * FROM prod_trlog_ibank.service_audit_temp WHERE par_dt = 20201202;\n```\n\n7. Open `impala-shell` for DR site and validate that row count of `prod_trlog_ibank.service_audit`  is the same as seen in PR.\n```\nimpala-shell -i dr1edge.mno,gr -k --ssl\nselect par_dt, count(*)  from prod_trlog_ibank.service_audit where par_dt in (20201105, 20201111, 20201120, 20201127, 20201202) group by 1 order by 1;\npar_dt count\n20201105 ...\n20201111 ...\n20201120 ...\n20201127 ...\n20201202 ...\n```",
        "Affected Systems": "Disaster Site IBANK",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "IM2010302-SD2070794.md": {
        "Description": "```\nService affected: Hosts\nBad Health issue for dr1edge01.mno.gr (Agent Status)\n```",
        "Actions Taken": "&nbsp;&nbsp; <a id=\"step-1\"></a> 1. Logged in to Cloudera Manager `https://dr1edge01.mno.gr:7183` with personal account in order to check the status of the host \"dr1edge01.mno.gr\".\n\n2. Checked `https://dr1edge01.mno.gr:7183/cmf/hardware/hosts` the host.\n\n3. Continued in Cloudera Manager on host \"dr1edge01.mno.gr\" and checked the disks.\n\n4. Logged in to \"dr1edge01.mno.gr\" node with personal account.\n\n&nbsp;&nbsp; <a id=\"step-5\"></a> 5. Executed the following command and disk usage check performed on the nodes.\n```bash\ndf -h\n```\n\n6. We saw that the usage in \"/var\" partition was 100%.\n\n7. We proceed to find wich directory has big enough size.\n```bash\nsudo du -sh /var/*\n```\nAs we noticed \"/var/lib/carbon\" directory was enormous, up to 199 G. This directory concerns **Graphite** application. <br/>\n\n8. We navigated to /var/lib/carbon/whisper/translog-api/spark\n```bash\ncd /var/lib/carbon/whisper/translog-api/spark\n```\n\nIn this directory we can delete without any confirmation all the \"DEV\" directories.\n\n```bash\n-bash-4.2$ sudo rm -rf IBank_IngestStream_DEV_mno IBank_MergeBatch_DEV_mno IBank_MergeBatch_DEV_mno_Hourly Online_IngestStream_DEV_mno Online_MergeBatch_DEV_mno Online_MergeBatch_DEV_mno_Hourly\n```\n\n9. Disk usage has been checked again as step [5](#step-5).\n\n10. Cloudera Manager has been checked to validate that the bad status host alert has been eliminated as step [1](#step-1).",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230329-IM2117067.md": {
        "Description": "Title: Batch Job Failed\nBody:\n```\nHello,\n\nToday 29/03 in Grafana application a failed Batch Job appeared.\n\nApplication : DWH_IBank\nJob_Name : Extract\nComponent: MAN_DATE\nDate: 28/03/23\nStatus: Failed\nDescription Code 1\n```\n\nInformation regarding the extract, logs etc. available [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-extract) \n\nInformation regarding the export, logs etc. available [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/datawarehouse-ibank.md#man-date-export) \n\n</b>Actions Taken:</b>\n\nWe tried to invoke the Extract MAN_DATE script but it failed multiple times. After investigating the logs of the executor using internal firefox and the logs of the DWH_Ibank_MAN_DATE script we saw the following :\n\n`Cannot insert dublicate key in object srcib.MandateDetails. The dublicate key value is (e5435435-4354254235-121nfdgd33)`\n\n\nThat means that the table already has records in it, so we have to drop this records or invoke the script with the `-f` option that gives the ability to trancate the table and then insert the records. \n\n```\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t manDate -f\n```\n\nAfter executing the extract script, we use the same `-f` to run the export script. The reason that we are running export is because in the above mentioned step we run the extract and trancated the table so the export is neccessary. \n\n```\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_export_details.sh -t manDate -f\n```\n\nThe issue has been resolved by executing the above two commands. The key in this case was to find in the logs that we have dublicate keys.",
        "Actions Taken": "",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20220707-IM1910783.md": {
        "Description": "```\nHBase Master Health\n\nMaster (dr1node06)\nHBase Regions In Transision Over Threshold\n\nRegion Server(dr1node05)\nUnexpected Exits\n```",
        "Actions Taken": "1. Login to `dr1edge01.mno.gr` with personal account and then from command line type `firefox`\n2. In firefox type: `https://dr1node06.mno.gr:16010/master-status`\n\nWe see that 7 RegionServers are in Transition\n\n3. Login to `dr1edge01.mno.gr` with personal account and then ssh to a node\n4. Move to the process folder:\n```bash\ncd /var/run/cloudera-scm-agent/process/\n```\n3. Find the latest hbase process and go to that folder. In our case is `12269-hbase-MASTER-togglebalanncer`. So move to that folder:\n```bash\ncd 12269-hbase-MASTER-togglebalanncer\n```\n4. Use the keytab you just found in that folder:\n```bash\nkinit -kt hbase.keytab hbase/`hostname`\n```\n5. Check the health of HBASE:\n```bash\nhbase hbck (On this command the status was not HEALTHY)\n```\n6. Remove the file that we found from above test\n```bash\nhdfs dfs rm hdfs://DRBDA-ns/hbase/.tmp/hbase-hbck.lock\n```\n7. After investigating logs at `/var/log/hbase/hbase-cmf-hbase-REGIONSERVER-dr1node01.mno.gr.log.out` for the time that the ticker occured we show the following error:\n```bash\n2022-07-07 19:30:07,874 WARN org.apache.hadoop.hdfs.DFSClient:Connectionfailure:Failedtoconnecttodrinode05.mno.gr/I\n999.999.999.999:50010 for file /hbase/data/PROD BANK/TAX _FREE_20220404/2c093lelcf6178a3548f7162a5a5965a/D/e2ea28cd22ad402394bd8\n6acb8002f58 for block BP-1157034308-999.999.999.999-1530642985707:blk_1164232343_90497946:0rg.apache.hadoop.hdfs.security.toke\n```\n\nAt that moment we realised that table `TAX _FREE_20220404` was removed but metadata weren't. So, let's remove them too!\n\n>**Ndef**: We have hbase 2.1 version and basic `hbase hbck` tool doesn't support basic options and flags like repair/fix. So we had to download HBCK2 from [here](https://jar-download.com/artifact-search/hbase-hbck2)\n\n8. Fix extra regions in hbase:meta region/table\n```bash\nhbase hbck -j hbase-hbck2-1.2.0.jar extraRegionsInMeta PROD_BANK:TAX_FREE_20220404 --fix\n```\n\n9. Get states of regions of PROD_BANK:TAX_FREE_20220404 table in hbase\n```bash\nhbase shell\n> scan 'hbase:meta',{FILTER=>\"PrefixFilter('PROD_BANK:TAX_FREE_20220404')\"}\n```\nOutput: No rows\n\n10. Remove table from hdfs:\n```bash\nhdfs dfs -rm -r hdfs://DRBDA-ns/hbase/data/PROD_BANK/TAX _FREE_20220404\nhdfs dfs -ls hdfs://DRBDA-ns/hbase/data/PROD_BANK/\n```\n\n11. Restart HBASE master and check health\n```bash\nhbase hbck\n```",
        "Affected Systems": "mno Bigstreamer DR1 HBASE",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20221005-SD2048346.md": {
        "Description": "```\nGood morning,\n\nthe submit script for the time deposits table (data warehouse) has crashed.\n\nwe will restart it via the scheduler and if it crashes again we will come back\n```",
        "Actions Taken": "1. Login to `dr1edge01.mno.gr` with personal account and then `sudo su - PRODUSER`\n2. We tried to re-run the script using the following command:\n```bash\n/opt/ingestion/PRODUSER/datawarehouse-ibank/extract_details/sched_extract_details.sh -t timeDeposit\n```\nBut we got the following error:\n\n**ERROR DESCRIPTION=ANOTHER_PROCESS_RUNNING**\n\n3. After investigation at impala queries and alert notification we realized that at `10/04/2022 8:29 AM` which was the time that the `extract` of `time_deposit` was running, the upsert that updates the status (SUCCEDED/RUNINNG/FAILED) of the job, failed due to hive metastore unavailability at that time. \n\n> Ndef: Hive metastore issue is discribed at **SD2046350**\n\n**Investigation Steps**\n\nWe found that with below steps:\n- From DR CM UI -> Impala -> Queries \n- In searh bar type: `statement rlike 'upsert into prod_trlog_ibank_analytical.*'` and click on `Search` to find the query\n\nLogin to dr1edge01 with your personal account and then `sudo su - PRODUSER`\n\nInvestigate impala tables with below commands:\n```bash\nimpala-shell -k -i dr1edge01.mno.gr --ssl\nSHOW CREATE TABLE prod_trlog_bank_analytical.dwh_monitoring;\nSELECT * FROM prod_trlog_bank_analytical.dwh_monitoring WHERE details_type='TIME_DEPOSIT' and\n'procedure'='EXTRACT' and par_dt > 20221002;\n```\n\nTherefore, at `10/05/2022` when the script executed again, the job crashed due to the fact that the value on the impala table was set to RUNNING. \n\nWe had to manually change he value of yesterday's job to `SUCCESS` using the following impala query.\n\nFirst, login to dr1edge01 with your presonal account, change to PRODUSER with `sudo su - PRODUSER` and login to impala-shell using following command:\n```bash\nimpala-shell -k -i dr1edge01.mno.gr --ssl\n```\n\nIn our case we run the following query:\n```bash\nUPSERT INTO prod_trlog_ibank_analytical.dwh_monitoring\n(details_type, procedure_par_dt,status.start_time,end_time,description) VALUES\n('TIME DEPOSIT','EXTRACT','20221003','SUCCESS','2022-10-04','08:32-42.000','2022-10-04','08-39:21.000',\")\n```\n\n> Ndef: In case you want to run the above query for a different job modify VALUES according the procedure_par_dt, details_type, etc\n\n4. Repeat **step 2** in order script to succeed or ask mno to do that.",
        "Affected Systems": "mno Bigstreamer",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20201014-IM1317401.md": {
        "Description": "```\nWe have the following alert msg on Grafana.\n[PR][IBANK] Query Average Response Time alert\n```",
        "Actions Taken": "1. Login to `https://dr1edge01.mno.gr:3000` with personal account\n2. Inspected `[PR][IBANK] Query Average Response Time alert` graph\n\n```\nI edited the graph temporarily to display the max response time instead of the mean value. Usually the problem affects 3-4 queries and is due to user actions in the PR site.\nThese problematic queries add bias to the mean time and create the alarm. By checking the max values, I saw that that was the case.\n```\n\n3. Login to pr1edge01.mno.gr/pr1edge02.mno.gr with personal account and check access logs under `/var/log/wildfly/prodrestib/access.log`\n4. Login to Primary Site Cloudera Manager `https://pr1edge01.mno.mno.gr:7183` and check that the cluster is in healthy status\n5. No action taken. The alarm will clear without the need for manual action.\n6. Our engineers were not active on the site, so I requested from mno to check for user activity",
        "Affected Systems": "Primary Site IBANK query",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20210430-IM1317401.md": {
        "Description": "```\nIt seems that the Data Warehouse jobs have not run, can you see this?\n\nselect * from prod_trlog_ibank.monitor_sched_jobs where par_dt=20210429\n\nIBank_Migration Historical JOB 20210429 SUCCESS 2021-04-30 02:00:01.000 2021-04-30 02:03:50.000 dr1edge01.mno.gr\n2 IBank_Migration Historical Sqoop_Import 20210429 SUCCESS 2021-04-30 02:00:01.000 2021-04-30 02:01:51.000 dr1edge01.mno.gr\n3 IBank_Migration Historical Impala_Insert 20210429 SUCCESS 2021-04-30 02:03:07.000 2021-04-30 02:03:50.000 dr1edge01.mno.gr\n4 IBank_Migration Historical to SA Impala_Insert 20210429 SUCCESS 2021-04-30 02:04:23.000 2021-04-30 02:06:21.000 dr1edge01.mno.gr\n5 IBank_Migration Historical to SA JOB 20210429 SUCCESS 2021-04-30 02:04:23.000 2021-04-30 02:06:21.000 dr1edge01.mno.gr\n6 IBank_Ingestion MergeBatch JOB 20210429 FAILED 2021-04-30 09:37:35.000 2021-04-30 09:37:35.000 dr1edge.mno.gr\n\nWe also see that the merge batch has also crashed.\n\n```",
        "Actions Taken": "1. Login to `https://dr1edge01.mno.gr:3000` with personal account\n2. Inspected `[PR][IBANK] Overview` graph\n3. Merge Batch job has FAILED\n4. MergeBatch job was not running : \"yarn application -list | grep -i merge | grep -v Hourly\"\n5. Yarn/Spark logs examined, foun that job was failed due to lack of RAM caused by large amount of Data\n6. MergeBatch job resubmited but failed again.\n7. Vi /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh\n8. Search for “coalesce” , Change/replace “-coalesce=$NUMBER_OF_EXECUTORS \\ “ , To : “-coalesce=96 \\ “\n9. Search for \"--spark.sql.shuffle.partitions=16  \\\"  to : \"--spark.sql.shuffle.partitions=96  \\\" \n10. As PRODREST user, re-run the /opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh cron script from the MERGE section onwards.",
        "Affected Systems": "DR Site IBANK",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20220620-SD1951890.md": {
        "Description": "```\nGood evening,\n\nToday 20/6/2022 at 12:40 pm the following message appeared in the cloudera system for dr1edge01:\n\nhdfs - DataNode(dr1node07) - Data Directory Status - Warning\n```",
        "Actions Taken": "After investigation we saw that the problem occurred due to disk issue on dr1node07.\n\nWe communicated with Oracle and disk replacement was scheduled.\n\n> Ndef that disk replacement perfomerd online so there was no downtime.\n\nAccording with to [this document](https://support.oracle.com/epmos/faces/DocumentDisplay?_afrLoop=134521948780510&parent=EXTERNAL_SEARCH&sourceId=REFERENCE&id=2642582.1&_afrvwxowMode=0&_adf.ctrl-state=150blaep6z_4) we perfomed the following steps. Feel free to read it extensively.\n\n\n\n1. Before proceed with the following procedure make sure that the partition is not used from any process with the following command:\n```bash\nlsof /u09\n```\nIf you get an active process you should stop it first and start it again after step 7.\n\nFor example, if there are processes running on YARN Node Manager then proceed with following the steps [here](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/208) about stopping impala daemon & YARN roles on the corresponding node, in more detail:\n\n- Connect from the corresponding drnode with `impala-shell -i dr1edge.mno.gr -k --ssl`\n- Give permissions from inside impala-shell to your user with `grant role icom_adm to group EXXXXX`\n- Run `:SHUTDOWN('FQDN')` from inside impala-shell\n\n2. Back up `efis1entry`:\n```bash\nls -l /opt/oracle/bda/compmon/efis1entry\nmv /opt/oracle/bda/compmon/efis1entry /opt/oracle/bda/compmon/efis1entry.BAK_`date +%d%b%Y%H%M%S`\n```\n3. Download \"45R.zip\" from above doc and and place in a temporary/staging location on the server where the disk will be or was replaced.\n4. Extract \"bdadiskutility\" into /opt/oracle/bda/bin/bdadiskutility and \"efis1entry\" into /opt/oracle/bda/compmon/efis1entry\n```bash\nunzip 45R.zip bdadiskutility -d /opt/oracle/bda/bin/\nunzip 45R.zip efis1entry -d /opt/oracle/bda/compmon/\nls -l /opt/oracle/bda/bin/bdadiskutility #Verify that it is extracted\nls -l /opt/oracle/bda/compmon/efis1entry #Verify that it is extracted\n```\n5. Set the permissions on both scripts and verify\n```bash\nchmod 0755 /opt/oracle/bda/bin/bdadiskutility\nchmod 0755 /opt/oracle/bda/compmon/efis1entry\nls -l /opt/oracle/bda/bin/bdadiskutility \nls -l /opt/oracle/bda/compmon/efis1entry \n```\n6. Confirm the latest bdadiskutility is in place\n```bash\nbdadiskutility -v #Must be 45R\n```\n7. Display a summary of all disk states\n```bash\nbdadiskutility -i\n```\n8. After the disk is replaced, and the slot and mount point of the replaced disk fully identified, configure the replaced disk. In our case is u01\n\n```bash\nbdadiskutility /u01\n```\nWith force option:\n\n```bash\nbdadiskutility -f /u01\n```\n\n9. Configure OS disk\n\n>Ndef: Perform this step only if the partition is mirrored. Otherwise skip this step. You can check that by running the following command:\n\nMake sure that you can see `/u09`\n```bash\nlsblk\n```\n\nAfter successfully configuring an OS disk with bdadiskutility confirm the  mirrored partitions are in an \"active sync\" state before rebooting the server.\n\nMonitor the state of the  mirrored partitions for the equivalent device paths for \"/boot\" and \"/\" on the system.\n\nFor example if \"/boot\" and \"/\" are /dev/md126 and /dev/md127 respectively, monitor the status with:\n```bash\ncat /proc/mdstat\nmdadm -Q --detail /dev/md126\nmdadm -Q --detail /dev/md127\n```\n\n\n*Congrats!* \n\nDisk replacement is complete",
        "Affected Systems": "Disaster Site  dr1node07",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230331-SD2180781.md": {
        "Description": "",
        "Actions Taken": "",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20211701-IM1769421.md": {
        "Description": "```\nWe notice that the user with E70529 cannot make queries in Hue either via impala-shell, while he has the permissions and was playing until Friday at noon.\nThere was no user group in the user management of hue and when I created it and went to sync it, it crashed.\n```",
        "Actions Taken": "1. Login to dr1node3 as root and check the groups of E70529 user\n\n`# id E70529 | grep 871556062`\nYou will notice that the name of the above group is empty.\nLets fix that!\n\nNdef1: This occurs for dr1node03, dr1node05, dr1node06, dr1node07,dr1node08,dr1node09,dr1node10\n\nNdef2: Perform the above actions for all nodes\n\n2. `# sss_cache -E;id E70529 | grep 871556062`\n\nNow you must be able to see the name of the group. If not, continue with the following steps in order to clear cache and restart sssd with the right name of the group:\n\n1.  `# dcli -c dr1node03, dr1node05, dr1node06, dr1node07,dr1node08,dr1node09.dr1node10  'mv /var/lib/sss/db/* /tmp;systemctl restart sssd'`\n\n2.  `# dcli -C 'id E70529 | grep -v \"CMS Way4Manager PROD RDS DevTOOLS\"'`\n\n3.  `# dcli -C 'id E70529 | grep -v \"CMS Way4Manager PROD RDS DevTOOLS\"' | wc -l`",
        "Affected Systems": "mno Bigstreamer",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20210127-IM1426379.md": {
        "Description": "```\nIBank_Migration - Enrich hbase tables - JOB & Impala_insert batch Job failed.\nFail SA daily upsert 2nd time.exit on dr1.edge01.mno.gr.\n```",
        "Actions Taken": "1. Login to Grafana `https://dr1edge02.mno.gr:3000` with personal account\n2. Inspected `LOCAL MONITORING`/`I-Bank Batch Jobs Overview` \n3. From the diagram stage `Enrich HBase Visible Tables` had failed\n4. According to [Hadoop Trac](http://999.999.999.999/trac/hadoop/wiki/dev/project/mno/support), execution is done with the following command:\n```\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n```\n5. Connected  on MobaXterm via ssh to `dr1edge01` and changed user to `PRODREST` using sudo.\n7. Check the logs of failed script.\n```\nless /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log\n...\nWARNING: java.lang.IllegalArgumentException: Row length 34XXX is > 32767\n```\n\n8. After investigation this error indicates that the key column for the HBase table is greater than the limit, so the insert fails.\n9. Find how many lines are causing this error by executing the `SELECT` part of the query and adding `WHERE length(...)>32767` at the end:\n```\nSELECT\n        concat(default.hbase_salt_md5hex(client_username,4,TRUE), chr(0), hex(CAST(CAST(u_timestamp AS DOUBLE) * 1000000 AS bigint)), chr(0), a.id),\n        'true',\n        'true',\n        originate_timestamp,\n        ...\nON a.service_name = b.name\nWHERE b.show_customer=TRUE\n       AND length(concat(default.hbase_salt_md5hex(client_username,4,TRUE), chr(0), hex(CAST(CAST(u_timestamp AS DOUBLE) * 1000000 AS bigint)), chr(0), a.id))>32767;\n```\n\nThis query returned 1 row. Copy the above query into a file for future reference.\n\n10. Copy script `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`.\n\n11. Edit the copy. Append `AND length(concat(default.hbase_salt_md5hex(client_username,4,TRUE), chr(0), hex(CAST(CAST(u_timestamp AS DOUBLE) * 1000000 AS bigint)), chr(0), a.id))<32767` at the end of the query.\n\n11. Execute the new script. Check the execution is successful.\n```\n/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh.pqr `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1\n```\n\n12. In the Grafana charts, check that stage is green now.",
        "Affected Systems": "Disaster Site IBANK",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20220103-IM1805149.md": {
        "Description": "```\nToday 1/3/2022 we have the following Failed Batch Job at Grafana monitoring tool.\n\napplication: IBank_Ingestion\njob_name: MergeBatch\ncomponent: Job\nStatus: Failed\nHost: dr1edge01.mno.gr\n```",
        "Actions Taken": "1. Login to `dr1edge01` and open firefox\n2. At the YARN UI search for `PRODREST` and sort by End date. You will find the failed application.\n3. From the UI we saw that Spark exited due to OOM errors.\n4. Using this [document] (KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#merge-batch) we executed Merge Batch in 3 steps in parallel:\n    ```\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2022-02-28 00:00:00\" \"2022-02-28 12:00:00\"\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2022-02-28 12:00:00\" \"2022-02-28 18:00:00\"\n    /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2022-02-28 18:00:00\" \"2022-01-03 00:00:00\"\n    ```\n5. We updated the monitoring postgres database in order for the entry to appear green in Grafana. You can get a success query for Merge batch from the log file of the master script and change the dates. To check it worker Grafana must so no failed merge batch jobs for that day.\n6. We created a copy of the master script as `PRODREST` at `dr1edge01`.\n7. Inside the copy script we erased the steps before the [Distinct join to Service Audit](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit) stage.\n8. We executed the copy script and performed checks as ndefd in the support document.",
        "Affected Systems": "Disaster Site IBank Batch",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20231117-IM2241809.md": {
        "Description": "```\nGood evening,\n\nAll Hosts in Dr1 and PR1 are in critical state\n\nPlease take action.\n```",
        "Actions Taken": "**Steps in order to investigate and make sure that the table is not created**\n1. Login to `PR` and `DR` cloudera manager in order to check the health of each cluster. The status was unhealthy for all services on both clusters.\n2. Login to `Grafana` in order to check that applications running. All the applications were running without errors.\n3. ssh to `pr1edge01.mno.gr` with personal account\n4. sudo to root\n5. Move to the log folder:\n```bash\ncd /var/log\n```\n6. Check messages file\n```bash\nless messages\n```\nThe output was:\n![image](.media/IM2241809/pr1edge01_messages.png)\n\n7. From the above output we saw that at `22:13:02 pr1edge01_kernel: nfs: server 999.999.999.999 not responding`.\n8. Now lets check the `agent logs` of an internal node.\n9. ssh to `pr1node03.mno.gr` with personal account\n10. sudo to root\n11. Move to the log folder:\n```bash\ncd /var/log/cloudera-scm-agent\n```\n12. Check `cloudera-scm-agent.log` file\n```bash\nless cloudera-scm-agent.log\n```\nThe output was:\n![image](.media/IM2241809/pr1node03_agent_logs.png)\n\n13. Due to unavaliability of `nfs storage`(responisibility of the customer to maintain), `Host Monitor` service of Cloudera management services had `timeout` errors because couldn't collect metrics from each filesystem of the nodes.\n14. Customer informed that `nfs storage` caused the issue on both clusters and the unhealthy state of all services was not real because `Host Monitor` was not able to collect metrics in order to be appeared on `CM`. Also all flows ran without errors during the issue.\n15. Customer informed us that the `nfs` storage was full and after their actions it's ok. We checked the `CM` and all the services now is healthy.",
        "Affected Systems": "Disaster/Primary Site\n\n\n**Well Done!!**",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "```\nGood evening,\n\nThe issue was caused by the nfs storage used on the nodes of both clusters becoming full. This resulted in the host monitor of the cloudera management services timeouting as it was unable to collect metrics for each filesystem of the nodes.\n\nRelevant screenshots are attached showing the above causes of the issue.\n\nThroughout the issue, the flows were up and running as seen in grafana as it was a malfunction of the management services resulting in the incorrect image of all services in Cloudera Manager PR & DR respectively.\n\nAfter space was freed up on the nfs, both clusters returned to good health.\n\nThank you.\n```",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "This problem occurred due to `nfs` unavaliability.",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20220504-IM1851937.md": {
        "Description": "```\nOn 5/5/2022 we had the following Failed Batch Job at Grafana monitoring tool.\n\nThe following batch job failed at Grafana\napplication: DWH_IBANK\njob_name: EXPORT\ncomponent: SERVICE_AUDIT\ndescription: Code 1\n```",
        "Actions Taken": "1. Login to grafana to make sure that the alert is about DR SITE\n2. Login to `dr1edge01` and open firefox\n3. At the YARN UI search for `PRODUSER`, sort by End date and search with \"PROD_IBANK_DWH_EXPORT_ServiceAudit. You will find the failed application.\n4. From the UI we noticed that the job with id application_1651064786946_8294 started manually before the completion of the automated job with id application_1651064786946_8190, which led to the failure of the second job.\n5. We informed the client that they should rerun the failed job manually from the scheduler after the completion of the manual step. Also, we pointed out that before proceding with manual actions they should make sure beforehand that all scheduled flows have completed.",
        "Affected Systems": "Disaster Site IBank Batch",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230307-IM2099957.md": {
        "Description": "```text\nGood evening,\n\nThe following alert appeared in the grafana system:\n\n[DR][IBANK] Spark Waiting Batches Alert\n```",
        "Actions Taken": "1. The following text has been sent to mno/PM and explains the problem, as well as the recommended actions\n\n```text\nSpark Waiting Batches Problem: The first and most important problem we have is the \"Spark Waiting Batches\" which opens a ticket for this monitoring. This is due to physical user actions (queries/jobs) that occupy/bind production resources on the disaster site mainly. This results in there being no resources available for the MySQL process (a central point that the entire cluster has a dependency on), the service that does the authorization is unable to process its data in the database and thus causes a delay in the spark topologies until the \"permission denied\" error is resolved. The spark topologies, while up and running, are unable to process the data, as a result of which they continue to execute after the execution of the jobs that occupied the resources on the server where the MySQL service is located has finished. We do not take any action in this nor can we do anything and mno has asked us to close the ticket directly.\nSuggestion: Disable Impala Daemon and YARN Node Manager on dr1node03.mno.gr, pr1node03.mno.gr where the primary MySQL service is located. This will not affect our cluster workload as, as you will see in the attached screenshots:\n1. impala_mean.png: The average memory occupied by Impala Daemon is much smaller than the limit (150GB) that we have set even in the evening hours when all the flows \"close\" the previous day and the largest load is concentrated on the cluster.\n```\n\n![impala mean usage](.media/IM2099957/impala_mean.PNG)\n\n``` text\n2. impala_total.png: The total memory commitment from Impala cumulatively for all nodes is at its peak about 500GB less than the total available, which means that by removing a node there will still be room for resources even with the addition of new flows\n```\n\n![impala total usage](.media/IM2099957/impala_total.PNG)\n\n```\n3. yarn.png: throughout the day, the available yarn resources are sufficient even with the removal of more than one node\nThe above applies to both sites and the screenshots are from the Disaster site, where the most resources are reserved compared to the 2 sites. It is important as in this MySQL has a dependency on the entire cluster.\n```\n![yarn usage](.media/IM2099957/yarn.PNG)\n\n``` text\nBy monitoring the remaining nodes of the 2 clusters, we see how they can manage the workload at CPU levels and with the above proposal we will reduce the CPU levels on critical node03 which anyway uses increased CPU for cluster management processes.\n```",
        "Affected Systems": "mno Disaster Site",
        "Action Points": "https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/66  \nhttps://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/196",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230313-way4streams-venia.md": {
        "Description": "```text\nReporting and investigation for this issue was performed in a teams call, since we did not have access to the server that Way4Streams was installed.\n\nThe error we were facing was something along the lines\n\n/way4/DEVUSER.keytab does not contain any keys for DEVUSER@BANK.CENTRAL.mno.GR\n```",
        "Actions Taken": "1. The new server hosting the application is RHEL 8 instead of Solaris. We tried to manually `kinit`\n\nFrom the server with `way4`\n\n``` bash\nkinit DEVUSER@BANK.CENTRAL.mno.GR -kt /way4/DEVUSER.keytab\n```\n\nOutput:\n\n```bash\nTicket cache: KCM:1500\nDefault principal: DEVUSER@BANK.CENTRAL.mno.GR\n\nValid starting       Expires              Service principal\n15/03/2023 12:35:29  16/03/2023 12:35:29  krbtgt/BANK.CENTRAL.mno.GR@BANK.CENTRAL.mno.GR\n\trenew until 22/03/2023 12:35:29\n```\n\n2. **Anything** but `FILE` ticket caches is sure to create a problem.\n\nFrom the server with `root`:\n\n``` bash\nvi /etc/krb5.conf\n```\n\nChange the following under `libdefaults` section:\n\n``` conf\ndefault_ccache_name = FILE:/tmp/krb5cc_%{uid}\n```\n\nAlso, remove `sssd-kcm`\n\n```bash\nyum remove sssd-kcm\n```\n\n3. After that the klist output used a `FILE` cache, but the problem persisted.\n\nSince the OS problems were resolved we focused the keytab.\n\nFrom the server with `way4`\n\n``` bash\nklist -kte /way4/DEVUSER.keytab\n```\n\nOutput:\n\n```\nKeytab name: FILE:/way4/DEVUSER.keytab\nKVNO Timestamp           Principal\n---- ------------------- ------------------------------------------------------\n  0 01/01/1970 00:00:00 DEVUSER@BANK.CENTRAL.mno.GR (DEPRECATED:arc4-hmac) \n```\n\nThat DEPRECATED flag is not a good sign. \n\n4. Searching for `rc4-hmac` and `OpenJDK11` we stumbled upon this link https://bugs.openjdk.org/browse/JDK-8262273\n\nFrom the server with `root`:\n\n``` bash\nvi /etc/krb5.conf\n```\n\nAdd the following under `libdefaults` section:\n\n``` conf\nallow_weak_crypto = true\n```\n\nThe issue was resolved!",
        "Affected Systems": "Way4Streams QA (Not supported by jkl)",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230726-IM2193241.md": {
        "Description": "",
        "Actions Taken": "1. Re-run the failed step as described [here](../supportDocuments/applicationFlows/online.md#batch)\n2. The flow completed successfully, we proceeded with the investigation\n\n   Logs from the application:\n\n   ```\n   Caused by: org.apache.kudu.client.NonRecoverableException: cannot complete before timeout: ScanRequest(scannerId=\"22c757bfcf674a05a08f14c316e745e9\", tablet=c42b07f18435403297fee37add478c0b, attempt=1, KuduRpc(method=Scan, tablet=c42b07f18435403297fee37add478c0b, attempt=1, TimeoutTracker(timeout=30000, elapsed=30004), Trace Summary(0 ms): Sent(1), Received(0), Delayed(0), MasterRefresh(0), AuthRefresh(0), Truncated: false \n   ```\n\n   Spark UI:\n\n   ![Spark UI](.media/IM2193241_1.png)\n\n3. Stage 0 should have 180 partitions not 468\n\n   ![Spark UI normal](.media/IM2193241_2.png)\n\n4. Informed development team to correct the number of partitions for `prod_trlog_online.service_audit_stream`. This deleted unnecessary data from Kudu's disks and next run (see 3) did not have any failed tasks.",
        "Affected Systems": "mno Primary Site",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230301-IM2095966.md": {
        "Description": "```\nGood evening,\n\nThe following failed batch job appeared in the Grafana system:\n\napplication :  IBank_Ingestion\njob_name : MergeBatch\ncomponent : JOB\ndate : 28-02-2023\nstatus : FAILED\ndescription :\nhost : pr1edge01.mno.gr\n```",
        "Actions Taken": "1. We identified the failed step using the alarm name. Steps `MSSQL Sqoop Import (Migration)` and `Insert to Service Audit` had been executed successfully. We rerun the `Merge Batch` according to [this](../supportDocuments/applicationFlows/ibank.md#merge-batch).\n2. The job had not completed at approximately 9.pm on 01/03/2023 we terminated the job after communication with the customer in order for the night flow to run without any problems. We scheduled to rerun the job in the following day after the completion of the daily MergeBatch.\n3. On 02/03/2023 we reran the job in 3 patches \n   ```bash\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2023-02-28 00:00:00\" \"2023-02-28 12:00:00\"\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2023-02-28 12:00:00\" \"2023-02-28 18:00:00\"\n   /opt/ingestion/PRODREST/ibank/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh \"2023-02-28 18:00:00\" \"2023-03-01 00:00:00\"\n   ```\n4. The `Upsert to HBase` stage that synchronises the `Visible` table caused an Impala problem during which Impala stopped to process this job as well as other requests.\n5. The problem is described below.",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "```\n03/03/23 11:17:49 Europe/Eastern (POULAS GIORGOS):\nAfter investigating yesterday's Impala issue, we found the following:\n\nDue to HBase quotas set in the PROD_IBANK namespace, we have limited the parallelism in the Impala query to run on an Impala daemon.\n\nThe daemon that ran the query to enrich the Service Audit Visible (pr1node04) encountered a problem as it did not have the resources required to process the large volume of records we had at the end of the month, while at the same time accepting requests from the REST APIs of the live streams.\n\nAs a result of the above, the queries from the live systems were not completing and accumulating, exhausting the available connections that Impala can accept. The malfunction of the live streams is also the problem you observed last night.\n\nWe propose as a workaround today after 9pm. disable quotas in the PROD_IBANK namespace and rerun the script without the single node limitation, so that the load is shared across all 9 available Impala daemons. We will then examine the alternatives for modifying the flow and re-enabling quotas.\n\nThere is no downtime required for the above actions.\n\nIf you need further information we can arrange a call.\n\nG. Poulas\n03/03/23 00:53:26 Europe/Eastern (MASTROKOSTA MARIA):\nGood evening,\n\nThe service audit has been filled on the PR Site. The job that fills the visible table is pending as it was canceled in the context of ticket SD2159021.\n\nThank you\n03/03/23 00:52:57 Europe/Eastern (MASTROKOSTA MARIA):\nGood evening,\n\nThe service audit has been filled on the PR Site. The job that fills the visible table is pending as it was canceled under ticket SD2159021.\n\nThank you\n02/03/23 15:56:38 Europe/Eastern (MASTROKOSTA MARIA):\nGood evening,\n\nThe execution on both sites started after the scheduled execution of the Merge Batch for 01/03/2023, which has been completed without a problem. At this time, the DR has processed until 18:00, while the PR has processed the data until 12:00. The executions on both sites are being monitored so that they can be resubmitted in case of a problem.\n\nThank you\n02/03/23 15:54:31 Europe/Eastern (MASTROKOSTA MARIA):\nGood evening,\n\nThe execution on both sites started after the scheduled Merge Batch execution on 01/03/2023, which has been completed without any problems. At the moment, the DR has processed until 18:00, while the PR has processed the data until 12:00. The executions on both sites are being monitored so that they can be resubmitted in case of a problem.\n\nThank you\n01/03/23 21:11:19 Europe/Eastern (MASTROKOSTA MARIA):\nFollowing our telephone communication, the job has been stopped and will be re-executed tomorrow in order to avoid problems with the evening streams.\n\nThank you\n```",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20211021-IM1681883.md": {
        "Description": "```\nToday 20/10/2021 appeared the following issue on cloudera manager\n\nhdfs: Datanode Data Directory Status\n```",
        "Actions Taken": "1. Login to Cloudera Manager in DR and check the alarm\n2. It was a block count alarm on `dr1node09`\n3. Login to `dr1edge01` with your personal account and execute `firefox` to view the Namenode UI.\n4. Go to `https://dr1node02.mno.gr:50470` and from there in the tab `Datanodes`. Order the datanodes using the block count by desceding order to overview the situation.\n5. Change user to PRODREST and use HDFS command or Impala query to check how many partitions exist in `prod_trlog_ibank.service_audit_old`. This is a big table that has no retention mechanism yet so its data are stored in many blocks. HDFS command is `hdfs dfs -ls /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit_old` and Impala query is `show partitions prod_trlog_ibank.service_audit_old`. In either case observe how many `par_dt` directories exist.\n6. In our case there were at least 6 months in there so we informed mno. Example mail:\n```\nGood evening,\nRegarding ticket SD1734269, the error shown in Cloudera Manager refers to a block count threshold exceeded on dr1node09 and is due to the increased number of blocks stored on the new nodes dr1node07-10. The impact on the datanode lies in the performance as the metadata for the blocks it has in memory does not fit.\nFrom the investigation we did in the HDFS data, we saw that there are many files in the prod_trlog_ibank.service_audit_old table. Specifically, there is history in it from 01/03.\nBy keeping only recent data through a retention mechanism, the total block count in HDFS and on each datanode individually will be reduced.\nAs an immediate solution, we suggest deleting data older than 40 days, as has been done in the past. Consequently, it will not be possible to investigate problems that occurred older than 40 days.\nPlease let us know if you agree with the above and when you would like us to proceed with the immediate solution.\nThank you,\n```\n\tRecipients are `ZEVGAROPOULOU.GEORGIA@mno.gr,krekoukias.konst@mno.gr,papakostas.athanasios@mno.gr`. Add our team and kbikos in CC.\n5. When mno agreed to the immediate action, we informed the monitoring team to ignore alarms in Cloudera Manager of DR site regarding HDFS, Hive, Impala and YARN. Their email address is `csocmonitoringops@jkl-telecom.com`. Example mail:\n```\nGood evening,\nWe will proceed with actions on the National Bank of Greece Disaster Site infrastructure.\nDuring this time, you will probably have alarms in the Cloudera Manager UI related mainly to the Health of HDFS, Hive, Impala, Yarn.\nThe work will start at 16:40 today. We will inform you after the work is completed.\nThank you,\n```\n8. When the time comes, login to Grafana and check charts of topologies running in DR. Login to Cloudera Manager and check throughout the process the health status of Cloudera services. Ndef that `Hive Metastore Canary` alerts will pop up as expected.\n9. Login to dr1edge01 with your personal account and change user to `PRODREST`.\n10. Open an impala shell `impala-shell -i pr1edge.mno.gr -k --ssl`.\n11. Change database to `prod_trlog_ibank` with `use prod_trlog_ibank;` and `show tables;` to check that `service_audit_old` is here.\n12. Drop partitions. Seperate the amount of days you have to delete in batches of 15 days in order to avoid disruptions of applications. `ALTER TABLE service_audit_old DROP IF EXISTS PARTITION(par_dt<=20200715) PURGE;`\n12. When the query is done, execute the next one and so on: `ALTER TABLE service_audit_old DROP IF EXISTS PARTITION(par_dt<=20200801) PURGE;`\n13. Can't stress this enough. **Throughout steps 12, 13 check Grafana and Cloudera Manager**.\n14. When done, inform mno and the monitoring team that actions have been successfully finished.",
        "Affected Systems": "mno Bigstreamer",
        "Action Points": "Develop retention mechanism to automatically drop old partitions.",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20221119-IM2024442.md": {
        "Description": "```bash\nHello\n\nA critical alarm appeared in Cloudera Application in dr1edge01.\n\nHive --> HiveServer2 (dr1node04) // Pause Duration.\n\nPlease for your attention.\n\nThanks\n```",
        "Actions Taken": "1. Check HiveServer2 JVM Heap Memory Usage and JVM Pause Time Charts from Cloudera Manager.\n\n    ```bash\n    cluster -> Hive -> HiveServer2 -> Charts\n    ```\n\n2. Restart HiveServer2 Instance if needed (workaround).\n\n    ``` bash\n    In our case the service had Unexpected Exits due to OutOfMemory. \n    ```\n\n3. Search for \"Java Heap Space\" failed Jobs in HiveServer2 Service Logs.\n\n    ```bash\n    grep Java /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node 04.mno-gr.log.out\n    ```\n\n    Example Output:\n    ![hiveServer2Logs.PNG](.media/hiveServer2Logs.PNG)\n\n4. Check failed Yarn Applications from Cloudera Manager that match those of the previous step.\n\n    ```bash\n    Cluster -> Yarn -> Applications -> Filter: \"application_type = MAPREDUCE\"\n    ```\n\n5. Search for GC Pause Duration in HiveServer2 Service Logs and make sure that the warnings started after the submission of the failed jobs.\n\n    ```bash\n    grep GC /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node 04.mno-gr.log.out\n    ```\n\n6. Compare the timestamps of all the above to be sure that you have found the queries that caused the problem.",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "```bash\nGood evening,\n\nHiveServer2 of dr1node04 is back up. Services and flows have been checked and there is no problem at this time.\n\nWe appreciate your checks and are continuing to analyze the root cause.\n\nThank you\n```\n\n```bash\nGood evening,\n\nThe following findings emerged from the analysis:\n\nHiveServer2 of dr1node04 crashed from OutOfMemory, because the Java Heap Space was full.\nThe Pause Duration messages in Cloudera Manager are related to the Garbage Collector delay.\n\nSpecifically, from the analysis of the logs we saw that between 14:19 and 15:24, HiveServer2 of dr1node04 was called to manage 8 Queries which crashed with a Java Heap Space error. The GC started throwing warnings from 15:08, as it could not clean the memory of the above. The service crashed with an OutOfMemory error, restarted and returned to normal operation.\n\nBelow are details for the specific queries:\n\n14:19 application_1665578283516_50081 user:E30825\n14:25 application_1665578283516_50084 user:E30825\n14:29 application_1665578283516_50085 user:E30825\n14:32 application_1665578283516_50088 user:E30825\n14:37 application_1665578283516_50089 user:E30825\n14:41 application_1665578283516_50090 user:E30825\n15:23 application_1665578283516_50095 user:E36254\n15:24 application_1665578283516_50096 user:E36254\n\nAll queries are for the table: dev_trlog_card.pmnt_response_stg_0.\n\nPlease confirm that we can proceed to close the ticket.\n\nThank you\n```",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20201026-IM1332456.md": {
        "Description": "```\nAfter checking the details tables of the prod_trlog_ibank_analytical database, it appears that the last partition is 20201023, while the details should have had up to yesterday's partition (20201025).\nThe extraction flows have a successful status in the monitoring table.\n\nPlease check.\n```",
        "Actions Taken": "1. Login to `https://dr1edge01.mno.gr:3000` with personal account\n2. Inspected `LOCAL MONITOR/Batch Jobs DR`\n3. DW JOBS `Check that all is SUCCESS`\n4. Open MobaXterm `dr1edge01` ssh with your personal account\n5. impala shell -i dr1edge.mno.gr -k --ssl\n6. Execute the query `select count(*),par_dt from service_audit where par_dt > 20200919 group by 2 order by 2;`\n7. Check that par_dt has inserted data\n8. After the above checking procedure, customer informed to refresh the above table before execute a spark/flow and that the spark topology was healthy.",
        "Affected Systems": "Disaster Site IBANK query",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20240306-IM2323192.md": {
        "Description": "```\nGood morning,\n\nThe following alert has appeared in Cloudera Manager (DR):\nDataNode (dr1node02)\nData Directory Status\n\nThank you.\n```",
        "Actions Taken": "There are references from the similar issue [20220620-SD1951890.md](20220620-SD1951890.md).\n\nAfter investigation we saw that the problem occurred due to disk issue on dr1node02.\n\nWe communicated with Oracle and disk replacement was scheduled.\n\n> Ndef that disk replacement perfomerd online so there was no downtime.\n\nWe followed the steps as described at [20220620-SD1951890.md](20220620-SD1951890.md) and [sync_mysql.md](sync_mysql.md), which include the following:\n\n1. Stopping the processes that specifically run at the disk slots `s1` and `s7` of the server `dr1node02`. On our case was the hdfs datanode and some yarn applications . We identified them with:\n\n2. Stopping the mysql slaves using the command:\n```\nmysql -u root -p\nSHOW SLAVE STATUS\\G;\n```\n\n3. Ensuring that the no processes are running at the partitions with the following commands:\n```bash\nlsof /u02\n```\n\n```bash\nlsof /u08\n```\n\n4. Unmounting the two partitions, so the disks can be replaced.\n\n```bash\numount <mountpoint>\n```\n\n5. Once the disks have been replaced we ran the following command for both partitions:\n\n```bash\nbdadiskutility /u02\n```\n\n6. After running the command, we got the following error:\n```\nVirtual Drive <VIRTUAL_DRIVE_NUMBER> is incorrectly mapped.\n<TIMESTAMP> : Error executing 'MegaCli64 CfgLdAdd r0[<ENCLOSURE>:<SLOT>] a0'\n<TIMESTAMP> : Error code is 84 . Response is <<\nAdapter 0: Configure Adapter Failed\n\nFW error description:\nThe current operation is not allowed because the controller has data in cache for offline or missing virtual disks.\n\nExit Code: 0x54>>\nFound a disk with a Firmware State of Unconfigured(good).\nSuccessfully cleared the cache for the logical drive.\nSuccessfully added the disk to its own RAID(0) volume.\n```\n\n7. After communicating with Oracle Support [SR 3-36895603206 : Wrong disk status after replacement](https://support.oracle.com/epmos/faces/SrDetail?_afrLoop=206254157461870&srNumber=3-36895603206&queryModeName=Technical&needSrDetailRefresh=true&_afrvwxowMode=0&_adf.ctrl-state=iwvcvrye_184), we ran the following commands to solve the issue:\n\n\n- `For s1 # The disk slot 1 of the server that corresponds to mount point /u02`\n- `For s7 # The disk slot 7 of the server that corresponds to mount point /u08`\n\n- Validated if there is a cache pinned for any device, running command:\n\n```bash\nMegaCli64 -GetPreservedCacheList -a0 \n ```\n\nIf the old disk has pinned the cache, the command will return something like:\n\n```\nAdapter #0\n \nVirtual Drive(Target ID 07): Missing.\n \nExit Code: 0x00\n```\n- In this case, the disk in slot 7 had the pinned cache and had to clear.\n\nRemove the pinned cache by running command:\n\n```bash\n#MegaCli64 -DiscardPreservedCache -L7 -force -a0 <<<< where -LX should be replaced by the Target ID number reported in previous step.\n```\nGet the `ENCLOSURE_NUMBER`\n```bash\nMegaCli64 LdPdInfo a0 | more\n```\n- Added the virtual disk back\n\n```bash\nMegaCli64 CfgLdAdd r0[ENCLOSURE_NUMBER:slot] a0\n```\nOn our case was:\n\nFor `s1`\n```bash\nMegaCli64 CfgLdAdd r0[252:1] a0\n```\n\nFor `s7`\n\n```bash\nMegaCli64 CfgLdAdd r0[252:7] a0\n```\n\nStarted configuring the disk at `slot1`\n\n```bash\nbdadiskutility -f /u02\n```\n\nWait until the mirroring is finished and after that.\n\nStarted configuring the disk at `slot7`\n\n```bash\nbdadiskutility -f /u08\n```\n\n- Checks:\n\nFor `s1`:\n\n```bash\nparted /dev/disk/by-hba-slot/s1 -s unit chs print\niscsi # Check that all disks appeared\nlsblk # Check that all disks appeared\n```\n\nFor `s7`:\n\n```bash\nparted /dev/disk/by-hba-slot/s7 -s unit chs print\niscsi # Check that all disks appeared\nlsblk # Check that all disks appeared\n```\n\n8. We proceed with the start of the `datanode` role of `dr1node02`",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230305-IM2098517.md": {
        "Description": "```\nGood morning,\n\nThe following health issue has occurred in the cloudera manager system:\n\nSpark on yarn - History Server (dr1node03) - Unexpected Exits\n```",
        "Actions Taken": "1. Login to Cloudera for DR Site\n2. We inspected logs for this role for the time that the problem arose: `Cloudera > Diagnostics > Logs` and chose `Service: Spark on Yarn` and `Role: History Server`. We could not identify the root cause by these logs\n3. ssh to dr1node03 as root, went to `/var/run/process` , and inspected logs from the process that ran at the time of the problem and found out that the process with pid 51291 was killed while a `OutOfMemoryError` occured\n   ![terminal_screenshot](.media/IM2098517/spark_on_yarn.png)\n\n4. We checked the  `java heap size` of the History Server through Cloudera UI configuration tab. It was set to 512M.\n5. We checked the respective option for the PR Site and it was set to 2G\n6. We set the `java heap size` of the History Server to 2G at the DR Site\n7. We proceeded to restart of the role after communication with the customer",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "```\n07/03/23 16:28:55 Europe/Eastern (MASTROKOSTA MARIA):\nWe have restarted the History Server after a phone call. There was no problem during the restart.\n\nPlease let us know if we can close the ticket.\n\nThank you\n07/03/23 16:21:21 Europe/Eastern (MASTROKOSTA MARIA):\nGood evening,\n\nThe exit occurred due to an out of memory error. We have changed the java heap size of the History Server from 512MB to 2GB as in the PR. We will need to restart the role. There will be no outage.\n\nThank you\n```",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "IM1908793.md": {
        "Description": "```\nGood morning.\n\nI am getting the following error while creating a temporary kudu table,\n\nERROR: ImpalaRuntimeException: Error creating Kudu table 'prod_trlog_card_analytical.opticash_dispencing_atm_tmp'\n\nCAUSED BY: NonRecoverableException: failed to wait for Hive Metastore notification log listener to catch up: failed to retrieve notification log events: failed to get Hive Metastore next notification: Thrift SASL frame is too long: 338.01M/100.00M\n```",
        "Actions Taken": "**Steps in order to investigate and make sure that the table is not created**\n1. Login to `dr1edge01.mno.gr` with personal account and then to `dr1node01.mno.gr`\n2. Move to the process folder:\n```bash\ncd /var/run/cloudera-scm-agent/process/\n```\n3. Find the latest process and go to that folder. In our case is 12200-kudu-KUDU_TSERVER. So move to that folder:\n```bash\ncd 12200-kudu-KUDU_TSERVER\n```\n4. Use the keytab you just found in that folder:\n```bash\nkinit -kt kudu.keytab kudu/`hostname`\n```\n5. Check kudu cluster health and specifically for `prod_trlog_card_analytical` database in order to check if the wanted table is created.\n```bash\nkudu cluster ksck dr1node04.mno.gr dr1node05.mno.gr dr1node06.mno.gr | grep -i prod_trlog_card_analytical\n```\n\nAs you can see the `prod_trlog_card_analytical.opticash_dispencing_atm_tmp` table is not created.\n\n**Optional**: You can also verify that from impala-shell running the following commands:\n- Login to `dr1edge01.mno.gr` with personal account\n- impala-shell -i dr1edge01 -k --ssl\n- `[dr1edge01.mno.gr:21000] default> use prod_trlog_card_analytical;`\n- `[dr1edge01.mno.gr:21000] default> show tables;`\n\nAs you can see the `prod_trlog_card_analytical.opticash_dispencing_atm_tmp` table is not created.\n\n6. Login to CM DR with your pesonal account > Go to impala > Queries\n7. In the search bar type the following in order to find the query:\n`STATEMENT RLIKE '.*prod_trlog_card_analytical.opticash_dispencing_atm_tmp'.*` and click on the query details for investigation.\n\nWe found that the query they try to run is the following:\n\n```bash\nCREATE TABLE IF NOT EXISTS prod_trog_card analytical.opticash ispencing_atm_tmp\ncashp id, STRING NOT NULL\n, transaction date STRING NOT NULL\n,denom id STRING\n, cassette STRING\n, crncy id STRING\n, open Bal BIGINT\n, norm del BIGINT\n, norm rtr BIGINT\n, unpl_ del BIGINT\n, unpl_tr BIGINT\n, wthdrwls BIGINT\n, pre_wdrw BIGINT\n, deposits BIGINT\n, clos_bal BIGINT\n, bal_disp BIGINT\n, bal_escr BIGINT\n, bal_unav BIGINT\n, opr_stat STRING\n, excld_fl STRING\n‚PRIMARY KEY (cashp_id, transaction date, denom_id, cassette)\n) STORED AS KUDU\n```\n\nWe try to rerun the above query and we get the following error:\n```bash\nERROR: ImpalaRuntimeException: Error creating Kudu table 'prod_trlog_card_analytical.opticash_dispencing_atm_tmp'\nCAUSED BY: NonRecoverableException: failed to wait for Hive Metastore notification log listener to catch up: failed to retrieve notification log events: failed to get Hive Metastore next notification: Thrift SASL frame is too long: 338.01M/100.00M\n```\n\n**Time to fix the problem**\n\nAs a first step, let's try to fix `Thrift SASL frame is too long: 338.01M/100.00M` error.\n\n1. Login to Cloudera Manager in DR site with your personal administrative account:\n\n`Kudu > Instances > Click on Master > Select Tab Configuration`\n\n2. In `Search` box write safety valvue and at `Master Advanced Configuration Snippet (Safety Valve for gflagfile)` add th following flag:\n\n```bash\n--hive_metastore_max_message_size_bytes=858993459\n```\n\n>**Important Ndef**:  The above step with flag must be set at all three masters\n\n3. Restart the three kudu masters (one at a time)\n4. After rerunning the query the table is not still created but this time we get the following error: \n```bash\nSASL decode failed: SASL(-1): generic failure:\nwO706 15:44:11.242372 109675 hms_notification_log_listener.cc:130] Hive Metastore notification log listener poll failed: Not authorized: failed to ret\nrieve notification log events: failed to get Hive Metastore next notification: SASL decode failed: SASL(-1): generie failure:\nw0706 15:44:35.127687 109673 hms_client.cc:345] Time spent get HMS notification events: real 8.885s user 0.000s sys 0.228s\n```\n\n5. Restarting all Tablet Servers (dr1node01-10),one at a time, fixed the problem. \n\n**Before Restarting Tablets the following Flows must be stopped !!!**\n```\nPROD_IBANK_IngestStream_Visible\nPROD_Online_IngestStream\nPROD_IBank_IngestStream\n```\n\nStop the flows:\n\n>Ndef: We used following command because flows were working fine. Otherwise, we you should kill the application.\n\n```bash\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest/topology_shutdown_marker/\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_IBank_Ingest_Visible/topology_shutdown _marker/\nhdfs dfs -put SHUTDOWN /user/PRODREST/service/PROD_Online_Ingest/topology_shutdown_marker/\n```\nWhen Tablets are all up and running make sure you start again the flows. \n\nVerify that Tablets and Kudu is up and running by checking graphs and CM UI (CM -> Kudu -> Charts Library)\n\nInformation about how to start flows can be found [here](http://https://metis.ghi.com/obss/oss/sysadmin-group/support/-/tree/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows \"here\")\n\nWe verified that the problem is fixed by running the querry and got Table has been created message.",
        "Affected Systems": "Disaster Site\n\n\n**Well Done!!**",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "This problem occurred due to dr1node07 disk replacement.\n\nPlease refer to *IM1893876* for more information.\n\nThe fact that kudu tablets were offline for more than 1 days resulted in networking issues between Tablets.",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20240129-IM2271635.md": {
        "Description": "```\nGood morning everyone.\n\nPlease check if the Navigator in PR is functional because we often see messages like\nhaproxy[23876]: proxy cn_vip has no server available!\n\nWe also noticed that most of the time, when entering the Navigator web application, we receive a message\nFailed to connect to the Navigator server, check your network connection and try again.\n```",
        "Actions Taken": "1. We executed the script located at `pr1node03.mno.gr:/opt/navigator_restart/apicluster.py` as `root`, documented at [Script for navigator metadata restart](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/200) which is used to restart the Cloudera Navigator Metadata server. However, the errors remained and we could not connect to the Navigator Metadata Server.\n\n\n2. We connected to Cloudera Manager and used it to restart the Navigator Metadata Service. This restored the connection to the service.",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20220617-SD1949713.md": {
        "Description": "```\nGood evening,\n\nWe received 120 emails from the DR cluster services saying they have a problem and when we entered the cloudera manager everything was red. It seems that some of our applications crashed as well. Can you tell us what happened?\n\nBest regards,\nThanos Papakostas\n```",
        "Actions Taken": "1. Login to grafana to make sure that the alert is about DR SITE. We noticed that there were alerts for IBANK Spark Waiting Batches but not for Visible which predisposes us for an issue with Kudu.\n![ibank_kudu_problem](.media/SD1949713/ibank_kudu_problem.PNG)\n2. Login to Cloudera UI for the DR Site.\n3. From `Charts>Impala Perf` we noticed increased resource commitment through Impala Pool Reserved and Threads charts.\n![Impala Pool Reserved](.media/Impala_pool_reserved.PNG)\n![Threads](.media/threads.PNG)\n4. From `Cloudera Manager>Impala>Queries` we searched for queries that took place at the time the problem raised. We found that the query with ID 6d44d9525a681fb8:5e536ffc00000000 had Threads:CPU Time 10.7h. Upon inspection through `Query Details` we saw that the query was of high complexity with conversions and comparisons with regex.\n![Query](.media/query.PNG)\n5. Through Cloudera logs, we noticed that the query impacted the services in the form of timeouts for Kudu and Hive due to slow communication with Sentry Service.\n![hive_problem](.media/SD1949713/hive_problem.PNG)\n![timeouts_kudu](.media/SD1949713/timeouts_kudu.PNG)\n![sentry_problem](.media/SD1949713/sentry_problem.PNG)\n6. The problem was resolved without any interference from our side. We informed the client that it was due to a high complexity query ran by a normal user that resulted in an increased undertaking of resources.",
        "Affected Systems": "DR Site",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "SD2146917_IM2085094_Merge_Branch_Online.md": {
        "Description": "We have the following alert msg on Grafana.\n[DR][IBANK] Online_Ingestion MergeBatch Failed",
        "Actions Taken": "Login to `dr1edge01` with your acount\n```\nsu - PRODREST\n```\nWe look at the script log:\n```\n/var/log/ingestion/PRODREST/online/logonExecutor_OnlineBatch_full.log\n```\nThe problem was :\n\n`Permission Denied on hdfs dir. Due to unavailability of mysql it could not check the sentry permissions which are certain`\n\nThe main problem was due to some tasks running `deν` there was communication with the server to get the correct Permission.\n\nwe will have to rerun the script manually. Before running the script we will see if there are records in the table for each `par_dt`\n\nEnsure that no records are present in prod_trlog_online.service_audit (eg 23_03_2023)\n\n```bash\nimpala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select count(*) from prod_trlog_online.service_audit where par_dt='20230223';\"\n```\n\nIf there is no record for the above `par_dt` then we run the script again.\n\n```bash\n/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh /user/PRODREST/lock/PROD_Online_MergeBatch LOCK_ONLINE_PROD_BATCH_MERGE_TRANS /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh PROD_Online_MergeBatch /opt/ingestion/PRODREST/online/lock/ FULL 1800 \"`date --date='-1 day' '+%Y-%m-%d 00:00:00'`\" \"`date '+%Y-%m-%d 00:00:00'`\" >> /var/log/ingestion/PRODREST/online/log/cronExecutor_onlineBatch_full.log 2>&1 &\n```\n\nBecause it had failed for the previous day, we ran the script for the previous day.\n\nThe script ran for over 4 hours.\n\nAfter the above script successfully executed we ran the step [Report Start to Graphite](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/online.md#report-stats-to-graphite)",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230127-IM2072206.md": {
        "Description": "```bash\nHello,\n\nA failed batch job appeared in Grafana today 27/01/23.\n\nApplication: DWH_IBank\nJob Name: EXTRACT\nComponent: MY BANK\nStatus: Failed\nDescription: Code 6\n\nPlease for your actions.\n\nThank you.\n```",
        "Actions Taken": "",
        "Affected Systems": "",
        "Action Points": "Solution has been given with [this](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/issues/101#ndef_94836) issue.",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "1. Login to `https://dr1edge01.mno.gr:3000` with personal account and confirm that Datawarehouse Flow failed from `Monitoring/Monitoring PR/DR` dashboard.\n\n   The flow failed with `Code: 6` which means that the control script has timed-out while monitoring the `EXTRACT` script.\n\n2. Check logs\n\n    From `dr1edge01.mno.gr` with personal account:\n\n    ``` bash\n    less /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log\n    ```\n\n    ![IM2072206_extract_logs](.media/IM2072206_extract_logs.png)\n\n    The monitoring database was updated with status FAILED due to `Check if app is running` timeout.\n\n3. Check the Spark application status from YARN UI\n\n   ![IM2072206_yarn_app](.media/IM2072206_yarn_app.png)\n\n   Spark App Status: SUCCEEDED.\n\n   The script waited for only 1,5min and updated the monitoring database with Failed Status. Spark app began its execution after almost 2,5 minutes.\n   > 2,5min is not considered as a noticeable or abnormal delay time, so we did not investigate further.\n\n4. Customer reran the job\n\n5. Check logs and YARN UI of second application\n\n   ![IM2072206_yarn_rerun](.media/IM2072206_yarn_rerun.png)\n\n   ![IM2072206_rerun_logs](.media/IM2072206_rerun_logs.png)\n\n   In this case the Spark app started immediately and the script updated the monitoring app with Running Status.",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "SD2146915-IM2085092.md": {
        "Description": "```\nWe have the following alert msg on Grafana.\n[DR][IBANK] IBank_Ingestion MergeBatch Failed\n```",
        "Actions Taken": "1. Login to `dr1edge01` with your acount\n```\nsu - PRODREST\n```\n\n2. We check the script log:\n\nScript Logs: `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`\n\nError code: `Log messages was for memory fault.`\n\nWe also look at the Spark logs:\n\nUse Firefox on dr1edge01.mno.gr/pr1edge01.mno.gr to access the logs via YARN Resource Manager UI\n\nScript: `/opt/ingestion/PRODREST/common/scripts/cronExecutor_MergeBatchWithLock_hdfs_STABLE.sh` on dr1edge01.mno.gr/pr1edge01.mno.gr (each   edge server submits to a different cluster)\n\n  **Troubleshooting Steps**:\n\n- Use the script logs `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log` to identify the cause of the failure\n\nIf we have mentioned `error code` then :\n```bash\nvi /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_batch_cluster_mno_STABLE.sh\n```\n\nChange `colaesce` from `6` to `12` and save changes. \n\n>Ndef: Inform the next day developers in order to update the git repo with the new value\n\n- Ensure that no records are present in prod_trlog_ibank.service_audit_old\n\n``` bash\n  # eg. 09-11-2019\n  impala-shell -k --ssl -i ${HOSTNAME/01/} -q \"select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';\"\n```\n\n- If no records exist and no other process is up, you can ran the script again.\n  - For the previous day:\n\n    ``` bash\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n    ```\n\n  - For a specified date:\n\n    ``` bash\n    # e.g. 09-11-2019\n    /opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1\n\nThe process runs for well over an hour under normal circumstances or even longer for heavy load. Use of screen command advised.\n\nAfter the above script completed we ran the next sub-steps manually:\n\n1. `Distinct join to Service Audit` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)\n2.  `Report stats to Graphite` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#report-stats-to-graphite)\n3. `Drop hourly partitions` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#drop-hourly-partitions)\n4. `Upsert to HBase (Migration)` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\n5. `Send reports to business users` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#send-reports-to-business-users)\n6. `Duplicates between Impala and Kudu/HBase` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#duplicates-between-impala-and-kuduhbase)",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230301-IM2097021.md": {
        "Description": "```\nToday 02/03/2023 & 23:30 the Following alarms appeared on Cloudera\n\n[Impala: Daemon (pr1node01)]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node02)]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node03)]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node04)]\n[Pause Duration]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node05)]\n[Impala Concurrent Client Connections]\n------------\n[Impala: Daemon (pr1node06)]\n[Impala Concurrent Client Connections]\n```\n\n</b>Actions Taken:</b>\n\n1. Login to Cloudera for PR Site \n2. To identify the Impala query from `Upsert to HBase` we can see logs from the script at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log` as `PRODREST` user. We cite a screenshot that shows the query. Also, we see the url where we can monitor the query progress (paste this url on a firefox opened through terminal), as well as the coordinator.\n   \n   ![logs_screenshot](.media/upsert_to_hbase_logs_query.PNG)\n   > Ndef: These are not logs from that specific script execution, just a sample to see where you can find the query information you need.\n\n3. From `Cloudera > Impala > Queries` we identified the query and noticed that it had stopped getting processed. In addition, we noticed that Impala had stopped processing other queries as well\n4. We cancelled the query that ran for `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` execution. We can cancel the query in two ways.\n   1. From `Cloudera > Impala > Queries` you can click `cancel` at the dropdown next to the query\n   2. From the url that we monitor the query\n5. We restarted Impala daemon role for pr1node01. This solved the problem with this specific node, however the service did not correspond\n6. We restarted Impala daemon role for pr1node04 that was the coordinator for the query. This solved the problem and recovered the service functionality\n7. Upon investigation, we concluded that the change to `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` that stops the parallel execution of the query by Impala daemons (set num_nodes = 1) was the cause of the problem\n8. We scheduled to rerun the `Upsert to HBase` stage the following day after reverting the script to use all Impala daemons for parallel execution.\n9.  On 03/03/2023 \n   - we disabled HBase quotas for ` PROD_IBANK` namespace on PR Site according to [this](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/blob/master/Documentation/MOP/22263_mno_HBASE_TUNING.docx) MoP\n   - we removed `set num_nodes = 1` from `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`\n   - reran the script\n10. The script ran successfully",
        "Actions Taken": "",
        "Affected Systems": "",
        "Action Points": "1. We opened [this](https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/67) issue to investigate and deploy a permanent fix for running `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh` alongside HBase quotas.",
        "Customer Update": "",
        "Our Ticket Response": "```\n03/03/23 00:49:29 Europe/Eastern (MASTROKOSTA MARIA):\nGood evening,\n\nThe root cause is the same as ticket SD2158913. The job that populates the visible table was canceled after consultation with the customer as it was affecting the live streams.\n\nThe job will be scheduled to be rerun after consultation\n\nThank you\n```",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230611-IM2165930.md": {
        "Description": "```\nGood evening\nThe following alert appeared in Grafana:\n\n[DR][ONLINE] Spark Waiting Batches alert\n\nThank you.\n```",
        "Actions Taken": "1. Login to grafana at https://dr1edge01.mno.gr:3000 with personal account\n2. Inspected Monitoring Alerts and Monitoring DR\\PR to confirm which topology has the issue. We saw that the alert line for online topology had no line.\n3. SSH to dr1nodeedge01 and open firefox with `firefox` without root privilege.\n4. From `Yarn` tab on the browser we ensured that the application had failed\n5. Resubmitted topology with `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` script as `PRODREST` user.\n6. From application's logs we concluded it was a kerberos issue.\n ![image](.media/kerberos_error_stream_online.PNG)\n7. The topology was down from ~8.30 pm on Sunday 11/06 and was resubmitted on Monday 12/06. Therefore the merge batch job for 11/06 had completed with lack of data. The devs' team checked that there were 9 transactions that were interrupted during the failure of the spark job so if we reran the script for merge batch there would be 9 double records in hive.\n8. We decided to transfer the prod_trlog_online.service_audit table for par_dt=20230611 from PR to DR Site with distcp according to the procedure described [here](./20201218-IM1389913.md)\n\nNdef: The data from kudu for the specific partition needed manual deletion on 15/06 by devs and HBase needs no manual action since it clears by CleanupHBaseSAS spark job.",
        "Affected Systems": "DR Site Online\n\n[def]: ./media/kerberos",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "```\n12/06/23 17:13:35 Europe/Eastern (MASTROKOSTA MARIA):\nGood evening,\n\nThe root cause is the same as in SD2228613.\n\nLet us know if you need anything else, otherwise we will proceed to close the ticket.\n\nThanks\n12/06/23 11:18:40 Europe/Eastern (MASTROKOSTA MARIA):\nGood morning,\n\nThe topology crashed due to a problem with kerberos as mentioned in SD2228613. It has been resubmitted.\n\nThanks\n```\n\nThe client confirmed that the issue with kerberos was because one of the two domain controllers was out due to patching",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20211105-SD1752317.md": {
        "Description": "```\nCan the functionality of Navigator be checked on the primary? It does not bring the information from Analytics.\n```",
        "Actions Taken": "1. Login to Cloudera Navigator on PR Site (https://xxxx:7187) with your Exxx account\n2. Check analytics tab if you have permissions to see the graphs (If you are in WBDADMIN group you should see)\n3. ssh Exxx@pr1edge01;ssh Exxx@pr1node03;\n4. less /var/log/cloudera-scm-navigator/ & check navigator logs for errors\n5. Errors for invalid columns appeared.\n6. To solve the issue we made the below steps:\n\n```\nPurging the Navigator Metadata Server Storage Directory\n\n\n\na) Identify and take ndef of the Navigator Metadata Server Storage Dir directory by Logging into Cloudera Manager and browsing to Cloudera Management Services > Configuration > Navigator Metadata Server > Navigator Metadata Server Storage Dir. Ndef the storage directory location.\n\nb) Stop Navigator Metadata Server by navigating to Cloudera Management Service > Instances, selecting Navigator Metadata Server > Actions > Stop # First Infrom Monitoring and after stop the role.\n\nc) Backup the storage Directory. Example:\n# sudo tar -cpzf /<backup-location/navms_data_backup-`date +%Y%m%d-%H%M`.gz /<Navigator Metadata Server Storage Dir location>\n\nd) Remove the Navigator Metadata Server Storage Dir location. Example:\n# sudo mv /<Navigator Metadata Server Storage Dir location>/ /root/<Navigator Metadata Server Storage Dir location>_bkp\n\ne) Starting 2.9 release whenever NMS Solr data directory is purged, database table need to be modified to make sure that state is in sync. To do this run following SQL command in NMS database:\nssh pr1edge01;\nssh pr1node03;\nmysql -uroot -p\nuse navigator_metadata;\ncreate table date_temp_nav_upgrade_ordinal as select * from NAV_UPGRADE_ORDINAL;\ndelete from NAV_UPGRADE_ORDINAL;\ninsert into NAV_UPGRADE_ORDINAL values(-1, -1);\n\nf) Start Navigator Metadata Server and it will recreate data directory and Solr schema on the first run. Cloudera Management Service > Instances, selecting Navigator Metadata Server > Actions > Start\n```\n\n7. Reproduce steps 1 & 2 to check if issue solved.\n8. Infrom Monitoring that all tasks completed.",
        "Affected Systems": "mno Bigstreamer Navigator",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20220415-SD1897262.md": {
        "Description": "```\nGood evening,\n\nWe notice that in some queries we cannot see the details from cloudera manager > impala > queries while they are in the executing phase.\nExample\nhttps://pr1node03.mno.gr:7183/cmf/impala/queryDetails?queryId=e7441b27715b1699%3Ad3527df300000000&serviceName=impala\n\nThanos\n```",
        "Actions Taken": "1. Check that that the problem truly occurs\n\n- Login to CM DR with your pesonal account and go to `Cloudera Manager > impala > queries`\n- Select a query that it on executing phase and click on `Query details`\n\nWhen we tried to checkup on Impala the querie we couldn't see the \"Query details\" and the message of every query was \"Waiting on Client\"\n\n\n2. As a part of the investigation we created `SR 3-29589386011`\n\n3. Acording to Oracle's suggestion problem occurred due to impala query and session timeout. So we had to set the below values: \n\n- Login to CM DR with your pesonal account and go to `Cloudera Manager > Hue >  Configuration > Hue Service Advanced Configuration Snippet (Safety Valve) for hue_safety_valve.in` and set the below values: \n\n```bash\n[impala]\nquery_timeout_s=60\nsession_timeout_s=60\nclose_queries =true\n\n[desktop]\n[[auth]]\nidle_session_timeout=300\n```\n\n4. `Restart` Hue Service\n\n5. From `Cloudera Manager > Impala > Configuration and change below values:\n- idle_query_timeout: 1 min\n- idle_session_timeout: 1 min\n\n6. `Restart` Impala Service",
        "Affected Systems": "Disaster Site",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20201120-IM1361249.md": {
        "Description": "```\n[DR][IBANK] Spark Waiting Batches alert\n[DR][ONLINE] Spark Waiting Batches alert\n\n```",
        "Actions Taken": "1. Login to `https://dr1edge01.mno.gr:3000` with personal account\n2. Inspected `Monitoring Alerts` and `Monitoring DR\\PR`\n3. From the `Monitoring Alerts` check the graphs `spark waiting batches` to find which topology has delays`\n4. Open MobaXterm `dr1edge01` ssh with your personal account\n5. Execute `firefox`\n6. Click the `DR` bookmark\n7. Check the logs of failed spark topology.\n8. Login to `DR cloudera manager` with your personal account\n9. Go to `CHARTS-->KAFKA_KUDU_DISK_UTIL` and see if abnormal rates on disk util exists\n10. Go to `firefox from moba` and check for every kudu tablet on `DR SITE` the `memory(detail)`. If its greater than 90% then restart the tablet instance on `DR KUDU`.\n11. Check again the graphs `Monitoring Alerts` and `Monitoring DR\\PR`\n12. If the alert appeared `(in our case appeared on PROD_Online_IngestStream topology)` restart the specific topology\n13. Open a new tab on MobaXterm `dr1edge01` ssh with your personal account\n14. sudo -iu `PRODREST`\n15. yarn application -list | grep \"PROD_Online_IngestStream\"\n16. yarn application -kill `<application_id>`\n17. Start again the topology `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh`\n18. Check from Graphana again the graphs `Monitoring Alerts` and `Monitoring DR\\PR`.\n19. Go to spark jobs history and click running.\n20. Click `Running-->Online_Ibank_IngestStream-->Streaming`\n21. Refresh every 1sec to until the `active batches` is 0.",
        "Affected Systems": "Disaster Site IBANK",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20220630-IM1896751.md": {
        "Description": "```\nAn Alert appeared in Grafana Application.\n\n[DR][ONLINE] Spark Waiting Batches alert.\n\nPlease for your attention.\n\nThank you.\n```",
        "Actions Taken": "1. Login to `dr1edge01` and open firefox\n2. At the YARN UI search for `PRODREST` and sort by End date. You will find the failed application.\n3. From the UI we saw that Spark exited due to errors related to HBase timeouts.\n4. From DR site's Cloudera Manager we saw that HBase Regionservers restarted due to Out-Of-Memory errors and are now healthy again.\n5. Using this [document](/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/online.md#spark-streaming) we re-submitted the failed topology and the alarm was cleared:\n    ```\n    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh\n    ```\n6. We informed the customer that a **workaround** has been implemented. The ticket status from this point is \"Workaround Provided\".\n7. The logs and resources charts from Cloudera Manager did not indicate a specific reason for the restarts.\n8. After investigating HBase tables we identified that 2 tables from the `PROD_BANK` namespace although they have low traffic overall, they have high traffic per region. Since each region is hosted on only one Regionserver, so this application pattern creates hotspots on specific servers and does not utilize the whole cluster properly. Below you can find the charts that show this behavior.\n\nWorkload per region\n\n```\nselect delete_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'\n```\n\n![DR_DELETE.PNG](.media/DR_DELETE.PNG)\n![DR_DELETE_12H.PNG](.media/DR_DELETE_12H.PNG)\n\n```\nselect get_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'\n```\n\n![DR_GET.PNG](.media/DR_GET.PNG)\n![DR_GET_12H.PNG](.media/DR_GET_12H.PNG)\n\n```\nselect increment_rate_across_hregions + append_rate_across_hregions + mutate_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'\n```\n\n![DR_WRITE.PNG](.media/DR_WRITE.PNG)\n![DR_WRITE_12H.PNG](.media/DR_WRITE_12H.PNG)\n\nUneven workload between Regionservers\n\n```\nselect read_requests_rate, write_requests_rate\n```\n\n![DR_LOAD_REGIONS.PNG](.media/DR_LOAD_REGIONS.PNG)\n![DR_LOAD_REGIONS_12H.PNG](.media/DR_LOAD_REGIONS_12H.PNG)\n\n9. Inform the customer of the findings\n```\nThe problem did not occur today. Looking at the overall picture of HBase, we notice that its usage has increased significantly. Specifically, we see tables in the Namespace \"PROD:BANK\" with a high workload throughout the day and with an application-level configuration that creates uneven load distribution between Region Servers.\n\nYou should immediately proceed with the actions that have been suggested for benchmarking and tuning HBase as a service and the applications that use it in order to optimize its use. If there is nothing else, please let us know if we can proceed with closing the ticket.\n```",
        "Affected Systems": "Disaster Site HBase",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20240302-IM2285747.md": {
        "Description": "```\nGood evening,\n\ncould you please check the merge batch in DR/pr for 2/29. It seems to be still running. We have disabled it on 1/31 until this is finished.\n\nThanks,\nThanos\n```",
        "Actions Taken": "",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "1. Since the data was already loaded into PR for 29/2, the rest of the steps were executed manually on it\n   according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#batch instructions for\n   steps beyond the Merge Batch .\n2. For all the remaining dates since 29/2 and because the cron jobs were stopped, each day was executed\n   manually in full in each site, half of them (days) in PR and half of them (days) in DR following\n   all the steps in the sub-steps guide from https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md .\n3. For each job completed in one site it was replicated over to the other using HDFS replication, through\n   the destination's Cloudera Manager for both `service_audit` and `service_audit_old` tables. **A similar\n   procedure for table replication exists in [Table Replication](./20201218-IM1389913.md) but not for HDFS replication.**\n   - /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit_old/par_dt=$date\n   - /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit/par_dt=$date\n4. Once replicated the HBase Upsert step was run on the destination site according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .\n5. HBase markers were manually set for each job that didn't complete automatically according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .\n6. Kudu range partitions were cleaned up manually for all previous days and 3 new partitions were created\n   for the 3 previous days, in order for the cleanup script to continue functioning as nornal. More details\n   on this can be found at the end of this document.\n\n\nResolution for 05/03 HBase Upsert:\n\nFor this operation specifically more resources had to be allocated to the spark-submit job at `/opt/ingestion/PRODREST/ibank/spark/submit/visible_trn_hbase_daily_upsert/submitVisibleTrnToHbaseIndexesDailyUpsert_STABLE.sh`. After its completion resources were reverted back to normal:\n```\nIncrease node count to 12\nDecrease core count to 1\nIncrease tasks to 40\n```\n\nKudu Range Partitions:\n\nRange partitions are created for the `service_audit_stream` table on the `u_timestamp` column. The commands to create and delete them\ncan be found below for some example dates. Ndef that those partitions are in UTC time, so the time to\ncreate/drop must be converted to local time, taking into account DST. For winter we are at GMT+2 so in order\nto include a full day it must range for 22:00 of the previous to 22:00 of the current (where current is the day you want to delete).\n\n```sh\n# Drop\nalter table prod_trlog_ibank.service_audit_stream drop range partition '2023-02-26T22:00:00.000000Z' <= VALUES < '2023-02-27T22:00:00.000000Z' ;\n\n# Create\nalter table prod_trlog_ibank.service_audit_stream add range partition '1970-01-01T00:00:00.000000Z' <= VALUES < '2023-02-28T22:00:00.000000Z' ;\n```\n\nThe example above includes the first partition, which tracks from 1970 to the date in question. When deleting previous partitions, the first one\nmust be recreated in order to include the time from 1970 to the first date, and then you can create the daily partitions. For example, lets say we have\nthe following partitions:\n\n```\n| 1970-01-01T00:00:00.000000Z <= VALUES < 2023-11-29T22:00:00.000000Z | # We want to drop this one in order to include a bigger range\n| 2023-11-29T22:00:00.000000Z <= VALUES < 2023-11-30T22:00:00.000000Z | # We want to drop this one due to policy\n| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\n| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\n| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.\n| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This must not be touched\n| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |\n| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |\n| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |\n```\n\nThe above will become:\n\n```\n| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # This was created by including all dates from 1970 to this one\n| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # This was dropped and recreated\n| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # This was dropped and recreated\n| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This was not touched\n| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |\n| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |\n| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |\n```",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "**Ndef**: All log paths and query executions are found/executed from pr1edge01/dr1edge01.\n\n1. Login to Grafana and make sure that the failed step is the Merge Batch.\n2. Login to DR/PR edge nodes and through the node's firefox check YARN at https://dr1node03:8090 and https://pr1node03:8090 for the PROD_Ibank merge batch job.\n3. Check the stages tab for stages that have been completed for this job. At PR the `insert into` stages had completed after 8h. The RDD stages had failed and continued failing.\n![Yarn UI](.media/IM2285747_1.png)\n![Spark Stages](.media/IM2285747_2.png)\n4. The 8 hour mark gives us some clues as to the failure. In essence after 8 hours kerberos tickets are dropped, leading to continuous authentication failures and timeouts as shown in the logs `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`.\n5. Checked the merge batch logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log` and `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log` and indeed we found authorization errors.\n6. Checked the size and count of the `service_audit_old` table through Impala and HDFS and it was found to be among the largest ever both in size (over 115 GB) and count (60 mil). The query used is `select  count(*) from prod_trlog_ibank.service_audit_old where par_dt=20240229`.\n7. Inspecting the submition script we can see that for each individual sub-script the date is generated anew in the sub-script invocation. This means that since the merge in question spanned multiple dates, all steps started after the day's end wouldn't have completed succesfully.\n8. Inspecting the range partitions through Impala with the query `show range partitions prod_trlog_ibank.service_audit_stream`, we can find multiple leftover range partitions since November. This added further computation time for the already large table.\n\nInvestigation for 05/03:\n\nFor this day, while the size and count were not unusually large the last step of the process, ie. the\nhbase upsert took unusually long and couldn't complete on its own. The process seemed to get stuck on 1\nsingular task as shown by the logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`, which leads us to conclude that a certain record was problematic because all tasks completed\nnormally except a specific one that kept failling even on re-runs.\n\nInvestigation - Summary:\n\n- Greater than most end of the month dates size(over 115 GB) and count (60 mil)\n- Multiple leftover range partitions\n- Execution spanning multiple days, triggering the dynamic date issue with the submition script\n- Stage execution surpassing the 8 hour mark, triggering a known kerberos authorization bug\n- Problematic record for 05/03",
        "References": "",
        "Nfgh": ""
    },
    "20230531-IM2158906.md": {
        "Description": "```\nGood evening\nFollowing ticket SD2221480, the failed job [IBank_Ingestion]-[MergeBatch]-[JOB] appeared again but for d1edge01.mno.gr\n\nThank you.\n```",
        "Actions Taken": "After communicating with the customer, we proceeded to manually kill the job as it was running for over 14 hours and was affecting live production flows.\n\nProceeding to investigate the issue, we saw the following:\n\n1. Going to Cloudera Manager => Yarn => Applications =>\n\n``\nname RLIKE '.*PROD_IBank_MergeBatch' and application_duration > 3h\n``\n\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/Yarn_Applications_Merge_Batch_Prod_Ibank.png)\n\nWe notice that the Merge batch for `20230531` was running `14 hours` without finishing.\n\n2. As a second step, let's check the number of records in `impala` and the space occupied in `hdfs` by each `par_dt` from `20230509` to `20230530`\n\nBelow we see the space at the `hdfs` level:\n\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/HDFS_du.png)\n\nThe number of records in `impala` as an example for the `par_dt` `20230511` and `20230512`:\n\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/impala_query_par_dt_count.PNG)\n\nWhat we observe above is that while the `par_dt` `20230511` and `20230512` they have no difference in the number of records, they are twice as large. Where did this come from and what impact does it have? We will analyze it in the next steps.\n\n3. Let's see how this increase came about:\n\nAnalyzing the sum of length for response_text_data for each service, we notice that from `12/05/2023` onwards the service_name `'/CAMPAIGNMANAGEMENT/GETCAMPAIGNS'` takes up much more space as shown below compared to previous days.\n\nAfter `20230512`\n\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/After_20230512.png)\n\nBefore `20230512`\n\n![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/Before_20230512.png)\n\nUntil 11/05/2023 the average avg(length(response_text_data)) of prod_trlog_ibank.service_audit is stable at ~12K while from 12/05/2023 we see it approximately doubling.\n\nThis resulted in the `Merge Batch` not finishing as since the size of each `json` has doubled it takes much longer to execute the spark job.\n\n4. How did we handle it to get it running?\n\nAs described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#merge-batch) in the subchapter `If the problem is with resources (out-of-memory errors):` we ran the `Merge Batch` in separate chunks of the day.\n\nThe process took 3 days to complete as each chunk of the day took ~9 hours.\n\n5. After it was completed on one site, `distcp` was performed as described in [issue](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/issues/20201218-IM1389913.md) for the `prod_trlog_ibank.service_audit` and `prod_trlog_ibank.service_audit_old` tables with `par_dt` `20230530` on the other site. Finally, on the other site, after `distcp` was completed, we ran [upsert-to-hbase-migration](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)\n\n6. Finally, after all the steps were completed on both sites, the developers manually deleted the old data in `kudu`.\n\nNdef:\n\nAfter all the above was completed, we should proceed with the consultation with the bank, to execute the `DWH` flows for the days that did not run due to the above issue. The `DWH` flow was removed, so we did not need to take any action.",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230611-IM2165544.md": {
        "Description": "",
        "Actions Taken": "1. Login to Cloudera Manager UI for both Sites\n2. We did not notice any other issues on either Site after checking services' charts and did not have any other alerts\n3. The clock offset relates to the ntpd service, so on the affected nodes (dr1edge02, pr1edge01, pr1edge02) we inspected the service's status\n ```bash\n systemctl status ntpd\n ```\n The service was running\n4. We restarted the service on the affected nodes\n ```bash\n systemctl restart ntpd\n ```\n5. After a while the alert cleared up\n6. We guessed that there were actions at the time servers",
        "Affected Systems": "DR Site, PR Site",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "```\n11/06/23 05:04:49 Europe/Eastern (MASTROKOSTA MARIA):\nGood morning,\n\nWe have restarted the ntpd service on dr1edge02, pr1edge01 and pr1edge02 that displayed the clock offset alert. After the restart, the alert cleared.\n\nHas any action been taken on the time servers?\n\nThank you\n```\nDuring communication with the customer it was confirmed that one of the two domain controllers was out due to patches.",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230126-IM2070630.md": {
        "Description": "```bash\nToday 26/1/2023 the following batch job failure appeared on Grafana\n\nApplication: DWH_IBank\nJob Name: EXPORT\nComponent: LOAN_PAYMENT\nStatus: Failed\nDescription: Code:1\nThanks\n```",
        "Actions Taken": "",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "```bash\nGood evening,\n\nAfter investigation, it was determined that there is a problem with two different transactions that occurred on 25/01/2023 which were declared with the same id.\n\nThe external SQL server LoanPaymentDetails restricts the id field to be UNIQUE, hence the expected error displayed by sqoop-export.\n\nSQLException: Violation of PRIMARY KEY constraint 'PK_LoanPaymentDetails'.\nCannot insert duplicate key in object 'srcib.LoanPaymentDetails'.\nThe duplicate key value is (0e86af89-f15c-4b78-8925-08ed8d237805)\n\nThe id of the problematic transaction is 0E86AF89-F15C-4B78-8925-08ED8D237805 and the first transaction has a timestamp of 09:38:13.072489000 while the second one has a timestamp of 2023-01-25 2023-01-25 09:38:13.476066000.\n\nWe suggest deleting one of the two transactions so that we can proceed with the export of the loan payment data for the remaining transactions. The information for both transactions remains in the big data environment\nso it can be retrieved if needed later.\n\nPlease let us know if you agree to proceed with the proposed action.\n\nThank you\n```\n\n```bash\nAfter communication via email, we proceeded to reconstruct the table, keeping only the record with timestamp 09:38:13.072489000 and reran the job. Please, investigate this on your part and take the necessary actions so that duplicate records are not sent to us. Please confirm that the data is in the SQLServer so that we can close the ticket.\n```",
        "Resolution": "Solution provided by @fgh and @adrint\n\n```bash\n[PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\n```\n\n```bash\n# == DWH LoanPayment ==\n\n# Create table with original data\ncreate table prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig like prod_trlog_ibank_analytical.dwh_details_loan_payment;\n\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt=20230125;\n\n# Create table and insert only required data\ncreate table prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp like prod_trlog_ibank_analytical.dwh_details_loan_payment;\n\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt = 20230125 and id != '0E86AF89-F15C-4B78-8925-08ED8D237805';\n\ninsert into prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp partition(par_dt) select * from prod_trlog_ibank_analytical.dwh_details_loan_payment where par_dt = 20230125 and id = '0E86AF89-F15C-4B78-8925-08ED8D237805' and tr_timestamp = '202301025 09:38:13.072489000' limit 1;\n\n# Overwrite normal table with correct data\ninsert overwrite prod_trlog_ibank_analytical.dwh_details_loan_payment partition(par_dt) select * from prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp where par_dt = 20230125;\n```\n\n```bash\n# Run Export procedure \n/opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/sched_export_to_dwh.sh -t loanPayment\n```\n\n```bash\n[PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\n```\n\n```bash\n# Drop temporary table\ndrop prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_tmp purge\n\n# TODO Check data and drop backup table with initial orginal data\ndrop prod_trlog_ibank_analytical.20230126_dwh_details_loan_payment_orig purge\n```",
        "Recommendations": "",
        "Root Cause Analysis": "Analysis was performed in collaboration with @lmn and @iaravant\n\n1. Check MapReduce logs from YARN UI - App Name: PROD_IBank_DWH_EXPORT_LoanPaymentDetails_*\n\n   ![IM2070630_yarn_app](.media/IM2070630_yarn_app.png)\n\n   ![IM2070630_yarn_mapreduce](.media/IM2070630_yarn_mapreduce.png)\n\n   There was a duplicate entry in prod_trlog_ibank_analytical.dwh_details_loan_payment_stg.\n   Duplicate Key Value: 0E86AF89-F15C-4B78-8925-08ED8D237805\n\n2. Check the tables from Impala Shell with PRODUSER\n\n   ```bash\n   [PRODUSER@dr1edge01 ~]$ impala-shell -k -i dr1edge.mno.gr --ssl\n   ```\n\n   Check for duplicates in dwh_details_loan_payment_stg and dwh_details_loan_payment with id=0E86AF89-F15C-4B78-8925-08ED8D237805\n\n   ![IM2070630_details_duplicates](.media/IM2070630_details_duplicates.PNG)\n\n   Check service_audit specifically for id=0E86AF89-F15C-4B78-8925-08ED8D237805\n\n   ![IM2070630_service_audit](.media/IM2070630_service_audit.PNG)\n\n   Check service_audit for duplicates\n\n   ![IM2070630_service_audit_monthly](.media/IM2070630_service_audit_monthly.PNG)\n\n   Service_audit contained two entries with the same id=0E86AF89-F15C-4B78-8925-08ED8D237805 and different timestamps.",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    },
    "20230131-IM2074270.md": {
        "Description": "```bash\nGood evening,\n\nThe following failed batch job appeared in the grafana system:\n\napplication : DWH_IBank\njob_name : EXPORT\ncomponent : LOAN_PAYMENT\ndate : 30-01-2023\nstatus : FAILED\ndescription : code 6\nhost : -\n```\n\n</b>Actions Taken:</b>\n\n1. Check Loan Payment - Export Status from Grafana\n\n   ```bash\n   LOCAL MONITOR -> Batch Jobs DR -> DW Jobs\n   ```\n\n   The script excited with Code 6, which means that it timed-out, waiting for the Sqoop MapReduce job to be in running state.\n\n2. Check Impala Query Status\n\n   ```bash\n   Cloudera Manager -> Impala -> Queries -> statement RLIKE '.*details_loan_payment.*' -> Search\n   ```\n\n   ![IM2074270_impala_queries](.media/IM2074270_impala_queries.png)\n\n   Impala query ran successfully.\n\n3. Check MapReduce job from YARN\n\n   ![IM2074270_yarn_apps](.media/IM2074270_yarn_apps.png)\n\n   There wasn't any job for Loan Payment,so it had never been submitted.\n\n4. Check export logs\n\n    ```bash\n    #from dr1edge01\n    less /var/log/datawarehouse-ibank/PRODUSER/sched_export.log\n    ```\n\n   ![IM2074270_export_logs](.media/IM2074270_export_logs.png)\n\n   There was a Connection Timed Out error from sqoop-eval which indicates a problem with the SQLServer.\n\n5. Check export script\n\n    ```bash\n    #from dr1edge01\n    less /opt/ingestion/PRODUSER/datawarehouse-ibank/export_to_dwh/export_to_dwh.sh\n    ```\n\n   ![IM2074270_export_script](.media/IM2074270_export_script.png)\n\n   As we can see in the image above, sqoop-export runs after sqoop-eval. In our case sqoop-export did not run because sqoop-eval exited with error.\n\n6. Inform Customer and ask for a rerun",
        "Actions Taken": "",
        "Affected Systems": "",
        "Action Points": "",
        "Customer Update": "",
        "Our Ticket Response": "```bash\nGood evening,\n\nfrom the analysis of the logs we see that sqoop was not submitted because the evaluation for SQL Server was run first, which crashed with a Network I/O exception. (Connection Timed Out)\n\nPlease rerun the job.\n\nWe also see that the DWH started today at 12. Because at this time we have an increased chance of the SQL Server evaluation crashing as the traffic on it is increased, could you inform us why the execution of the DWH was delayed?\n\nThank you\n```",
        "Resolution": "",
        "Recommendations": "",
        "Root Cause Analysis": "",
        "Investigation": "",
        "References": "",
        "Nfgh": ""
    }
}