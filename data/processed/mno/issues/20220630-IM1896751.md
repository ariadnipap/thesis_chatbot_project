---
title: HBase RegionServer OOM and Spark Waiting Batches Alert on DR Site
description: Spark streaming job in DR cluster failed due to HBase RegionServer OOM and high per-region traffic in PROD:BANK namespace; issue mitigated by restarting streaming job, root cause identified as uneven HBase load.
tags:
  - mno
  - bigstreamer
  - hbase
  - regionserver
  - dr1edge01
  - spark
  - streaming
  - online ingestion
  - grafana
  - spark waiting batches
  - prodrest
  - hbase hotspotting
  - workload imbalance
  - workaround
  - hbase performance
  - region load
  - memory issue
  - im1896751
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM1896751
  system: mno BigStreamer - DR Site
  root_cause: Spark job failure caused by HBase RegionServer Out-of-Memory errors due to hotspotting in PROD:BANK HBase namespace
  resolution_summary: Streaming job resubmitted; HBase cluster stabilized; client advised to tune table design to reduce per-region load
  workaround: Job manually restarted
  downstream_risks: Potential recurrence unless application logic or table distribution is improved
---
# mno - BigStreamer - IM1896751 - Alert in Grafana Application
## Description
A [DR][ONLINE] Spark Waiting Batches alert appeared in Grafana, indicating a failure in the Spark streaming job on the DR site.
## Actions Taken
1. Login to `dr1edge01` and open firefox
2. At the YARN UI search for `PRODREST` and sort by End date. You will find the failed application.
3. From the UI we saw that Spark exited due to errors related to HBase timeouts.
4. From DR site's Cloudera Manager we saw that HBase Regionservers restarted due to Out-Of-Memory errors and are now healthy again.
5. Using this [document](/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/online.md#spark-streaming) we re-submitted the failed topology and the alarm was cleared:
```bash
/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh
```
6. We informed the customer that a **workaround** has been implemented. The ticket status from this point is "Workaround Provided".
7. The logs and resources charts from Cloudera Manager did not indicate a specific reason for the restarts.
8. After investigating HBase tables we identified that 2 tables from the `PROD_BANK` namespace although they have low traffic overall, they have high traffic per region. Since each region is hosted on only one Regionserver, so this application pattern creates hotspots on specific servers and does not utilize the whole cluster properly. Below you can find the charts that show this behavior.
Workload per region
```sql
select delete_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'
```
![DR_DELETE.PNG](.media/DR_DELETE.PNG)
![DR_DELETE_12H.PNG](.media/DR_DELETE_12H.PNG)
```sql
select get_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'
```
![DR_GET.PNG](.media/DR_GET.PNG)
![DR_GET_12H.PNG](.media/DR_GET_12H.PNG)
```sql
select increment_rate_across_hregions + append_rate_across_hregions + mutate_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'
```
![DR_WRITE.PNG](.media/DR_WRITE.PNG)
![DR_WRITE_12H.PNG](.media/DR_WRITE_12H.PNG)
Uneven workload between Regionservers
```sql
select read_requests_rate, write_requests_rate
```
![DR_LOAD_REGIONS.PNG](.media/DR_LOAD_REGIONS.PNG)
![DR_LOAD_REGIONS_12H.PNG](.media/DR_LOAD_REGIONS_12H.PNG)
## Our Ticket Response
9. Inform the customer of the findings
```
The problem did not occur today. Looking at the overall picture of HBase, we notice that its usage has increased significantly. Specifically, we see tables in the Namespace "PROD:BANK" with a high workload throughout the day and with an application-level configuration that creates uneven load distribution between Region Servers.
You should immediately proceed with the actions that have been suggested for benchmarking and tuning HBase as a service and the applications that use it in order to optimize its use.
```
## Affected Systems
Disaster Site HBase