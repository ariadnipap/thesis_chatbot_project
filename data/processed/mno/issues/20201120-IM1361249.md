---
title: [DR][IBANK] Spark Waiting Batches Alert and IngestStream Topology Recovery
description: Alert from Grafana about waiting Spark streaming batches on DR site for IBANK and ONLINE systems. Root cause traced to a bottleneck in the PROD_Online_IngestStream topology. Issue resolved by checking Kudu memory usage and restarting the faulty Spark topology.
tags:
  - bigstreamer
  - mno
  - ibank
  - dr site
  - grafana
  - spark
  - spark streaming
  - spark waiting batches
  - ingeststream
  - kudu
  - memory usage
  - topology restart
  - yarn
  - cloudera manager
  - application id
  - streaming recovery
  - spark troubleshooting
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM1361249
  system: MNO BigStreamer - DR IBANK
  root_cause: High memory usage in Kudu tablets and backlog in the `PROD_Online_IngestStream` topology led to delayed Spark batches
  alert_source: Grafana Monitoring Alerts dashboard
  triggered_alerts:
    - [DR][IBANK] Spark Waiting Batches
    - [DR][ONLINE] Spark Waiting Batches
  action_taken:
    - Investigated Grafana dashboards for delayed topologies
    - Checked Kudu tablet memory usage and disk utilization
    - Restarted `PROD_Online_IngestStream` Spark topology
    - Verified resolution through Spark UI and Grafana batch metrics
  outcome: IngestStream topology recovered and Spark batches processed normally
---
# mno - BigStreamer - IM1361249 - Spark Waiting Batches Alert - Grafana
## Description
Grafana raised critical alerts for `[DR][IBANK]` and `[DR][ONLINE]` due to prolonged Spark waiting batches. Investigation revealed issues with the `PROD_Online_IngestStream` topology on the DR site, likely caused by resource bottlenecks in Kudu or Kafka ingestion delays.
## Actions Taken
1. Login to `https://dr1edge01.mno.gr:3000` with personal account
2. Inspected `Monitoring Alerts` and `Monitoring DR\PR`
3. From the `Monitoring Alerts` check the graphs `spark waiting batches` to find which topology has delays`
4. Open MobaXterm `dr1edge01` ssh with your personal account
5. Execute `firefox`
6. Click the `DR` bookmark
7. Check the logs of failed spark topology.
8. Login to `DR cloudera manager` with your personal account
9. Go to `CHARTS-->KAFKA_KUDU_DISK_UTIL` and see if abnormal rates on disk util exists
10. Using Firefox on `dr1edge01`, navigate to the Kudu tablet Web UI for each DR node and inspect the **Memory Usage (detailed)** section. If any tablet exceeds 90% memory usage, restart the affected Kudu tablet via Cloudera Manager or systemd.
11. Check again the graphs `Monitoring Alerts` and `Monitoring DR\PR`
12. If the alert appeared `(in our case appeared on PROD_Online_IngestStream topology)` restart the specific topology
13. Open a new tab on MobaXterm `dr1edge01` ssh with your personal account
14. sudo -iu `PRODREST`
15. yarn application -list | grep "PROD_Online_IngestStream"
16. yarn application -kill `<application_id>`
17. Start again the topology `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh`
18. Check from Graphana again the graphs `Monitoring Alerts` and `Monitoring DR\PR`.
19. Go to spark jobs history and click running.
20. Click `Running-->Online_Ibank_IngestStream-->Streaming`
21. On the Spark Job UI for `Online_Ibank_IngestStream`, monitor the **Active Batches** section and ensure the count drops to 0, indicating no backlog.
## Affected Systems
Disaster Site IBANK