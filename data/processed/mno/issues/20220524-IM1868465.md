---
title: iBank_Migration Historical Batch Job Failed Due to Impala OOM
description: The iBank_Migration Historical batch job failed on the Primary site due to Impala out-of-memory (OOM) errors triggered by high-memory queries running during the same window. The issue was mitigated by backing up the data, re-running the script manually, and analyzing Impala query memory usage via Cloudera Manager.
tags:
  - mno
  - bigstreamer
  - ibank
  - impala_insert
  - ibank_migration
  - batch job
  - historical
  - oom
  - out of memory
  - grafana alert
  - cloudera manager
  - pr1edge01
  - sqoop import
  - ibank_histMigrate_aggr
  - merge batch
  - impala
  - memory usage
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM1868465
  system: mno BigStreamer - Primary Site
  root_cause: Impala out-of-memory (OOM) due to high-memory queries overlapping with batch job execution
  user_visible_error: Historical batch job and Impala_Insert component failed on Grafana
  detection_method:
    - Grafana alert
    - Log analysis from ibank_histMigrate_aggr_MergeBatchWithLock_v2.log
    - Impala query chart in Cloudera Manager
  action_taken:
    - Verified Grafana failure for Impala_Insert
    - Identified Impala OOM from log and CM charts
    - Backed up HDFS files for raw data
    - Manually re-executed batch job script
    - Investigated Impala query load during the failure window
  outcome: Job re-executed successfully; customer notified to avoid heavy queries during processing hours
---
# mno - IBANK - IM1868465 - Batch Job failed on Grafana
## Description
On 23/05/2022, the iBank_Migration Historical batch job failed on the Primary Site, as shown in Grafana. The Impala_Insert step failed due to an out-of-memory (OOM) error in Impala, which occurred during a high-load period where multiple large queries were running. This caused the final validation step to compare fewer Impala records than expected from the Sqoop import.
```
application: iBank_Migration
job_name: Historical
component: Impala_Insert
date: 23/05/2022
status: Failed
description:
host: pr1edge01.mno.gr

application: iBank_Migration
job_name: Historical
component: JOB
date: 23/05/2022
status: Failed
description: Impala rcords are less than retrieved sqoop records
host: pr1edge01.mno.gr
```
## Actions Taken
Workaround Steps
1. Check Grafana
Grafana -> LOCAL MONITOR/ Batch Jobs PR
```bash
Historical | Sqoop_Import : SUCCESS 
Historical | Impala_Insert : FAILED
```
2. Check Main script logs
```bash
sudo su - PRODREST
crontab -l #find log file
less /var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log
```
We saw the following:
```bash
restval_sqoop_2=1
sqoop import failed again
```
According to the log file, the first step of the Main Script failed with exit code 1.
More Info for the Steps here:[<https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#mssql-sqoop-import>]
3. Check "MSSQL Sqoop Import" step logs:
```bash
less /opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log
```
We found out the the INSERT Impala query failed with error: Memory limit exceeded.
4. Backup prod_trlog_ibank.historical_service_audit_raw_v2 files
```bash
hdfs dfs -mkdir /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/20220524_import_historical_service_audit_raw_v2_dvoul
hdfs dfs -mv /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2/* /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/20220524_import_historical_service_audit_raw_v2_dvoul/
```
5. Run Main script with PRDREST user
```bash
screen
/opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh >> /var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log
```
A copy of the main batch script was used to re-run the process safely. The historical data files were backed up before reprocessing, ensuring no data loss.
## Root Cause Analysis
Now that we are certain that we have an Impala OOM issue, we have to check Impala resources specifically.
1. From Cloudera Manager select Impala Service and check the Out of Memory Impala Queries Chart
In our case we had a large number of OOM quries between 23:00 and 02:00.
2. Check for large queries in the corresponding time slot
Cloudera Manager -> Impala -> Queries
Select those that have a lot of Aggregate Peak Memory Usage from the filters on the left.
## Our Ticket Response
```
The topology has been re-executed and is in running status. The analysis revealed that there was an Out of Memory problem in Impala due to some queries that ran between 23:00 and 02:00 and requested many MB of memory.
These specific queries are hindering the production process. Attached you will find the relevant screenshots.
We ask for your immediate actions as well as for the control of your own flows during the above period.
```
![PR_OOM_Impala_Queries.png](.media/PR_OOM_Impala_Queries.png)
![PR_Queries1.png](.media/PR_Queries1.png)
![PR_Queries2.png](.media/PR_Queries2.png)
## Affected Systems
Primary Site IBank Batch