---
title: HiveServer2 OutOfMemory Crash on dr1node04 Due to Large Queries on pmnt_response_stg_0
description: HiveServer2 on dr1node04 crashed with OutOfMemory errors after executing multiple large queries on dev_trlog_card.pmnt_response_stg_0; identified via JVM heap/GC logs and Cloudera monitoring charts, then resolved by service restart.
tags:
  - mno
  - bigstreamer
  - hiveserver2
  - dr1node04
  - outofmemory
  - java heap space
  - garbage collector
  - pause duration
  - cloudera manager
  - jvm heap
  - gc logs
  - dev_trlog_card
  - pmnt_response_stg_0
  - query failure
  - yarn
  - application timeout
  - hive crash
  - im2024442
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM2024442
  system: mno BigStreamer DR1 HiveServer2
  root_cause: Multiple concurrent heavy queries caused Java heap saturation in HiveServer2, leading to long GC pauses and service crash
  resolution_summary: HiveServer2 on dr1node04 was restarted and returned to normal operation; queries from E30825 and E36254 on pmnt_response_stg_0 table identified as root cause
  affected_component: HiveServer2
  crashing_queries:
    - application_1665578283516_50081
    - application_1665578283516_50084
    - application_1665578283516_50085
    - application_1665578283516_50088
    - application_1665578283516_50089
    - application_1665578283516_50090
    - application_1665578283516_50095
    - application_1665578283516_50096
  affected_table: dev_trlog_card.pmnt_response_stg_0
---
# mno - BigStreamer - IM2024442 - Critical alarm in Cloudera Application - dr1edge01
## Description
A critical alarm appeared in Cloudera Application in dr1edge01.
Hive --> HiveServer2 (dr1node04) // Pause Duration.
## Actions Taken
1. Check HiveServer2 JVM Heap Memory Usage and JVM Pause Time Charts from Cloudera Manager.
```bash
cluster -> Hive -> HiveServer2 -> Charts
```
2. Restart HiveServer2 Instance if needed (workaround).
``` bash
In our case the service had Unexpected Exits due to OutOfMemory. 
```
3. Search for "Java Heap Space" failed Jobs in HiveServer2 Service Logs.
```bash
grep Java /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node 04.mno-gr.log.out
```
Example Output:
![hiveServer2Logs.PNG](.media/hiveServer2Logs.PNG)
4. Check failed Yarn Applications from Cloudera Manager that match those of the previous step.
```bash
Cluster -> Yarn -> Applications -> Filter: "application_type = MAPREDUCE"
```
5. Search for GC Pause Duration in HiveServer2 Service Logs and make sure that the warnings started after the submission of the failed jobs.
```bash
grep GC /var/log/hive/hadoop-cmf-hive-HIVESERVER2-dr1node 04.mno-gr.log.out
```
6. Compare the timestamps of all the above to be sure that you have found the queries that caused the problem.
## Our Ticket Response
```
HiveServer2 of dr1node04 is back up. Services and flows have been checked and there is no problem at this time.
```
```
The following findings emerged from the analysis:
HiveServer2 of dr1node04 crashed from OutOfMemory, because the Java Heap Space was full.
The Pause Duration messages in Cloudera Manager are related to the Garbage Collector delay.
Specifically, from the analysis of the logs we saw that between 14:19 and 15:24, HiveServer2 of dr1node04 was called to manage 8 Queries which crashed with a Java Heap Space error. The GC started throwing warnings from 15:08, as it could not clean the memory of the above. The service crashed with an OutOfMemory error, restarted and returned to normal operation.
Below are details for the specific queries:
14:19 application_1665578283516_50081 user:E30825
14:25 application_1665578283516_50084 user:E30825
14:29 application_1665578283516_50085 user:E30825
14:32 application_1665578283516_50088 user:E30825
14:37 application_1665578283516_50089 user:E30825
14:41 application_1665578283516_50090 user:E30825
15:23 application_1665578283516_50095 user:E36254
15:24 application_1665578283516_50096 user:E36254
All queries are for the table: dev_trlog_card.pmnt_response_stg_0.
```