---
title: All Hosts Report Critical State in Cloudera Due to NFS Unavailability
description: Cloudera Manager reported all nodes on PR and DR clusters as critical due to NFS unavailability, which prevented the Host Monitor from collecting filesystem metrics; confirmed false alarm as flows ran successfully and issue cleared after freeing up NFS space.
tags:
  - mno
  - bigstreamer
  - cloudera manager
  - nfs
  - host monitor
  - node metrics
  - pr1edge01
  - dr1edge01
  - grafana
  - critical state
  - cloudera-scm-agent
  - timeout
  - cluster health
  - false positive
  - im2241809
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM2241809
  system: mno BigStreamer PR & DR Sites
  root_cause: NFS mount point on all nodes became unavailable due to full capacity, preventing Host Monitor from retrieving metrics
  resolution_summary: NFS space was freed, restoring monitoring functionality; services remained operational throughout the incident
  affected_nodes:
    - pr1edge01
    - dr1edge01
    - all PR/DR hosts
  false_alert: true
---
# mno - BigStreamer - IM2241809 - Every Host on PR1 and DR1 are in critical state
## Description
All nodes in PR and DR clusters appeared as critical in Cloudera Manager due to NFS becoming unavailable. This prevented the Host Monitor from retrieving node metrics. However, all jobs continued to run correctly and the issue was resolved after the NFS storage was freed.
All Hosts in Dr1 and PR1 are in critical state
## Actions Taken
### Investigation Steps
1. Login to `PR` and `DR` cloudera manager in order to check the health of each cluster. The status was unhealthy for all services on both clusters.
2. Login to `Grafana` in order to check that applications running. All the applications were running without errors.
3. ssh to `pr1edge01.mno.gr` with personal account
4. sudo to root
5. Move to the log folder:
```bash
cd /var/log
```
6. Check messages file
```bash
less messages
```
The output was:
![image](.media/IM2241809/pr1edge01_messages.png)
7. From the above output we saw that at `22:13:02 pr1edge01_kernel: nfs: server 999.999.999.999 not responding`.
8. Now lets check the `agent logs` of an internal node.
9. ssh to `pr1node03.mno.gr` with personal account
10. sudo to root
11. Move to the log folder:
```bash
cd /var/log/cloudera-scm-agent
```
12. Check `cloudera-scm-agent.log` file
```bash
less cloudera-scm-agent.log
```
The output was:
![image](.media/IM2241809/pr1node03_agent_logs.png)
13. Due to unavaliability of `nfs storage`(responisibility of the customer to maintain), `Host Monitor` service of Cloudera management services had `timeout` errors because couldn't collect metrics from each filesystem of the nodes.
14. Customer informed that `nfs storage` caused the issue on both clusters and the unhealthy state of all services was not real because `Host Monitor` was not able to collect metrics in order to be appeared on `CM`. Also all flows ran without errors during the issue.
15. Customer informed us that the `nfs` storage was full and after their actions it's ok. We checked the `CM` and all the services now is healthy.
## Root Cause Analysis
This problem occurred due to `nfs` unavaliability.
## Our Ticket Response
```
The issue was caused by the nfs storage used on the nodes of both clusters becoming full. This resulted in the host monitor of the cloudera management services timeouting as it was unable to collect metrics for each filesystem of the nodes.
Relevant screenshots are attached showing the above causes of the issue.
Throughout the issue, the flows were up and running as seen in grafana as it was a malfunction of the management services resulting in the incorrect image of all services in Cloudera Manager PR & DR respectively.
After space was freed up on the nfs, both clusters returned to good health.
```
## Affected Systems
Disaster/Primary Site