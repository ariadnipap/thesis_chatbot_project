---
title: Spark Waiting Batches Alert Due to Resource Contention on MySQL Node (dr1node03)
description: Spark Waiting Batches alerts appeared due to physical user jobs monopolizing resources on dr1node03, where MySQL service is hosted, causing permission denied errors; recommendation was made to disable Impala and YARN roles on this node to prevent contention.
tags:
  - mno
  - bigstreamer
  - spark
  - spark waiting batches
  - impala
  - yarn
  - mysql
  - resource contention
  - dr1node03
  - impala daemon
  - node manager
  - grafana
  - alert
  - performance tuning
  - cluster optimization
  - disaster site
  - im2099957
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM2099957
  system: mno BigStreamer DR Site
  root_cause: User-initiated jobs on dr1node03 consumed system resources, affecting MySQL and delaying Spark jobs
  resolution_summary: No action taken directly; recommendation made to disable Impala Daemon and YARN NodeManager on dr1node03 to avoid future contention
  affected_node: dr1node03
  recommendation_links:
    - https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/66
    - https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/196
---
# mno - BigStreamer - IM2099957 - Alert on Grafana
## Description
A `[DR][IBANK] Spark Waiting Batches Alert` was triggered in Grafana due to prolonged Spark job delays. The issue stems from physical user workloads (queries, jobs) monopolizing critical system resources—specifically on `dr1node03` where MySQL is hosted. This results in Spark topologies entering a stalled state due to delayed authorization responses from the overloaded MySQL service.
The following alert appeared in the grafana system:
```
[DR][IBANK] Spark Waiting Batches Alert
```
## Actions Taken
1. The following text has been sent to mno/PM and explains the problem, as well as the recommended actions:
```text
Spark Waiting Batches Problem: The first and most important problem we have is the "Spark Waiting Batches" which opens a ticket for this monitoring. This is due to physical user actions (queries/jobs) that occupy/bind production resources on the disaster site mainly. This results in there being no resources available for the MySQL process (a central point that the entire cluster has a dependency on), the service that does the authorization is unable to process its data in the database and thus causes a delay in the spark topologies until the "permission denied" error is resolved. The spark topologies, while up and running, are unable to process the data, as a result of which they continue to execute after the execution of the jobs that occupied the resources on the server where the MySQL service is located has finished. We do not take any action in this nor can we do anything and mno has asked us to close the ticket directly.
Suggestion: Disable Impala Daemon and YARN Node Manager on dr1node03.mno.gr, pr1node03.mno.gr where the primary MySQL service is located. This will not affect our cluster workload as, as you will see in the attached screenshots:
1. impala_mean.png: The average memory occupied by Impala Daemon is much smaller than the limit (150GB) that we have set even in the evening hours when all the flows "close" the previous day and the largest load is concentrated on the cluster.
```
![impala mean usage](.media/IM2099957/impala_mean.PNG)
``` text
2. impala_total.png: The total memory commitment from Impala cumulatively for all nodes is at its peak about 500GB less than the total available, which means that by removing a node there will still be room for resources even with the addition of new flows
```
![impala total usage](.media/IM2099957/impala_total.PNG)
```
3. yarn.png: throughout the day, the available yarn resources are sufficient even with the removal of more than one node
The above applies to both sites and the screenshots are from the Disaster site, where the most resources are reserved compared to the 2 sites. It is important as in this MySQL has a dependency on the entire cluster.
```
![yarn usage](.media/IM2099957/yarn.PNG)
``` text
By monitoring the remaining nodes of the 2 clusters, we see how they can manage the workload at CPU levels and with the above proposal we will reduce the CPU levels on critical node03 which anyway uses increased CPU for cluster management processes.
```
## Affected Systems
mno Disaster Site
## Action Points
- [Issue 66 – Resource contention on MySQL node (Disable roles)](https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/66)  
- [Issue 196 – Cluster role optimization for Spark performance](https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/196)
