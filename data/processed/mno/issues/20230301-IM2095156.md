---
title: YARN NodeManager Failure Due to Full Root Partition from krb5kdc Log Growth
description: Critical YARN NodeManager alert on pr1node01 caused by krb5kdc logs filling the root partition; issue resolved by log cleanup and adjusting logrotate retention policy across nodes.
tags:
  - mno
  - bigstreamer
  - yarn
  - nodemanager
  - pr1node01
  - krb5kdc
  - logrotate
  - root partition full
  - process status alert
  - disk usage
  - cloudera manager
  - logs cleanup
  - log retention
  - kerberos
  - IM2095156
  - hadoop-yarn
  - weekly rotation
  - ibank
  - streaming crash
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM2095156
  system: mno BigStreamer PR YARN Cluster
  root_cause: krb5kdc logs grew uncontrollably, filling up the root partition on pr1node01 and causing NodeManager process failure
  resolution_summary: Deleted oversized krb5kdc logs from 2022 and implemented new weekly rotation policy across PR and DR nodes
  affected_nodes:
    - pr1node01
    - pr1node02
    - dr1node01
    - dr1node02
  secondary_impact:
    - Streaming and batch jobs failed (see SD2157107 and SD2157111)
  remediation_action:
    - Restarted NodeManager on pr1node01
    - Logrotate policy modified to prevent recurrence
---
# mno - BigStreamer - IM2095156 - Alarm on PRDBA  Cloudera Manager
## Description
A **YARN NodeManager Process Status critical alert** was triggered on `pr1node01` in the PR Cloudera Manager cluster. This caused streaming and batch job failures in the iBank environment.
```
YARM -- Node Manager (pr1node03)
Process Status
```
## Actions Taken
1. Login to Cloudera UI for the PR Site
2. Cloudera > Yarn
3. Upon inspection we noticed that the alert was about pr1node01 (Node Manager) and not pr1node03 (JobHistory Server)
4. Ssh pr1node01 and inspect logs at /var/loh/hadoop-yarn. We could not find the root cause from logs
5. Restart the Node Manager role for the specific node. After the restart the alert disappeared.
6. During further investigation, from Cloudera UI we saw that prior to ```Process Status``` alert there was a ```NODE_MANAGER_LOG_FREE_SPACE``` alert
7. From pr1node01 as root `df -h /`. The usage of `/` was at 98% at that time
8. Upon inspection we noticed that the krb5kdc logs had increased over the last months peaking the monthly log file to ~80G.
9. We proceeded to the removal of krb5kdc log files for 2022.
10. As a permanent solution, we implemented changes to retention policy for krb5kdc logs. Specifically, we changed the rotation to weekly from monthly and the storage to 7 old logs from 12 logs files that it was prior the change.  This change was implemented at pr1node02, dr1node01 and dr1node02 as well.
![logrotate_krb5kdc](.media/IM2095156/IM2095156_logrotate_krb5kdc.PNG)
## Our Ticket Response
```
09/03/23 15:47:41 Europe/Eastern (MASTROKOSTA MARIA):
Following the investigation, we have changed the retention for krb5kdc logs. Specifically, we have set the rotation to be weekly instead of monthly and to keep 7 log files. Note that the monthly krb5kdc log file had reached 80G.
Please let us know if we can proceed with closing the ticket.
```
```
01/03/23 07:16:41 Europe/Eastern (MASTROKOSTA MARIA):
There was a malfunction in the yarn node manager since 3.42, resulting in the ibank and online streaming topologies falling as recorded in ticket SD2157107.
We proceeded to restart at 4:53 to get it back up. During the restart, the online merge batch crashed, which was resubmitted (related ticket SD2157111).
At this time, yarn and the flows are running normally.
From the investigation it appears that the root partition on pr1node01 had filled up, which was caused by the local kdc logs. We have proceeded to clean the corresponding log files and are investigating changes to the retention of the logs to avoid future problems.
```