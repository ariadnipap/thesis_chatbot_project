---
title: Spark Waiting Batches Alert on DR Online Topology Due to Kerberos Failure
description: Spark Waiting Batches alert triggered on DR Online topology due to Kerberos authentication failure caused by domain controller downtime; topology was resubmitted and affected data was restored via distcp.
tags:
  - mno
  - bigstreamer
  - spark
  - spark waiting batches
  - online topology
  - kerberos
  - grafana
  - prodrest
  - dr1edge01
  - hbase
  - kudu
  - distcp
  - yarn
  - cluster monitoring
  - ticket im2165930
  - sd2228613
  - authentication error
  - domain controller
  - merge batch
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM2165930
  related_issue: SD2228613
  system: mno BigStreamer DR Site
  root_cause: Spark Streaming job failed due to Kerberos errors caused by domain controller being down for patching
  resolution_summary: Resubmitted Spark topology, avoided duplicate merge batch inserts by transferring Hive data from PR via distcp and cleaning Kudu manually
  affected_components:
    - Spark Streaming Online
    - Kerberos
    - prod_trlog_online.service_audit
    - DR Site Merge Batch
  remediation_actions:
    - Resubmitted topology
    - Avoided duplicate inserts
    - Manual Kudu cleanup
    - Verified HBase cleared via CleanupHBaseSAS
---
# mno - BigStreamer - IM2165930 - Alert at Grafana
## Description
A Spark Waiting Batches alert was triggered on the DR Online topology due to a Kerberos failure caused by a domain controller outage. The topology was restarted and missing data was synced from PR via distcp, avoiding duplication in downstream merge batch jobs.
The following alert appeared in Grafana:
```
[DR][ONLINE] Spark Waiting Batches alert
```
## Actions Taken
1. Login to grafana at https://dr1edge01.mno.gr:3000 with personal account
2. Inspected Monitoring Alerts and Monitoring DR\PR to confirm which topology has the issue. We saw that the alert line for online topology had no line.
3. SSH to dr1nodeedge01 and open firefox with `firefox` without root privilege.
4. From `Yarn` tab on the browser we ensured that the application had failed
5. Resubmitted topology with `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh` script as `PRODREST` user.
6. From application's logs we concluded it was a kerberos issue.
 ![image](.media/kerberos_error_stream_online.PNG)
7. The topology was down from ~8.30 pm on Sunday 11/06 and was resubmitted on Monday 12/06. Therefore the merge batch job for 11/06 had completed with lack of data. The devs' team checked that there were 9 transactions that were interrupted during the failure of the spark job so if we reran the script for merge batch there would be 9 double records in hive.
8. We decided to transfer the prod_trlog_online.service_audit table for par_dt=20230611 from PR to DR Site with distcp according to the procedure described [here](./20201218-IM1389913.md)
> Ndef: The data from kudu for the specific partition needed manual deletion on 15/06 by devs and HBase needs no manual action since it clears by CleanupHBaseSAS spark job.
## Our Ticket Response
```
12/06/23 17:13:35 Europe/Eastern (MASTROKOSTA MARIA):
The root cause is the same as in SD2228613.
12/06/23 11:18:40 Europe/Eastern (MASTROKOSTA MARIA):
The topology crashed due to a problem with kerberos as mentioned in SD2228613. It has been resubmitted.
```
The client confirmed that the issue with kerberos was because one of the two domain controllers was out due to patching
## Affected Systems
DR Site Online
[def]: ./media/kerberosthis