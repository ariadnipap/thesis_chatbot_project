---
title: DR Cluster Service Disruptions Due to High-Complexity Impala Query
description: DR Cloudera services appeared unhealthy, causing mass alerts and crashing some applications. Root cause was a high-complexity Impala query that overloaded threads and caused timeouts in Hive, Kudu, and Sentry.
tags:
  - mno
  - bigstreamer
  - dr cluster
  - cloudera
  - impala
  - high complexity query
  - threads cpu time
  - sentry
  - hive timeout
  - kudu timeout
  - grafana
  - service disruption
  - resource spike
  - SD1949713
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: SD1949713
  system: mno BigStreamer - DR Cluster
  root_cause: A complex Impala query with excessive regex and conversions spiked resource consumption and triggered cascading timeouts across Hive, Kudu, and Sentry
  user_visible_error: All Cloudera services reported unhealthy (red) status, with ~120 alert emails received
  detection_method:
    - Grafana alert
    - Cloudera Manager Impala queries tab
    - Service logs showing timeouts and resource exhaustion
  action_taken:
    - Validated alerts in Grafana
    - Identified problem query in Cloudera Manager
    - Investigated thread and resource usage
    - No manual action taken; services recovered after query completed
  outcome: Root cause identified and customer informed; system stabilized automatically after resource usage dropped
---
# mno - BigStreamer - SD1949713 - DR Cluster Service Disruptions
## Description
The DR cluster triggered ~120 alert emails reporting unhealthy service status across the board. On logging into Cloudera Manager, all services were marked red. Some dependent applications had crashed due to this. Some of the applications crashed as well.
## Actions Taken
1. Login to grafana to make sure that the alert is about DR SITE. We noticed that there were alerts for IBANK Spark Waiting Batches but not for Visible which predisposes us for an issue with Kudu.
![ibank_kudu_problem](.media/SD1949713/ibank_kudu_problem.PNG)
2. Login to Cloudera UI for the DR Site.
3. From `Charts>Impala Perf` we noticed increased resource commitment through Impala Pool Reserved and Threads charts.
![Impala Pool Reserved](.media/Impala_pool_reserved.PNG)
![Threads](.media/threads.PNG)
4. From `Cloudera Manager>Impala>Queries` we searched for queries that took place at the time the problem raised. We found that the query with ID 6d44d9525a681fb8:5e536ffc00000000 had Threads:CPU Time 10.7h. Upon inspection through `Query Details` we saw that the query was of high complexity with conversions and comparisons with regex.
![Query](.media/query.PNG)
5. Through Cloudera logs, we noticed that the query impacted the services in the form of timeouts for Kudu and Hive due to slow communication with Sentry Service.
![hive_problem](.media/SD1949713/hive_problem.PNG)
![timeouts_kudu](.media/SD1949713/timeouts_kudu.PNG)
![sentry_problem](.media/SD1949713/sentry_problem.PNG)
6. The issue resolved itself once the query completed execution and the resource usage dropped. No manual recovery steps were required. We informed the client that it was due to a high complexity query ran by a normal user that resulted in an increased undertaking of resources.
## Affected Systems
DR Site
