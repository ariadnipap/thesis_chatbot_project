---
title: IBank_Migration Job Failure - Enrich SA from SA_old (Memory Limit Exceeded)
description: The 'Enrich SA from SA_old' batch job on 30-03-2023 failed due to Impala memory exhaustion during execution on pr1node05; rerun manually after verifying service_audit partition and updating Grafana monitoring DB.
tags:
  - mno
  - bigstreamer
  - ibank
  - service_audit
  - impala
  - memory limit exceeded
  - grafana
  - enrichment
  - upsert to hbase
  - duplicates
  - kudu
  - hbase
  - impala query failure
  - retry job
  - monitoring update
  - prodrest
  - im2180781
  - pr1edge01
  - date_20230330
  - manual rerun
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: SD2180781
  system: mno BigStreamer PR Site
  root_cause: Impala memory limit exceeded on pr1node05 during join query for service_audit enrichment
  resolution_summary: Reran enrichment script after verifying missing par_dt=20230330, then manually updated monitoring DB and ran HBase upsert and duplication report scripts
  related_scripts:
    - ibank_service_audit_insert_join_distinct.sh
    - ibank_visible_trn_hbase_daily_upsert_STABLE.sh
    - report_duplicates_kudu_hbase_impala_STABLE.sh
  related_tables:
    - prod_trlog_ibank.service_audit
    - prod_trlog_ibank.service_audit_stream
    - prod_trlog_ibank.service_audit_old
  monitoring_updated: true
---
# mno - BigStreamer - SD2180781 - Failed job at Grafana 
## Description
On 30-03-2023, the "Enrich SA from SA_old" job failed due to an Impala memory allocation error on pr1node05.
The following failed job appeared in Grafana:
```
Application: IBank_Migration
Job_name: Enrich SA from SA_old
Componment: JOB
Date: 30-03-2023
Status: FAILED
Host: pr1edge01.mno.gr
```
## Actions Taken
1. We have to check first the logs from the failed job `Enrich SA from SA_old` as described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)
Detailed information from the above link:
**User**: `PRODREST`
**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`
**Script**: `/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh` on `dr1edge01.mno.gr`/`pr1edge01.mno.gr` (each edge server submits to a different cluster)
2. The error was the below from logs:
```
ERROR: Memory limit exceeded: Failed to allocate row batch
EXCHANGE_NODE (id=5) could not allocate 1.00 MB without exceeding limit.
Error occurred on backend pr1node05.mno.gr:22000
Memory left in process limit: 27.82 GB
```
3. To verify the below error also checked from Cloudera > Impala > Queries
![impala_query_error](.media/SD2180781/SD2180781_IMPALA_QUERY_ERROR.PNG)
4. Now we have to go to `troubleshooting steps` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit) in order to check that no records are present in `prod_trlog_ibank.service_audit`
``` bash
# eg. 09-11-2019
impala-shell -k --ssl -i ${HOSTNAME/01/} -q "select  count(*) from prod_trlog_ibank.service_audit where par_dt='20191109';"
```
> Ndef : The par_dt is -1 from today
5. No records exists on `par_dt` `20230330`
6. Now we have to run the command `For the previous day:` from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#distinct-join-to-service-audit)
**User**: `PRODREST`
**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log`
If no records exist and no other process is up, you can ran the script again.
- For the previous day:
``` bash
/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh `date -d '-1 day' '+%Y%m%d'` >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1
```
- For a specified date:
``` bash
# e.g. 09-11-2019
/opt/ingestion/PRODREST/historical/ibank_service_audit_insert_join_distinct.sh 20191109 >> /var/log/ingestion/PRODREST/ibank/log/ibank_service_audit_insert_join_distinct.log 2>&1
```
7. When the job finished `succesfully` we have to updated the monitoring postgres database in order for the entry `Enrich SA from SA_old` to appeared green/success in Grafana.
```bash
ssh Exxxx@pr1edge01.mno.gr
sudo -i -u postgres
psql -d monitoring
select * from prod.monitoring where par_dt = 20230330;
INSERT INTO prod.monitoring (application, job_name,component,status,par_dt,start_time,end_time,description,params,host) VALUES ('IBank_Migration','Enrich SA from SA_old','JOB',0,20230330,'2023-03-31 03:18:30.000','2023-03-31 05:00:42.000','','','pr1edge01.mno.gr') ON CONFLICT (application, job_name,component,par_dt) DO UPDATE SET status=0, start_time='2023-03-31 03:18:30.000', end_time='2023-03-31 05:00:42.000',description='';
```
8. Check from Grafana that the job `Enrich SA from SA_old` is now `succeded`
9. After i execute the below steps mannualy from [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md)
a. [Upsert to HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)
**User**: `PRODREST`
**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`
**Script**: `/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh`
**Alerts**:
- IBank_Migration Enrich hbase tables JOB
- IBank_Migration Enrich hbase tables Impala_insert
- IBank_Migration Enrich hbase tables Spark
**Troubleshooting Steps**:
- Use the script logs to identify the cause of the failure
> Ndef: If job failed and the following error appears :`ERROR: RetriesExhaustedWithDetailsException: Failed <num> actions: CallTimeoutException: <num> times, servers with issues: [dr/pr]1node02.mno.gr`,  execute script again. The error has to do with HBase merging/spliting on a region server, but a detailed reason is unknown.
The script uses upsert and can be safely run many times.
- For the previous day:
``` bash
/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh `date -d '-1 day' '+%Y%m%d'`  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1
```
- For a specified date:
``` bash
# e.g. 09-11-2019
/opt/ingestion/PRODREST/common/scripts/ibank_visible_trn_hbase_daily_upsert_STABLE.sh 20191109  >> /var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log 2>&1
```
b. [Duplicates between Impala and Kudu/HBase](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#duplicates-between-impala-and-kuduhbase)
**User**: `PRODREST`
**Script Logs**: `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log`
**Script**: `/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh`
**Alerts**:
- **Not Monitored**
**Troubleshooting Steps**:
- Check `/var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log` for errors
- You can safely skip this step if not running for the previous day
- Sample execution:
``` bash
/opt/ingestion/PRODREST/common/scripts/report_duplicates_kudu_hbase_impala_STABLE.sh `date --date='-1 day' '+%Y%m%d'` prod_trlog_ibank.service_audit_stream prod_trlog_ibank.service_audit_old ibank >> /var/log/ingestion/PRODREST/ibank/log/report_duplicates_kudu_hbase_impala.log 2>&1
```