---
title: MergeBatch Job Failure Due to Increased JSON Payload in response_text_data
description: The IBank_Ingestion MergeBatch job failed after running over 14 hours due to a spike in data volume caused by large JSON payloads in the response_text_data field; the issue was mitigated by splitting the job into three time-based segments and performing distcp and HBase upsert operations afterward.
tags:
  - mno
  - bigstreamer
  - ibank
  - mergebatch
  - spark
  - job failure
  - response_text_data
  - json size
  - campaignmanagement
  - getcampaigns
  - distcp
  - hdfs
  - impala
  - yarn
  - grafana
  - hbase
  - kudu
  - sd2221480
  - im2158906
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM2158906
  related_issue: SD2221480
  system: mno BigStreamer PR Site
  root_cause: The service_name /CAMPAIGNMANAGEMENT/GETCAMPAIGNS caused JSON payloads in response_text_data to nearly double, resulting in prolonged Spark execution times
  resolution_summary: Job was manually terminated, then re-executed in 3 time-based chunks over 3 days; followed by distcp sync to the second site, upsert to HBase, and manual cleanup of old data in Kudu
  affected_component: IBank_Ingestion MergeBatch
  spark_chunk_strategy: 00:00–12:00, 12:00–18:00, 18:00–00:00
  spark_issue_type: long-running job
---
# mno - BigStreamer - IM2158906 - Failed job at Grafana
## Description
On 30/05/2023, the IBank_Ingestion MergeBatch Spark job failed due to large JSON payloads returned by the /CAMPAIGNMANAGEMENT/GETCAMPAIGNS service, doubling data sizes and exceeding resource limits. The issue was resolved by running the job in chunks and syncing results via distcp.
Following ticket SD2221480, the failed job [IBank_Ingestion]-[MergeBatch]-[JOB] appeared again but for d1edge01.mno.gr
## Actions Taken
After communicating with the customer, we proceeded to manually kill the job as it was running for over 14 hours and was affecting live production flows.
Proceeding to investigate the issue, we saw the following:
1. Going to Cloudera Manager => Yarn => Applications =>
``
name RLIKE '.*PROD_IBank_MergeBatch' and application_duration > 3h
``
![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/Yarn_Applications_Merge_Batch_Prod_Ibank.png)
We notice that the Merge batch for `20230531` was running `14 hours` without finishing.
2. As a second step, let's check the number of records in `impala` and the space occupied in `hdfs` by each `par_dt` from `20230509` to `20230530`
Below we see the space at the `hdfs` level:
![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/HDFS_du.png)
The number of records in `impala` as an example for the `par_dt` `20230511` and `20230512`:
![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/impala_query_par_dt_count.PNG)
What we observe above is that while the `par_dt` `20230511` and `20230512` they have no difference in the number of records, they are twice as large. Where did this come from and what impact does it have? We will analyze it in the next steps.
3. Let's see how this increase came about:
Analyzing the sum of length for response_text_data for each service, we notice that from `12/05/2023` onwards the service_name `'/CAMPAIGNMANAGEMENT/GETCAMPAIGNS'` takes up much more space as shown below compared to previous days.
After `20230512`
![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/After_20230512.png)
Before `20230512`
![alt text](KnowledgeBase/mno/BigStreamer/issues/.media/IM2158906/Before_20230512.png)
Until 11/05/2023 the average avg(length(response_text_data)) of prod_trlog_ibank.service_audit is stable at ~12K while from 12/05/2023 we see it approximately doubling.
This resulted in the `Merge Batch` not finishing as since the size of each `json` has doubled it takes much longer to execute the spark job.
4. How did we handle it to get it running?
As described [here](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#merge-batch) in the subchapter `If the problem is with resources (out-of-memory errors):` we ran the `Merge Batch` in separate chunks of the day.
The process took 3 days to complete as each chunk of the day took ~9 hours.
5. After it was completed on one site, `distcp` was performed as described in [issue](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/issues/20201218-IM1389913.md) for the `prod_trlog_ibank.service_audit` and `prod_trlog_ibank.service_audit_old` tables with `par_dt` `20230530` on the other site. Finally, on the other site, after `distcp` was completed, we ran [upsert-to-hbase-migration](https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#upsert-to-hbase-migration)
6. Finally, after all the steps were completed on both sites, the developers manually deleted the old data in `kudu`.
> Ndef: After all the above was completed, we should proceed with the consultation with the bank, to execute the `DWH` flows for the days that did not run due to the above issue. The `DWH` flow was removed, so we did not need to take any action.