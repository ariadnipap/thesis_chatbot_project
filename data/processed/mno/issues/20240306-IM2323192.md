---
title: HDFS Data Directory Status Alert on dr1node02 Due to Failing Disk
description: Cloudera Manager triggered a Data Directory Status alert on dr1node02 caused by a failing disk; resolved through disk replacement, RAID reconfiguration, and validation using bdadiskutility and MegaCli64.
tags:
  - mno
  - bigstreamer
  - cloudera
  - hdfs
  - datanode
  - disk replacement
  - data directory status
  - dr1node02
  - bdadiskutility
  - megacli
  - raid
  - nfs
  - oracle support
  - dr site
  - disk mirroring
  - sd2389640
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: SD2389640
  system: mno BigStreamer DR Site
  affected_node: dr1node02
  root_cause: Data Directory Status alert caused by failing disk in slots s1 and s7 on dr1node02
  resolution_summary: Disks were replaced online with no downtime using bdadiskutility and MegaCli64 to clear cache, reconfigure RAID, and validate partitions; DataNode role restarted successfully
  maintenance_window_required: false
  impact: No downtime
---
# mno - BigStreamer - SD2389640 - hdfs - Data Directory Status
## Description
An HDFS Data Directory Status alert on dr1node02 was triggered due to a failing disk in slots s1 and s7. The issue was resolved through coordinated disk replacement, cache clearing, and RAID reinitialization using MegaCli64.
The following alert has appeared in Cloudera Manager (DR):
```
DataNode (dr1node02)
Data Directory Status
```
## Actions Taken
There are references from the similar issue [20220620-SD1951890.md](20220620-SD1951890.md).
After investigation we saw that the problem occurred due to disk issue on dr1node02.
We communicated with Oracle and disk replacement was scheduled.
> Ndef: that disk replacement perfomerd online so there was no downtime.
We followed the steps as described at [20220620-SD1951890.md](20220620-SD1951890.md) and [sync_mysql.md](sync_mysql.md), which include the following:
1. Stopping the processes that specifically run at the disk slots `s1` and `s7` of the server `dr1node02`. On our case was the hdfs datanode and some yarn applications. We identified them with:
2. Stopping the mysql slaves using the command:
```bash
mysql -u root -p
SHOW SLAVE STATUS\G;
```
3. Ensuring that the no processes are running at the partitions with the following commands:
```bash
lsof /u02
```
```bash
lsof /u08
```
4. Unmounting the two partitions, so the disks can be replaced.
```bash
umount <mountpoint>
```
5. Once the disks have been replaced we ran the following command for both partitions:
```bash
bdadiskutility /u02
```
6. After running the command, we got the following error:
```
Virtual Drive <VIRTUAL_DRIVE_NUMBER> is incorrectly mapped.
<TIMESTAMP> : Error executing 'MegaCli64 CfgLdAdd r0[<ENCLOSURE>:<SLOT>] a0'
<TIMESTAMP> : Error code is 84 . Response is <<
Adapter 0: Configure Adapter Failed
FW error description:
The current operation is not allowed because the controller has data in cache for offline or missing virtual disks.
Exit Code: 0x54>>
Found a disk with a Firmware State of Unconfigured(good).
Successfully cleared the cache for the logical drive.
Successfully added the disk to its own RAID(0) volume.
```
7. After communicating with Oracle Support [SR 3-36895603206 : Wrong disk status after replacement](https://support.oracle.com/epmos/faces/SrDetail?_afrLoop=206254157461870&srNumber=3-36895603206&queryModeName=Technical&needSrDetailRefresh=true&_afrvwxowMode=0&_adf.ctrl-state=iwvcvrye_184), we ran the following commands to solve the issue:
- `For s1 # The disk slot 1 of the server that corresponds to mount point /u02`
- `For s7 # The disk slot 7 of the server that corresponds to mount point /u08`
- Validated if there is a cache pinned for any device, running command:
```bash
MegaCli64 -GetPreservedCacheList -a0 
```
If the old disk has pinned the cache, the command will return something like:
```
Adapter #0
Virtual Drive(Target ID 07): Missing.
Exit Code: 0x00
```
- In this case, the disk in slot 7 had the pinned cache and had to clear.
Remove the pinned cache by running command:
```bash
#MegaCli64 -DiscardPreservedCache -L7 -force -a0 <<<< where -LX should be replaced by the Target ID number reported in previous step.
```
Get the `ENCLOSURE_NUMBER`
```bash
MegaCli64 LdPdInfo a0 | more
```
- Added the virtual disk back
```bash
MegaCli64 CfgLdAdd r0[ENCLOSURE_NUMBER:slot] a0
```
On our case was:
For `s1`
```bash
MegaCli64 CfgLdAdd r0[252:1] a0
```
For `s7`
```bash
MegaCli64 CfgLdAdd r0[252:7] a0
```
Started configuring the disk at `slot1`
```bash
bdadiskutility -f /u02
```
Wait until the mirroring is finished and after that.
Started configuring the disk at `slot7`
```bash
bdadiskutility -f /u08
```
- Checks:
For `s1`:
```bash
parted /dev/disk/by-hba-slot/s1 -s unit chs print
iscsi # Check that all disks appeared
lsblk # Check that all disks appeared
```
For `s7`:
```bash
parted /dev/disk/by-hba-slot/s7 -s unit chs print
iscsi # Check that all disks appeared
lsblk # Check that all disks appeared
```
8. We proceed with the start of the `datanode` role of `dr1node02`