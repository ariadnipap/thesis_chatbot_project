---
title: CDSW Job Failures with "Engine exited with status 34" Due to Pod Sandbox Issues
description: Multiple jobs in Cloudera Data Science Workbench (CDSW) failed with "Engine exited with status 34" due to Kubernetes pod sandboxing issues caused by networking plugin failures on wrkcdsw1. The issue was mitigated by restarting the Docker daemon via supervisor.
tags:
  - bigstreamer
  - cdsw
  - engine status 34
  - job failure
  - sandbox error
  - pod sandbox changed
  - kubernetes
  - kubectl events
  - networking
  - weave
  - docker
  - supervisorctl
  - wrkcdsw1
  - mncdsw1
  - engine restart
  - cni plugin
  - job diagnostics
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IMxxxxxx
  system: abc BigStreamer CDSW
  root_cause: Kubernetes pod sandbox creation failed due to CNI plugin (Weave) failing to assign IP, resulting in engine crash (status 34)
  affected_nodes:
    - wrkcdsw1.bigdata.abc.gr
  user_visible_error: "Engine exited with status 34"
  log_trace: "Failed to create pod sandbox: networkPlugin cni failed to set up pod network: connect: connection refused"
  action_taken:
    - Inspected job logs via CDSW admin UI
    - Retrieved event logs using `kubectl get events`
    - Identified weave network errors from pod sandbox events
    - Restarted CDSW Docker service via `supervisorctl` on wrkcdsw1
    - Validated recovery via `cdsw status` and re-run jobs
  outcome: Jobs executed successfully post-restart; no further failures observed
---
# abc - IM2363704 - CDSW issue -  Engine exited with status 34
## Description
Noticed since 4/8 many job executions in CDSW fail with "Engine exited with status 34" message.
In the logs --> There are no logs for this engine node at this time.
With re-run the jobs are executed normally.
It is observed that many jobs fail during the day.
for example:
https://mncdsw1.bigdata.abc.gr/ccharisis/hardware_failures/engines/0nohdhssxz6uebit
https://mncdsw1.bigdata.abc.gr/ccharisis/forecasting/engines/kx29vx7l91i7x567
## Actions Taken
1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'
2. Select the `Site Administration` tab (must have admin privileges)
3. Select the `Usage` tab which displays the jobs
4. Inspect the jobs in question.
The jobs are in `FAILED` status. The logs for the failed applications are missing.
5. Troubleshoot from the command line:
From `mncdsw1` as root (use personal account and then sudo):
```bash
kubectl get pods -w -A # Wait a pod to fail (namespace should be like default-user-XXX)
# After a while, a pod has failed, describe it
kubectl describe pod -n default-user-XXX XXXXXXXX
```
In some cases the pod has failed but it cannot be seen by the `kubectl describe pod` command. In those cases, we use the `kubectl get events` command on the same namespace and search for the appropriate pod name.
```bash
kubectl get events -n default-user-XXX
```
In our case, we used the `kubectl get events` command:
```logs
60m         Normal    Scheduled                pod/t804rlnpej08xzcg                  Successfully assigned default-user-49/t804rlnpej08xzcg to wrkcdsw1.bigdata.abc.gr
60m         Warning   FailedCreatePodSandBox   pod/t804rlnpej08xzcg                  Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container "166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e" network for pod "t804rlnpej08xzcg": networkPlugin cni failed to set up pod "t804rlnpej08xzcg_default-user-49" network: unable to allocate IP address: Post "
http://127.0.0.1:6784/ip/166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e":
dial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container "166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e" network for pod "t804rlnpej08xzcg": networkPlugin cni failed to teardown pod "t804rlnpej08xzcg_default-user-49" network: Delete "
http://127.0.0.1:6784/ip/166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e":
dial tcp 127.0.0.1:6784: connect: connection refused]
33s         Normal    SandboxChanged           pod/t804rlnpej08xzcg                  Pod sandbox changed, it will be killed and re-created.
```
The jobs were running for about an hour before failing with the message `Pod sandbox changed, it will be killed and re-created.`
6. Restart the docker daemon to restart all containers on `wrkcdsw1`
_At the time of the issue, CDSW had stale configuration that required full restart (outage) which was not desirable_
To avoid applying the settings, restart the service with the same configuration by triggering a restart by `supervisord` deployed as part of the Cloudera agent
<details> ![Danger ahead](https://media3.giphy.com/media/vvzMdSygQejBIejeRO/200w.gif?cid=6c09b952aacsm9yssw6k6q0z5v8ejuy82rjpvw6qdhglcwpu&rid=200w.gif&ct=g) </details>
From wrkcdsw4 as root (use personal account and then sudo):

```bash
/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf status | grep DOCKER
# Sample
# 145071-cdsw-CDSW_DOCKER          RUNNING   pid 39353, uptime 29 days, 0:40:20
/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart 145071-cdsw-CDSW_DOCKER
```
8. Check that the node is operational after the restart
From `mncdsw1` as root (use personal account and then sudo):
```bash
cdsw status # You might have to wait a few minutes
```
9. Inform the customer about the problem
```text
We noticed that some jobs on the wrkcdsw1 node were running for about an hour before they failed with the error "Pod sandbox changed, it will be killed and re-created". To solve this particular error, we restarted the cdsw service on the wrkcdsw1 node and noticed that the failures with a duration of 1 hour stopped and the corresponding jobs were executed normally.
```
## Affected Systems
abc Bigstreamer CDSW