---
title: HDFS Bad Health Due to Zookeeper Timeout on Failover Controllers
description: HDFS health degraded due to Failover Controller roles shutting down on mn1 and mn2 after timeout connecting to Zookeeper. No hardware or network issues were found. High RPC latency likely caused the timeout. Automatic restart was enabled for failover controllers to prevent recurrence.
tags:
  - bigstreamer
  - hdfs
  - failover controller
  - zkfc
  - zookeeper
  - mn1
  - mn2
  - hdfs bad health
  - cloudera manager
  - automatic restart
  - tick time
  - rpc latency
  - failover timeout
  - log investigation
  - service availability
  - high availability
  - root cause analysis
  - failover resilience
last_updated: 2025-05-01
author: ilpap
context:
  issue_id: IM2158478
  system: abc BigStreamer HDFS
  nodes_affected:
    - mn1.bigdata.abc.gr
    - mn2.bigdata.abc.gr
  roles_down: HDFS Failover Controller (ZKFC)
  root_cause: Temporary RPC latency spike led to Zookeeper timeout (tickTime 2000ms)
  fix: Enabled automatic restart for Failover Controller via Cloudera Manager
  log_paths:
    - /var/log/hadoop-hdfs/
    - /var/log/zookeeper/
  references:
    - https://actorsfit.com/a?ID=01750-52f7ffb1-84f3-4d85-a855-e06d619799ce
    - https://community.cloudera.com/t5/Support-Questions/Failover-Controllers-Health-Bad-leads-to-complete-HDFS/m-p/51717
---
# abc - BigStreamer - IM2158478 - HDFS Failover Controller Timeout on mn1 & mn2
## Description
The HDFS health alert was triggered due to failover controller role failures on mn1 and mn2. Below is the original request:
HDFS status is bad as HDFS Failover Controller role is down on nodes mn1, mn2
## Actions Taken
1. Since the failover controller roles were down, we investigated their logs under `/var/log/hadoop-hdfs/` on each host and found that they received a timeout in their connection to zookeeper leading them to shutdown until they were manually restarted.
2. Checking the zookeper server logs under `/var/log/zookeeper/` we observed that they report that the connection had been closed client side. Additionally at the same time frame we checked to see if there were any issues with other services hosted on these nodes, mainly if any Namenodes had any issues, and found none.
3. Similarly from Cloudera Manager we checked the host's event log for any red flags and found none. Lastly we didn't find any network errors on both hosts.
4. Through Cloudera Manager health checking we see certain RPC latency alerts popping up with values above 1500 ms. Additionally we checked Zookeeper Server's tick time under `ZooKeeper->Configuration->Tick Time`.
4. Thus we concluded that, given the zookeeper's tick time of 2000 ms and no other issues found, a spike in latency led to the above timeout and after communicating with the customer we enabled the failover controller's automatic restart to avoid having the failover controllers down for prolonged periods of time. This was done without needing any services or redeploy any configuration by checking the box under `HDFS->Failover Controller->Automatically Restart Processes`.
## References
- [Failover Controller Connection Loss](https://actorsfit.com/a?ID=01750-52f7ffb1-84f3-4d85-a855-e06d619799ce#:~:text=Modify%20the%20zookeeper%20configuration%20file.%20In%20zoo.cfg%2C%20modify,ticktime%20to%204000ms%2C%20and%20the%20default%20is%202000ms.)
- [ZKFC Failure](https://community.cloudera.com/t5/Support-Questions/Failover-Controllers-Health-Bad-leads-to-complete-HDFS/m-p/51717)