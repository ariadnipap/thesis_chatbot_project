---
title: CDSW Jobs Fail to Schedule Due to Resource Exhaustion
description: CDSW jobs in abc BigStreamer failed to schedule due to CPU, memory, and GPU exhaustion; resolved by force-deleting pending and stuck pods after identifying a burst job submission.
tags:
  - bigstreamer
  - abc
  - cdsw
  - kubernetes
  - pod-pending
  - unschedulable
  - cpu
  - memory
  - gpu
  - resource-exhaustion
  - cluster-utilization
  - kubectl
  - stuck-jobs
  - pod-cleanup
  - container-scheduling
last_updated: 2025-05-01
author: ilpap
context:
  environment: BigStreamer
  issue_id: IM1476278
  system: CDSW
  nodes:
    - mncdsw1.bigdata.abc.gr
  tools_used:
    - kubectl
    - CDSW Admin Console
  failure_symptom: CDSW jobs stuck in 'Scheduling' with "Unschedulable: No host has enough CPU, memory and GPU"
  root_cause:
    - Excessive pod creation (800+ jobs on 2025-03-14 07:00)
    - Resource starvation
  resolution:
    - Force-deletion of Pending and Init:0/1 pods
  preventative_actions:
    - Resource monitoring from pod scheduler
    - Alerting when system hits high pending pod count
---
# abc - IM1476278 - CDSW Not enough CPU/GPU/Memory 
## Description
For two consecutive days, no CDSW job could be scheduled due to node resource exhaustion. CDSW displayed the message:
"Unschedulable: No host in the cluster currently has enough CPU, memory and GPU to run the engine."
This was traced to a large-scale job submission event that saturated available resources.
## Actions Taken
1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'
2. Navigate to the Admin console.
3. Select `Activity` tab.
4. Check `CPU` and `Memory` graphs.
5. In our case all scheduled jobs were stuck at `Scheduling` due to a job that ran `800 times at 14/03 07:00`
- Delete all PENDING pods to free resources. This releases blocked resources and allows new jobs to be scheduled.
```bash
[root@mncdsw1 ~]# kubectl get pods
[root@mncdsw1 ~]# kubectl get pods | grep Pending | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force
[root@mncdsw1 ~]# kubectl get pods | grep "Init:0/1" | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force
[root@mncdsw1 ~]# kubectl get pods
```
## Affected Systems
abc Bigstreamer CDSW
## Action Points
If the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a place can be found. However, it will not be killed for excessive CPU usage.
As an action point we could `monitoring compute & memory resource usage` from the Pod status via our monitoring tool.