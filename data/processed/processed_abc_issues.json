[
    {
        "Issue Number": "IM2225720",
        "Description": "Neighbor Tool - Solvatio interface failure",
        "Keywords": [
            "neighb_user",
            "wildfly"
        ],
        "Owner": "u7",
        "Date": "20230928",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230928-IM2225720.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\n\nThey informed us that the Solvatio interface with the Neighbor tool does not work.\n\nThe password of the user neighbor_user expired, we reset it but when we hit the url https://cne.fgh.gr/bigstreamer-api/rest/neighbourAlgorithm/2109700572 (the fixed one you see is mine and you can use it for testing reasons) returns us:\n\n{\"error\": \"org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: [Cloudera][ImpalaJDBCDriver](500164) Error initialized or created transport for authentication: [Cloudera][ImpalaJDBCDriver](500592) Authentication failed..\", \"message\": \"Could not get JDBC Connection; nested exception is java.sql.SQLException: [Cloudera][ImpalaJDBCDriver](500164) Error initialized or created transport for authentication: [Cloudera][ImpalaJDBCDriver](500592) Authentication failed..\"}\n\nPlease for your checks.\n\nThanks\n```",
            "Actions Taken": "```\nhttps://172.25.37.247:8543/serviceweaver/jmx/\n```\n[login_info](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/abc-syspasswd.kdbx?ref_type=heads)\n\nAnd under `servers -> com.xyz.abc_nts.bigstreamer_api.util -> mbeans -> name=BigStreamerApiJmxConfig -> attributes -> ImpalaURL` we change the password :\n\n```\njdbc:impala://172.25.37.237:21090;SSL=1;AuthMech=3;UID=neighb_user;PWD=HERE_ENTER_PASSWORD;\n```\n\nRetrieve the password from the abc Admin or xyz's PM\n\n\nThen start the wildfly service or restart:\n\nstart wildfly:\n`ekl-start`\n\n\nFor checks of the api from Haproxy : \n```\nhttps://cne.fgh.gr/bigstreamer-api/rest/neighbourAlgorithm/phone_number_provided_from_abc\n```\n \nFor checks of the API directly from the servers:\n\n```\nhttps://unekl1/bigstreamer-api/rest/neighbourAlgorithm/phone_number_provided_from_abc\nhttps://unekl2/bigstreamer-api/rest/neighbourAlgorithm/phone_number_provided_from_abc\n```",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "The problem was that the password has expired. So in order to change it after we received a new one from abc was to connect to the following UI :",
            "Recommendations": "Can be found on the following [issue](https://metis.xyztel.com/obss/bigdata/abc/nts/nts-devops-bigstreamer/-/issues/4)",
            "Root Cause Analysis": "Not specified",
            "Investigation": "In order to check the logs of this we did the following:\n```\nssh unekl1/unekl2\nsu - wildfly\nbash\n```\nckeck logs:\n`ekltaillog`",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "GI7",
        "Description": "Change the ssh configuration in all servers in order not to permit root ssh login",
        "Keywords": [
            "ssh",
            "root",
            "backup_user",
            "cron"
        ],
        "Owner": "u1",
        "Date": "20201012",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201012-GI7.md",
        "Detailed Description": {
            "Description": "```\nSecurity requirements from abc mandates that we should change the ssh configuration in all servers in order not to permit root ssh login\n```",
            "Actions Taken": "1. Login to `admin.bigdata.abc.gr` with personal account and change to root with sudo\n2. Inspect the status of sshd on all nodes\n\n``` bash\nsalt '*' cmd.run 'grep \"^PermitRootLogin\" /etc/ssh/sshd_config'\n```\n\n3. Inspect cronjobs of root user on all nodes\n\n``` bash\nsalt '*' cmd.run 'cronjob -l'\n```\n\n4. After collecting the scripts executed by root user, we checked the scripts that contain the words `ssh`, `scp` and `rsync`\n\n``` bash\nfor i in {'/root/disk_balance_with_check.sh' '/usr/local/bin/CM_Config_Backup.sh' '/etc/elasticsearch/elasticsearch_monitoring.sh' '/etc/keepalived/scripts/mysql_check_crontab.sh' '/home/intra/scripts/MySQL_Dump_All_DBs.sh' '/usr/local/bin/krb5prop.sh' '/root/scripts/ldap_check.sh' '/root/send_haproxy_statistics.sh' '/root/send_haproxy_weekly_statistics.sh' '/home/intra/MySQL_Dump_All_DBs.sh' '/home/intra/dfs-backup.sh' '/usr/lib/icom/scripts/cdsw_rsync_backup.sh' '/usr/lib/icom/scripts/cdsw_tar_backup.sh' '/home/intra/MySQL_Dump_spec_DB.sh' '/home/intra/scripts/hue_workflows_all.sh' '/root/hive_logs_retention.sh'}\ndo\nsalt \"*\" cmd.run \"if [ -f $i ]; then grep -e ssh -e scp -e rsync $i; fi\" | grep -B1 -e ssh -e scp -e rsync\ndone\n```\n\n5. User `backup_user` was created by abc admins to substitute the root login in the scripts above. From the above investigation we determined that `backup_user` needs to be able to login to nodes `un1.bigdata.abc.gr` and `wrkmncdsw1.bigdata.abc.gr`\n\n```bash\n  # un1.bigdata.abc.gr\n  setfacl -R -m d:u:backup_user:rwx /data/1/cm_backup/cmdeploys/\n  setfacl -R -m u:backup_user:rwx /data/1/cm_backup/cmdeploys/\n  setfacl -R -m d:u:backup_user:rwx /data/1/cm_backup/db-backups/db-vip\n  setfacl -R -m u:backup_user:rwx /data/1/cm_backup/db-backups/db-vip\n  setfacl -R -m u:backup_user:rwx /data/1/cm_backup/dfs_backup/\n  setfacl -R -m d:u:backup_user:rwx /data/1/cm_backup/dfs_backup/\n\n  # Add backup_user to AllowedGroups in /etc/ssh/sshd_config\n```\n\n``` bash\n  # wrkmncdsw1.bigdata.abc.gr\n  setfacl -R -m u:backup_user:rwx /backup\n  setfacl -R -m d:u:backup_user:rwx /backup\n\n  # Add backup_user to AllowedGroups in /etc/ssh/sshd_config\n```\n\n6. Created `backup user` saltstack state that installs a new private key for passwordless ssh under `/root/backup_user_id/id_backup_user_rsa` on all nodes\n7. Change the following scripts to use `backup_user` instead of `root`:\n\n- /usr/local/bin/CM_Config_Backup.sh\n- /home/intra/scripts/MySQL_Dump_All_DBs.sh\n- /home/intra/MySQL_Dump_All_DBs.sh\n- /home/intra/dfs-backup.sh\n- /usr/lib/icom/scripts/cdsw_rsync_backup.sh\n\n```conf\n# This script uses rsync. For rsync to use another user than the logged in one create /root/.ssh/config with the following contents:\nHost wrkcdsw1.bigdata.abc.gr\n    User backup_user\n    IdentityFile /root/backup_user_id/id_backup_user_rsa\nHost wrkcdsw1.bigdata.abc.gr\n    User root \n    IdentityFile ~/.ssh/id_rsa\n```\n\n- /home/intra/MySQL_Dump_spec_DB.sh\n\n8. Change `PermitRootLogin` on all hosts. The actions mentioned below are executed as `root` from `admin.bigdata.abc.gr`\n\n- Get `/etc/ssh/sshd_config to a uniform state\n\nContents of `/etc/salt/salt/prepare_sshd_config.sh`:\n\n``` bash\ngrep -e \"^PermitRootLogin\" /etc/ssh/sshd_config &> /dev/null\ni=$?\nif [ $i -eq 1 ]; then\n  echo \"The config file is ok\"\nelse \n  sed -i -e 's/^#PermitRootLogin/PermitRootLogin/' /etc/ssh/sshd_config # From step 2 we know that on some files the entry was commented\nfi\n```\n\n- Get `/etc/ssh/sshd_config to a uniform state\n\nContents of `/etc/salt/salt/disable_root_login.sh`:\n\n``` bash\ngrep -e \"^PermitRootLogin no\" /etc/ssh/sshd_config &> /dev/null\nif [ $i -eq 0 ]; then \n  echo \"The config file is ok\"\nelse \n  sed -i -e 's/^PermitRootLogin.*yes/PermitRootLogin no/' /etc/ssh/sshd_config\nfi\n```\n\n- Apply the two scripts on all nodes and reload sshd:\n\n``` bash\nsalt '*' cmd.script salt://prepare_sshd_config.sh\nsalt '*' cmd.script salt://disable_root_login.sh\nsalt '*' cmd.run 'service sshd reload'\n```",
            "Affected Systems": "abc Bigstreamer OS",
            "Action Points": "While investigating the impact of disallowing the root ssh login, we found the following port forwards:\n\n``` bash\n  ssh -g -f gbenet@unekl2 -L 18636:10.255.240.20:3306 -N\n  ssh -g -f gbenet@admin -L 8889:admin:5900 -N\n  ssh -g -f u15@admin -L 8888:172.25.37.237:3000 -N\n  ssh -g -f gbenet@unc1 -L 8743:172.25.37.241:8743 -N\n  ssh -g -f gbenet@unc1 -L 9743:172.25.37.241:8743 -N\n  ssh -g -f root@omnm -L 8888:omnm:5901 -N\n  ssh -g -f root@hedge1 -L 8998:10.255.240.142:8998 -N\n  ssh -g -f intra@un2 -L 2525:172.18.20.205:25 -N\n  ssh -g -f intra@un2 -L 22255:un1:22222 -N\n  ssh -g -f intra@un2 -L 22255:un1:22222 -N\n  ssh -g -f u15@un2 -L 6536:172.25.150.68:5432 -N\n  ssh -g -f gbenet@un2 -L 227:undt1:8522 -N\n  ssh -g -f root@mncdsw1 -L 5555:172.19.53.146:5555 -N\n  ssh -g -f gbenet@undt2 -L 21050:10.255.241.239:3306 -N\n  ssh -g -f u3@undt1 -L 9191:10.95.129.200:9191 -N\n  ssh -g -f gbenet@undt1 -L 9191:10.95.129.200:9191 -N\n  ssh -g -f gbenet@undt1 -L 9621:10.53.166.37:1521 -N\n  ssh -g -f gbenet@undt1 -L 4040:10.255.241.220:3306 -N\n  ssh -g -f gbenet@undt1 -L 9521:10.53.192.187:1521 -N\n  ssh -g -f gbenet@undt1 -L 21050:10.53.192.187:1521 -N\n  ssh -g -f gbenet@undt1 -L 3579:172.26.131.15:3579 -N\n  ssh -g -f gbenet@undt1 -L 8521:10.53.192.192:1521 -N\n  ssh -g -f gbenet@undt1 -L 8522:10.53.192.191:1521 -N\n  ssh -g -f gbenet@undt1 -L 8523:10.53.192.190:1521 -N\n  ssh -g -f gbenet@un1 -L 3579:172.26.131.15:3579 -N\n  ssh -g -f intra@un1 -L 6654:10.255.240.20:3306 -N\n  ssh -g -f intra@un1 -L 6634:10.101.1.230:1521 -N\n  ssh -g -f intra@un1 -L 6433:172.16.109.237:1433 -N\n  ssh -g -f intra@un1 -L 7536:10.255.241.239:3306 -N\n  ssh -g -f intra@un1 -L 6721:172.21.4.68:1521 -N\n  ssh -g -f intra@un1 -L 3389:10.101.6.41:389 -N\n  ssh -g -f intra@un1 -L 6644:10.101.16.169:1521 -N\n  ssh -g -f intra@un1 -L 6633:10.255.240.13:6533 -N\n  ssh -g -f intra@un1 -L 7535:172.24.104.100:5432 -N\n  ssh -g -f intra@un1 -L 6646:10.95.129.43:1521 -N\n  ssh -g -f intra@un1 -L 5525:172.18.20.205:25 -N\n  ssh -g -f intra@un1 -L 6645:10.95.129.41:1521 -N\n  ssh -g -f intra@un1 -L 21060:sn38:21050 -N\n  ssh -g -f intra@un1 -L 7183:10.255.243.215:80 -N\n  ssh -g -f intra@un1 -L 25020:sn88:25020 -N\n  ssh -g -f ipvpn@un1 -L 7180:undt2:22 -N\n  ssh -g -f ipvpn@un1 -L 6531:172.25.119.82:1521 -N\n  ssh -g -f gbenet@un1 -L 8888:undt2:22 -N\n```\n\nBoth `intra` and `root` are no longer allowed to ssh and in a future restart most of the above will not be able to be implemented in the same way.",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2211937",
        "Description": "Add all cyberark subnets into virtualop hosts.allow",
        "Keywords": [
            "ssh",
            "virtualop",
            "hosts.allow",
            "cyberark"
        ],
        "Owner": "u44",
        "Date": "20230904",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230904-IM2211937.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\n\nWe notice that on the virtualop node - 172.26.169.11, sometimes we can connect via Cyberark with ssh and other times it shows us the attached error message.\n\nFrom a first check we did not find anything strange in the ssh configuration. We also restarted the sshd service, but the behavior remains.\n\nCan you check it?\n\nThanks\n```\n\n![IM2211937](.media/IM2211937.jpg)",
            "Actions Taken": "1. After confirming the same problem on our end, ie connecting to the host through Cyberark, we login to the\n   `admin` server first and then ssh into `virtualop`. We checked the sshd config under `/etc/ssh/sshd_config`\n   for any glaring issues, but as was communicated by the customer, none were found.\n\n2. The next step is to check `/var/log/secure` in order to see if the authentication issue is due to the host\n   or if it doesn't even reach it. In there, while performing a test connection, we identified the following\n   log entry `refused connect from 10.53.134.71 (10.53.134.71)` which signifies that the host itself refused\n   the connection.\n\n3. We check for the existence of `/etc/host.allow` and we added the above address, before testing again.\n   The test failed again, this time with a diffrent IP being refused inside `/var/log/secure`, which lead\n   us to believe that Cyberark is using multiple IPs and subnets to facilitate connections.\n\n4. We requested all subnets used by Cyberark from the customer in order to add them to `/etc/hosts.allow`\n   and after doing so no more connection issues appeared. One thing to nfgh here is that for any new subnets\n   that Cyberark will use they must also be added to `/etc/hosts.allow` or the same connection issues will\n   reappear.",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2352302",
        "Description": "idm2.bigdata.abc.gr Change to Faulty State",
        "Keywords": [
            "os"
        ],
        "Owner": "u1",
        "Date": "20240722",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20240722-IM2352302.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\n\nidm2.bigdata.abc.gr --> Not Healthy state.\n```",
            "Actions Taken": "1. After checking the error on Cloudera Manager idm2 was briefly in bad health with error `Swapping`. Checking the graphs for the hosts reported over \nGB (out of 64GB) of physical memory in use.\n2. Logged in with SSH and checked memory per process\n\n    ``` bash\n    ps aux --sort -rss\n    ```\n\n3. The top process consuming memory was `cmf-agent`, which is the Cloudera Manager Agent process. After a quick search we found the following bug [Cloudera bug: OPSAPS-59764: Memory leak in the Cloudera Manager agent while downloading the parcels](https://docs.cloudera.com/cdp-private-cloud-base/7.1.8/manager-release-nfghs/topics/cm-known-issues-773.html)\n4. To verify that this bug is triggered we checked the parcel page of Cloudera Manager and we found that two hosts (`idm1.bigdata.abc.gr`/`idm2.bigdata.abc.gr`) where constantly trying to download some of the parcels that are distributed, but not available anymore.\n5. We proposed to the client to remove the two hosts from the logical cluster, since they did not have any roles.\n\n    ```\n    From Cloudera Manager\n    Hosts > All Hosts > Check `idm1.bigdata.abc.gr` and `idm2.bigdata.abc.gr` > Actions > Remove from Cluster\n    ```\n\n    ```bash\n    systemctl restart cloudera-scm-agent\n    ```\n\n6. After removing the two hosts from the cluster and restarting the Cloudera Manager Agent memory consumption for the two hosts has remained stable.",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1363402",
        "Description": "We see that sn87 has a problem with the CPU (attached). It has been removed from the cluster. Please take action and contact DELL.",
        "Keywords": [
            "hw",
            "host",
            "cpu"
        ],
        "Owner": "u27",
        "Date": "20201125",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201125-IM1363402.md",
        "Detailed Description": {
            "Description": "```\nWe see that sn87 has a problem with the CPU (attached). It has gone out of cluster. Please for your actions and communication with DELL.\n```",
            "Actions Taken": "1. Check Idrac logs for the description error `Overview-->Server-->Logs`\n2. Export the lifecycle logs `Overview-->Server-->Troubleshooting-->Support Assist-->Export Collection` and save the TSR*.zip\n3. Open a case on DELL SUPPORT(2108129800). Dell need the service tag from `Overview` of Idrac\n4. Send them the TSR*.zip\n5. In this case necessary was the update of BIOS & Lifecycle Controller of Idrac\n6. Dell send us the right update files based on our servers `PowerEdge C6320`\n7. Updated the BIOS base on the link `https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Updating%20BIOS%20on%20Dell%2013G%20PowerEdge%20Servers.pdf`\n(to update the BIOS via OS-CLI, see APPENDIX below )\n8. Updated the Lifecycle Controller base on the link `https://dl.dell.com/topicspdf/idrac_2-75_rn_en-us.pdf`\n(to update the Lifecycle Controller via OS-CLI, see APPENDIX below )\n9. After the update of both versions the host was up with the roles stopeed for 1 day.\n10. After 1 day send the lifecycle logs like `Step 2` and forward the zip file to Dell.\n11. If any error exist start the roles.",
            "Affected Systems": "abc Bigstreamer HW",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "APPENDIX  - Procedure for BIOS & F/W upgrade for C6320 with CLI (via OS shell)\n-------------------------------------------------------------------------------\n\n\n- Download new iDRAC FW from link below (Nfgh: download the \".bin\" format, not the \".exe\" format):\n \n https://www.dell.com/support/home/en-us/drivers/driversdetails?driverid=5hn4r&oscode=naa&productcode=poweredge-c6320\n \n  eg: iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \n \n \n- Download new BIOS from (Nfgh: download the \".bin\" format, not the \".efi\" format):\n \n https://www.dell.com/support/home/en-us/drivers/driversdetails?driverid=cctdp&oscode=naa&productcode=poweredge-c6320\n \n eg : BIOS_CCTDP_LN64_2.13.0.BIN\n\n\nProcedure :\n---------------\nLogin to C6320 eg sn75 as root\n\nStore the downloaded files under /tmp/\n\nProcedure executed via OS shell\n\n\nGet current BIOS version \n---------------------------\n[root@sn75 /]# ipmitool  mc getsysinfo system_fw_version\n\n2.3.4\n\n\nGet current iDRAC version\n---------------------------\n[root@sn75 /]# ipmitool   mc info | grep Firmware\n\nFirmware Revision         : 2.40\n\n\n\n\nUPDATE iDRAC PROCEDURE (mc cold restart is preformed automatically) :\n------------------------------------------------------------------------\n```\n[root@sn75 /]# \n\n[root@sn75 tmp]# ll iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \n\n  -rw-r--r-- 1 root root 111350247 Dec  6 14:17 iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN\n\n[root@sn75 tmp]# chmod +x iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \n\n[root@sn75 tmp]# ./iDRAC-with-Lifecycle-Controller_Firmware_5HN4R_LN64_2.81.81.81_A00.BIN \n  Update Package 21.04.200 (BLD_1123)\n  Copyright (C) 2003-2021 Dell Inc. All Rights Reserved.\n  Release Title:\n  iDRAC 2.81.81.81, A00\n  Release Date:\n  July 02, 2021\n  Default Log File Name:\n  5HN4R_A00\n  Reboot Required:\n  No\n  Running validation...\n  iDRAC\n  The version of this Update Package is newer than the currently installed version.\n  Software application name: iDRAC\n  Package version: 2.81.81.81\n  Installed version: 2.40.40.40\n  Continue? Y/N:Y\n  Executing update...\n  WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER PRODUCTS WHILE UPDATE IS IN PROGRESS.\n  THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE!\n  ...............................................................   USB Device is not found\n  ..............................................................   USB Device is not found\n  ...............................................................   USB Device is not found\n  Device: iDRAC\n    Application: iDRAC\n    Failed to reach virtual device. This could be caused by BitLocker or other security software being enabled. For more information, see the\n    Update Package Userâ€™s Guide.\n  The update completed successfully.\n```",
            "Nfgh": "------\n  IF THE ABOVE ERROR IS SHOWN, THEN REBOOT THE iDRAC (\"#ipmitool  -U root -P c0sm0t31 mc reset cold\") \n  \n  and REPEAT to get the below correct output, without the \"Failed to reach virtual device.\" message:!!!\n  \n  \n```  Device: iDRAC\n  Application: iDRAC\n  Update Successful.\n  The update completed successfully.\n```\n\n\nUpdate BIOS PROCEDURE (REBOOT REQUIRED !!!)\n----------------------------------------------\n```\n[root@sn75 /]# cd /tmp\n\n[root@sn75 /]#  ll BIOS_CCTDP_LN64_2.13.0.BIN \n\n[root@sn75 /]#  chmod +x BIOS_CCTDP_LN64_2.13.0.BIN \n\n[root@sn75 /]#  ./BIOS_CCTDP_LN64_2.13.0.BIN \n  Running validation...\n  \n  PowerEdge C6320 BIOS\n  \n  The version of this Update Package is newer than the currently installed version.\n  Software application name: BIOS\n  Package version: 2.13.0\n  Installed version: 2.3.4\n  Continue? Y/N:Y\n  Executing update...\n  WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER PRODUCTS WHILE UPDATE IS IN PROGRESS.\n  THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE!\n  ................................................................................................................\n  Device: PowerEdge C6320 BIOS\n    Application: BIOS\n    The BIOS image file is successfully loaded. Do not shut down, cold reboot, power cycle, or turn off the system, till the BIOS update is complete otherwise the\n    system will be corrupted or damaged. Bios update takes several minutes and it may be unresponsive during that time. Nfgh: If OMSA is installed on the system,\n    the OMSA data manager service stops if it is already running.\n  \n  Would you like to reboot your system now?\n  \n  Continue? Y/N:Y \n``` \n\n\n\n\n\nGet NEW BIOS version \n------------------------\n[root@sn75 /]# ipmitool  mc getsysinfo system_fw_version\n\n2.3.4\n\n\nGet NEW iDRAC version\n------------------------\n[root@sn75 /]# ipmitool   mc info | grep Firmware\n\nFirmware Revision         : 2.40"
        }
    },
    {
        "Issue Number": "IM1333238",
        "Description": "IT files (RA Dsession / RA Dtraffic)",
        "Keywords": [
            "Location Mobility",
            "Router Analytics",
            "Trust Center",
            "flow",
            "Mediation"
        ],
        "Owner": "u15",
        "Date": "20201026",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201026-IM1333238.md",
        "Detailed Description": {
            "Description": "```\nRA_Dsession/RA_Dtraffic are not exported\n```",
            "Actions Taken": "1. Login to `un2.bigdata.abc.gr` with personal account and change to `mtuser` with sudo\n2. Inspect logs of *RA* flow\n``` bash\ncd /shared/abc/location_mobility/log/\nless ra_export_bs_01.oozie.20201026.log\nless ra_export_bs_02.oozie.20201026.log\n```\n3. Check if max partition of source tables is greater or equal than the export date.\nIn `ra_export_bs_01.oozie.20201026.log`:\n``` bash\nQuery: SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201024'\n...\n[2020/10/26 09:00:22] - INFO: max_date=NULL and export_date=20201024\n```\nIn `ra_export_bs_02.oozie.20201026.log`:\n``` bash\nQuery: SELECT MAX(par_dt) FROM device_traffic WHERE par_dt >= '20201024'\n...\n[2020/10/26 09:00:22] - INFO: max_date=NULL and export_date=20201024\n```\nThe above messages show that data from the source tables have been exported to files already.\n\n4. Validate the results of the query:\nFor `RA_Dsession`:\n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"SELECT MAX(par_dt) FROM device_session WHERE par_dt >= '20201023';\";\n...\n+-------------+\n| max(par_dt) |\n+-------------+\n| 20201023    |\n+-------------+\nFetched 1 row(s) in 1.38s\n```\nFor `RA_Dtraffic`:\n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d npce -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"SELECT MAX(par_dt) FROM device_traffic WHERE par_dt >= '20201023';\";\n...\n+-------------+\n| max(par_dt) |\n+-------------+\n| 20201023    |\n+-------------+\nFetched 1 row(s) in 1.38s\n```\n5. Files will be exported at the next execution if the source tables contain new entries. Due to the size of the exported files runs only for the previous day `par_dt`.\n\nIf the customer requests to generate the files for the missing days:\n\n``` bash\ncd /shared/abc/location_mobility/run\n./export_ra_bs_01.sh -t 20201115 # Run for specific date\n./export_ra_bs_02.sh -t 20201115 # Run for specific date\n```",
            "Affected Systems": "abc Bigstreamer Backend",
            "Action Points": "N/A",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1332709",
        "Description": "SM-MISSING DATA",
        "Keywords": [
            "Ipvpn",
            "csv files",
            "Service Manager",
            "EEM",
            "CPU_LOAD",
            "MEM_USAGE"
        ],
        "Owner": "u15",
        "Date": "20201029",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201029-IM1332709.md",
        "Detailed Description": {
            "Description": "```\nFiles CPU_LOAD and MEMORY_USAGE are exported in the wrong order or not at all\nMissing files for 4:25 and 5:05 a.m.\n```",
            "Actions Taken": "1. Login to `un2.bigdata.abc.gr` with personal account and change to `ipvpn` with sudo\n2. Inspect logs of *export component files* flow\n``` bash\ncd /shared/abc/ip_vpn/log/\nless initiate_export_components.cron.20201025.log\n```\n\n3. Check messages for files with 04:25 and 05:05 metrics\n``` bash\n[2020/10/25 04:32:56] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-25_04.25.00.csv is empty.\n[2020/10/25 04:32:56] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-25_04.25.00.csv is empty.\n...\n[2020/10/25 05:12:57] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-25_05.05.00.csv is empty.\n[2020/10/25 05:12:57] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-25_05.05.00.csv is empty.\n```\n\n4. Check Impala queries execution for that files\nFor `CPU_LOAD`:\n``` bash\nless compute_cpu_kpis.20201025.log\n...\nQuery: SELECT straight_join a.min_5 AS time,\n...\n    a.component_type='CPU' AND\n    a.min_5='2020-10-25 04:25:00' AND\n    a.par_dt='20201025'\n...\nFetched 0 row(s) in 12.20s\nINFO: CPU file exported.\n```\nFor `MEM_USAGE`:\n``` bash\nless compute_memory_kpis.20201025.log\n...\nQuery: SELECT straight_join a.min_5 AS time,\n...\na.component_type='MEMORY' AND\na.min_5='2020-10-25 04:25:00' AND\na.par_dt='20201025'\n...\nFetched 0 row(s) in 12.03s\nINFO: Memory file exported.\n```\n\n5. Check input metrics table\nExecute the Impala query either from Hue or impala-shell\n```bash\nSELECT count(*)\nFROM bigcust.nnm_ipvpn_componentmetrics_hist a\nWHERE        \n    a.min_5='2020-10-25 04:25:00' AND\n    a.par_dt='20201025';\nResult = 0\n```\n\n6. Inspect logs of input metrics ingestion\nFiles are transferred from NNM node to a local spool directory every 5 minutes.\n```bash\nless /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20201025.log\n...\n[2020/10/25 04:31:05] - INFO - Checking file: /data/1/nnm_components_LZ/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.csv.gz\n...\n[2020/10/25 04:31:05] - INFO - /bin/mv /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv.tmp /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv\n```\n\n7. Inspect logs of Flume agent\n```bash\nless /var/log/flume-ng/flume-cmf-flume5-AGENT-un2.bigdata.abc.gr.log\n...\n2020-10-28 22:46:05,308 INFO org.kitesdk.morphline.stdlib.LogInfoBuilder$LogInfo: After Split record:[{IPAddress=[87.203.132.214], ..., file=[/data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv]}]\n2020-10-28 22:46:05,308 INFO org.kitesdk.morphline.stdlib.LogInfoBuilder$LogInfo: After extractTimeBucket record:[{IPAddress=[87.203.132.214], ..., field_min_5=[2020-10-25 04:30:00], field_par_dt=[20201025], file=[/data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201025042603327.20201025_023104UTC.csv]}]\n```\nThe issue is that field_min_5 should have been `2020-10-25 04:25:00`, not `2020-10-25 04:30:00`.\n\n8. Alter Morphline configuration\nChange rounding to `mathematical` so field_min_5 is the timestamp of the metric rounded to the nearest 5-minute interval. \n```bash\nvim /shared/abc/ip_vpn/flume/nnm_component_metrics/morphline_nnmMetricsCsvToRecord_ipvpn_user.conf\n...\n          { extractTimeBucket { field : file, bucket:5, rounding:mathematical } }\n```\n\n9. Restart Flume Agent\nWhen the agent is not processing any files, restart `FLUME-IPVPN` at `un2.bigdata.abc.gr` from Cloudera Manager. Monitor that field_min_5 is rounded to the nearest 5-minute interval.\n```bash\ntail -f less /var/log/flume-ng/flume-cmf-flume5-AGENT-un2.bigdata.abc.gr.log\n```\n\n10. Investigate delays in export\n```bash\nless initiate_export_components.cron.20201025.log\n...\n[2020/10/25 03:12:56] - INFO: Searching for output files..\n[2020/10/25 03:13:04] - INFO: cpu_output_file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-25_03.05.00.csv\n[2020/10/25 03:13:04] - INFO: mem_output_file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-25_03.05.00.csv\n```\nSearching for the files is taking too long. \n\n11. Inspect host resources at that time\nLogin to Cloudera Manager. CPU, memory and network did not show anomalies, however disk queue was increased for sdc device. Peaks were nfghd every 2 hours. \n\n12. Find processes with heavy disk i/o\nBased on a previous investigation the processes that write to sdc run for Location Mobility. Communication with the development team to change disk/device.",
            "Affected Systems": "abc Bigstreamer Backend",
            "Action Points": "",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1336999",
        "Description": "SM-MISSING DATA",
        "Keywords": [
            "Ipvpn",
            "csv files",
            "Service Manager",
            "EEM",
            "CPU_LOAD",
            "MEM_USAGE"
        ],
        "Owner": "u15",
        "Date": "20201029",
        "Status": "Open",
        "Info": "abc/BigStreamer/issues/20201029-IM1336999.md",
        "Detailed Description": {
            "Description": "```\nSince 27/10/2020 12:40 pm 3 files have not been registered to EEM due to delays. Normal offset is 8 minutes e.g. metrics for 13:05 have to be transferred to the exchange directory before 13:13.\n```",
            "Actions Taken": "1. In the screenshot sent via email there is one file is missing for 13:05 and three files have been delayed from 13:55 to 14:05\n2. Login to `un2.bigdata.abc.gr` with personal account and change to `ipvpn` with sudo\n3. Inspect logs of *export component files* flow\n``` bash\ncd /shared/abc/ip_vpn/log/\nless initiate_export_components.cron.20201027.log\n```\n\n3. Check messages for missing file\n``` bash\n[2020/10/27 13:13:00] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_13.05.00.csv is empty.\n[2020/10/27 13:13:00] - ERROR: file=/shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_13.05.00.csv is empty.\n```\n\n4. Check Impala queries execution for that file\nFor `CPU_LOAD`:\n``` bash\nless compute_cpu_kpis.20201027.log\n...\nQuery: SELECT straight_join a.min_5 AS time,\n...\n    a.component_type='CPU' AND\n   a.min_5='2020-10-27 13:05:00' AND\n    a.par_dt='20201027'\n...\nQuery submitted at: 2020-10-27 13:12:51\n...\nFetched 0 row(s) in 8.68s\nINFO: CPU file exported.\nTue Oct 27 13:13:00 EET 2020\n```\nFor `MEM_USAGE`:\n``` bash\nless compute_memory_kpis.20201025.log\n...\nQuery: SELECT straight_join a.min_5 AS time,\n...\na.component_type='MEMORY' AND\na.min_5='2020-10-27 13:05:00' AND\na.par_dt='20201027'\n...\nQuery submitted at: 2020-10-27 13:12:50\n...\nFetched 0 row(s) in 9.00s\nINFO: Memory file exported.\nTue Oct 27 13:13:00 EET 2020\n```\n\n5. Check input metrics table\nExecute the Impala query either from Hue or impala-shell\n```bash\nSELECT count(*)\nFROM bigcust.nnm_ipvpn_componentmetrics_hist a\nWHERE        \n    a.min_5='2020-10-27 13:05:00' AND\n    a.par_dt='20201027';\nResult = 4286\n```\n\n6. Inspect logs of input metrics ingestion\n```bash\nless /shared/abc/ip_vpn/log/nnm_component_metrics.cron.20201027.log\n...\n[2020/10/27 13:11:03] - INFO - /bin/mv /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201027130601110.20201027_111102UTC.csv.tmp /data/1/nnm_components_LZ/spooldir/BIG-CUSTOMERS-CPU-MEM-UTIL_20201027130601110.20201027_111102UTC.csv\n...\n```\nInput metrics file has been loaded before the execution of the export query. So queries in step 4 should have returned about 1260 rows. This needs further investigation on why even after the table has been refreshed the query returns the wrond result. Nfghd as Action Point 1.\n\n3. Check messages for delayed files\n``` bash\n[2020/10/27 14:03:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_13.55.00.csv at /shared/abc/ip_vpn/out/saismpm\n[2020/10/27 14:03:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_13.55.00.csv at /shared/abc/ip_vpn/out/saismpm\n...\n[2020/10/27 14:08:09] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_14.00.00.csv at /shared/abc/ip_vpn/out/saismpm\n[2020/10/27 14:08:09] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_14.00.00.csv at /shared/abc/ip_vpn/out/saismpm\n...\n[2020/10/27 14:14:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/CPU_LOAD_2020-10-27_14.05.00.csv at /shared/abc/ip_vpn/out/saismpm\n[2020/10/27 14:14:04] - INFO: Copied files /shared/abc/ip_vpn/nnm_flume/MEM_USAGE_2020-10-27_14.05.00.csv at /shared/abc/ip_vpn/out/saismpm\n```\nSince SFTP GET occurs on 3rd, 8th, 13th, 18th etc minute of every hour, the files above should have been transferred in `/shared/abc/ip_vpn/out/saismpm` before 14:03, 14:08, 14:13 respectively.\n\n4. Check Impala queries execution for those files\nFor `CPU_LOAD`:\n``` bash\nless compute_cpu_kpis.20201027.log\n...\nTue Oct 27 14:02:02 EET 2020\nStarting Impala Shell using LDAP-based authentication\n...\nINFO: CPU file exported.\nTue Oct 27 14:03:00 EET 2020\n...\n...\nTue Oct 27 14:07:07 EET 2020\nStarting Impala Shell using LDAP-based authentication\n...\nINFO: CPU file exported.\nTue Oct 27 14:08:09 EET 2020\n...\n...\nTue Oct 27 14:12:06 EET 2020\nStarting Impala Shell using LDAP-based authentication\n...\nINFO: CPU file exported.\nTue Oct 27 14:14:03 EET 2020\n```\nSame for `MEM_USAGE`.\nThe cause of the delays is the duration time of the Impala queries. To reduce the times we need to investigate if the schema of bigcust.nnm_ipvpn_componentmetrics_hist can be improved, if we can delete the `REFRESH nnmnps.nms_node ...` queries etc. Nfghs as Action Point 2.",
            "Affected Systems": "abc Bigstreamer Backend",
            "Action Points": "1. Investigate empty response of Impala query for 13:05 even after refreshing the table\n2. Investigate slightly increased response times of Impala queries and ways to reduce them.",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1363226",
        "Description": "csi_fixed coollection issue",
        "Keywords": [
            "Brond",
            "Cube",
            "Indicators"
        ],
        "Owner": "u15",
        "Date": "20201125",
        "Status": "Open",
        "Info": "abc/BigStreamer/issues/20201125-IM1363226.md",
        "Detailed Description": {
            "Description": "```\n CSI_fix_11222020_w47.txt was exported empty\n```",
            "Actions Taken": "1. Login to `un2.bigdata.abc.gr` with personal account and change to `mtuser` with sudo\n\n2. Inspect logs of *CSI fix* flow. Nfgh that filename format is `CSI_fix_<mmddyyyy>_w<week>.txt` and that there is a 2 day delay between the export time and the exported data. In this case there was a problem with the file containing data for 2020-11-22 which was exported at 2020-11-24. So we checked the logs for 2020-11-24. \n```bash\ncd /shared/abc/export_sai_csi/log\nless sai_csi.cron.20201124.log\n```\n3. Check if the source table contained data for the export date.\nIn `sai_csi.cron.20201124.log`:\n``` bash\nQuery: use `sai`\nQuery: select nvl ... from sai.cube_indicators_it where par_dt='20201122'\n...\nFetched 0 row(s) in 0.06s\nProblem with 20201122.\n```\n\n4. Check if the source table contains data for this date. Please nfgh that the source table is just a view of table `brond.cube_indicators`.\n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d brond -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"select count(*), par_date from brond.cube_indicators where par_date >= '20201118' group by 2 order by 2;\";\n...\ncount(*),par_date\n2454925,20201118\n2453089,20201119\n2458393,20201123\n```\n\n5. Since 3 dates are missing 2020/11/20-22, we need to run the workflow that populates `brond.cube_indicators`. But first we have to make sure all its table dependecies are loaded. Execute the following queries in an impala-shell or an Impala editor in Hue and make sure not only that partitions exist for those dates but also that there are an identical amount of lines.\n```\nselect count(*), par_dt\nfrom brond.brond_retrains_hist\nwhere par_dt >= '20201118'\ngroup by 2\norder by 2;\n...\ncount(*),par_dt\n2499833,20201118\n2497948,20201119\n*2496522,20201120*\n*2497810,20201121*\n*2497480,20201122*\n2496932,20201123\n2497130,20201124\n2505791,20201125\n``` \nExecute the same query for tables:\n  - brond.fixed_radio_matches_unq_inp\n  - brond.fixed_brond_customers_daily_unq\n  - radius.radacct_hist\n  - brond.brond_retrains_hist\n  - brond.dsl_stats_week_xdsl_hist\n\n6. Run Cube_Indicators workflow. Login to Hue as intra and navigate to Workflows > Dashboards > Coordinators. Search for `Coord_Cube_Spark_Indicators` and click on the coordinator to view its executions. Select **one** at a time of the executions that need to be repeated. Nfgh that `brond.cube_indicators` is populated with a 2 day delay so if we want to load data for 2020-11-20 we have to execute the workflow of 2020-11-22. Select the execution from the check box and click `Rerun`. \n7. After each workflow execution repeat the query of step 4 to verify that table has been loaded. \n``` bash\nimpala-shell -i un-vip.bigdata.abc.gr:22222 -d brond -l --ssl -u mtuser --ldap_password_cmd /home/users/mtuser/check.sh  --query=\"select count(*), par_date from brond.cube_indicators where par_date >= '20201118' group by 2 order by 2;\";\n...\ncount(*),par_date\n2454925,20201118\n2453089,20201119\n2454321,20201120\n2458393,20201123\n```\n8. Gather dates that need to be exported. As we saw in step 5 the source table was empty for 3 partitions: 20201120, 20201121, 20201122. The files produced for them was empty. Verify by checking `/shared/abc/export_sai_csi/logging/CSI_fix_reconciliation.log`:\n```bash\n2020-11-21 09:01:39  CSI_fix_11212020_w47.txt  20201119  2453089\n2020-11-22 09:00:43  CSI_fix_11222020_w47.txt  20201120  0\n2020-11-23 09:28:14  CSI_fix_11232020_w47.txt  20201121  0\n2020-11-24 09:01:01  CSI_fix_11242020_w47.txt  20201122  0\n2020-11-25 09:03:13  CSI_fix_11252020_w48.txt  20201123  2458393\n```\n*CSI fix* provides a mechanism to avoid manual re-export of empty files. The most recent date of an empty file is stored in a table and automatically exported with the next execution. To view the date stored issue the query:\n```bash\nselect * from refdata.mediation_csi_load_info;\n...\nload_time,flow_name\nNULL,sai.mob\n20201122,sai.fix\n```\nSo we don't need to export manually date 20201122, only dates 20201120 & 20201121.\n9. Finally, to export the files we have to repeat step 1 and execute the export script **sequentially** for the desired dates + 2 days:\n```bash\n/shared/abc/export_sai_csi/export_csi_fix.sh 20201122 >> /shared/abc/export_sai_csi/log/sai_csi.cron.$(date '+%Y%m%d').log 2>&1 &\n/shared/abc/export_sai_csi/export_csi_fix.sh 20201123 >> /shared/abc/export_sai_csi/log/sai_csi.cron.$(date '+%Y%m%d').log 2>&1 &\n```\n10. Afterwards check the reconciliation log file that files have been exported:\n```bash\nless /shared/abc/export_sai_csi/logging/CSI_fix_reconciliation.log\n...\n2020-11-25 10:46:40  CSI_fix_11222020_w47.txt  20201120  4915294\n2020-11-25 10:51:17  CSI_fix_11232020_w47.txt  20201121  2457858\n```\nThe first one contains more lines as it includes dates 20201120 & 20201122.",
            "Affected Systems": "abc Bigstreamer Backend",
            "Action Points": "N/A",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1364500",
        "Description": "oozie job hive2script failed / stale metadata",
        "Keywords": [
            "Osix",
            "SIP topology",
            "Spark",
            "Impala",
            "Hive",
            "Parquet"
        ],
        "Owner": "u15",
        "Date": "20201124",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201124-IM1364500.md",
        "Detailed Description": {
            "Description": "```\nImpala queries finish with error for table osix.sip and partition 20201123\nMessage:\nERROR processing query/statement. Error Code: 0, SQL state: File 'hdfs://nameservice1/ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/par_method=REGISTER/part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000' has an invalid version number: .??6\nThis could be due to stale metadata. Try running \"refresh osix.sip\".\n```",
            "Actions Taken": "1. Checked that same query results in error using Hive.\n2. Checked that the problem occurs only with par_hr=08 partition. \n```bash\nselect distinct sip.callinguser \nas callinguser \nfrom OSIX.sip where par_dt='20201123' \nAND par_hr != '08' \nAND sip.callingUser IS NOT NULL;\n...\nFetched X rows in X seconds.\n```\n\n2. Inspected logs of Osix SIP application for that time. Login in `unosix1.bigdata.abc.gr`, switch user to `osix` and kinit first.\n```bash\n$ sudo su - osix\n$ cd\n$ kinit -kt osix.keytab osix\n$ yarn logs -applicationId application_1599948124043_405502\n```\n3. As `sn87.bigdata.abc.gr` was running a Spark executor of this application the time it was forced to shutdown, inspected if there are any corrupt files in the table. Login to any datanode first.\n``` bash\n$ cd /var/run/cloudera-scm-agent/process/ \n$ ls -lahtr | grep -i hdfs\n$ cd <last directory>\n$ kinit -kt hdfs.keytab hdfs/`hostname -f`\n$ hdfs fsck /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08 -includeSnapshots\n...\nStatus healthy\n```\n4. Inspected format of written files. After communication with the dev team the batch id was retrieved so only a few files were checked. Login to un2.  \n```\n$ hdfs dfs -copyToLocal /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/part-*-17ead666-d5cb-437e-a849-c08ef825bec4.* .\n$ parquet-tools meta part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000\nfile:/home/users/u15/part-00006-17ead666-d5cb-437e-a849-c08ef825bec4.c000 is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [46, -19, -49, 54]\n```\n4. Some of the files didn't have a correct parquet format so we removed them from the table.\n``` bash\n$ hdfs dfs -mv hdfs dfs -mv  /ez/warehouse/osix.db/sip/par_dt=20201123/par_hr=08/par_method=OTHER/part-00005-17ead666-d5cb-437e-a849-c08ef825bec4.c000 /ez/landingzone/tmp/osix_sip/other\n...\n```\n\n5. Refresh the table and check that problem is fixed.\n```\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='REGISTER');\nREFRESH osix.sip PARTITION (par_dt='20201123', par_hr='08', par_method='OTHER');\nselect count(*) from OSIX.sip where par_dt='20201123' ;\n```",
            "Affected Systems": "abc Bigstreamer Backend",
            "Action Points": "N/A",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1367129",
        "Description": "We notice that no data is being loaded since 25/11/2020 at 07:00 on osix.sip.",
        "Keywords": [
            "Osix",
            "SIP topology",
            "Spark",
            "Impala"
        ],
        "Owner": "u27",
        "Date": "20201126",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201126-IM1367129.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\n\nWe notice that no data is being loaded from 25/11/2020 at 07:00 on osix.sip.\n```",
            "Actions Taken": "1. ssh unosix1 with your personal account\n2. sudo -iu osix\n3. kinit -kt osix.keytab osix\n4. yarn application -list | grep OSIX-SIP-NORM\n5. In our case the topology was down and the kudu script didn't resubmit it.\n6. Check if `coord_OsixStreaming_SIP_MonitorResubmit` is running.\n7. Check if listeners is healthy and data inserted `http://172.25.37.251/dashboard/#osix_listeners`\n8. The rate for `listen_sip_core` should be between 12K and 22K messages.If there is an extreme problem e.g. the rate is 0, consider restarting the problematic listener\n9. Check the logs of monitor script `hdfs dfs -ls /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM` and `hdfs dfs -cat /user/osix/resubmit_topology/logs/logs_OSIX-SIP-NORM/date_monitor_sip_norm.log`\n10. Start again the topology `unosix1:/home/users/osix/topologies/binary-input-impala-output/sip_norm/` and execute `./submit_sip_norm.sh` until the state appeared `RUNNING`\n11. yarn application -list | grep OSIX-SIP-NORM\n12. Connect to impala-shell or Hue and execute `SELECT count(*), par_dt FROM osix.sip WHERE par_dt>'20201124' group by par_dt;` to check if the data inserted on the table.",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1382364",
        "Description": "Please inform us whenever the pollaploi table in the energy efficiency schema is updated. Also, investigate why it has not been updated based on the latest file.",
        "Keywords": [
            "Energy",
            "efficiency",
            "Impala",
            "Table",
            "sftp"
        ],
        "Owner": "u27",
        "Date": "20201210",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201211-IM1382364.md",
        "Detailed Description": {
            "Description": "```\nPlease let us know whenever the pollaploi table in schema energy efficiency is updated. \nAlso to investigate why an update has not been made based on the latest file.\n```",
            "Actions Taken": "1. ssh un2 with your personal account\n2. sudo -iu intra\n3. sftp `bigd@172.16.166.30`\n4. cd energypm\n5. ls -ltr\n6. Open HUE dashboard and search for `energy_efficiency_load_pollaploi` Workflow\n7. Check if workflow failed.\n8. ssh `un2` with your personal account.\n9. sudo -i\n10. less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.date.log` and less `/shared/abc/energy_efficiency/load_pollaploi/log/pollaploi.next_date.log`. The next date should returned no changes.\n11. At un2 `wc -l /shared/abc/energy_efficiency/load_pollaploi/pollaploi_curr/2020_10_pollaploi.txt`\n12. Connect to impala-shell and execute `select count(*) from energy_efficiency.pollaploi`\n13. The count should be the same on `11 & 12`\n14. Check on Impala Queries UI if the queries ran without exception `STATEMENT RLIKE '.*energy_efficiency_load_pollaploi.*'`",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1391585",
        "Description": "Please check immediately if BigStreamer is working properly. We have received a lot of alerts over the weekend and today regarding HDFS, for various nodes as well as services (e.g. oozie). We have also noticed problems with geolocation streams and loc mob files.",
        "Keywords": [
            "Cluster",
            "Impala",
            "HDFS"
        ],
        "Owner": "u27",
        "Date": "20201220",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201220-IM1391585.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\n\nPlease check immediately if BigStreamer is working properly.\n\nWe have received many alerts over the weekend and today regarding HDFS, for various nodes as well as for services (eg oozie). We also noticed problems with geolocation streams and loc mob files.\n\nThanks,\nSK\n```",
            "Actions Taken": "1. Connect with personal creds `https://172.25.37.232:7183` Cloudera Manager\n2. Namenodes were both on standby mode which caused bad health on HDFS,HBASE,OOZIE,IMPALA. After nn1 restarted nn2 became the Active and nn1 the Standby namenode. All the other services was stable after this manual action except HBASE which restarted.\n3. Since all services were stable check HUE `https://172.25.37.236:8888/oozie/list_oozie_workflows/` to ensure that all workflows running.\n4. The specific timeline which namenodes crashed the load,cpu,network,hdfs_read/write,nodes health,,namenodes health,impala queries if something heavy executed,yarn applications if something heavy executed `http://10.20.9.82:5601/app/kibana`\n5. Opened a case on Cloudera with namenodes diagnostics.",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "Monitor the status/health of services and inform with mail/alert when a service/role is down.",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1391612",
        "Description": "Data loading",
        "Keywords": [
            "Cluster",
            "Impala",
            "radius",
            "radacct_hist",
            "radarchive_hist"
        ],
        "Owner": "u27",
        "Date": "20201220",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201220-IM1391612.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\n\nPlease immediately load all the data for all the hours and for all the flows (sai,vantage,radius etc) for the period of time the system was not working and run all the aggregated tables. We should also be sent a summary table with the relevant information so that we know if and in which streams there are data deficiencies.\n```",
            "Actions Taken": "1. Login to `un2.bigdata.abc.gr` with personal account and change to `intra` with sudo.\n2. Check files that have not been ingested. As you can see, there is a gap for radacct 20201220 and for radarchive between 3:00 - 16:30\n```bash\n[intra@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radacct_orig_files\n...\n-rwxrwx--x+  3 hive hive  838634257 2020-12-20 02:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_01-30.csv.20201220_021002.utc\n-rwxrwx--x+  3 hive hive  837624575 2020-12-20 03:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_03-00.csv.20201220_031003.utc\n-rwxrwx--x+  3 hive hive  840322537 2020-12-20 17:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_16-30.csv.20201220_171002.utc\n-rwxrwx--x+  3 hive hive  839948348 2020-12-20 18:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_18-00.csv.20201220_181002.utc\n-rwxrwx--x+  3 hive hive  840668651 2020-12-20 20:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_19-30.csv.20201220_201002.utc\n-rwxrwx--x+  3 hive hive  840847248 2020-12-20 21:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2020-12-20_21-00.csv.20201220_211002.utc\n...\n[intra@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radarchive_orig_files\nFound 30 items\n...\n-rwxrwx--x+  3 hive hive 1694918420 2020-12-17 05:14 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-16.csv.20201217_051003.utc\n-rwxrwx--x+  3 hive hive 1635182557 2020-12-18 05:14 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-17.csv.20201218_051002.utc\n-rwxrwx--x+  3 hive hive 1618497473 2020-12-19 05:14 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-18.csv.20201219_051002.utc\n-rwxrwx--x+  3 hive hive 1522580860 2020-12-21 05:13 /ez/warehouse/radius.db/radarchive_orig_files/RAD___radarchive_2020-12-20.csv.20201221_051002.utc\n```\n\n3. Change directory and transfer missing files from sftp. \n``` bash\n[intra@un2 radius]$ cd /shared/radius_repo/cdrs\n[intra@un2 cdrs]$ sftp prdts@79.128.178.35\nConnecting to 79.128.178.35...\nsftp> get radacct_2020-12-20_04-30.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_04-30.csv.bz2 to radacct_2020-12-20_04-30.csv.bz2\n/home/prdts/radacct_2020-12-20_04-30.csv.bz2                                                                                                                              100%  201MB  28.8MB/s   00:07    \nsftp> get radarchive_2020-12-19.csv.bz2\nFetching /home/prdts/radarchive_2020-12-19.csv.bz2 to radarchive_2020-12-19.csv.bz2\n/home/prdts/radarchive_2020-12-19.csv.bz2                                                                                                                                 100%  207MB  25.8MB/s   00:08    \nsftp> get radacct_2020-12-20_06-00.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_06-00.csv.bz2 to radacct_2020-12-20_06-00.csv.bz2\n/home/prdts/radacct_2020-12-20_06-00.csv.bz2                                                                                                                              100%  201MB  25.2MB/s   00:08    \nsftp> get radacct_2020-12-20_07-30.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_07-30.csv.bz2 to radacct_2020-12-20_07-30.csv.bz2\n/home/prdts/radacct_2020-12-20_07-30.csv.bz2                                                                                                                              100%  201MB  33.6MB/s   00:06    \nsftp> get radacct_2020-12-20_09-00.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_09-00.csv.bz2 to radacct_2020-12-20_09-00.csv.bz2\n/home/prdts/radacct_2020-12-20_09-00.csv.bz2                                                                                                                              100%  201MB  25.1MB/s   00:08    \nsftp> get radacct_2020-12-20_10-30.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_10-30.csv.bz2 to radacct_2020-12-20_10-30.csv.bz2\n/home/prdts/radacct_2020-12-20_10-30.csv.bz2                                                                                                                              100%  201MB  28.8MB/s   00:07    \nsftp> get radacct_2020-12-20_12-00.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_12-00.csv.bz2 to radacct_2020-12-20_12-00.csv.bz2\n/home/prdts/radacct_2020-12-20_12-00.csv.bz2                                                                                                                              100%  202MB  25.2MB/s   00:08    \nsftp> get radacct_2020-12-20_13-30.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_13-30.csv.bz2 to radacct_2020-12-20_13-30.csv.bz2\n/home/prdts/radacct_2020-12-20_13-30.csv.bz2                                                                                                                              100%  202MB  28.8MB/s   00:07    \nsftp> get radacct_2020-12-20_15-00.csv.bz2\nFetching /home/prdts/radacct_2020-12-20_15-00.csv.bz2 to radacct_2020-12-20_15-00.csv.bz2\n/home/prdts/radacct_2020-12-20_15-00.csv.bz2 \nsftp> exit\n```\n\n4. Check that /shared/radius_repo/radius_date.dat.local points to an older file:\n``` bash\n[intra@un2 cdrs]$ cat /shared/radius_repo/radius_date.dat.local\n[File]\nlatest_file=\"/shared/radius_repo/cdrs/radarchive_2019-08-14.csv.bz2\"\n[intra@un2 cdrs]$ ll\ntotal 2357692\n-rw-r--r-- 1 intra intra 211140333 Dec 21 16:27 radacct_2020-12-20_04-30.csv.bz2\n-rw-r--r-- 1 intra intra 211020434 Dec 21 16:28 radacct_2020-12-20_06-00.csv.bz2\n-rw-r--r-- 1 intra intra 211125062 Dec 21 16:28 radacct_2020-12-20_07-30.csv.bz2\n-rw-r--r-- 1 intra intra 210696825 Dec 21 16:28 radacct_2020-12-20_09-00.csv.bz2\n-rw-r--r-- 1 intra intra 211175805 Dec 21 16:29 radacct_2020-12-20_10-30.csv.bz2\n-rw-r--r-- 1 intra intra 211440564 Dec 21 16:29 radacct_2020-12-20_12-00.csv.bz2\n-rw-r--r-- 1 intra intra 211670525 Dec 21 16:29 radacct_2020-12-20_13-30.csv.bz2\n-rw-r--r-- 1 intra intra 211765933 Dec 21 16:29 radacct_2020-12-20_15-00.csv.bz2\n-rw-r--r-- 1 intra intra 172240773 Jun 19  2019 radarchive_2019-06-12.csv.bz2\n-rw-r--r-- 1 intra intra 162087027 Jul 30  2019 radarchive_2019-07-27.csv.bz2\n-rw-r--r-- 1 intra intra 168362647 Aug 16  2019 radarchive_2019-08-14.csv.bz2\n-rw-r--r-- 1 intra intra 216716584 Dec 21 16:27 radarchive_2020-12-19.csv.bz2\n```\nSince we want to load files newer than `/shared/radius_repo/cdrs/radarchive_2019-08-14.csv.bz2` we don't have to edit the file.\n\n5. Change configuration file of ingestion script.\n```bash\n[intra@un2 cdrs]$ vim /shared/abc/radius/DataParser/scripts/transferlist/radius.trn\nDefault Status:\n...\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n#-- local executions --\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n...\nWhen local file is used:\n...\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n#-- local executions --\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n...\n```\n\n6. Execute ingestion scripts when making sure it is not executed at the moment. When scripts have finished, revert changes in the .trn file.\n```bash\n[intra@un2 cdrs]$ tail /shared/abc/radius/DataParser/scripts/log/radius_20201221.log\n...\n--------------END------------\n[intra@un2 cdrs]$ /shared/abc/radius/DataParser/scripts/radius.pl -l -d -D -o >> /shared/abc/radius/DataParser/scripts/log/radius_cron_manual_20201221.log 2>&1\n[intra@un2 cdrs]$ /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.manual_20201221.log 2>&1\n[intra@un2 cdrs]$ vim /shared/abc/radius/DataParser/scripts/transferlist/radius.trn\n```\n\n7. Check that files have been loaded to hist tables.\n```bash\n[u15@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radacct_hist/par_dt=20201220\n...\n-rwxrwx--x+  3 hive hive   65350341 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000013_2054504955_data.0.\n-rwxrwx--x+  3 hive hive  134217741 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000014_1021750110_data.0.\n-rwxrwx--x+  3 hive hive  134217750 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000028_924374267_data.0.\n-rwxrwx--x+  3 hive hive  134217617 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000008_1102168495_data.0.\n-rwxrwx--x+  3 hive hive  134217769 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000001b_924374267_data.0.\n-rwxrwx--x+  3 hive hive  134217906 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000000b_1400128216_data.0.\n-rwxrwx--x+  3 hive hive  134217799 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000005_183542376_data.0.\n-rwxrwx--x+  3 hive hive  134217146 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000001d_1808120301_data.0.\n-rwxrwx--x+  3 hive hive  134217812 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000000d_1400128216_data.0.\n-rwxrwx--x+  3 hive hive  165905440 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000020_2125534478_data.0.\n-rwxrwx--x+  3 hive hive  166171908 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000026_694498725_data.0.\n-rwxrwx--x+  3 hive hive  166671557 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000009_1829852461_data.0.\n-rwxrwx--x+  3 hive hive  134217919 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000027_1432252135_data.0.\n-rwxrwx--x+  3 hive hive  134217610 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000006_975299616_data.0.\n-rwxrwx--x+  3 hive hive  134217617 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-703759660000000c_361805242_data.0.\n-rwxrwx--x+  3 hive hive  134217918 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000023_361805242_data.0.\n-rwxrwx--x+  3 hive hive  134217774 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000002_361805242_data.0.\n-rwxrwx--x+  3 hive hive  134217914 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000004_2125534478_data.0.\n-rwxrwx--x+  3 hive hive  134217443 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000011_1541315014_data.0.\n-rwxrwx--x+  3 hive hive  168449504 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000018_924374267_data.0.\n-rwxrwx--x+  3 hive hive  134217940 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000007_975299616_data.0.\n-rwxrwx--x+  3 hive hive  134217515 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000010_2101034182_data.0.\n-rwxrwx--x+  3 hive hive  134217798 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000001_1541315014_data.0.\n-rwxrwx--x+  3 hive hive  134217909 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000024_738850578_data.0.\n-rwxrwx--x+  3 hive hive  134216978 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000017_361805242_data.0.\n-rwxrwx--x+  3 hive hive  134217450 2020-12-21 16:53 /ez/warehouse/radius.db/radacct_hist/par_dt=20201220/7242bd6c6df667fc-7037596600000016_559180009_data.0.\n[u15@un2 ~]$ hdfs dfs -ls -t -r /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219\n...\n-rwxrwx--x+  3 hive hive  134217959 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000001_564661313_data.0.\n-rwxrwx--x+  3 hive hive  134217962 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000002_564661313_data.0.\n-rwxrwx--x+  3 hive hive   78601928 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000000_564661313_data.0.\n-rwxrwx--x+  3 hive hive  134217573 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000006_1446698864_data.0.\n-rwxrwx--x+  3 hive hive  134217391 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000004_929468556_data.0.\n-rwxrwx--x+  3 hive hive  134217788 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000007_1446698864_data.0.\n-rwxrwx--x+  3 hive hive  134218011 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000005_1259058186_data.0.\n-rwxrwx--x+  3 hive hive  134217788 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000008_1446698864_data.0.\n-rwxrwx--x+  3 hive hive  134217753 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de0000000a_1960091991_data.0.\n-rwxrwx--x+  3 hive hive  134217789 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000003_2144981155_data.0.\n-rwxrwx--x+  3 hive hive  134217904 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de00000009_1259058186_data.0.\n-rwxrwx--x+  3 hive hive  134217360 2020-12-21 16:53 /ez/warehouse/radius.db/radarchive_hist/par_dt=20201219/7949b11b446efef7-ab0819de0000000b_1446698864_data.0.\n```",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "Section `Radius` in https://edn2.bigdata.intranet.gr/abc/BigStreamer/cluster_monitoring/blob/master/future_steps/flows_applications.md",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1421557",
        "Description": "CSI_fix_01212021_w03.txt file with no data",
        "Keywords": [
            "cube_indicators",
            "brond"
        ],
        "Owner": "u15",
        "Date": "20210122",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20210122-IM1421557.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\n\nYesterday's file was zero. Can you check it?\n\n-1 172.25.37.240 CSI_FIXED CSI_fix_01212021_w03.txt 0 1/21/2021 11:00:10 AM 35 1/21/2021 11:00:10 AM CSI_FIXED:CSI_fix_01212021_w03.txt:20210121110010035918\n\nThanks,\nSK\n```",
            "Actions Taken": "1. Execute steps 1-5 from [this doc](knowledge-base/abc/BigStreamer/20201125-IM1363226.md)\n2. After the check we saw that table `brond.dsl_stats_week_xdsl_hist` had no data for 2 partitions\n```bash\nselect count(*), par_dt\nfrom brond.dsl_stats_week_xdsl_hist\nwhere par_dt >= '20210115'\ngroup by 2\norder by 2;\nResult:\ncount(*)\tpar_dt\t\n...\n2491814\t20210117\t\n2491872\t20210118\t\n2494261\t20210121\t\n...\n```\n\n3. Check coordinator `coord_brond_load_dsl_daily_stats` that populates this table as explained in [here](systems-info/abc/BigStreamer/Brond/cube_indicators_pipeline.md). \n\n4. Check that source tables `brond.brond_vdsl_stats_week` and `brond.brond_adsl_stats_week` have data for these partitions. Same query for `brond.brond_adsl_stats_week`\n``` bash\nselect count(*), par_dt\nfrom brond.brond_vdsl_stats_week\nwhere par_dt >= '20210115'\ngroup by 2\norder by 2;\nResult:\ncount(*)\tpar_dt\t\n...\n1806006\t20210116\t\n1806256\t20210117\t\n1806306\t20210118\t\n1808049\t20210119\t\n1808918\t20210120\t\n1810234\t20210121\t\n1811401\t20210122\t\n```\nAs source tables have data, we have to execute only the inserts that failed.\n\n5. Impala insert queries are under `/user/intra/brond_dsl_stats/impala-shell/populate*.sql`. Change conditions `..par_dt > '20210127..` to get the dates that are missing. In this case `20210119`. Execute the queries using Hue or impala-shell.\n\n6. Repeat step 2 to validate that data are loaded correctly for missing dates.\n\n7. Complete steps 6-10 from [this doc](knowledge-base/abc/BigStreamer/20201125-IM1363226.md).",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "N/A",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1443515",
        "Description": "Radius Radacct Hist",
        "Keywords": [
            "radius",
            "radacct_hist",
            "radacct_orig_files",
            "radacct_load",
            "intra",
            "radius_ops",
            "hive",
            "metastore"
        ],
        "Owner": "u100",
        "Date": "20210211",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20210211-IM1443515.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\n\nPlease investigate the three-day reduction in registrations at 06:00 and 07:00 (02/08 - 02/10) and inform us about it.\n```",
            "Actions Taken": "- Login to `un2.bigdata.abc.gr` with personal account and change to `intra` with sudo.\n- Compare the count of the inserted data between the radius.radacct_hist and the original files radius.radacct_orig_files\n\n```bash\n[intra@un2 ~]$ secimp\n[un-vip.bigdata.abc.gr:22222] > select par_dt,substr(acctupdatetime,1,13),count(*) from radius.radacct_hist where par_dt>'20210209' group by 1,2 order by 1,2;\n| 20210209 | 2021-02-09 06 | 597 |\n| 20210209 | 2021-02-09 07 | 697082 |\n\n[un-vip.bigdata.abc.gr:22222] > select substr(acctupdatetime,1,13),count(*) from radius.radacct_orig_files where acctupdatetime>'2021-02-09' group by 1 order by 1;\n| 2021-02-09 06                 | 1430757  |\n| 2021-02-09 07                 | 1393639  |\n```\nSo, the files had been correctly ingested but the radacct_hist table has a problem.\n\n- Compare the total ingested lines with the total inserted lines for the provided dates/hours (the provided hours are in UTC time - Impala)\n\n```bash\n[intra@un2 ~]$ for i in {08..09};do grep -E \"2021/02/09 ${i}.*Total lines\" /shared/abc/radius/DataParser/scripts/log/radius_cron.log;done\n[2021/02/09 08:12:01] - info - Total lines :  <2130925>\n[2021/02/09 09:12:37] - info - Total lines :  <2136145>\n\n[intra@un2 ~]$ for i in {08..09};do grep -B 5 Modified /shared/abc/radius/log/000_radius_ops.20210209.log | grep -A 6 \"insert into radius.radacct_hist\" | grep -C 3 \"Query submitted at: 2021-02-09 ${i}\" | grep Modified;done\nModified 0 row(s) in 0.58s\nModified 2136145 row(s) in 12.15s\n```\n\nAs you can see, the data had been correctly inserted into radius.radacct_load (2130925) but the insert into the radius.radacct_hist had insert 0 rows @  09/02/2021 08:18:15.\n\nDo the same process for 08/02/2021 and 10/02/2021 and observe the same outcome.\n\n- The ingested files have been backed up in the radius.radacct_orig_files. Find the appropriate files for the given dates/hours\n\n```bash\n[intra@un2 ~]$ for i in {08..10};do hdfs dfs -ls /ez/warehouse/radius.db/radacct_orig_files/ | grep 202102${i}_08;done\n-rwxrwx--x+  3 hive hive  839787710 2021-02-08 08:11 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-08_07-30.csv.20210208_081002.utc\n-rwxrwx--x+  3 hive hive  844035825 2021-02-09 08:12 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-09_07-30.csv.20210209_081002.utc\n-rwxrwx--x+  3 hive hive  844035825 2021-02-09 08:12 /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-10_07-30.csv.20210210_081001.utc\n```\n\n- Copy the files to the load table\n\n```bash\n[intra@un2 ~]$ hdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-02-08_07-30.csv.20210208_081002.utc /ez/warehouse/radius.db/radacct_load/\n```\n\nDo the same for the other two files.\n\n- Run the radius procedure again\n\n```bash\n[intra@un2 ~] /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.20210211.log.manual 2>&1\n```",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "Resolution of Cloudera Issue 752877 - Hive Metastore innodb lock await time out which is the root cause of this issue.",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1500475",
        "Description": "agama schema missing data 02/04",
        "Keywords": [
            "agama",
            "probe_live_tv_per_channel_hourly_hist",
            "probe_dvb_s_per_channel_hourly_hist",
            "probe_dvb_s_timeshift_per_channel_hourly_hist",
            "intra",
            "missing_data"
        ],
        "Owner": "u27",
        "Date": "20210409",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20210409-IM1500475.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\n\nPlease investigate if primary data has been created for 02/04/2021 and if so upload it.\n\nThank you\n```",
            "Actions Taken": "1. ssh with you personal ldap account at `un2` from `admin`\n2. sudo -iu intra\n3. cronta -l | grep agama\n4. Check logs of the script that data is missing.\n5. Also check if the files exist at `sftp_server:directory`\n6. If files exist modify script for the <table> un2:/shared/abc/agama/bin/`table`.sh\n7. comment `yest_sftp`,`yest`,`dt_sftp`,`dt`. Uncomment the static `dt_sftp`,`dt` (e.g `##dt_sftp=2021/07/07 ##dt=20210707`)\n8. Run the script and when is finished connnect to impala-sheel `select count(*),par_dt from agama.table where par_dt >= '2021xxxx' group by 2;`\n9. Edit again the script on un2 with the default values to run tomorrow",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "GI12",
        "Description": "Integration of xyz services with nnmprd01",
        "Keywords": [
            "ipvpn",
            "syzefxis",
            "nnm",
            "ingestion",
            "workflow",
            "oozie"
        ],
        "Owner": "u15",
        "Date": "20210421",
        "Status": "Closed",
        "Info": "abc/BigStreamer/issues/20210421-GI12.md",
        "Detailed Description": {
            "Description": "```\nWe are currently at the point where NNMi on the production server (NYMA) has been upgraded to its final version (NNMi 2020.11). This means that there is - on your part - the possibility of testing integration with nnmprd01 for the following:\n\n1. SLA Custompoller primary service (syzeyxis & ipvpn)  in new nnmprd01\n2. Bigdata flows from nnmprd01 (Oozie workflows που παίρνουν τα NNMi postgress tables για reference data)\n```",
            "Actions Taken": "for SLA Custompoller:\n\n1. Login at `un2` with your personal account and switch to `ipvpn` user:\n```bash\n[u15@un2 ~]$ sudo su - ipvpn\n[ipvpn@un2 ~]$ \n```\n\n2. Check ssh to `nnmprd01`. Communication must be passwordless:\n```bash\n[ipvpn@un2 ~]$ ssh custompoller@nnmprd01\npassword:\n```\nWe were asked for the password. We had to execute the following and try again:\n```bash\n[ipvpn@un2 ~]$ ssh-copy-id custompoller@nnmprd01\n[ipvpn@un2 ~]$ ssh custompoller@nnmprd01\nActivate the web console with: systemctl enable --now cockpit.socket\n\nThis system is not registered to Red Hat Insights. See https://cloud.redhat.com/\nTo register this system, run: insights-client --register\n\nLast login: Wed Apr 21 14:23:46 2021 from 172.25.37.236\n[custompoller@nnmprd01 ~]$ \n```\nSsh connectivity good to go!\n\n3. Execute Custompoller for IP-VPN and inspect output and logs:\n```bash\n[custompoller@nnmprd01 ~]$ crontab -l\n#*/5 * * * * /home/custompoller/run/run_syzeyksis_standby.sh &>> /home/custompoller/log/syzeyksis-`date +\"\\%Y-\\%m-\\%d\"`.log\n#17 03 * * * /home/custompoller/run/zip_folders_syzeyksis.sh &>> /home/custompoller/log/syzeyksis-`date +\"\\%Y-\\%m-\\%d\"`.log\n\n######## IP VPN #################################\n#*/5 * * * * /home/custompoller/ipvpn/run/run_ipvpn.sh &>> /home/custompoller/ipvpn/log/ipvpn-`date +\"\\%Y-\\%m-\\%d\"`.log\n#03 */6 * * * /home/custompoller/ipvpn/run/zip_folders_ipvpn.sh &>> /home/custompoller/ipvpn/log/ipvpn-`date +\"\\%Y-\\%m-\\%d\"`.log\n[custompoller@nnmprd01 ~]$ cat /home/custompoller/ipvpn/run/run_ipvpn.sh\n#!/bin/bash\n\nexport JAVA_HOME=/home/custompoller/jdk1.8.0_144\nexport PATH=$JAVA_HOME/bin:$PATH\ncd /home/custompoller/ipvpn/run \n\n/home/custompoller/ipvpn/run/java -Xms1024m -Xmx4096m -DlogFilename=/home/custompoller/ipvpn/log/ipvpn -Dlog4j.configurationFile=/home/custompoller/ipvpn/conf/log4j2.xml -cp ./bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner -config /home/custompoller/ipvpn/conf/vpn.config -directorytomove /home/custompoller/ipvpn/out/ -version 1 -timeout 1500 -retries 2\n\nsleep 10\necho \"[`date '+%Y/%m/%d %T'`] - INFO: Executing ssh command..\"\nssh ipvpn@172.25.37.236 'nohup /shared/abc/nnm_custompoller_ipvpn/DataParser/scripts_nnmprod/nnm_custompoller_ipvpn.pl -r -m -po >> /shared/abc/nnm_custompoller_ipvpn/log/nnmcustompoller_ipvpn_cron.`date \"+%Y%m%d\"`.log 2>&1 &'\n\n[custompoller@nnmprd01 ~]$ export JAVA_HOME=/home/custompoller/jdk1.8.0_144\n[custompoller@nnmprd01 ~]$ export PATH=$JAVA_HOME/bin:$PATH\n[custompoller@nnmprd01 ~]$ cd /home/custompoller/ipvpn/run \n[custompoller@nnmprd01 run]$ \n[custompoller@nnmprd01 run]$ /home/custompoller/ipvpn/run/java -Xms1024m -Xmx4096m -DlogFilename=/home/custompoller/ipvpn/log/ipvpn -Dlog4j.configurationFile=/home/custompoller/ipvpn/conf/log4j2.xml -cp ./bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner -config /home/custompoller/ipvpn/conf/vpn.config -directorytomove /home/custompoller/ipvpn/out/ -version 1 -timeout 1500 -retries 2\n[custompoller@nnmprd01 run]$ \n[custompoller@nnmprd01 run]$ less /home/custompoller/ipvpn/log/ipvpn-2021-04-21.log \n...\n  14:26:50.446 ERROR [Thread-25] [athe-saa-new] SNMPWalkTool: snmpWalkByOidsException: \n  java.io.IOException: No such file or directory\n        at java.io.UnixFileSystem.createFileExclusively(Native Method) ~[?:1.8.0_144]\n        at java.io.File.createNewFile(File.java:1012) ~[?:1.8.0_144]\n        at com.xyz.bigstreamer.snmp.tools.snmp4jwalk.SNMPWalkTool.createLockFile(SNMPWalkTool.java:183) ~[bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar:1.0.1-SNAPSHOT]\n        at com.xyz.bigstreamer.snmp.tools.snmp4jwalk.SNMPWalkTool.snmpWalkByOids(SNMPWalkTool.java:68) [bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar:1.0.1-SNAPSHOT]\n        at com.xyz.bigstreamer.snmp.tools.wrapper.runnables.NodeRunner.run(NodeRunner.java:33) [bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar:1.0.1-SNAPSHOT]\n```\nOups! Are all directories ok?\n```\n[custompoller@nnmprd01 run]$ ll /home/custompoller/ipvpn/\ntotal 12\nlrwxrwxrwx 1 custompoller custompoller   28 Feb 25 11:07 backup -> /opt/OV/_CUSTOM_POLLER/ipvpn       ### it's red and blinking\ndrwxrwxr-x 2 custompoller custompoller   42 Apr 21 12:23 conf\ndrwxrwxr-x 2 custompoller custompoller   90 Apr 21 12:12 log\nlrwxrwxrwx 1 custompoller custompoller   33 Feb 25 11:07 out -> /opt/OV/_CUSTOM_POLLER_PROD/ipvpn     ### it's red and blinking\ndrwxrwxr-x 2 custompoller custompoller 4096 Apr 21 12:24 out2\ndrwxrwxr-x 2 custompoller custompoller 4096 Jul 30  2020 run\n[custompoller@nnmprd01 ipvpn]$ ll /opt/OV/_CUSTOM_POLLER/ipvpn\nls: cannot access '/opt/OV/_CUSTOM_POLLER/ipvpn': No such file or directory\n[custompoller@nnmprd01 ipvpn]$ ll /opt/OV/_CUSTOM_POLLER/\nls: cannot access '/opt/OV/_CUSTOM_POLLER/': No such file or directory\n[custompoller@nnmprd01 ipvpn]$ mkdir /opt/OV/_CUSTOM_POLLER/\nmkdir: cannot create directory ‘/opt/OV/_CUSTOM_POLLER/’: Permission denied\n### custompoller is not a sudoer. I tried :(\n```\nOk, so first nfgh: System administrator of this server must give us some directories to store our data. Why?\n```bash\n[custompoller@nnmprd01 run]$ df -h .\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/mapper/vg0-home   10G  800M  9.3G   8% /home\n```\nWell, 10GB will be done in the first day or 2.\nOk, but just to check the files, lets create a second directory for output files and execute the java thingy again.\n```bash\n[custompoller@nnmprd01 ipvpn]$ pwd\n/home/custompoller/ipvpn\n[custompoller@nnmprd01 ipvpn]$ mkdir out2\ncustompoller@nnmprd01 ~]$ export JAVA_HOME=/home/custompoller/jdk1.8.0_144\n[custompoller@nnmprd01 ~]$ export PATH=$JAVA_HOME/bin:$PATH\n[custompoller@nnmprd01 ~]$ cd /home/custompoller/ipvpn/run \n[custompoller@nnmprd01 ipvpn]$ /home/custompoller/ipvpn/run/java -Xms1024m -Xmx4096m -DlogFilename=/home/custompoller/ipvpn/log/ipvpn -Dlog4j.configurationFile=/home/custompoller/ipvpn/conf/log4j2.xml -cp ./bigstreamer-snmp-tools-1.0.1-SNAPSHOT.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner -config /home/custompoller/ipvpn/conf/vpn.config -directorytomove /home/custompoller/ipvpn/out2/ -version 1 -timeout 1500 -retries 2\n[custompoller@nnmprd01 run]$ less /home/custompoller/ipvpn/log/ipvpn-2021-04-21.log \n...\n    12:24:16.796 INFO [Thread-12] [athe-saa13] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:16.796 INFO [Thread-12] [athe-saa13] NodeRunner: Ended Thread for output file = [athe-saa13]\n    12:24:16.943 INFO [Thread-14] [athe-saa15] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:16.943 INFO [Thread-14] [athe-saa15] NodeRunner: Ended Thread for output file = [athe-saa15]\n    12:24:17.020 INFO [Thread-17] [athe-saa18] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:17.020 INFO [Thread-17] [athe-saa18] NodeRunner: Ended Thread for output file = [athe-saa18]\n    12:24:17.156 INFO [Thread-16] [athe-saa17] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:17.156 INFO [Thread-16] [athe-saa17] NodeRunner: Ended Thread for output file = [athe-saa17]\n    12:24:19.262 ERROR [Thread-5] [athe-saa2] SNMPWalkTool: For ip=62.103.1.232, Table OID=1.3.6.1.4.1.9.9.42.1.3.5.1.12, error=Request timed out.\n    12:24:22.656 INFO [Thread-20] [athe-saa21] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:22.656 INFO [Thread-20] [athe-saa21] NodeRunner: Ended Thread for output file = [athe-saa21]\n    12:24:24.462 ERROR [Thread-5] [athe-saa2] SNMPWalkTool: For ip=62.103.1.232, Table OID=1.3.6.1.4.1.9.9.42.1.3.5.1.31, error=Request timed out.\n    12:24:25.162 INFO [Thread-5] [athe-saa2] SNMPWalkTool: END: newdirectory=/home/custompoller/ipvpn/out2/, version=1\n    12:24:25.162 INFO [Thread-5] [athe-saa2] NodeRunner: Ended Thread for output file = [athe-saa2]\n    12:24:25.162 INFO [main] [] SNMPWrapperRunner: END SNMPWrapperRunner\n[custompoller@nnmprd01 ipvpn]$ ll out2/\ntotal 179892\n-rw-rw-r-- 1 custompoller custompoller    70882 Apr 21 12:17 nnmcp.athe384g.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller    70924 Apr 21 12:24 nnmcp.athe384g.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  1028483 Apr 21 12:17 nnmcp.athe384o.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  1028930 Apr 21 12:24 nnmcp.athe384o.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller   621224 Apr 21 12:17 nnmcp.athe384q.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller   621550 Apr 21 12:24 nnmcp.athe384q.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4183130 Apr 21 12:17 nnmcp.athe-saa10.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4183386 Apr 21 12:24 nnmcp.athe-saa10.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4477609 Apr 21 12:17 nnmcp.athe-saa11.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4478033 Apr 21 12:24 nnmcp.athe-saa11.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  3422809 Apr 21 12:17 nnmcp.athe-saa1.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  3422557 Apr 21 12:24 nnmcp.athe-saa1.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  3940362 Apr 21 12:17 nnmcp.athe-saa12.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  3940483 Apr 21 12:24 nnmcp.athe-saa12.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4800030 Apr 21 12:17 nnmcp.athe-saa13.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4798949 Apr 21 12:24 nnmcp.athe-saa13.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4635658 Apr 21 12:17 nnmcp.athe-saa14.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4636384 Apr 21 12:24 nnmcp.athe-saa14.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4941176 Apr 21 12:17 nnmcp.athe-saa15.202104211215.txt\n-rw-rw-r-- 1 custompoller custompoller  4940436 Apr 21 12:24 nnmcp.athe-saa15.202104211220.txt\n-rw-rw-r-- 1 custompoller custompoller  4546605 Apr 21 12:17 nnmcp.athe-saa16.202104211215.txt\n...\n```\nOk, seems better! \n\n4. Compare the number and lines of the files with `nnmdis01`:\n```\n[custompoller@nnmprd01 ipvpn]$ wc -l out2/*202104211220*\n     496 out2/nnmcp.athe384g.202104211220.txt\n    7060 out2/nnmcp.athe384o.202104211220.txt\n    4268 out2/nnmcp.athe384q.202104211220.txt\n   28170 out2/nnmcp.athe-saa10.202104211220.txt\n   30532 out2/nnmcp.athe-saa11.202104211220.txt\n   23134 out2/nnmcp.athe-saa1.202104211220.txt\n   26692 out2/nnmcp.athe-saa12.202104211220.txt\n   32426 out2/nnmcp.athe-saa13.202104211220.txt\n   31288 out2/nnmcp.athe-saa14.202104211220.txt\n   33530 out2/nnmcp.athe-saa15.202104211220.txt\n   30788 out2/nnmcp.athe-saa16.202104211220.txt\n   31782 out2/nnmcp.athe-saa17.202104211220.txt\n   33070 out2/nnmcp.athe-saa18.202104211220.txt\n   25454 out2/nnmcp.athe-saa19.202104211220.txt\n   25920 out2/nnmcp.athe-saa20.202104211220.txt\n   73738 out2/nnmcp.athe-saa21.202104211220.txt\n   22634 out2/nnmcp.athe-saa2.202104211220.txt\n   24054 out2/nnmcp.athe-saa22.202104211220.txt\n   19006 out2/nnmcp.athe-saa23.202104211220.txt\n   28160 out2/nnmcp.athe-saa24.202104211220.txt\n    5516 out2/nnmcp.athe-saa25.202104211220.txt\n   30868 out2/nnmcp.athe-saa7.202104211220.txt\n   26232 out2/nnmcp.athe-saa8.202104211220.txt\n   28730 out2/nnmcp.athe-saa9.202104211220.txt\n    1304 out2/nnmcp.athe-saa-new.202104211220.txt\n      96 out2/nnmcp.n3400-ekal9kb.202104211220.txt\n      96 out2/nnmcp.n3400-maro9ka.202104211220.txt\n      48 out2/nnmcp.n3400-thes9ka.202104211220.txt\n      48 out2/nnmcp.n3400-toub9ka.202104211220.txt\n  625140 total\n[custompoller@nnmprd01 ipvpn]$ logout\nConnection to nnmprd01 closed.\n[ipvpn@un2 ~]$ ssh custompoller@nnmdis01\nLast login: Wed Apr 21 13:37:06 2021 from un2e.bigdata.abc.gr\n[custompoller@nnmdis01 ~]$ wc -l ipvpn/out/*202104211220*\n     496 ipvpn/out/nnmcp.athe384g.202104211220.txt.LOADED\n    7060 ipvpn/out/nnmcp.athe384o.202104211220.txt.LOADED\n    4268 ipvpn/out/nnmcp.athe384q.202104211220.txt.LOADED\n   28170 ipvpn/out/nnmcp.athe-saa10.202104211220.txt.LOADED\n   30532 ipvpn/out/nnmcp.athe-saa11.202104211220.txt.LOADED\n   23134 ipvpn/out/nnmcp.athe-saa1.202104211220.txt.LOADED\n   26692 ipvpn/out/nnmcp.athe-saa12.202104211220.txt.LOADED\n   32426 ipvpn/out/nnmcp.athe-saa13.202104211220.txt.LOADED\n   31288 ipvpn/out/nnmcp.athe-saa14.202104211220.txt.LOADED\n   33530 ipvpn/out/nnmcp.athe-saa15.202104211220.txt.LOADED\n   30788 ipvpn/out/nnmcp.athe-saa16.202104211220.txt.LOADED\n   31782 ipvpn/out/nnmcp.athe-saa17.202104211220.txt.LOADED\n   33070 ipvpn/out/nnmcp.athe-saa18.202104211220.txt.LOADED\n   25454 ipvpn/out/nnmcp.athe-saa19.202104211220.txt.LOADED\n   25920 ipvpn/out/nnmcp.athe-saa20.202104211220.txt.LOADED\n   73736 ipvpn/out/nnmcp.athe-saa21.202104211220.txt.LOADED\n   23526 ipvpn/out/nnmcp.athe-saa2.202104211220.txt.LOADED\n   24054 ipvpn/out/nnmcp.athe-saa22.202104211220.txt.LOADED\n   19006 ipvpn/out/nnmcp.athe-saa23.202104211220.txt.LOADED\n   28160 ipvpn/out/nnmcp.athe-saa24.202104211220.txt.LOADED\n    5516 ipvpn/out/nnmcp.athe-saa25.202104211220.txt.LOADED\n   30868 ipvpn/out/nnmcp.athe-saa7.202104211220.txt.LOADED\n   26232 ipvpn/out/nnmcp.athe-saa8.202104211220.txt.LOADED\n   28730 ipvpn/out/nnmcp.athe-saa9.202104211220.txt.LOADED\n    1304 ipvpn/out/nnmcp.athe-saa-new.202104211220.txt.LOADED\n      96 ipvpn/out/nnmcp.n3400-ekal9kb.202104211220.txt.LOADED\n      96 ipvpn/out/nnmcp.n3400-maro9ka.202104211220.txt.LOADED\n      48 ipvpn/out/nnmcp.n3400-thes9ka.202104211220.txt.LOADED\n      48 ipvpn/out/nnmcp.n3400-toub9ka.202104211220.txt.LOADED\n  626030 total\n```\nSeems fine for a first check.\n\n5. Repeat the same for Syzefxis. Changes are for the java process and the test directory.\n```\n[custompoller@nnmprd01 ~]$ mkdir out2\n[custompoller@nnmprd01 ~]$ java -Xms1024m -Xmx4096m -DlogFilename=/home/custompoller/log/syzeyksis -Dlog4j.configurationFile=/home/custompoller/conf/log4j2.xml -cp ./bigstreamer-snmp-tools-1.1.0.jar com.xyz.bigstreamer.snmp.tools.wrapper.SNMPWrapperRunner -config /home/custompoller/conf/syzeyksis.config -directorytomove /home/custompoller/out2/ -version 1 -timeout 1500 -retries 2\n```\nApart from the directories in step 3 nothing came up at this first check.\n\nfor Oozie Workflows:\n\n1. Login at Hue with your personal account and switch to Hue 3. Then go to `https://un1.bigdata.abc.gr:8889/oozie/list_oozie_coordinators/#`\n2. Search for `c_nms` coordinators.\n3. Click on coordinator `C_NMS Config, mplsdb.nms_node` and then its latest execution. At the top there is the name of the workflow, `NNM_Workflow`.\n4. At the left bar, under `Variables` we will obtain anything we need for a manual execution. Specifically:\n    * SPLIT_COLUMN\n    * partition\n    * tablename (Impala)\n    * password\n    * sourceTable\n    * schema\n5. We will first check for source table `nms_node`. Go to Impala and issue the following query to create a look-alive test table:\n```bash\nCREATE table nnmnps.nms_node_test\nlike nnmnps.nms_node;\n```\n6. Switch back to Hue 4. On the top search bar, write the workflow name `NNM_Workflow` and click on the result.\n7. Before `Submit` change the iptables@un2 to bind the nnmprd01 \n```\nFrom nnmdis01:\n\n-A PREROUTING -i bond0.300 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.25.150.68:5432\n-A OUTPUT -d 10.255.240.13/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.25.150.68:5432\n-A OUTPUT -d 10.255.240.14/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.25.150.68:5432\n-A OUTPUT -d 10.255.240.15/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.25.150.68:5432\n\nTo nnmprd01:\n\n-A PREROUTING -i bond0.300 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\n-A OUTPUT -d 10.255.240.13/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\n-A OUTPUT -d 10.255.240.14/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\n-A OUTPUT -d 10.255.240.15/32 -p tcp -m tcp --dport 6535 -j DNAT --to-destination 172.24.104.100:5432\n\nservice iptables reload\n```\n8. At the workflow page, click on `Submit`. Fill variables with the information taken at step 4. Change `tablename` to `nms_node_test` so that you don't overwrite the data. Click `Submit`.\n9. The first time the sqoop command failed. Inspect logs by clicking the `View logs` of the sqoop 1 stage. At the default logs page we will see the following error: `org.postgresql.util.PSQLException: FATAL: no pg_hba.conf entry for host \"172.25.37.236\", user \"postgres\", database \"nnm\"`\nOups vol.2! Second nfgh: abc must enable us to login to the database.",
            "Affected Systems": "abc Bigstreamer IPVPN Syzefxis",
            "Action Points": "After abc fixes the above we have a series of checks to perform. Check out the GI for details.",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1622139",
        "Description": "abc - BigStreamer - IM1622139 - CSI_FIXED CSI_MOBILE Collection problem",
        "Keywords": [
            "sai",
            "workflow",
            "csi"
        ],
        "Owner": "u27",
        "Date": "20210824",
        "Status": "Closed",
        "Info": "abc/BigStreamer/issues/20210824-IM1622139.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\n\nWe have not received any CSI_MOBILE files today.\n\nThe latest we have received are the following:\n172.25.37.240 CSI_MOBILE CSI_mob_08162021_08232021.txt 208229499 8/23/2021 11:00:01 AM 20690 8/23/2021 11:00:22 AM\n172.25.37.240 CSI_MOBILE CSI_mob_08162021_08222021.txt 197819583 8/22/2021 11:00:02 AM 23619 8/22/2021 11:00:25 AM\n172.25.37.240 CSI_MOBILE CSI_mob_08162021_08212021.txt 190247108 8/21/2021 11:00:01 AM 21643 8/21/2021 11:00:23 AM\n\nPlease for your checks.\n\nThanks\n```",
            "Actions Taken": "1. ssh un2 with your personal account\n2. sudo -iu mtuser\n3. less /shared/abc/export_sai_csi/log/sai_csi.cron.20210913.log\n4. Check the logs for the issue date. If any problem exist for par_dt on table `sai.sub_aggr_csi_it` or received any mail like this ```Mediation/IT Daily Flows: Unavailable Data Table sai.sub_aggr_csi_it has less lines than 3500000. Actual number is 0 for date 23/08/2021``` then follow the below steps:\n\n5. ssh unc2;sudo -iu intra;crontab -l | grep -i cron_aggregation_parallel.sh; less /shared/abc/traffica/log/SAI_AGGREGATION_LOOP.20210823_20210823.log `IMPROTANT: If the hour that the aggr finished were after 07:30 then we have issue bro`\n6. ssh un2\n7. su - intra\n8. crontab -l | grep -i 'csi_weekly_load'\n```\n##### EZ Population of CSI table (par_dt is always a monday and contains data between (now -1) day and its respective monday)  #####\n30 07 * * * /shared/abc/csi/bin/csi_weekly_load.sh >> /shared/abc/csi/log/csi_weekly_load.`date '+\\%Y\\%m\\%d'`.log 2>&1\n```\n10. secimp\n11. select count(*),par_dt from sai.sub_aggr_csi_it where par_dt >= '20210xxx'group by par_dt; `IMPROTANT` `1. sai.sub_aggr_csi_it is a view of vantage.sub_aggr_csi 2.Table has weekly par_dt but updated every day with new data`\n\nSteps to restore the data:\n\n1. First try on temp tables:\n    - ssh un2;sudo -iu intra;cd /shared/abc/csi/bin/;\n\n    - ./csi_weekly_load.sh.manual 2 `If 2 days passed then run the script with parameter 3 instead of 2. If no day has passed run the script without any parameter `\n    ```\n    Nfgh:tables temp.sub_aggr_csi & temp.sub_aggr_csi_lastmeasure \n    ```\n2. select count(*),par_dt from temp.sub_aggr_csi where par_dt >= '20210xxx'group by par_dt; `(Check if par_dt created)`\n\n3. If data imported on test tables then go to production tables:\n    - ssh un2;sudo -iu intra;cd /shared/abc/csi/bin/;\n    - ./csi_weekly_load 2 `If 2 days passed then run the script with parameter 3 instead of 2. If no day has passed run the script without any parameter `\n4. Run the CSI fix&mob after the above steps completed:\n    - - ssh un2;sudo -iu mtuser;\n    - /shared/abc/export_sai_csi/export_csi_mob_daily.sh `<missing_date>`\n5. After the successfully execution of step `4` then run again `./csi_weekly_load`",
            "Affected Systems": "abc Bigstreamer CSI&MOB Flows",
            "Action Points": "Change the hour of the crontab execution for `SAI_AGGREGATIONS` to avoid the below issues.\n```\nssh unc2;sudo -iu intra;crontab -l | grep -i cron_aggregation_parallel.sh;\n#### SAI/TRAFFICA raw Stats, aggregations, aggr Stats (initial schedule 07:01:00)\n1 6 * * * /shared/abc/traffica/bin/cron_aggregation_parallel.sh >> /shared/abc/traffica/log/cron_aggregation_parallel.`date '+\\%Y\\%m\\%d' -d \"yesterday\"`.log 2>&1\n```",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1631218",
        "Description": "abc - BigStreamer - IM1631218 - huawei tv",
        "Keywords": [
            "play",
            "vod",
            "tvod"
        ],
        "Owner": "u15",
        "Date": "20210902",
        "Status": "Open",
        "Info": "abc/BigStreamer/issues/20210902-IM1631218.md",
        "Detailed Description": {
            "Description": "```\nGood morning, please load the tables huawei_tv.rel_play_tv_hist, huawei_tv.rel_play_tvod_hist, huawei_tv.rel_play_vod_hist for 31/8-1/9 and huawei_tv.rel_vod_info_hist for 1/9 with data.\n```",
            "Actions Taken": "1. Login to Hue and go to `Editor` > `Impala`\n2. Check for missing partitions as stated in description\n```bash\nselect count(*), par_dt \nfrom huawei_tv.rel_vod_info_hist  \nwhere par_dt between '20210825' and '20210905' \ngroup by par_dt \norder by par_dt;\nResult:\ncount(*)\tpar_dt\t\n...\n15026\t20210830\t\n14728\t20210901\t\n14748\t20210902\t\n...\n```\n\n3. Partition for `20210831` is actually missing but `20210901` is here and seems fine.\n\n4. Check logs of flow but first login to `un2`. And yes, huawei tv loads data for yesterday's partition from today's table #crazyright\n\n``` bash\n$ su - intra\n$ cd /shared/abc/huawei_tv/\n$ less log/huawei_tv_load.20210901.log\n...\nSFTP get files from : ./export/20210901\nConnected to 172.28.128.150.\nsftp> get export/20210901/*.csv /data/1/huawei_tv_LZ/\nFetching /export/20210901/EPG_SCHEDULE.csv to /data/1/huawei_tv_LZ/EPG_SCHEDULE.csv\nFetching /export/20210901/REL_PLAY_TV.csv to /data/1/huawei_tv_LZ/REL_PLAY_TV.csv\nFetching /export/20210901/REL_PLAY_TVOD.csv to /data/1/huawei_tv_LZ/REL_PLAY_TVOD.csv\nFetching /export/20210901/REL_PLAY_VOD.csv to /data/1/huawei_tv_LZ/REL_PLAY_VOD.csv\nFetching /export/20210901/REL_VOD_INFO.csv to /data/1/huawei_tv_LZ/REL_VOD_INFO.csv\n\n```\n\n5. Let's inspect those files\n```bash\n$ sftp bigdata@172.28.128.150:/export\nsftp> cd 20210901\nsftp> ls -l\n-rw-r--r--    1 0        0          476541 Sep  1 05:13 EPG_SCHEDULE.csv\n-rw-r--r--    1 0        0               0 Sep  1 05:11 REL_PLAY_TV.csv\n-rw-r--r--    1 0        0               0 Sep  1 05:10 REL_PLAY_TVOD.csv\n-rw-r--r--    1 0        0               0 Sep  1 05:11 REL_PLAY_VOD.csv\n-rw-r--r--    1 0        0         3470903 Sep  1 05:11 REL_VOD_INFO.csv\n-rw-r--r--    1 0        0        11414886 Sep  1 05:13 SubscriberID_STBMACAddress_Relationship.csv\n```\nAs it is obvious, there are empty files, so abc needs to reload data into the remfgh server and we need to run the script for the missing partition:\n```bash\n$ /shared/abc/huawei_tv/bin/huawei_tv_load.sh 20210831\n```",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "We have informed abc about each case and wait for their response.\nPartition `20210901` is fine but `20210831` needs manual action as nfghd in step 5.",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1742741",
        "Description": "abc - BigStreamer - IM1742741 - prometheus",
        "Keywords": [
            "prometheus",
            "dwh22_last",
            "flow"
        ],
        "Owner": "u27",
        "Date": "20211215",
        "Status": "Closed",
        "Info": "abc/BigStreamer/issues/20211215-IM1742741.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\nthe table prometheus.dwh22_last , and by extension the view prometheus.prom_total_subscrs is empty.\nPlease check the corresponding stream.\n```",
            "Actions Taken": "1. ssh un2 with your personal account; sudo -iu intra\n2. crontab -l | grep prometheus\n```\n0 6 * * * /shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.`date '+\\%Y\\%m\\%d'`.log 2>&1\n```\n3. Check the latest log file to find the root cause `/shared/abc/prometheus/log/Cron_Prometheus_Load.date_of_issue.log`\n4. If the issue date didn't pass then run again the script `/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.issue_date.log`\nIf the issue date passed then comment the `yesterday_dt=` and replace it with `yesterday_dt=<issue date -1>`. Then run the script ``/shared/abc/prometheus/bin/Cron_Prometheus_Load.sh >> /shared/abc/prometheus/log/Cron_Prometheus_Load.issue_date.log``\n5. When the script finished replace the old value of `yestarday_dt` on script.\n6. Checks:\n```\nselect count(*),par_dt from prometheus.table where par_dt >= 'issue_date -1' group by 2;\n```",
            "Affected Systems": "abc Bigstreamer Prometheus Tables\n\n```\nprometheus.DWH22_hist - IMPALA\nprometheus.DWH22_last - IMPALA\nrometheus.dwh3_hist - HIVE\nprometheus.DWH3_hist - IMPALA\nprometheus.DWH3_last - IMPALA\nprometheus.dwh4_hist - HIVE\nprometheus.DWH4_hist  - IMPALA\nprometheus.DWH4_last - IMPALA\nprometheus.dwh9_hist - HIVE\nprometheus.DWH9_hist - IMPALA\nprometheus.DWH9_last - IMPALA\nprometheus.dwh11_hist - HIVE\nprometheus.DWH11_hist - IMPALA\nprometheus.DWH11_last - IMPALA\nprometheus.dwh14_hist - HIVE\nprometheus.DWH14_hist - IMPALA\nprometheus.DWH14_last - IMPALA\nprometheus.dwh17_hist - HIVE\nprometheus.DWH17_hist - IMPALA\nprometheus.DWH17_last - IMPALA\nprometheus.dwh2_hist - HIVE\nprometheus.DWH2_hist - IMPALA\nprometheus.DWH2_last - IMPALA\nprometheus.dwh43_hist - HIVE\nprometheus.DWH43_hist - IMPALA\nprometheus.DWH43_last - IMPALA\n```",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "SD1811951",
        "Description": "abc - Bigstreamer - SD1811951 - brond.brond_adsl/vdsl_stats_week",
        "Keywords": [
            "prometheus",
            "dwh22_last",
            "flow"
        ],
        "Owner": "u27",
        "Date": "20220103",
        "Status": "Open",
        "Info": "abc/BigStreamer/issues/20220103-SD1811951.md",
        "Detailed Description": {
            "Description": "```\nGood morning and happy new year,\n\nPlease load the tables for 01/01 and 02/01 with today's data available\n\nbrond.brond_adsl_stats_week\nbrond.brond_vdsl_stats_week\n\nThanks\n```",
            "Actions Taken": "1. ssh un2;\n2. sudo -iu intra; secimp\n3. Check that source tables brond.brond_vdsl_stats_week and brond.brond_adsl_stats_week have data for these partitions.\n```\nselect count(*), par_dt from brond.brond_vdsl_stats_week where par_dt >= 'xxxxx' group by 2 order by 2;\nselect count(*), par_dt from brond.brond_adsl_stats_week where par_dt >= 'xxxxx' group by 2 order by 2;\nexit;\n```\n4. cd /shared/abc/brond_dsl_stats/DataParser/scripts/log/ ; check logs of the missing date\n5. less load_dsl_stats.missing_date.log\n6. The filename should be like DWH_ADSL.number_year_month_day.csv.gz but for the missing data was DWH_ADSL_year_month_day.csv.gz. Furthermore, VDSL files did missing on the sftp server.\n7. Inform abc for missing files on sftp server.\n8. The next execution will be automated get the files and create the missing partitions. If not the check if any DWH_ file with wrong pattern exist under /ez/landingzone/brond_dsl_stats/ . If yes remove it and re-run the workflow.\n9. When step 7 completed continue with https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/20210122-IM1421557.md",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1793457",
        "Description": "SM-MISSING DATA",
        "Keywords": [
            "cfgh_bs_ipvpn"
        ],
        "Owner": "u77",
        "Date": "20220215",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201029-IM1793457.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1773928",
        "Description": "Please investigate why yesterday's file (2022-01-24) uploaded to the REF_DATA folder was not uploaded and upload it.",
        "Keywords": [
            "refdata.rd_cells"
        ],
        "Owner": "u13",
        "Date": "20220121",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20220121-ΙΜ1773928.md",
        "Detailed Description": {
            "Description": "```\nPlease investigate why yesterday's file (2022-01-24) uploaded to the REF_DATA folder was not loaded and upload it.\n```",
            "Actions Taken": "1. Check the size of current partition from Impala-Shell\n``` bash\n[un-vip.bigdata.abc.gr:22222] default> show files in refdata.rd_cells_load partition (par_dt>='20220118');\n```\nexample output\n``` bash\n+----------------------------------------------------------------------------------------------+---------+-----------------+\n| Path                                                                                         | Size    | Partition       |\n+----------------------------------------------------------------------------------------------+---------+-----------------+\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220118/cells_20220118.csv | 44.00MB | par_dt=20220118 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220119/cells_20220119.csv | 44.12MB | par_dt=20220119 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220121/cells_20220121.csv | 43.72MB | par_dt=20220121 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220122/cells_20220122.csv | 43.64MB | par_dt=20220122 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220123/cells_20220123.csv | 43.59MB | par_dt=20220123 |\n+----------------------------------------------------------------------------------------------+---------+-----------------+\nFetched 5 row(s) in 0.02s\n```\n\nAs you can see there is a missing partition for 20220120\n\n2. At first, lets try to reload the \n``` bash\n[un-vip.bigdata.abc.gr:22222] default> refresh refdata.rd_cells_load;\n```\n\n3. Repeat step #2 in ordet to check that the missing partition is fixed. If not keep on with the follwing steps\n\n4. Check logs \n``` bash\n[intra@un2]$ less /shared/abc/refdata/log\n```\n\nNotice that there is nothing for partition 20220120\n\nFirst lines of this log must be something like \n\n``` bash\n===[Sat Jan  1 00:05:01 EET 2022, 20211231 --> 20211231, 010_refData_Load.sh]===\nKINIT_INFO: 2022-01-01 00:05:01, check for valid kerberos ticket\n```\nAt /shared/vantage_ref-data/REF-DATA/ you will see the following lines \n\n``` bash\n-rw-r--r--   1 vantagerd external  46258798 Jan 19 17:50 cells_20220119.csv.20220120.LOADED\n-rw-r--r--   1 vantagerd external  46289460 Jan 20 17:50 cells_20220120.csv.gz NOT_LOADED\n-rw-r--r--   1 vantagerd external  46258798 Jan 19 17:50 cells_20220121.csv.20220122.LOADED\n```\n\n5. So lets check 10_refData_Load.sh script and run this script for the missing partition \n``` bash\n[intra@un2 bin]$ /shared/abc/refdata/bin/010_refData_Load.sh 20220120\n```\n\n6. Verify that the missing partion is loaded \n``` bash\n[un-vip.bigdata.abc.gr:22222] default> show files in refdata.rd_cells_load partition (par_dt>='20220118');\nQuery: show files in refdata.rd_cells_load partition (par_dt>='20220118')\n+----------------------------------------------------------------------------------------------+---------+-----------------+\n| Path                                                                                         | Size    | Partition       |\n+----------------------------------------------------------------------------------------------+---------+-----------------+\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220118/cells_20220118.csv | 44.00MB | par_dt=20220118 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220119/cells_20220119.csv | 44.12MB | par_dt=20220119 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220120/cells_20220120.csv | 44.15MB | par_dt=20220120 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220121/cells_20220121.csv | 43.72MB | par_dt=20220121 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220122/cells_20220122.csv | 43.64MB | par_dt=20220122 |\n| hdfs://nameservice1/ez/warehouse/refdata.db/rd_cells_load/par_dt=20220123/cells_20220123.csv | 43.59MB | par_dt=20220123 |\n+----------------------------------------------------------------------------------------------+---------+-----------------+\nFetched 6 row(s) in 0.02s\n```",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "obss/bigdata/common-dev/apps/monitoring/monitoring-devops#14",
        "Description": "abc: Slow insert DB queries.",
        "Keywords": [
            "mysql"
        ],
        "Owner": "u1",
        "Date": "20221128",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20221128-monitoring-devops14.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2076207",
        "Description": "Fraport measurement registration problem",
        "Keywords": [
            "cfgh_bs_ipvpn"
        ],
        "Owner": "u1",
        "Date": "20230205",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230205-IM2076207.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\n\nWe've noticed that we're not getting metrics at the following new points in the Fraport client (there don't seem to be any records at all)\n\nFrap-PVK1\nFrap-KVA1\nFrap-SKG1\nFrap-JMK1\nFrap-RHO1\n\nFrap-PVK2\nFrap-KVA2\nFrap-SKG2\nFrap-JMK2\nFrap-RHO2\n\nFrom a check made through the saa-csr5 probe, it appears that the router takes measurements from the specific points.\n\nIndicative:\n\nsaa-csr5#sh ip sla configuration 4891 | i addre\nTarget address/Source address: 80.106.132.34/212.205.74.72\n\nsaa-csr5#sh ip sla statistics 4891\nIPSLAs Latest Operation Statistics\nIPSLA operation id: 4891\n        Latest RTT: 8 milliseconds\nLatest operation start time: 12:56:10 EET Thu Feb 2 2023\nLatest operation return code: OK\nNumber of successes: 302\nNumber of failures: 0\nOperation time to live: Forever\n\nFanis also sees that NNM takes the measurements but they are not sent to Bigstreamer.\n\nPlease for your checks.\n\nThanks\n```",
            "Actions Taken": "1. As seen from [here](../supportDocuments/applicationFlows/ip_vpn.md) branch metrics are exported by querying `bigcust.nnmcp_ipvpn_slametrics_hist`. We need to investigate if this table contains metrics for the branches mentioned in the description.\n\n    From `un2.bigdata.abc.gr` with root (cyberark login):\n\n    ``` bash\n    su - ipvpn\n    impala-shell -i un-vip.bigdata.abc.gr -k --ssl\n    ```\n\n    ``` sql\n    select distinct qa_probe_name from bigcust.nnmcp_ipvpn_slametrics_hist where par_dt='20230202' and customer = 'fraport';\n    ```\n\n    |                     qa_probe_name                      |\n    | :----------------------------------------------------: |\n    |                     _Other probes_                     |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-RHO1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-RHO2_64_ce   |\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-fgh1_64_ce |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-HQs1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JSI2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KGS2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CHQ1_64_ce   |\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-fgh2_64_ce |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-HQs2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-EFL2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CFU1_64_ce   |\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-LH1_64_ce  |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-EFL1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KVA1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JMK2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CFU2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JSI1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JTR1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JTR2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-JMK1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-MJT1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-PVK2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-CHQ2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SMI1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-PVK1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KGS1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-MJT2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SKG1_64_ce   |\n    | avail_saa-csr5_ip-sla-probe_fraport_Frap-DC-LH2_64_ce  |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-KVA2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SKG2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-ZTH1_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-SMI2_64_ce   |\n    |  avail_saa-csr5_ip-sla-probe_fraport_Frap-ZTH2_64_ce   |\n    |                     _Other probes_                     |\n\n2. These probes were checked in the CSVs created by `custompoller`\n\n    From `un2.bigdata.abc.gr` with root (cyberark login):\n\n    ``` bash\n    su - ipvpn\n    ssh custompoller@nnmdis01\n    grep fraport_Frap-ZTH2_64_ce ipvpn/out/*.LOADED | head -10\n    # Here we see that the probe name is the same as the one we see in bigcust.nnmcp_ipvpn_slametrics_hist\n    ```\n\n3. After reviewing the [business documentation](https://metis.xyztel.com/obss/bigdata/abc/ipvpn/ip-vpn-sla/ipvpnsla-customers-devops/-/tree/master/docs) we found that valid device types for export are `'ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes'` and that the `device_type` field is derived from the probe name.\n\n    We checked the probe names with valid device type.\n\n    From `un2.bigdata.abc.gr` with root (cyberark login):\n\n    ``` bash\n    su - ipvpn\n    impala-shell -i un-vip.bigdata.abc.gr -k --ssl\n    ```\n\n    ``` sql\n   select distinct qa_probe_name from bigcust.nnmcp_ipvpn_slametrics_hist where par_dt='20230202' and customer = 'fraport' and  device_type  IN ('ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes');\n    ```\n\n    |                             qa_probe_name                             |\n    | :-------------------------------------------------------------------: |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-HDQ-01_cpe_fraport      |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-KGS-01_cpe_fraport      |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-KGS-02_cpe_fraport      |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-RHO-02_cpe_fraport      |\n    |      avail_saa-csr1_ip-sla-probe_fraport_FG4G-SMI-01_cpe_fraport      |\n    |        avail_saa-csr1_ip-sla-probe_fraport_Frap12_cpe_fraport         |\n    |        avail_saa-csr1_ip-sla-probe_fraport_Frap13_cpe_fraport         |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-CFU-01_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-CFU-02_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-JMK-01_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-JSI-01_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-SKG-01_cpe_fraport      |\n    |      avail_saa-csr2_ip-sla-probe_fraport_FG4G-SKG-02_cpe_fraport      |\n    |        avail_saa-csr2_ip-sla-probe_fraport_Frap15_cpe_fraport         |\n    |      avail_saa-csr3_ip-sla-probe_fraport_FG4G-CHQ-02_cpe_fraport      |\n    |      avail_saa-csr3_ip-sla-probe_fraport_FG4G-EFL-01_cpe_fraport      |\n    |      avail_saa-csr3_ip-sla-probe_fraport_FG4G-RHO-01_cpe_fraport      |\n    |      avail_saa-csr4_ip-sla-probe_fraport_FG4G-CHQ-01_cpe_fraport      |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap01_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap01_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap02_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap02_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap03_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap03_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap04_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap04_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap05_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap05_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap06_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap06_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap07_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap07_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap09_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap09_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap10_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap10_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap11_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap11_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap12_ce_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap13_ce_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap14_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap14_cpe_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap15_ce_fraport         |\n    |         avail_saa-csr5_ip-sla-probe_fraport_Frap16_ce_fraport         |\n    |        avail_saa-csr5_ip-sla-probe_fraport_Frap16_cpe_fraport         |\n    |    avail_saa-csr5_ip-sla-probe_fraport_frap-21p2000380_ce_fraport     |\n    | avail_saa-csr5_ip-sla-probe_fraport_fraport-DIA-21N1003241_ce_fraport |\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap11-backup_ce_fraport     |\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap12-backup_ce_fraport     |\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap13-backup_ce_fraport     |\n    |    rttd-pl_saa-csr1_ip-sla-probe_fraport_Frap14-backup_ce_fraport     |\n    |    rttd-pl_saa-csr2_ip-sla-probe_fraport_Frap15-backup_ce_fraport     |\n    |    rttd-pl_saa-csr3_ip-sla-probe_fraport_Frap06-backup_ce_fraport     |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap01-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap01_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap02-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap02_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap03-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap03_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap04-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap04_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap05-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap05_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap06_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap07-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap07_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap09-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap09_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap10-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap10_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap11_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap12_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap13_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap14_ce_fraport        |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap15_ce_fraport        |\n    |    rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap16-backup_ce_fraport     |\n    |        rttd-pl_saa-csr5_ip-sla-probe_fraport_Frap16_ce_fraport        |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap01_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap02_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap03_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap04_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap05_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap06_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap07_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap09_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap10_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap11_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap12_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap13_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap14_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap15_ce_fraport       |\n    |      udp-jitter_saa-csr5_ip-sla-probe_fraport_Frap16_ce_fraport       |\n\n4. By comparing the probe from the two queries, we see that the device type for the new devices is `64` and therefore it is omitted from the CSVs. The probe name is configured by the customer on their end\n\n5. Inform the customer about the problem\n\n``` text\nGood evening,\n\nWe see that for the new points you mention there are SLA metrics in the respective tables. These points are excluded when generating the CSV as the probe name does not match what is expected. Specifically:\n\nThe delimeter in the probe name is the \"_\" character, the device type is extracted from the probe name, and the measurements included in the CSVs are for the device types 'ce', 'ce 1024Bytes', 'cpe', 'cpe 1024Bytes', 'nte', 'nte 1024Bytes'.\n\nΑυτό το probe είναι valid:\navail_saa-csr1_ip-sla-probe_fraport_Frap12_cpe_fraport\nWhile the following belonging to one of the points you mentioned is not:\navail_saa-csr5_ip-sla-probe_fraport_Frap-PVK1_64_ce\nAs you can see in the device type position it has the value 64 and is therefore excluded in the generated CSVs.\n\nPlease correct the probe names. If there is nothing else please let us know so we can close the ticket.\n```",
            "Affected Systems": "abc Bigstreamer CDSW",
            "Action Points": "",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2131290",
        "Description": "LOCATION_MOBILITY - LM_02_LTE failure",
        "Keywords": [
            "LM - Location Mobility"
        ],
        "Owner": "u7",
        "Date": "20230421",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230420-IM2131290.md",
        "Detailed Description": {
            "Description": "Our team has been assigned with the following issue:\n\n```\nGood morning,\n\nAs of yesterday noon at 15:00 we noticed that the creation of the LM_02_LTE file of the location mobility stream fails. From the HUE jobs you can see that the workflow (export_Location_Mobility_files_to_mediat...) is killed.\n\nWe also saw from the logs (lm_export_lte_v2_mon.cron.20230419.log) that at 15:00 when the problem starts we have the following error\n\nQuery submitted at: 2023-04-19 15:00:31 (Coordinator: http://sn72.bigdata.abc.gr:25000)\nQuery progress can be monitored at: http://sn72.bigdata.abc.gr:25000/query_plan?query_id=c74df6d614d535ea:4de432ac00000000\nERROR: Failed due to unreachable impalad(s): sn102.bigdata.abc.gr:22000\n\nCould not execute command: SELECT\nachievable_thr_bytes_down_1,\nachievable_thr_bytes_up_1,\nachievable_thr_time_down_1,\n\n..................................................................\n\n[2023/04/19 15:19:13] - ERROR: Impala shell command for par_msisdn= failed.\n[2023/04/19 15:19:13] - ERROR: Clean up and exit.\n% Total % Received % Xferd Average Speed Time Time Time Current\n\n\nFrom there onwards we observe errors of the form:\n\nQuery submitted at: 2023-04-19 17:00:31 (Coordinator: http://sn64.bigdata.abc.gr:25000)\nQuery progress can be monitored at: http://sn64.bigdata.abc.gr:25000/query_plan?query_id=094fdeda997b8d44:5826172200000000\nERROR: Disk I/O error on sn62.bigdata.abc.gr:22000: Failed to open HDFS file hdfs://nameservice1/ez/warehouse/npce.db/eea_hour/pardt=2023041910/ef4d08b3073e8531-d909e37b0000010\ne_1525512597_data.0.txt\nError(2): No such file or directory\nRoot cause: RemfghException: File does not exist: /ez/warehouse/npce.db/eea_hour/pardt=2023041910/ef4d08b3073e8531-d909e37b0000010e_1525512597_data.0.txt\nat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:85)\nat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)\nat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1909)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:736)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:415)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)\nat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:869)\n\n\nWe have similar errors in today's log file lm_export_lte_v2_mon.cron.20230420.log\n\nPlease for your checks.\n\nThanks\n```",
            "Actions Taken": "No restart needed of the flow. There were some adjustments from dev team that took place. \n\nDev Resolution:\n\n```\nIt was necessary to make some corrections in the configuration table of the refdata.mediation_loc_mobility_load_info flow to synchronize the data sets that will be exported.\nAdditionally, due to the following exception a refresh table was added before the select in npce.eea_hour.\nERROR: Disk I/O error on sn111.bigdata.abc.gr:22000: Failed to open HDFS file hdfs://nameservice1/ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\nError(2): No such file or directory\nRoot cause: RemfghException: File does not exist: /ez/warehouse/npce.db/eea_hour/pardt=2023041912/6547744514f77d2d-0dbb27f700000123_1150579282_data.0.txt\n```",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "First thing that we have checked were the comments on the [md](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/trustcenter_flows.md) that exists for this flow.\nAfter checking the logs of the flow we saw that the flow was running successfully after it had failed for half day. The reason that it was able to run  was the retention that had already taken place the day that we received the ticket.\n\nWe were able to identify the issue that caused the problem. After checking the logs of the host we saw that the host didn't have enough memory at the time.\n![sn102_memory](.media/sn102_memory.JPG) because of the user `ranai-geo` that had run an impala query \n![query](.media/query.JPG) that took all the resources of the impala deamon on sn102.\n![query_details](.media/q_details.JPG)",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2215792",
        "Description": "Refdata_rd_cells not loaded properly",
        "Keywords": [
            "Refdata_rd_cells"
        ],
        "Owner": "user1",
        "Date": "20230912",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230912-IM2215792.md",
        "Detailed Description": {
            "Description": "The following issue has been assigned to our team with subject:\n\n```\nGood evening,\n\nNoticed that refdata.rd_cells was not loaded by refdata.rd_cells_load\n\nselect max(par_dt) from refdata.rd_cells_load  --> 20230911\n\nselect max(refdate) from refdata.rd_cells -->  20230910\n```",
            "Actions Taken": "At path `/shared/abc/refdata/bin` there is the script `210_refData_Load.sh` that at the beginning of each day loads the reference data at the `refdata.rd_cells_load` table and then updates the table so that this data appears as the latest data for other tables. At the same path there is the script `220_refData_Daily_Snapshot.sh` that loads the reference data from the `refdata.rd_cells_load` table to the `refdata.rd_cells` table, so that the two tables have the latest data recorded. These actions are handled by different server coordinators of cloudera manager. \n\nWe checked the log files for each month at the `/shared/abc/refdata/log` path to see which coordinator was responsible for these processes at September 12. After that we logged in Cloudera Manager, and checked the log files at the specific time interval. We found that due to synchronization issues , the coordinator that updated `refdata.rd_cells` updated it before the procedure that refreshed the data at  `refdata.rd_cells_load` so it read the data from the previous date.\n\nThe solution was to add the parameter **set SYNC_DDL=1** at the necessary scripts so there are no synchronization issues.\n\nThe logs at cloudera manager are deleted after a certain period of time, so they need to be checked soon after the ticket.",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2217968",
        "Description": "Location mobility : Files export issue",
        "Keywords": [
            "LM - Location Mobility"
        ],
        "Owner": "user1",
        "Date": "20230915",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230915-IM2217968.md",
        "Detailed Description": {
            "Description": "The following issue has been assigned to our team with subject:\n\n```\nGood evening,\n\nFrom 15/9 11:00 Location Mobility Files are not exported.\n\nun2 /shared/abc/location_mobility/logging\n\nrw-rw-r-- 1 mtuser mtuser  867968 Sep 15 11:03 LM_02_lte_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1254780 Sep 15 11:01 LM_03_smsIn_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1254098 Sep 15 11:01 LM_04_smsOut_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1333387 Sep 15 11:03 LM_05_voiceInOut_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1285358 Sep 15 11:04 LM_06_voiceIn_reconciliation.log\n-rw-rw-r-- 1 mtuser mtuser 1302383 Sep 15 11:04 LM_07_voiceOut_reconciliation.log\n\nfor example:\n2023-09-15 05:02:15 LM_02_lte_20230915_00001.txt 2023091502 7742068\n2023-09-15 07:02:27 LM_02_lte_20230915_00002.txt 2023091504 5880766\n2023-09-15 09:02:37 LM_02_lte_20230915_00003.txt 2023091506 8227530\n2023-09-15 11:03:21 LM_02_lte_20230915_00004.txt 2023091508 19753878\n```\n\nThe location mobility files are updated each day by running the following script:\n\n```\nssh -o \"StrictHostKeyChecking no\" -i ./id_rsa mtuser@un-vip.bigdata.abc.gr \"script\"\n```",
            "Actions Taken": "After investigating we found that the **mtuser** user did not have the necessary permissions and was not authorized to connect to the main server, so the script never ran.\n\nAfter updating the permissions for the user, the script started running again.\n\nWe can check if the script is running , by logging in to `Hue Server` and checking the `Oozie Editor`  for workflows of `mtuser`.",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2278275",
        "Description": "Hue error",
        "Keywords": [
            "hue",
            "kerneros"
        ],
        "Owner": "u13",
        "Date": "20240212",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20240212-IM2278275.md",
        "Detailed Description": {
            "Description": "```\nImpala & Hue queries from Hue fail with\n\n\nCould not start SASL: Error in sasl_client_start (-1) SASL(-1): generic failure: GSSAPI Error: Unspecified GSS failure. Minor code may provide more information (Ticket expired) (code THRIFTTRANSPORT): TTransportException('Could not start SASL: Error in sasl_client_start (-1) SASL(-1): generic failure: GSSAPI Error: Unspecified GSS failure. Minor code may provide more information (Ticket expired)',)\n\nApplies to https://172.25.37.236:8888/hue ( un2) & Virtual https://172.25.37.237 :8888/hue .\nNot for https://172.25.37.235:8888/hue (un1)\n```",
            "Actions Taken": "As read from [here](https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/procedures/manage_idm_replication.md#a-brief-history-of-preauthentication) we proceeded to the following steps:\n\n1. List crontab entries and find entry for hue:\n\n```bash\n[root@un2 ~]# crontab -l | grep hue\n```\nOuput:\n\n```bash\n### Hue Workaround ###\n5 */2 * * * sudo -u hue /usr/bin/kinit -k -t /var/run/cloudera-scm-agent/process/159288-hue-KT_RENEWER/hue.keytab -c /var/run/hue/hue_krb5_ccache hue/un2.bigdata.abc.gr@BIGDATA.abc.GR\n```\n\n2. Check if this file exists:\n\n```bash\n[root@un2 ~]# ll /var/run/cloudera-scm-agent/process/159835-hue-KT_RENEWER/\nls: cannot access /var/run/cloudera-scm-agent/process/159835-hue-KT_RENEWER/: No such file or directory\n```\n\nThis occurs due to the fact that hue service had been restarted.\n\n3. Find latest process for Kerberos ticket cache of Hue with ` ll -ltra /var/run/cloudera-scm-agent/process/` and edit crontab with `crontab -e` and replace with the correct directory\n\n4. Verify that problem is resolved by running below kinit command:\n\n```bash\n[root@un2 ~]#  sudo -u hue /usr/bin/kinit -k -t /var/run/cloudera-scm-agent/process/159836-hue-KT_RENEWER/hue.keytab -c /var/run/hue/hue_krb5_ccache hue/un2.bigdata.abc.gr@BIGDATA.abc.GR\n```\n\n5. Go to hue `https://172.25.37.236:8888/hue` and run a sample command like `show databases;` to verify that you can perform querries",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1663315",
        "Description": "abc - RStudio - login failure 04/10",
        "Keywords": [
            "Rstudio",
            "license",
            "expired"
        ],
        "Owner": "u27",
        "Date": "20211005",
        "Status": "Closed",
        "Info": "abc/BigStreamer/issues/20211005-IM1663315.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\n\nSince yesterday afternoon 04/10 we cannot connect to the RStudio application receiving the message:\n\nAn error has occurred\nLicense Expired\n\nYour RStudio Connect license has expired. Please contact your Customer Success representative or email sales@rstudio.com to obtain a current license.\n\nPlease for your actions.\n```",
            "Actions Taken": "1. ssh unrstudio1\n2. Make sure the time zone is correct for the machine. (sudo timedatectl)\n3. Resync the date and time of the machine. (sudo hwclock -w)\n4. /opt/rstudio-connect/bin/license-manager deactivate\n```\nError deactivating product key: (19): Connection to the server failed. Ensure that you have a working internet connection, you've configured any required proxies, and your system's root CA certificate store is up to date; see https://rstudio.org/links/licensing_ssl for more information.\n```\n5. /opt/rstudio-connect/bin/license-manager activate <product-key>\n```\nError verify: (19): The product is activated however the license manager is currently unable to connect to the license server to verify the activation.\n\n Please ensure that you can make a connection to the activation server and then re-activate the product.\n```\n6. To pass the error from the step 4&5 then export `export http_proxy=<ip:port>` & 'export https_proxy=<ip:port>' 'export http_proxy=<ip:port>'\n7. Try again to deactivate like step 4\n8. Try again to activate like step 5 and then run the below commands:\n```\nsudo /opt/rstudio-connect/bin/license-manager status\nsudo /opt/rstudio-connect/bin/license-manager verify\n/opt/rstudio-connect/bin/license-manager verify #run without sudo\n```\n9. systemctl restart rstudio-connect # ONLY IF the Activaton Status on step 8 was `Activated`\n10. systemctl status rstudio-connect",
            "Affected Systems": "abc Bigstreamer Rstudio-Connect",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "**In case you  receive the following error while executing step 5:**\n\n```\nError activating product key: (13): The activation has expired or the system time has been tampered with. Ensure your time, timezone, and date settings are correct. If you're sure the license is not expired, try performing the following steps, in order: \n1. Fix the timezone on your system.\n2. Fix the date on your system.\n3. Fix the time on your system.\n4. Perform a system restart (important!)\n```\nYou must **reboot** your node and then repeat 1-10 steps"
        }
    },
    {
        "Issue Number": "IM1962926",
        "Description": "abc - RStudio - login failure - Ldap check",
        "Keywords": [
            "Rstudio",
            "license",
            "",
            "ldap",
            "expired"
        ],
        "Owner": "u7",
        "Date": "20210910",
        "Status": "Closed",
        "Info": "abc/BigStreamer/issues/20220909-IM1962926.md",
        "Detailed Description": {
            "Description": "`RStudio user applications not working We would like to inform you that the applications of the user kmpoletis are not running showing the following error. This morning we reset the user's password because it had expired. It can connect normally with the credentials. Also let me know that the applications of the other users (Charisis, Ploskas) are running normally.`",
            "Actions Taken": "- Ssh to **unrstudio1**\n\n- Do an ldap search using `t1-svc-cnebind` as the following in order to check if `t1-svc-cnebind` password is still valid:\n\n```\nldapsearch -H ldaps://PVDCAHR01.groupnet.gr -D \"t1-svc-cnebind\" -W -b \"dc=groupnet,dc=gr\" '(sAMAccountName=t1-svc-cnebind)'\n```\n\n-  In the above case, if you get an error that you can't connect with the ldap server, that means it has an expired `t1-svc-cnebind` password, so you will need to replace it with a new one.\n\n- Check the configuration file below for the current ldap t1-svc-cnebind password & replace it with new **(Responsible for that abc Admin)**:\n```\nssh unrstudio1\nvi /etc/rstudio-connect/rstudio-connect.gcfg\n```",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1599907",
        "Description": "As of 24/6, open_weather_map.openweathermap_final has very few entries daily. Please check the flow.",
        "Keywords": [
            "Streamsets",
            "openweather",
            "pipeline"
        ],
        "Owner": "u27",
        "Date": "20210804",
        "Status": "Closed",
        "Info": "abc/BigStreamer/issues/20210804-IM1599907.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\nas of 6/24 open_weather_map.openweathermap_final has very few entries daily. Please check the stream.\n```",
            "Actions Taken": "1. Changed on the `SFTP FTP Client bullet(Tab)` `Max Batch Size(records)` from `1000` to `100000` and `Batch Wait Time(ms)` from `1000` to `30000`\n2. On the `Hadoop-FS 1` and `Hadoop-FS 2` bullets changed the `Idle Timeout` from `${1 * MINUTES}` to `${5 * MINUTES}`",
            "Affected Systems": "abc Bigstreamer Streamsets",
            "Action Points": "Reference from devs:\nhttps://metis.xyztel.com/obss/bigdata/abc/devops/devops-projects/-/issues/58#nfgh_44105",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1829518",
        "Description": "Good evening,\nin the archive_data table of the aums schema, the data for 30/03/2022 has not been loaded\nThank you",
        "Keywords": [
            "Streamsets",
            "aums",
            "pipeline"
        ],
        "Owner": "u13",
        "Date": "20220331",
        "Status": "Closed",
        "Info": "abc/BigStreamer/issues/20220331-IM1829518.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\naums schema archive_data table has not loaded data for 03/30/2022\nThank you\n```",
            "Actions Taken": "1.  Login to un2 and change to intra user with `sudo su - intra ` command\n2.  Give the following command in order to check the wanted partition\n\n`[intra@un2 ~]$ sudo -u hdfs hdfs dfs -ls /ez/warehouse/aums.db/archive_data/par_dt=20220330`\n\nYou must be able to see the following ouput \n`ls: /ez/warehouse/aums.db/archive_data/par_dt=20220330': No such file or directory `\n\n3.  Connect to impala with `intra` user in order to refresh the table \n\n` > refresh aums.archive_data;`\n\n4.  Check if you can see the missing data with the following command from impala using `intra` user:\n\n`> show files in aums.archive_data partition (par_dt>='20220329');`\n\n\nIf not then let's check the sftp server. You will notice that files for 31/03/2022 also missing.\n\n5.  From un2: `ssh bigd@172.16.166.30/;`\n\n ` sftp> ls aums`\n\nYou must be able to see the zip files : aems_data_20220329233417.zip and aems_data_20220330233347.zip for 30/03/2022 and 31/03/2022.\n\n6. Lets try to put those file to a local directory, remove them and upload them with the following commands: \n\n`[intra@un2 data_aums]$ sftp bigd@172.16.166.30`\nConnected to 172.16.166.30.\n\nLocally transfer the file for 30/03/2022:\n\n`sftp> get aems_data_20220330233347.zip`\n\nRemove file:\n\n`sftp> rm  aems_data_20220330233347.zip `\n\nLocally transfer the file for 31/03/2022:\n\n`sftp> get aems_data_20220331233417.zip `\n\nRemove file:\n\n`sftp> rm  aems_data_20220331233417.zip `\n\nNow, let's upload those files again:\n\n`sftp> put aems_data_20220330233347.zip`\n\n`sftp> put aems_data_20220331233417.zip `\n\n7. Streamsets won't upload those files simultaneously. You will be able to see first the partition for 30/03/2022 and secondly partition for 31/03/2022.\n\nFrom impala shell with `intra` user run the following command and make sure you will be able to see the missing partitions\n\n`> show files in aums.archive_data partition (par_dt>='20220330');`\n\n8. Check logs at un2:/shared/sdc/log",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2201796",
        "Description": "missing data  aums.archive_data",
        "Keywords": [
            "Streamsets",
            "aums",
            "pipeline",
            "cfgh_bs_aums"
        ],
        "Owner": "u140",
        "Date": "20230814",
        "Status": "Closed",
        "Info": "abc/BigStreamer/issues/20230814-IM2201796.md",
        "Detailed Description": {
            "Description": "```\nThe aums.archive_metadata table is not loaded since 10/8.\nPlease for your control.\n```",
            "Actions Taken": "We followed the troubleshooting steps described in [this](../supportDocuments/applicationFlows/streamsets.md) support document.\n\n1. Logged in to Streamsets (https://172.25.37.236:18636/) with `sdc` user and credentials found [here](../../abc-devpasswd.kdbx)\n2. Checked `AUMS Metadata File Feed` pipeline status. It was running.\n3. Checked pipeline's logs. There were no logs and there were no records found.\n4. Login to un2 and then switch to user `intra`\n5. Check files on sftp server\n   ```bash\n   sftp bigd@172.16.166.30\n   cd aums\n   ls\n   ```\n   There were multiple zip files that had not been processed by the pipeline.\n6. Open an impala connection and check if there are any partitions created for the days the client had reported\n   ```\n   secimp\n   \n   select count(*) from aums.archive_metadata where par_dt = '20230811';\n   select count(*) from aums.archive_metadata where par_dt = '20230812';\n   select count(*) from aums.archive_metadata where par_dt = '20230813';\n   ```\n   The queries returned zero rows.\n7. Then we proceeded with the manual data insertion described [here](../supportDocuments/applicationFlows/streamsets.md#manually-inserting-missing-data-in-hive-and-impala). More specifically:\n   After the **put command at step 3** of the mentioned quide, continue with the following steps below(8-10).\n8. Refreshed table on impala with `refresh aums.archive_metadata` and checked if new partitions were created (impaala queries at step 6). No new partitions had been created.\n9. From Streamsets UI, stop and then start  `AUMS Metadata File Feed` pipeline. If stopping takes too long you can **Force Stop** the pipeline.\n10. Then refresh impala again and ran queries about the days that the data was missing. The queries verified that data had loaded.\n11. Since the pipepline started and the metadata were loaded:\n      - remove the <filename>_tmp.csv/zip file from the remfgh sftp directory with the sftp command rm <filename>_tmp.csv/zip. !IMPORTANT\n      - clear the local directory from the unnecessary fetched data.",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "```\n16/08/23 15:20:30 Europe/Eastern (dsimantirakis):\nThank you. The ticket can be closed.\n16/08/23 15:09:29 Europe/Eastern (linker):\nRelated Incident IM2201796 has been updated.\nGood evening,\n\nSuccessfully loaded the aums.archive_metadata table.\n\nIf you don't need anything else, let us know to proceed with closing the ticket.\n\nThanks\n16/08/23 15:09:29 Europe/Eastern (linker):\nRelated Incident IM2201796 has been updated.\nIncident Status Change to Pending Customer from Work In Progress\n14/08/23 16:53:50 Europe/Eastern (linker):\nRelated Incident IM2201796 has been updated.\nGood evening,\n\nWe will get back to you when the investigation is complete.\n\nThank you\n```",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "- https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/supportDocuments/applicationFlows/streamsets.md\n- https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/abc/BigStreamer/issues/20220331-IM1829518.md",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "GI9",
        "Description": "abc Bigstreamer Geolocation mail for Spark job failure",
        "Keywords": [
            "geolocation",
            "spark",
            "failure",
            "phase cron"
        ],
        "Owner": "u1",
        "Date": "202010118",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20201118-GI9.md",
        "Detailed Description": {
            "Description": "```\nThe following mail arrived:\n\nGeolocation ALERT:[WARN] - Phase #4b, Spark job GeoViavi-LTE failed (1605552391)\n\nS550.Geo_Tech_Load_Data.sh\n2020-11-16 20:54:05 --> Phase #4\nSpark job GeoViavi-LTE failed with return_code 1.\nApplicationID:application_1599948124043_370289.\nLoadID:1605552391.\nStart time of job :1605552487.\n\nThis is an automated e-mail.\nPlease do not reply.\n```",
            "Actions Taken": "The [full documentation](http://10.124.161.38/trac/hadoop/wiki/dev/project/abc/geolocation_viavi) provided by the developers' team\n\nFollowed the `Failure Handling Manual Mechanism` section of the guide above\n\nIf there are other dates in the `/ez/warehouse/geolocation.db/geo_<technology>_fail/` HDFS folder that are older than two days, they can be deleted. Retention for the geolocation tables is 2 days for the eponymus and 1 day for the anonymous table.",
            "Affected Systems": "abc Bigstreamer Geolocation",
            "Action Points": "None, failures of that kind are rare and not worth the extra effort.",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2178255",
        "Description": "CDSW unavailability 29062023",
        "Keywords": [
            "cdsw"
        ],
        "Owner": "u7",
        "Date": "20230629",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230629-IM2178255.md",
        "Detailed Description": {
            "Description": "The following issue has been assigned to our team with subject:\n\n```\nGood evening,\nCDSW is not available. From the Cloudera manager we see the following.\nPods not ready in cluster default ['role/ds-reconciler', 'role/tcp-ingress-controller', 'role/web']. * Web is not up yet.\nThe nodes are all started and in good health.\nPlease for your actions.\nThanks\n\n```",
            "Actions Taken": "After investigation we found out that in the time the issue has been assigned to our team **the resources of the CDSW hosts(mncdsw1,wrkcdsw1,wrkcdsw2,wrkcdsw3,wrkcdsw4,wrkcdsw5,wrkcdsw6) were maxed out** as we can see in the following pictures. The results of the following screenshot has been aggregated. \nAlso, another step is the check @ mncdsw1 the cdsw status:\n`cdsw status` and the health of the pods: `kubectl get pods -A` . \nAt the time that we check the above mentioned status, all services and pods were healthy. \n\n![cfgh_cdsw_host_memory](.media/cfgh_cdsw.JPG)\n![cfgh_cdsw_host_memory](.media/cfgh_cdsw_host_memory.JPG)\n![cfgh_cdsw_host_load](.media/cfgh_cdsw_host_load.JPG)\n\nDue to increased load in the above mentioned time frame the service was down",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1476278",
        "Description": "abc - IM1476278 - CDSW Not enough CPU/GPU/Memory",
        "Keywords": [
            "cdsw",
            "pods",
            "scheduled",
            "jobs"
        ],
        "Owner": "u27",
        "Date": "20210317",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20210317-IM1476278.md",
        "Detailed Description": {
            "Description": "```\nGood morning, \n\nFor 2 days now all jobs in CDSW fail (stay in status schedule) with the message\n\"Unschedulable: No host in the cluster currently has enough CPU, memory and GPU to run the engine\"\n```",
            "Actions Taken": "1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'\n2. Go to last tab(admin).\n3. Select `Activity` tab.\n4. Check `CPU` and `Memory` graphs.\n5. In our case all scheduled jobs stucked at `Scheduling` due to a job that ran `800 times at 14/03 07:00`\n \n- Delete all PENDING pods to free resources\n\n```bash\n[root@mncdsw1 ~]# kubectl get pods\n[root@mncdsw1 ~]# kubectl get pods | grep Pending | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n[root@mncdsw1 ~]# kubectl get pods | grep \"Init:0/1\" | awk '{print $1}' | xargs kubectl delete pod --grace-period=0 --force\n[root@mncdsw1 ~]# kubectl get pods\n```",
            "Affected Systems": "abc Bigstreamer CDSW",
            "Action Points": "If the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a place can be found. However, it will not be killed for excessive CPU usage.\n\nAs an action point we could `monitoring compute & memory resource usage` from the Pod status via our monitoring tool.",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1900072",
        "Description": "abc - BigStreamer - IM1900072 - Execution schedule job",
        "Keywords": [
            "cdsw"
        ],
        "Owner": "u1",
        "Date": "20220629",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20220629-IM1900072.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\nIt was observed that there is a problem with the jobs in cloudera data science, specifically the Energy Bills project and the Set_Point_Automation job at the time and date that it is scheduled to run, it was observed that it starts and runs 2 times at the same time.\nThis is easy to understand from the job history and the cases are the ones I have attached with a screenshot.\nThank you and for any clarification I am at your disposal\n```",
            "Actions Taken": "1. Login to CDSW on https://mncdsw1.bigdata.abc.gr with personal account.\n2. From the Admin tab select usage.\n3. Confirm duplicate submitted jobs.\n4. SSH to `mncdsw1.bigdata.abc.gr` with personal account and change to `root` with sudo\n5. Find the scheduler pod\n``` bash\nkubectl get pods\nNAME                                          READY   STATUS             RESTARTS   AGE\narchiver-7c6656cf45-nklb2                     1/1     Running            0          159d\ncdsw-compute-pod-evaluator-849b98f9fd-rvg58   1/1     Running            0          159d\ncron-7d5f5656c7-ws77j                         1/1     Running            0          159d                 <---- This looks good\ndb-84f767b84c-tnr2j                           1/1     Running            0          159d\ndb-migrate-f260849-x6f9z                      0/1     Completed          0          159d\nds-cdh-client-6bd7476b5b-r268f                1/1     Running            0          159d\nds-operator-576c7459d6-wr4hc                  1/1     Running            1          159d\nds-reconciler-5cd476896d-8lnn7                1/1     Running            1          159d\nds-vfs-7f6578594b-dgnrx                       1/1     Running            0          159d\nfeature-flags-54f7f97948-zd4gw                1/1     Running            0          159d\ngrafana-cml-dashboards-f260849-4v7k4          0/1     Completed          0          159d\ngrafana-core-cd44d8dff-b2hhg                  1/1     Running            0          159d\nimage-puller-5cxg2                            1/1     Running            1          159d\nimage-puller-5khss                            1/1     Running            31         159d\nimage-puller-cgbls                            1/1     Running            35         159d\nimage-puller-f8876                            1/1     Running            34         159d\nimage-puller-vjkcp                            1/1     Running            40         159d\nimage-puller-vs6b7                            1/1     Running            38         159d\nimage-puller-w8wc2                            1/1     Running            1          159d\ningress-controller-78fc7d87b8-jntf8           1/1     Running            0          159d\nkube-state-metrics-656687dd48-zh66p           1/1     Running            0          159d\nlivelog-85fb8d8974-fnwkr                      1/1     Running            0          159d\nlivelog-cleaner-1656288000-gjfjw              0/1     Completed          0          2d11h\nlivelog-cleaner-1656374400-5pqd7              0/1     Completed          0          35h\nlivelog-cleaner-1656460800-vnjfw              0/1     Completed          0          11h\nlivelog-publisher-5rhbv                       1/1     Running            39         159d\nlivelog-publisher-f68qs                       1/1     Running            37         159d\nlivelog-publisher-j9p22                       1/1     Running            1          159d\nlivelog-publisher-rp4pp                       1/1     Running            39         159d\nlivelog-publisher-rv6h5                       1/1     Running            3          159d\nlivelog-publisher-wb6cn                       1/1     Running            43         159d\nlivelog-publisher-xc2wm                       1/1     Running            40         159d\nmodel-proxy-69867f6ff6-ljcdv                  1/1     Running            1          159d\nprometheus-core-686874bbbc-nzn9p              0/1     CrashLoopBackOff   35926      159d                 <---- This looks not good\nprometheus-node-exporter-d6n5v                1/1     Running            0          159d\nprometheus-node-exporter-flhq7                1/1     Running            23         159d\nprometheus-node-exporter-gxh2h                1/1     Running            0          159d\nprometheus-node-exporter-kvvjv                1/1     Running            24         159d\nprometheus-node-exporter-n47w5                1/1     Running            23         159d\nprometheus-node-exporter-sxtxp                1/1     Running            23         159d\nprometheus-node-exporter-wb4lf                1/1     Running            23         159d\nruntime-repo-puller-74f488b875-dj8f8          1/1     Running            0          159d\ns2i-builder-775cc65845-28k88                  1/1     Running            0          159d\ns2i-builder-775cc65845-qww92                  1/1     Running            0          159d\ns2i-builder-775cc65845-t8rp6                  1/1     Running            0          159d\ns2i-client-7979d87646-skh8m                   1/1     Running            0          159d\ns2i-git-server-5b6c4c4df9-8jczc               1/1     Running            0          159d\ns2i-queue-65cc5dd86b-6sckk                    1/1     Running            0          159d\ns2i-registry-75565bc6d4-zls79                 1/1     Running            0          159d\ns2i-registry-auth-58c4b8ddb-lgbf5             1/1     Running            0          159d\ns2i-server-6549bc9f86-zbxl9                   1/1     Running            1          159d\nsecret-generator-76994558c6-fl8sn             1/1     Running            0          159d\nspark-port-forwarder-29gfq                    1/1     Running            0          159d\nspark-port-forwarder-5w9hr                    1/1     Running            0          159d\nspark-port-forwarder-jss7r                    1/1     Running            23         159d\nspark-port-forwarder-kpkrh                    1/1     Running            23         159d\nspark-port-forwarder-r2lrj                    1/1     Running            23         159d\nspark-port-forwarder-tm757                    1/1     Running            23         159d\nspark-port-forwarder-zkb2h                    1/1     Running            24         159d\ntcp-ingress-controller-647b484f4c-fl6tr       1/1     Running            1          159d\nusage-reporter-d46bcdb59-cswll                1/1     Running            0          159d\nweb-6c75f94ff4-k2z7m                          1/1     Running            8          159d\nweb-6c75f94ff4-vfb2h                          1/1     Running            9          159d\nweb-6c75f94ff4-vl4p4                          1/1     Running            8          159d\n```\n6. Confirm that jobs were submitted by the CDSW scheduler\n``` bash\nkubectl logs cron-7d5f5656c7-ws77j | grep job=624 \n# Job ID can be obtained from the URL when inspecting the job from the Web GUI\n\n2022-06-29 06:00:00.001\t1\tINFO   \tCron                          \tStart  submitting cron job\tdata = {\"jobId\":624,\"spec\":\"0 0 9 * * *\",\"timezone\":\"Europe/Athens\",\"url\":\"http://web.default.svc.cluster.local/api/v1/tasks/start-job?job=624\"}\n2022-06-29 06:00:00.001\t1\tINFO   \tCron                          \tStart  submitting cron job\tdata = {\"jobId\":624,\"spec\":\"0 0 9 * * *\",\"timezone\":\"Europe/Athens\",\"url\":\"http://web.default.svc.cluster.local/api/v1/tasks/start-job?job=624\"}\n```\n8. Find out how scheduling works\n``` bash\nkubectl describe pod cron-7d5f5656c7-ws77j \nName:         cron-7d5f5656c7-ws77j\nNamespace:    default\nPriority:     0\nNode:         mncdsw1.bigdata.abc.gr/10.255.241.130\nStart Time:   Fri, 21 Jan 2022 02:24:28 +0200\nLabels:       app=cron\n              hash=f260849\n              pod-template-hash=7d5f5656c7\n              role=cron\n              version=f260849\nAnnotations:  <none>\nStatus:       Running\nIP:           100.66.0.9\nIPs:\n  IP:           100.66.0.9\nControlled By:  ReplicaSet/cron-7d5f5656c7\nContainers:\n  cron:\n    Container ID:   docker://c7f4ef220646d428b24a6f3fbc2460605997509ee3db4e2abea472e165b85178\n    Image:          docker-registry.infra.cloudera.com/cdsw/cron:f260849\n    Image ID:       docker://sha256:c18d89235586e85434bd1fd3878317926d337c27c0e59ab360bed04f33c9c904\n    Port:           <none>\n    Host Port:      <none>\n    State:          Running\n      Started:      Fri, 21 Jan 2022 02:28:34 +0200\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      memory:  100Mi\n    Requests:\n      cpu:     50m\n      memory:  100Mi\n    Environment:\n      POSTGRESQL_USER:         <set to the key 'postgresql.user' in secret 'internal-secrets'>  Optional: false\n      POSTGRESQL_PASS:         <set to the key 'postgresql.pass' in secret 'internal-secrets'>  Optional: false\n      POSTGRESQL_DB:           <set to the key 'postgresql.db' in secret 'internal-secrets'>    Optional: false\n      ZONEINFO:                /zoneinfo.zip\n      WEB_IP:                  web.default.svc.cluster.local\n      DB_IP:                   db.default.svc.cluster.local                                                                    <---- This looks good\n      SERVICE_ACCOUNT_SECRET:  <set to the key 'service.account.secret' in secret 'internal-secrets'>  Optional: false\n      DOMAIN:                  mncdsw1.bigdata.abc.gr\n      LOG_LEVEL:               INFO\n    Mounts:                    <none>\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:            <none>\nQoS Class:          Burstable\nNode-Selectors:     <none>\nTolerations:        node.kubernetes.io/not-ready:NoExecute for 300s\n                    node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:             <none>\n```\n7. Find the database pod\n```bash\nkubectl get svc -o wide\nNAME                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                         AGE    SELECTOR\narchiver                     ClusterIP   100.77.53.223    <none>        4444/TCP                                        159d   role=archiver\ncdsw-compute-pod-evaluator   ClusterIP   100.77.186.84    <none>        443/TCP                                         159d   app.kubernetes.io/instance=cdsw-compute,app.kubernetes.io/name=pod-evaluator\ndb                           ClusterIP   100.77.236.38    <none>        5432/TCP                                        159d   role=db       <---- This looks good         \nds-cdh-client                ClusterIP   100.77.254.36    <none>        80/TCP                                          159d   role=ds-cdh-client\nds-operator                  ClusterIP   100.77.155.3     <none>        80/TCP                                          159d   role=ds-operator\nds-vfs                       ClusterIP   100.77.33.62     <none>        80/TCP                                          159d   role=ds-vfs\nfeature-flags                ClusterIP   100.77.113.165   <none>        80/TCP                                          159d   role=feature-flags\ngrafana                      ClusterIP   100.77.6.156     <none>        3000/TCP                                        159d   app=grafana,component=core\nkube-state-metrics           ClusterIP   100.77.234.114   <none>        8080/TCP                                        159d   app=kube-state-metrics\nkubernetes                   ClusterIP   100.77.0.1       <none>        443/TCP                                         159d   <none>\nlivelog                      ClusterIP   100.77.177.53    <none>        80/TCP                                          159d   app=livelog\nmodel-proxy                  ClusterIP   100.77.170.230   <none>        80/TCP                                          159d   role=model-proxy\nprometheus                   ClusterIP   100.77.188.189   <none>        9090/TCP                                        159d   app=prometheus,component=core\nprometheus-node-exporter     ClusterIP   None             <none>        9100/TCP                                        159d   app=prometheus,component=node-exporter\nruntime-repo-puller          ClusterIP   100.77.192.194   <none>        3000/TCP                                        159d   role=runtime-repo-puller\ns2i-builder                  ClusterIP   100.77.191.97    <none>        5051/TCP                                        159d   role=s2i-builder\ns2i-client                   ClusterIP   100.77.185.37    <none>        5051/TCP                                        159d   role=s2i-client\ns2i-git-server               ClusterIP   100.77.212.112   <none>        80/TCP                                          159d   role=s2i-git-server\ns2i-queue                    ClusterIP   100.77.175.149   <none>        5672/TCP                                        159d   role=s2i-queue\ns2i-registry                 ClusterIP   100.77.0.134     <none>        5000/TCP                                        159d   role=s2i-registry\ns2i-registry-auth            ClusterIP   100.77.0.139     <none>        5001/TCP                                        159d   role=s2i-registry-auth\ns2i-server                   ClusterIP   100.77.232.217   <none>        5051/TCP                                        159d   role=s2i-server\ntcp-ingress-controller       ClusterIP   100.77.133.250   <none>        80/TCP                                          159d   role=tcp-ingress-controller\nusage-reporter               ClusterIP   100.77.187.214   <none>        3000/TCP                                        159d   role=usage-reporter\nweb                          ClusterIP   100.77.204.152   <none>        80/TCP,9229/TCP,35729/TCP,20050/TCP,20051/TCP   159d   role=web\n\nkubectl get pods -l role=db\nNAME                  READY   STATUS    RESTARTS   AGE\ndb-84f767b84c-tnr2j   1/1     Running   0          159d\n```\n8. Connect to the database\n```\n kubectl exec -it db-84f767b84c-tnr2j bash\nbash-4.4$ psql\npsql (12.1)\nType \"help\" for help.\n\npostgres=# \\l\n```\n\n| Name      | Owner    | Encoding | Collate    | Ctype      | Access privileges      |\n| --------- | -------- | -------- | ---------- | ---------- | ---------------------- |\n| postgres  | postgres | UTF8     | en_US.utf8 | en_US.utf8 |                        |\n| sense     | postgres | UTF8     | en_US.utf8 | en_US.utf8 |                        |\n| template0 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          + |\n|           |          |          |            |            | postgres=CTc/postgres  |\n| template1 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          + |\n|           |          |          |            |            | postgres=CTc/postgres  |\n\n9.  Database sense is the only viable candidate\n```sql\npostgres=# \\c sense\nYou are now connected to database \"sense\" as user \"postgres\".\nsense=# select * from pg_tables;\n```\n\n| schemaname    | tablename               | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity |\n| ------------- | ----------------------- | ---------- | ---------- | ---------- | -------- | ----------- | ----------- |\n| feature_flags | client_applications     | sense      |            | t          | f        | f           | f           |\n| feature_flags | migrations              | sense      |            | t          | f        | f           | f           |\n| feature_flags | client_instances        | sense      |            | f          | f        | f           | f           |\n| feature_flags | client_metrics          | sense      |            | t          | f        | f           | f           |\n| feature_flags | events                  | sense      |            | t          | f        | f           | f           |\n| feature_flags | features                | sense      |            | t          | f        | f           | f           |\n| pg_catalog    | pg_statistic            | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_type                 | postgres   |            | t          | f        | f           | f           |\n| feature_flags | strategies              | sense      |            | t          | f        | f           | f           |\n| public        | batch_runs              | sense      |            | t          | f        | t           | f           |\n| public        | authorized_keys         | sense      |            | t          | f        | t           | f           |\n| public        | build_details           | sense      |            | t          | f        | t           | f           |\n| public        | applications            | sense      |            | t          | f        | t           | f           |\n| public        | access_keys             | sense      |            | t          | f        | t           | f           |\n| public        | dashboard_pods          | sense      |            | t          | f        | t           | f           |\n| public        | crons                   | sense      |            | t          | f        | f           | f           |\n| pg_catalog    | pg_foreign_server       | postgres   |            | t          | f        | f           | f           |\n| public        | clusters                | sense      |            | t          | f        | t           | f           |\n| public        | custom_quota            | sense      |            | t          | f        | f           | f           |\n| pg_catalog    | pg_authid               | postgres   | pg_global  | t          | f        | f           | f           |\n| public        | engine_images           | sense      |            | t          | f        | t           | f           |\n| public        | dashboards_usage        | sense      |            | t          | f        | f           | f           |\n| public        | default_quota           | sense      |            | t          | f        | f           | f           |\n| public        | engine_images_editors   | sense      |            | t          | f        | t           | f           |\n| pg_catalog    | pg_statistic_ext_data   | postgres   |            | t          | f        | f           | f           |\n| public        | engine_sizes            | sense      |            | t          | f        | f           | f           |\n| public        | followers               | sense      |            | t          | f        | t           | f           |\n| public        | flow_uploads            | sense      |            | t          | f        | t           | f           |\n| public        | licenses                | sense      |            | t          | f        | t           | f           |\n| public        | invitations             | sense      |            | t          | f        | t           | f           |\n| public        | kerberos                | sense      |            | t          | f        | t           | f           |\n| public        | jobs                    | sense      |            | t          | f        | t           | f           |\n| public        | migrations              | sense      |            | t          | f        | f           | f           |\n| public        | flow_upload_chunks      | sense      |            | t          | f        | t           | f           |\n| public        | engine_statuses         | sense      |            | t          | f        | t           | f           |\n| public        | job_notifications       | sense      |            | t          | f        | t           | f           |\n| public        | model_builds            | sense      |            | t          | f        | t           | f           |\n| pg_catalog    | pg_user_mapping         | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_subscription         | postgres   | pg_global  | t          | f        | f           | f           |\n| public        | models                  | sense      |            | t          | f        | t           | f           |\n| pg_catalog    | pg_attribute            | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_proc                 | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_class                | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_attrdef              | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_constraint           | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_inherits             | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_index                | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_operator             | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_opfamily             | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_opclass              | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_am                   | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_amop                 | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_amproc               | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_language             | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_largeobject_metadata | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_aggregate            | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_largeobject          | postgres   |            | t          | f        | f           | f           |\n| public        | model_deployments       | sense      |            | t          | f        | t           | f           |\n| pg_catalog    | pg_statistic_ext        | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_rewrite              | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_trigger              | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_event_trigger        | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_description          | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_cast                 | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_enum                 | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_namespace            | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_conversion           | postgres   |            | t          | f        | f           | f           |\n| pg_catalog    | pg_depend               | postgres   |            | t          | f        | f           | f           |\n\n10. Insepect the public.projects table \n```sql\nsense=# select * from public.projects limit 1;\n```\n| id  | user_id | parent_id | name     | slug     | description | repository | created_at                 | updated_at                 | environment | organization_permission | project_visibility | creator_id | shared_memory_limit | size | crn                                   | default_project_engine_type | creation_status | creation_error_message |\n| --- | ------- | --------- | -------- | -------- | ----------- | ---------- | -------------------------- | -------------------------- | ----------- | ----------------------- | ------------------ | ---------- | ------------------- | ---- | ------------------------------------- | --------------------------- | --------------- | ---------------------- |\n| 311 | 31      |           | testRpac | testrpac |             |            | 2021-09-17 07:37:01.476738 | 2021-09-20 14:37:37.470235 |             |                         | private            | 31         |                     |      | /5c173548-19ed-4a05-8546-e99fdf4fc35f | legacy_engine               |                 |                        |\n11. Insepect the public.jobs table \n```sql\nsense=# select * from public.jobs limit 1;\n```\n| id  | project_id | creator_id | name                    | description | script                                                                                               | schedule | parent_id | timeout | timeout_kill | paused | created_at                 | updated_at                 | type   | original_parent_id | original_id | environment | timezone        | success_recipients | failure_recipients | timeout_recipients | include_logs | report_attachments | send_from_creator | share_token | cluster_id | director_instance_type | director_job_type | director_worker_count | reply_to | stopped_recipients | memory | cpu | engine_image_id | kernel  | nvidia_gpu | shared_view_visibility | arguments | runtime_id |\n| --- | ---------- | ---------- | ----------------------- | ----------- | ---------------------------------------------------------------------------------------------------- | -------- | --------- | ------- | ------------ | ------ | -------------------------- | -------------------------- | ------ | ------------------ | ----------- | ----------- | --------------- | ------------------ | ------------------ | ------------------ | ------------ | ------------------ | ----------------- | ----------- | ---------- | ---------------------- | ----------------- | --------------------- | -------- | ------------------ | ------ | --- | --------------- | ------- | ---------- | ---------------------- | --------- | ---------- |\n| 28  | 91         | 19         | training_until_20180117 |             | Aris_tmp_dev/20180209_Training in all measurements from Dec to 17 Jan, Scoring on 18 Jan/Training.py |          |           |         | f            | f      | 2018-02-09 17:28:05.412371 | 2018-02-09 17:28:05.412371 | manual |                    |             | {}          | Europe/Helsinki |                    |                    |                    | t            |                    | f                 |             |            |                        |                   |                       |          |                    | 8      | 4   | 2               | python2 | 0          | private                |           |\n12. Inspect the public.cron\n```sql\nsense=# select * from public.crons limit 1;\n--- The deleted column seems to be a boolean\n```\n\n| id  | schedule      | url                     | description | timezone        | deleted | processed | job_id |\n| --- | ------------- | ----------------------- | ----------- | --------------- | ------- | --------- | ------ |\n| 85  | 0 0 20 10 * * | /tasks/start-job?job=35 |             | Europe/Helsinki | t       | f         | 35     |\n\n13. Check for duplicate active jobs\n```sql\nsense=# select job_id from public.crons where deleted='f' group by job_id having count(job_id) > 1;\n```\n\n| job_id |\n| ------ |\n| 617    |\n| 514    |\n| 450    |\n| 451    |\n| 504    |\n| 529    |\n| 575    |\n| 543    |\n| 456    |\n| 574    |\n| 67     |\n| 571    |\n| 443    |\n| 598    |\n| 523    |\n| 459    |\n| 463    |\n| 544    |\n| 586    |\n| 572    |\n| 606    |\n| 618    |\n| 442    |\n| 614    |\n| 429    |\n| 619    |\n| 628    |\n\n14. Find which jobs correspond to the IDs using the tables above\n```sql\nsense=# select b.name as project,\n    a.name as job\nfrom public.jobs a\n    join public.projects b on a.project_id = b.id\nwhere a.id in (\n        select job_id\n        from public.crons\n        where deleted = 'f'\n        group by job_id\n        having count(job_id) > 1\n    );\n```\n\n| project                           | job                                       |\n| --------------------------------- | ----------------------------------------- |\n| Machine_Learning                  | NetworkInventory_update                   |\n| Machine_Learning                  | Predictive_Dslam_Actor_ Siebel_Check      |\n| Machine_Learning                  | Predictive_BNG_Cisco_Alcatel              |\n| Machine_Learning                  | Predictive_Maintenance_Ericsson_VSWR      |\n| Machine_Learning                  | Critical_Huawei_Dslam                     |\n| Energy Bills                      | Pollaploi                                 |\n| Hardware_Failures                 | Burned FXs                                |\n| Energy Bills                      | Truncate & Refresh BTS Live Measurements  |\n| Mini_Events_&_NPA                 | NPA                                       |\n| Energy Efficiency                 | Element_buildings_actions_second_phase    |\n| Energy Efficiency                 | Set_Point_Automation                      |\n| CNEA                              | NeighbourAlgorithm_Start                  |\n| Cloudera_Training                 | Monthy dataset                            |\n| Power_Outage_Detector_Load_To_QPM | Power Detector Load to QPM 1              |\n| Power_Outage_Detector_Load_To_QPM | Power Detector Load to QPM 2              |\n| Power_Outage_Detector_Load_To_QPM | Power Detector Load to QPM 4              |\n| GeoFencing                        | GeoFencing                                |\n| Machine_Learning                  | Report_Penitas                            |\n| Aumms                             | air_condition_Setpoints_to_HUE            |\n| Mini_Events_&_NPA                 | Mini_Events                               |\n| Energy Bills                      | features_export                           |\n| Energy Bills                      | refresh_11                                |\n| Monitoring Flows                  | Flows_update_all_counters_15:00_no_par_dt |\n| Monitoring Flows                  | Flows_update_all_counters_07:00           |\n| Monitoring Flows                  | Flows_update_all_counters_12:00_no_par_dt |\n| Monitoring Flows                  | Flows_update_all_counters_07:00_no_par_dt |\n| Sample Project                    | test                                      |\n\n15. Reproduce the problem\n\nThe `test` job on `Sample Project` was created for testing how duplicate jobs are submitted. It turns out that if you change the job scheduling and you can manage to click the `Update` job button more than one times, it creates more than one job schedules. See the answer below:\n\n```text\nWe were able to reproduce the problem. It seems that when updating the job's execution time, the update button has been pressed twice, resulting in duplicate entries being created in the database that holds the schedules.\n\nTo fix the problem, please update the schedules by pressing the update job only once, even if it seems that no action is being performed. Also make sure you are using one of the app's supported browsers https://docs.cloudera.com/cdsw/1.9.2/requirements/topics/cdsw-supported-browsers.html.\n```",
            "Affected Systems": "abc Bigstreamer CDSW",
            "Action Points": "N/A",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1923742",
        "Description": "abc - BigStreamer - IM1923742 - Job's problem",
        "Keywords": [
            "cdsw"
        ],
        "Owner": "u1",
        "Date": "20220722",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20220722-IM1923742.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\nit has been observed that jobs show the problem Engine exited with status 34.\nsome of them are:\n• Set_Point_Automation job in the Set Point Automation project (error today 22/7)\n• Cabins Live Measurements job in the Energy Bills project (error yesterday 21/7)\n• Flows_update_all_counters_12:00_no_par_dt job in the Monitoring Flows project (error yesterday 7/15)\n```",
            "Actions Taken": "1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'\n2. Go to last tab(admin).\n3. Select `Activity` tab.\n4. Inspect the Jobs in question.\n\nThe jobs are in `FAILED` status. The logs for the failed applications are missing.\n \n5. Troubleshoot from the command line:\n\nFrom `mncdsw1` as root (use personal account and then sudo):\n\n```bash\nkubectl get pods -w -A # Wait a pod to fail (namespace should be like default-user-XXX)\n\n# After a while, a pod has failed, describe it\n\nkubectl describe pod -n default-user-XXX XXXXXXXX\n```\n\n``` logs\nEvents\nWarning  FailedCreatePodSandBox  10s                    kubelet, wrkcdsw4.bigdata.abc.gr  Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"...\" network for pod \"XXXXXXXX\": networkPlugin cni failed to set up pod \"XXXXXXXX_default\" network: unable to allocate IP address: Post http://127.0.0.1:6784/ip/....: dial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container \"....\" network for pod \"XXXXXXXX\": networkPlugin cni failed to teardown pod \"XXXXXXXX_default\" network: Delete http://127.0.0.1:6784/ip/....: dial tcp 127.0.0.1:6784: connect: connection refused]\n```\n\nThis error points us to the CNI plugin\n\nCheck the logs for the weave pods\n\n``` bash\nkubectl logs -n kube-system weave-net-XXXXX\n# Weave pod in wrkcdsw4 has stopped logging events\n```\n\nThe pod was not responding and could not be deleted.\n\n7. Restart the docker daemon to restart all containers on `wrkcdsw4`\n\n_At the time of the issue, CDSW had stale configuration that required full restart (outage) which was not desirable_\n\nTo avoid applying the settings, restart the service with the same configuration by triggering a restart by `supervisord` deployed as part of the Cloudera agent\n\n<details> ![Danger ahead](https://media3.giphy.com/media/vvzMdSygQejBIejeRO/200w.gif?cid=6c09b952aacsm9yssw6k6q0z5v8ejuy82rjpvw6qdhglcwpu&rid=200w.gif&ct=g) </details>\n\nFrom wrkcdsw4 as root (use personal account and then sudo):\n\n``` bash\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf status | grep DOCKER\n# Sample\n# 145071-cdsw-CDSW_DOCKER          RUNNING   pid 39353, uptime 29 days, 0:40:20\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart 145071-cdsw-CDSW_DOCKER\n```\n\n8. Check that the node is operational after the restart\n\nFrom `mncdsw1` as root (use personal account and then sudo):\n\n```bash\ncdsw status # You might have to wait a few minutes\n```\n\n9. Inform the customer about the problem\n\n``` text\nA component of CDSW on worker node 4 encountered a problem resulting in jobs running on that node not being able to start. The function of the component has been restored and the jobs are now running normally.\n\nPlease let us know if you need anything else or if we can proceed to close the ticket.\n```",
            "Affected Systems": "abc Bigstreamer CDSW",
            "Action Points": "",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2073052",
        "Description": "Application not working",
        "Keywords": [
            "cdsw"
        ],
        "Owner": "u77",
        "Date": "20230130",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230130-IM2073052.md",
        "Detailed Description": {
            "Description": "```bash\nGood morning,\n\nIn CDSW we get error \"Unexpected Error. An unexpected error occurred\" when connecting. We saw that the node mncdsw1.bigdata.abc.gr was down. We did a restrart, just that, and it now appears to be in good health status.\n\nHowever, we still get the same error.\n\nIn CDSW status it has the following message\n\nFailed to run CDSW Nodes Check. * Failed to run CDSW system pods check. * Failed to run CDSW application pods check. * Failed to run CDSW services check. * Failed to run CDSW secrets check. * Failed to run CDSW persistent volumes check. * Failed to run...\n\nPlease for your checks.\n\nThanks\n```",
            "Actions Taken": "1. Restart CDSW\n\n   The customer had already restarted CDSW, so we tried it once more in order to live monitor it.\n\n   ```bash\n   Cloudera Manager -> CDSW -> Restart\n   ```\n\n2. Check status\n\n   We followed the logs until CDSW was available again.\n\n   ```bash\n   #from mncdsw1\n   cdsw status\n   ...\n   Cloudera Data Science Workbench is ready!\n   ```\n\n   Since CDSW was up and running, we continued with root cause analysis.\n\n3. Check logs\n\n   ```bash\n   less /var/log/cdsw/cdsw_health.log\n   ```\n\n   Firstly, we noticed an abnormal behavior with some of the control plane pods:\n\n   ```bash\n   2023-01-29 05:50:53,868 ERROR cdsw.status:Pods not ready in cluster kube-system ['component/kube-controller-manager', 'component/kube-scheduler'].\n   ```\n\n   And after that, CDSW lost connection with apiserver pod completely:\n\n   ```bash\n   2023-01-29 05:51:42,392 WARNING urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549bb50>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\n   2023-01-29 05:51:42,735 WARNING urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b710>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\n   2023-01-29 05:51:43,065 WARNING urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b050>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes\n   2023-01-29 05:51:43,371 ERROR cdsw.status:Failed to run CDSW Nodes Check.\n   ```\n\n4. Check node resources\n\n   From Cloudera Manager we saw that CPU and Memory were not increased but Disk I/O reached 100%.\n\n   ![IM2073052_diskio](.media/IM2073052_diskio.png)\n\n   From the image above we noticed that the issue occured on dm-7.\n\n   ```bash\n   [root@mncdsw1 ~]# ll /dev/mapper/cdsw-var_lib_cdsw\n   lrwxrwxrwx 1 root root 7 Dec 16  2021 /dev/mapper/cdsw-var_lib_cdsw -> ../dm-7\n   ```\n\n   ```bash\n   [root@mncdsw1 ~]# lsblk | grep cdsw-var\n   └─cdsw-var_lib_cdsw                                                                         253:7    0   931G  0 lvm  /var/lib/cdsw\n   ```\n\n   ```bash\n   [root@mncdsw1 ~]# kubectl get pv\n   NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS                 REASON   AGE\n   0c9df8bb         1Ti        RWX            Retain           Bound    default-user-120/b128af5f   cdsw-storageclass-whiteout            83m\n   1214923b         1Ti        RWX            Retain           Bound    default-user-98/1ec1e99a    cdsw-storageclass-whiteout            11m\n   1297834a         1Ti        RWX            Retain           Bound    default-user-9/740094c3     cdsw-storageclass-whiteout            54s\n   1a2f7a8a         1Ti        RWX            Retain           Bound    default-user-9/92acb87f     cdsw-storageclass-whiteout            55s\n   1f498fe8         1Ti        RWX            Retain           Bound    default-user-120/588500de   cdsw-storageclass-whiteout            106s \n   ```\n\n   ```bash\n   [root@mncdsw1 ~]# kubectl get pv 0c9df8bb -o yaml\n\n   apiVersion: v1\n   kind: PersistentVolume\n   metadata:\n     name: 0c9df8bb\n   spec:\n     accessModes:\n     - ReadWriteMany\n     capacity:\n       storage: 1Ti\n     mountOptions:\n     - nfsvers=4.1\n     nfs:\n       path: /var/lib/cdsw/current/projects/cdn/4xsyzsv0lnij00ob\n       server: 10.255.241.130\n   persistentVolumeReclaimPolicy: Retain\n     storageClassName: cdsw-storageclass-whiteout\n     volumeMode: Filesystem \n   ```\n\n   It seems that every CDSW project uses mncdsw1:/var/lib/cdsw for storage.\n\n5. Check kubelet logs\n\n   ```bash\n   ll /run/cloudera-scm-agent/process/ | grep -i master\n   ```\n\n   ```bash\n   [root@mncdsw1 ~]# ll /run/cloudera-scm-agent/process/145081-cdsw-CDSW_MASTER/logs/\n   total 111880\n   -rw-r--r-- 1 root root  9658036 Jan 30 10:24 stderr.log\n   -rw-r--r-- 1 root root 10485841 Jan 30 05:42 stderr.log.1\n   -rw-r--r-- 1 root root 10485989 Jan  4 19:40 stderr.log.10\n   -rw-r--r-- 1 root root 10485928 Jan 30 00:20 stderr.log.2\n   -rw-r--r-- 1 root root 10486166 Jan 29 18:58 stderr.log.3\n   -rw-r--r-- 1 root root 10485841 Jan 29 13:36 stderr.log.4\n   -rw-r--r-- 1 root root 10485790 Jan 29 08:06 stderr.log.5\n   -rw-r--r-- 1 root root 10485858 Jan 25 16:41 stderr.log.6\n   -rw-r--r-- 1 root root 10485835 Jan 21 08:56 stderr.log.7\n   -rw-r--r-- 1 root root 10485760 Jan 15 14:47 stderr.log.8\n   -rw-r--r-- 1 root root 10485805 Jan 10 11:57 stderr.log.9\n   -rw-r--r-- 1 root root    12055 Nov 21 14:58 stdout.log\n   ```\n\n   In stderr.log.5 file there were many log entries indicating a problem with etcd.\n\n   ```bash\n   I0129 05:50:11.022246   89953 prober.go:117] Liveness probe for \"etcd-mncdsw1.bigdata.abc.gr_kube-system(ef618d8c591c98ed7bd7d66b177d34f7):etcd\" failed (failure): HTTP probe failed with statuscode: 503\n   ```\n\n   ```bash\n   E0129 05:51:22.881553   89953 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"etcd-mncdsw1.bigdata.abc.gr.17299b09446544c4\", GenerateName:\"\", Namespace:\"kube-system\", SelfLink:\"\", UID:\"\", ResourceVersion:\"27938507\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Pod\", Namespace:\"kube-system\", Name:\"etcd-mncdsw1.bigdata.abc.gr\", UID:\"ef618d8c591c98ed7bd7d66b177d34f7\", APIVersion:\"v1\", ResourceVersion:\"\", FieldPath:\"spec.containers{etcd}\"}, Reason:\"Unhealthy\", Message:\"Liveness probe failed: HTTP probe failed with statuscode: 503\", Source:v1.EventSource{Component:\"kubelet\", Host:\"mncdsw1.bigdata.abc.gr\"}, FirstTimestamp:time.Date(2022, time.November, 21, 15, 0, 1, 0, time.Local), LastTimestamp:time.Date(2023, time.January, 29, 5, 50, 41, 21788692, time.Local), Count:700, Type:\"Warning\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'rpc error: code = Unknown desc = OK: HTTP status code 200; transport: missing content-type field' (will not retry!)\n   ```\n\n   Apparently the high DiskI/O was affecting the etcd server.\n\n6. Check warn file logs\n\n   ```bash\n   less /var/log/warn\n   ...\n   Jan 29 05:51:22 mncdsw1.bigdata.abc.gr dockerd[86247]: W0129 03:51:22.867126       1 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {https://127.0.0.1:2379  <nil> 0 <nil>}. E\n   rr :connection error: desc = \"transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused\". Reconnecting..\n   ```\n\n   Run the following to check if this error occured in the past.\n\n   ```bash\n   cat /var/log/warn | grep  -e \"Jan.*29.*grpc: addrConn.createTransport failed to connect to\" | less\n   ```\n\n   ![IM2073052_warn_logs1](.media/IM2073052_warn_logs1.png)\n\n   ![IM2073052_warn_logs2](.media/IM2073052_warn_logs2.png)\n\n   The errors appear every Sunday morning.",
            "Affected Systems": "Not specified",
            "Action Points": "We opened [this](https://metis.xyztel.com/obss/oss/sysadmin-group/abc/BigStreamer/bigstreamer/-/issues/25) issue to re-evaluate CDSW disk architecture.",
            "Customer Update": "Not specified",
            "Our Ticket Response": "```bash\nGood evening,\n\nfrom the analysis we saw that CDSW crashed as there was a problem in the Control Plane pods of the Kubernetes Cluster in which CDSW is deployed.  We notice in the logs that the problem started with timeouts in requests to the etcd of the cluster, which seem to be due to a high Disk I/O of the sdb disk of mncdsw1 at that moment. As a result we have the inability to synchronize the control plane pods with the base of the cluster, which led to their termination and by extension the entire service. Attached you will also find the screenshot that describes the high Disk I/O at that time.\n\nContinuing the analysis we noticed that this behavior is periodic, and more specifically it happens every Sunday at 6 am. Attached you will also find the screenshots that show that there was the same problem on January 15, 22 and 29. Before January the logs are clean. Could you tell us if you have any job set up every Sunday morning that needs a lot of Disk I/O?\n\nThank you\n```",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2172470",
        "Description": "energy_efficiency.pollaploi",
        "Keywords": [
            "cdsw"
        ],
        "Owner": "u13",
        "Date": "20230621",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230621-IM2172470.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\nwe have a problem with the pollaploi job located in the Energy Bills project in impala\nthe frequency of it failing has increased quite a bit\n```",
            "Actions Taken": "After communication with customer we undestand that the issue occurs for job at workbench and not for flow. So:\n\n1. Login to Cloudera Data Science Workbench with your personal account (https://mncdsw1.bigdata.abc.gr/)\n\n2. Click on the left **Sessions** tab and then on **Scope** select **All Projects** and click on **Energy Bills** Project and find **Pollaploi** job.\n\n\n3. Go on **History** tab and you will see that there are a lot of pollaploi jobs with status Failure\n\n4. Click on one job with status Failure and then go to **See job details** and then click on **Script: Energy_Bills_Automation/Energy_Bills_Automation.py**\n\n5. When investigated the script we saw below snippet of spark configuration:\n\n```bash\nspark = SparkSession.builder\\\n.master(\"yarn\")\\\n.config(\"spark.submit.deployMode\", \"client\")\\\n.config(\"spark.eventLog.enabled\", \"true\")\\\n.config(\"spark.executor.instances\", \"100\")\\\n.config(\"spark.executor.cores\", \"2\")\\\n.config(\"spark.executor.memory\", \"4g\")\\\n.config(\"spark.rpc.message.maxSize\", \"1024\")\\\n.config(\"spark.executor.memoryOverhead\", \"800\")\\\n.config(\"spark.driver.memory\", \"4g\")\\\n.config(\"spark.driver.memoryOverhead\", \"800\")\\\n.config(\"spark.spark.driver.maxResultSize\", \"4g\")\\\n.config(\"spark.executor.dynamicAllocation.initialExecutors\", \"4\")\\\n.config(\"spark.executor.dynamicAllocation.minExecutors\", \"4\")\\\n.config(\"spark.executor.dynamicAllocation.maxExecutors\", \"4\")\\\n.config(\"spark.sql.broadcastTimeout\", \"1000\")\\\n.config(\"spark.kryoserializer.buffer.max\", \"1024m\")\\\n.getOrCreate()\n```\n\nSo, there are 100 instances * 2 cores = 200 vcores\n\nand 100 instances * 4G ram = 400GB ram\n\nThe cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.",
            "Affected Systems": "abc Bigstreamer CDSW",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "```bash\nGood evening,\n\nUpon investigation we noticed that the job you mentioned fails with an out-of-memory error.\n\nAdditionally, according to the spark configuration snippet below in your job:\n\n```\n.master(\"yarn\")\\\n.config(\"spark.submit.deployMode\", \"client\")\\\n.config(\"spark.eventLog.enabled\", \"true\")\\\n.config(\"spark.executor.instances\", \"100\")\\\n.config(\"spark.executor.cores\", \"2\")\\\n.config(\"spark.executor.memory\", \"4g\")\\\n```\n\nWe see that you have given 100 instances * 2 cores = 200 vcores and 100 instances * 4Gram = 400GB ram\n\nThe cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.\n\nTherefore, the problem concerns the specific job. For this reason, jobs should be optimized according to the guidance given in an earlier communication for a similar issue, taking into account the configuration that has already been done in the cluster.\n\nIf you don't need anything else please if we can proceed to close the ticket.\n\nThanks\n```",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2363704",
        "Description": "abc - BigStreamer - IM2363704 - CDSW issue -  Engine exited with status 34",
        "Keywords": [
            "cdsw"
        ],
        "Owner": "user1",
        "Date": "20240819",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20240819-IM2363704.md",
        "Detailed Description": {
            "Description": "Good morning,\n\nNoticed since 4/8 many job executions in CDSW fail with \"Engine exited with status 34\" message.\nIn the logs --> There are no logs for this engine node at this time.\nWith re-run the jobs are executed normally.\nIt is observed that many jobs fail during the day.\n\nfor example:\n\nhttps://mncdsw1.bigdata.abc.gr/ccharisis/hardware_failures/engines/0nohdhssxz6uebit\n\nhttps://mncdsw1.bigdata.abc.gr/ccharisis/forecasting/engines/kx29vx7l91i7x567",
            "Actions Taken": "1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'\n2. Select the `Site Administration` tab (must have admin privileges)\n3. Select the `Usage` tab which displays the jobs\n4. Inspect the jobs in question.\n\nThe jobs are in `FAILED` status. The logs for the failed applications are missing.\n\n5. Troubleshoot from the command line:\n\nFrom `mncdsw1` as root (use personal account and then sudo):\n\n```bash\nkubectl get pods -w -A # Wait a pod to fail (namespace should be like default-user-XXX)\n\n# After a while, a pod has failed, describe it\n\nkubectl describe pod -n default-user-XXX XXXXXXXX\n```\n\nIn some cases the pod has failed but it cannot be seen by the `kubectl describe pod` command. In those cases, we use the `kubectl get events` command on the same namespace and search for the appropriate pod name.\n\n```bash\nkubectl get events -n default-user-XXX\n```\n\nIn our case, we used the `kubectl get events` command:\n\n```logs\n60m         Normal    Scheduled                pod/t804rlnpej08xzcg                  Successfully assigned default-user-49/t804rlnpej08xzcg to wrkcdsw1.bigdata.abc.gr\n60m         Warning   FailedCreatePodSandBox   pod/t804rlnpej08xzcg                  Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e\" network for pod \"t804rlnpej08xzcg\": networkPlugin cni failed to set up pod \"t804rlnpej08xzcg_default-user-49\" network: unable to allocate IP address: Post \"\nhttp://127.0.0.1:6784/ip/166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e\":\ndial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container \"166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e\" network for pod \"t804rlnpej08xzcg\": networkPlugin cni failed to teardown pod \"t804rlnpej08xzcg_default-user-49\" network: Delete \"\nhttp://127.0.0.1:6784/ip/166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e\":\ndial tcp 127.0.0.1:6784: connect: connection refused]\n33s         Normal    SandboxChanged           pod/t804rlnpej08xzcg                  Pod sandbox changed, it will be killed and re-created.\n```\n\nThe jobs were running for about an hour before failing with the message `Pod sandbox changed, it will be killed and re-created.`\n\n6. Restart the docker daemon to restart all containers on `wrkcdsw1`\n\n_At the time of the issue, CDSW had stale configuration that required full restart (outage) which was not desirable_\n\nTo avoid applying the settings, restart the service with the same configuration by triggering a restart by `supervisord` deployed as part of the Cloudera agent\n\n<details> ![Danger ahead](https://media3.giphy.com/media/vvzMdSygQejBIejeRO/200w.gif?cid=6c09b952aacsm9yssw6k6q0z5v8ejuy82rjpvw6qdhglcwpu&rid=200w.gif&ct=g) </details>\n\nFrom wrkcdsw4 as root (use personal account and then sudo):\n\n``` bash\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf status | grep DOCKER\n# Sample\n# 145071-cdsw-CDSW_DOCKER          RUNNING   pid 39353, uptime 29 days, 0:40:20\n/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart 145071-cdsw-CDSW_DOCKER\n```\n\n8. Check that the node is operational after the restart\n\nFrom `mncdsw1` as root (use personal account and then sudo):\n\n```bash\ncdsw status # You might have to wait a few minutes\n```\n\n9. Inform the customer about the problem\n\n```text\nGood morning,\n\nWe noticed that some jobs on the wrkcdsw1 node were running for about an hour before they failed with the error \"Pod sandbox changed, it will be killed and re-created\". To solve this particular error, we restarted the cdsw service on the wrkcdsw1 node and noticed that the failures with a duration of 1 hour stopped and the corresponding jobs were executed normally.\n\nPlease let us know if you need anything else or if we can proceed to close the ticket.\n```",
            "Affected Systems": "abc Bigstreamer CDSW",
            "Action Points": "",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2379531",
        "Description": "abc - BigStreamer - IM2379531 - CDSW failed jobs",
        "Keywords": [
            "cdsw"
        ],
        "Owner": "u27",
        "Date": "20240923",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20240923-IM2379531.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\n\nFailed CDSW jobs with a common error have been observed since yesterday (and today).\n\nFailed setting up spark (node: wrkcdsw1.bigdata.abc.gr) (error: dial unix /run/cloudera/data-science-workbench/port-forwarder/port-forwarder.sock: connect: connection refused)\nxEngine exited with status 33.\n________________________________________\n\nCDSW status\n\n|             spark-port-forwarder-w9zjv            |    1/1    |    Running    |      1       |   2024-09-19 09:06:51+00:00   |   10.255.241.133   |   10.255.241.133   |       spark-port-forwarder       |\n|             spark-port-forwarder-z7cdt            |    1/1    |    Running    |      1       |   2024-09-19 09:07:00+00:00   |   10.255.241.132   |   10.255.241.132   |       spark-port-forwarder       |\n|      tcp-ingress-controller-5b46dd4877-qm77x      |    1/1    |    Running    |      0       |   2024-09-19 09:21:22+00:00   |    100.66.0.22     |   10.255.241.130   |      tcp-ingress-controller      |\n|          usage-reporter-55b457bccd-nbt7q          |    1/1    |    Running    |      0       |   2024-09-19 09:06:41+00:00   |    100.66.0.37     |   10.255.241.130   |          usage-reporter          |\n|                web-7db65dccd9-g49qt               |    1/1    |    Running    |      0       |   2024-09-19 09:19:18+00:00   |    100.66.0.10     |   10.255.241.130   |               web                |\n|                web-7db65dccd9-ksff4               |    1/1    |    Running    |      0       |   2024-09-19 09:20:15+00:00   |    100.66.0.21     |   10.255.241.130   |               web                |\n|                web-7db65dccd9-xcxs2               |    1/1    |    Running    |      0       |   2024-09-19 09:20:15+00:00   |    100.66.0.11     |   10.255.241.130   |               web                |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nAll required pods are ready in cluster default.\nAll required Application services are configured.\nAll required secrets are available.\nPersistent volumes are ready.\nPersistent volume claims are ready.\nIngresses are ready.\nChecking web at url: https://mncdsw1.bigdata.abc.gr\nOK: HTTP port check\nCloudera Data Science Workbench is ready!\n\n```",
            "Actions Taken": "1. After checking logs of `wrkcdsw1.bigdata.abc.gr` `SparkPortForwarder` we saw that the latest request that handled was:\n\n```bash\ncdsw status | grep wrkcdsw1.bigdata.abc.gr\nkubectl logs spark-port-forwarder-thrr9 -n <namespace>\n```\n\nThe output of the logs:\n\n```bash\n2024-09-21 22:35:16.863 11 INFO SparkPortForwarder Failed to dial onward connection data = {\"err\":\"dial tcp 100.66.1.227:30742: connect: connection refused\",\"name\":\"spark-driver\",\"podId\":\"2liofp42ubkcj7yc\",\"port\":30742,\"target\":\"100.66.1.227:30742\"}\n2024-09-22 02:25:23.457 11 INFO SparkPortForwarder Returning port mappping data = {\"mapping\":{\"spark-blockmanager\":26577,\"spark-driver\":22768}}\n2024-09-22 02:26:29.689 11 INFO SparkPortForwarder Garbage collecting forwarders for pod data = {\"podId\":\"z48obsz9bocvu2wz\"}\n```\n\n2. We tried to delete the pod of `wrkcdsw1.bigdata.abc.gr` `SparkPortForwarder` but it stucked on `Terminating` status.\n\n```bash\nkubectl delete pod <pod_name> -n <namespace>\n```\n\n3. Thus, CDSW Application(mncdsw1) from Cloudera UI was down.\n\n4. From [Cloudera Manager UI](https://172.25.37.232:7183/cmf/home) we have restarted the `Docker Deamon Worker` of `wrkcdsw1.bigdata.abc.gr` and `Application` role of `mncdsw1`. After that we have checked the logs and `wrkcdsw1.bigdata.abc.gr` `SparkPortForwarder` handled succefully all the requests\n\nActions:\n\n```bash\nCloudera Manager -> CDSW -> `Docker Deamon Worker` Role of `wrkcdsw1 -> Restart\nCloudera Manager -> CDSW -> `Application` role of `mncdsw1` -> Restart\n```\nLogs \n\n1. Checks:\n\n```bash\ncdsw status | grep wrkcdsw1.bigdata.abc.gr\nkubectl logs <pod-name-spark-forwarder> -n <namespace>\n```\n\n2. Output:\n```\n2024-09-23 09:20:40.579 11 INFO SparkPortForwarder Start mapping ports data = {\"podId\":\"1t47ok1jnxqb1pi9\"}\n2024-09-23 09:20:40.579 11 INFO SparkPortForwarder Start trying to forward port data = {\"name\":\"spark-driver\",\"podId\":\"1t47ok1jnxqb1pi9\",\"port\":26404}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Finish trying to forward port, success data = {\"name\":\"spark-driver\",\"podId\":\"1t47ok1jnxqb1pi9\",\"port\":26404}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Start trying to forward port data = {\"name\":\"spark-blockmanager\",\"podId\":\"1t47ok1jnxqb1pi9\",\"port\":30123}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Finish trying to forward port, success data = {\"name\":\"spark-blockmanager\",\"podId\":\"1t47ok1jnxqb1pi9\",\"port\":30123}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Finish mapping ports data = {\"podId\":\"1t47ok1jnxqb1pi9\"}\n2024-09-23 09:20:40.580 11 INFO SparkPortForwarder Returning port mappping data = {\"mapping\":{\"spark-blockmanager\":30123,\"spark-driver\":26404}}\n2024-09-23 09:21:29.302 11 INFO SparkPortForwarder Garbage collecting forwarders for pod data = {\"podId\":\"1t47ok1jnxqb1pi9\"}\n```\n\n5. Additional checks made from the [CDSW UI](https://mncdsw1.bigdata.abc.gr). We reviewed the status of running jobs and examined the logs of them.\n\n```bash\nSite Administration -> Usage -> Select job Name -> Logs Tab\n```\n\nIn the logs of an example job we searched for `SparkPortForwarder` entries for `wrkcdsw1` in order to evaluate that no errors appeared.",
            "Affected Systems": "CDSW",
            "Action Points": "",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2231148",
        "Description": "pod failure at RAN.AI",
        "Keywords": [
            "ranai-geo",
            "airflow",
            "kubernetes"
        ],
        "Owner": "u33",
        "Date": "20231011",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20231011-IM2231148.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\n\nWe saw that the airflow scheduler is down due to access rights in the logs folder.\npod airflow-scheduler-0 had entered a restart loop as shown below.\n\nroot@kubemaster1:~# kubectl get pods -n ranai-geo\nNAME READY STATUS RESTARTS AGE\nairflow-postgresql-0 1/1 Running 0 326d\nairflow-scheduler-0 0/2 CrashLoopBackOff 970 (68s ago) 187d\nairflow-statsd-85d5d8768b-hgzzc 1/1 Running 0 326d\nairflow-webserver-6c8448476d-hs4nb 1/1 Running 0 187d\nΤο έκανα restart αλλά είναι σε status Init:0/1 πλέον. Επισυνάπτω logs και τα events μετά το restart του pod.\nroot@kubemaster1:~# kubectl get pods -n ranai-geo\nNAME READY STATUS RESTARTS AGE\nairflow-postgresql-0 1/1 Running 0 326d\nairflow-scheduler-0 0/2 Init:0/1 0 67s\nairflow-statsd-85d5d8768b-hgzzc 1/1 Running 0 326d\nairflow-webserver-6c8448476d-hs4nb 1/1 Running 0 187d\nabc-prod-ranai-geo-be-f7f8fc5c4-rv66z 1/1 Running 0 25h\nabc-prod-ranai-geo-clustering-68975fb5b5-qvztf 1/1 Running 0 25h\nabc-prod-ranai-geo-fe-9c5c7bc7c-72fpn 1/1 Running 0 141d\nabc-prod-ranai-geo-postgres-0 1/1 Running 0 165d\nroot@kubemaster1:~# kubectl get events -n ranai-geo\nLAST SEEN TYPE REASON OBJECT MESSAGE\n25m Warning BackOff pod/airflow-scheduler-0 Back-off restarting failed container\n35m Warning Unhealthy pod/airflow-scheduler-0 Liveness probe failed: Unable to load the config, contains a configuration error....\n21m Normal Scheduled pod/airflow-scheduler-0 Successfully assigned ranai-geo/airflow-scheduler-0 to kubeworker1.bigdata.abc.gr\n13m Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[kerberos-keytab connectors-config jssecacerts kube-api-access-2df5h config logs]: timed out waiting for the condition\n19m Warning FailedMount pod/airflow-scheduler-0 MountVolume.MountDevice failed for volume \"pvc-c826d577-e764-470e-9904-3986042810aa\" : rpc error: code = DeadlineExceeded desc = context deadline exceeded\n8m59s Warning FailedMount pod/airflow-scheduler-0 MountVolume.MountDevice failed for volume \"pvc-c826d577-e764-470e-9904-3986042810aa\" : rpc error: code = Internal desc = format of disk \"/dev/longhorn/pvc-c826d577-e764-470e-9904-3986042810aa\" failed: type:(\"ext4\") target:(\"/var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-c826d577-e764-470e-9904-3986042810aa/globalmount\") options:(\"defaults\") errcode:(exit status 1) output:(mke2fs 1.45.5 (07-Jan-2020)...\n9m26s Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[kube-api-access-2df5h config logs kerberos-keytab connectors-config jssecacerts]: timed out waiting for the condition\n15m Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[connectors-config jssecacerts kube-api-access-2df5h config logs kerberos-keytab]: timed out waiting for the condition\n9m24s Normal Scheduled pod/airflow-scheduler-0 Successfully assigned ranai-geo/airflow-scheduler-0 to kubeworker1.bigdata.abc.gr\n32s Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[connectors-config jssecacerts kube-api-access-nrb25 config logs kerberos-keytab]: timed out waiting for the condition\n2m58s Warning FailedMount pod/airflow-scheduler-0 MountVolume.MountDevice failed for volume \"pvc-c826d577-e764-470e-9904-3986042810aa\" : rpc error: code = Internal desc = format of disk \"/dev/longhorn/pvc-c826d577-e764-470e-9904-3986042810aa\" failed: type:(\"ext4\") target:(\"/var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-c826d577-e764-470e-9904-3986042810aa/globalmount\") options:(\"defaults\") errcode:(exit status 1) output:(mke2fs 1.45.5 (07-Jan-2020)...\n5m4s Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[jssecacerts kube-api-access-nrb25 config logs kerberos-keytab connectors-config]: timed out waiting for the condition\n2m47s Warning FailedMount pod/airflow-scheduler-0 Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[kerberos-keytab connectors-config jssecacerts kube-api-access-nrb25 config logs]: timed out waiting for the condition\n9m24s Normal SuccessfulCreate statefulset/airflow-scheduler create Pod airflow-scheduler-0 in StatefulSet airflow-scheduler successful\n\nPlease for your checks.\nThanks\n```",
            "Actions Taken": "Deleted problematic PVCs in Longhorn under instance-manager-r namespace.\n`kubectl delete pvc pvc-c826d577-e764-470e-9904-3986042810aa -n ranai-geo`",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "To resolve the issue and restore the airflow-scheduler pod's functionality, the following steps were taken:\n\nDeletion of Problematic PVC: The three problematic replicas of airflow-scheduler PVC in Longhorn, which were stuck in a deleting state, were the problem, as a result we deleted the airflow's PVC.\nNfgh: It is important to mention that this issue might be related to a bug within the Longhorn storage system, as it caused replicated PVCs to get stuck in a deleting state. Further investigation and monitoring of Longhorn may be necessary to prevent such issues from recurring. Also see this [thread](https://github.com/longhorn/longhorn/issues/4278)",
            "Recommendations": "Monitor Longhorn for any recurring issues with PVC management to prevent similar incidents. \n\nTicket Status:\nResolved successfully",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Pod Initialization: The initial issue was identified as the airflow-scheduler pod getting stuck during initialization.\n\ncmd: `kubectl describe <airflow-scheduler-pod> -n ranai-geo`\n\n![error_mount](.media/mounterror.JPG)\n\nConfigmap Mounting Issues: Further examination of the pod's logs and configuration revealed that some required configmaps were failing to mount correctly, causing a disruption in the pod initialization process.\n\nIdentify the airflow pv\n\n![pv](.media/getpv.JPG)\n\n`kubectl logs instance-manager-r-28535c55 -n instance-manager-r`\n\nPVC logs\n\n![pvclogs1](.media/pvc1logs.JPG)\n![pvclogs2](.media/pvc2logs.JPG)\n![pvclogs3](.media/pv3logs.JPG)\n\nLonghorn Investigation: To investigate possible underlying storage issues, we accessed [Longhorn](https://kubemaster-vip.bigdata.abc.gr/longhorn/) and discovered that the PVC (Persistent Volume Claim) associated with the airflow-scheduler had three replicatas of airflow-scheduler pvc stuck in a deleting state.\n\n![longhornUI](.media/longhornbug.JPG)",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1957832",
        "Description": "abc - Cloudera Alert Publisher",
        "Keywords": [
            "cloudera",
            "alert_publisher",
            "mail"
        ],
        "Owner": "u7",
        "Date": "20210910",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20220901-IM1957832.md",
        "Detailed Description": {
            "Description": "_No alerts from cloudera. We noticed that since yesterday morning, alerts have stopped coming from the cloudera manager.\nI sent a test mail (attached) today from the cloudera manager which came normally but the automatic mails we receive every day have stopped._\n\n\nabc opened the above ticket to our team.",
            "Actions Taken": "- Ssh to **un5**  Alert Publisher Node and check the following files for error:\n\n```\nless /var/log/messages\nless /var/log/mail.err \nless /var/log/mail.info \nless /var/log/cloudera-scm-alertpublisher\nless mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out\n```\n\n- Use the following commands as example to check the above log file in order to count how many alerts are in the logs and you can grep depending on month,date and the message. \n```\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Generated subject [Cloudera Alert]' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Cloudera Alert' | wc -l;done\nfor i in {20..31}; do echo 2022-08-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-08-${i} | grep 'Cloudera Alert' | wc -l;done\nfor i in {20..31}; do echo 2022-08-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-08-${i} | grep 'has become bad' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Collected new alert' | wc -l;done\nfor i in {20..31}; do echo 2022-08-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-08-${i} | grep 'Collected new alert' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'has become bad' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Cloudera Alert' | wc -l;done\nfor i in {01..05}; do echo 2022-09-$i; cat mgmt-cmf-mgmt-ALERTPUBLISHER-un5.bigdata.abc.gr.log.out | grep 2022-09-${i} | grep 'Collected new alert' | wc -l;done\n```\n\n- _Preventively step_ Restart Alert Publisher through Cloudera Manager\n\n- From the above investigation we came up to the result that we did not had a problem **as we did not find any errors** with the email alerting system because it was during a vacation period, so it was clearly that it had less warnings and alerts because of that.",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2158478",
        "Description": "abc - BigStreamer - IM2158478 - HDFS bad health",
        "Keywords": [
            "cloudera",
            "failover_controller",
            "zookeeper"
        ],
        "Owner": "u44",
        "Date": "20230531",
        "Status": "Resolved",
        "Info": "abc/BigStreamer/issues/20230531-IM2158478.md",
        "Detailed Description": {
            "Description": "```\nGood morning,\n\nHDFS status is bad as HDFS Failover Controller role is down on nodes mn1, mn2\n\nPlease for your checks.\n\nThanks\n```",
            "Actions Taken": "1. Since the failover controller roles were down, we investigated their logs under `/var/log/hadoop-hdfs/`\n   on each host and found that they received a timeout in their connection to zookeeper leading them to\n   shutdown until they were manually restarted.\n\n2. Checking the zookeper server logs under `/var/log/zookeeper/` we observed that they report that the\n   connection had been closed client side. Additionally at the same time frame we checked to see if\n   there were any issues with other services hosted on these nodes, mainly if any Namenodes had any\n   issues, and found none.\n\n3. Similarly from Cloudera Manager we checked the host's event log for any red flags and found none.\n   Lastly we didn't find any network errors on both hosts.\n\n4. Through Cloudera Manager health checking we see certain RPC latency alerts popping up with values\n   above 1500 ms. Additionally we checked Zookeeper Server's tick time under `ZooKeeper->Configuration->Tick Time`.\n\n4. Thus we concluded that, given the zookeeper's tick time of 2000 ms and no other issues found\n, a spike in latency led to the above timeout and after communicating with the customer\nwe enabled the failover controller's automatic restart to avoid having the failover controllers\ndown for prolonged periods of time. This was done without needing any services or redeploy any configuration\nby checking the box under `HDFS->Failover Controller->Automatically Restart Processes`.",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "- [Failover Controller Connection Loss](https://actorsfit.com/a?ID=01750-52f7ffb1-84f3-4d85-a855-e06d619799ce#:~:text=Modify%20the%20zookeeper%20configuration%20file.%20In%20zoo.cfg%2C%20modify,ticktime%20to%204000ms%2C%20and%20the%20default%20is%202000ms.)\n- [ZKFC Failure](https://community.cloudera.com/t5/Support-Questions/Failover-Controllers-Health-Bad-leads-to-complete-HDFS/m-p/51717)",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2237832",
        "Description": "abc - BDC - IM2237832 - Cannot login to Cloudera Manager",
        "Keywords": [
            "Cloudera Manager",
            "ldap",
            "Impala"
        ],
        "Owner": "u140",
        "Date": "20231024",
        "Status": "Resolved",
        "Info": "abc/BDC/issues/20231024-IM2237832.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2238305",
        "Description": "abc - BDC - IM2238305 - Kudu Rebalance",
        "Keywords": [
            "kudu",
            "rebalance"
        ],
        "Owner": "u140",
        "Date": "20231025",
        "Status": "Resolved",
        "Info": "abc/BDC/issues/20231025-IM2238305.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2162210",
        "Description": "abc - BDC - IM2119025- IM2162210- yarn jobs in sn04 are failling",
        "Keywords": [
            "yarn",
            "node_manager"
        ],
        "Owner": "u140",
        "Date": "20230606",
        "Status": "Resolved",
        "Info": "abc/BDC/issues/20230606-IM2162210.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2119025",
        "Description": "abc - BDC - IM2119025- Hive beeline error in edge1,2",
        "Keywords": [
            "hive",
            "beeline"
        ],
        "Owner": "u7",
        "Date": "20230431",
        "Status": "Resolved",
        "Info": "abc/BDC/issues/20230330-IM2119025.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1550556",
        "Description": "abc - BDC - IM1550556- HDFS issue 3/6/2021",
        "Keywords": [
            "hdfs",
            "cloudera",
            "hbase",
            "namenodes"
        ],
        "Owner": "u27",
        "Date": "20210603",
        "Status": "Resolved",
        "Info": "abc/BDC/issues/20210603-IM1550556.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1758382",
        "Description": "abc - BDC - IM1758382- Cluster has stale Kerberos client configuration.",
        "Keywords": [
            "hdfs",
            "keytabs",
            "cloudera",
            "datanodes"
        ],
        "Owner": "u27",
        "Date": "20220104",
        "Status": "Resolved",
        "Info": "abc/BDC/issues/20220104-IM1758382.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1798698",
        "Description": "abc - BDC - IM1798698 - HUE has different permissions/behavior per node",
        "Keywords": [
            "hue"
        ],
        "Owner": "u15",
        "Date": "20220223",
        "Status": "Resolved",
        "Info": "abc/BDC/issues/20220223-IM1798698.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2024779",
        "Description": "abc - BDC - External Hive table from Hbase Table return duplicates",
        "Keywords": [
            "Hive",
            "Hbase",
            "duplicates"
        ],
        "Owner": "u3",
        "Date": "20230222",
        "Status": "Resolved",
        "Info": "KnowledgeBase/abc/BDC/issues/SD2084840-IM2024779.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2110389",
        "Description": "abc - BDC - IM2110389 - Hbase issue 20/3",
        "Keywords": [
            "Hbase",
            "Region",
            "inconsistencies"
        ],
        "Owner": "u13",
        "Date": "20230320",
        "Status": "Resolved",
        "Info": "KnowledgeBase/abc/BDC/issues/20230320-IM2110389.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2153698",
        "Description": "abc - BDC - IM2153698 - Servers sn01 sn02 sn04 are in bad health",
        "Keywords": [
            "Cloudera Agent",
            "Navigator",
            "Solr",
            "space",
            "panic",
            "spark lineage logs",
            ""
        ],
        "Owner": "u44",
        "Date": "20230524",
        "Status": "Resolved",
        "Info": "KnowledgeBase/abc/BDC/issues/20230524-IM2153698.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2275988",
        "Description": "abc - BDC - IM2275988 - Yarn GC error",
        "Keywords": [
            "Cloudera Manager",
            "Yarn",
            "Resource Manager",
            ""
        ],
        "Owner": "user1",
        "Date": "20240205",
        "Status": "Resolved",
        "Info": "KnowledgeBase/abc/BDC/issues/20240205-IM2275988.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2278876",
        "Description": "BDC - BDC kerberos authentication issues",
        "Keywords": [
            "Kerberos",
            "HAProxy",
            "Hue",
            "Hive",
            "impala"
        ],
        "Owner": "u13",
        "Date": "20240212",
        "Status": "Resolved",
        "Info": "KnowledgeBase/abc/BDC/issues/20240212-IM2278876.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2276108",
        "Description": "abc - BDC - IM2276108 - Cannot create table in Kudu",
        "Keywords": [
            "Kudu",
            "Hive Metastore",
            "Impala",
            ""
        ],
        "Owner": "user1",
        "Date": "20240205",
        "Status": "Resolved",
        "Info": "KnowledgeBase/abc/BDC/issues/20240205-IM2276108.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2276995",
        "Description": "abc - BDC - IM2276995 - BDC - HUE and CM unresponsive, Metastore in master3 errors",
        "Keywords": [
            "Cloudera Manager",
            "Hue",
            "Oozie",
            "Hive Metastore",
            ""
        ],
        "Owner": "user1",
        "Date": "20240207",
        "Status": "Resolved",
        "Info": "KnowledgeBase/abc/BDC/issues/20240207-IM2276995.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2304908",
        "Description": "abc - BDC - IM2304908 - Hue - 403 Client Error: GSSException",
        "Keywords": [
            "Oozie",
            "keytab",
            "edge-vip"
        ],
        "Owner": "u140",
        "Date": "20240405",
        "Status": "Closed",
        "Info": "KnowledgeBase/abc/BDC/issues/20240405-IM2304908.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2303252",
        "Description": "abc - BDC - IM2303252 - Hive beeline error communicating with Thrift server",
        "Keywords": [
            "hive",
            "hostname",
            "haproxy"
        ],
        "Owner": "u33",
        "Date": "20240329",
        "Status": "Closed",
        "Info": "KnowledgeBase/abc/BDC/issues/20240329-IM2303252.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2347849",
        "Description": "abc - BDC - IM2347849 - Edge2 issue on Hue service due to python package",
        "Keywords": [
            "hue",
            "python"
        ],
        "Owner": "u2",
        "Date": "20241107",
        "Status": "Resolved",
        "Info": "KnowledgeBase/abc/BDC/issues/SD2414951_IM2347849.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2364882",
        "Description": "abc - BDC - IM2364882 - Kafka issue with lagging replicas",
        "Keywords": [
            "kafka brokers",
            "lagging replicas"
        ],
        "Owner": "u13",
        "Date": "20240823",
        "Status": "Resolved",
        "Info": "KnowledgeBase/abc/BDC/issues/20240823_IM2364882.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1317401",
        "Description": "[PR][IBANK] Query Average Response Time alert appeared in Grafana",
        "Keywords": [
            "Query",
            "Response",
            "IBANK"
        ],
        "Owner": "u1",
        "Date": "20201014",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20201014-IM1317401.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1332456",
        "Description": "DR][IBANK] Internet Banking Data Warehouse - Weekend extraction jobs",
        "Keywords": [
            "Query",
            "Response",
            "IBANK"
        ],
        "Owner": "u27",
        "Date": "20201026",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20201026-IM1332456.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1361249",
        "Description": "Spark Waiting Batches Alert - Grafana",
        "Keywords": [
            "Query",
            "Response",
            "IBANK"
        ],
        "Owner": "u27",
        "Date": "20201120",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20201120-IM1361249.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1426379",
        "Description": "2 Batch Job failed on dr1edge01.dcd.gr",
        "Keywords": [
            "HBase",
            "enrichment",
            "Impala",
            "keylength"
        ],
        "Owner": "u15",
        "Date": "20210127",
        "Status": "Open",
        "Info": "dcd/BigStreamer/issues/20210127-IM1426379.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1389913",
        "Description": "Registrations prod_trlog_ibank.service_audit_stream",
        "Keywords": [
            "IBank",
            "distcp",
            ""
        ],
        "Owner": "u1 u15",
        "Date": "20201218",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20201218-IM1389913.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1681883",
        "Description": "hdfs: Datanode Block Count Status",
        "Keywords": [
            "retention",
            "service_audit_old"
        ],
        "Owner": "u15",
        "Date": "20211021",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20211021-IM1681883.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1769421",
        "Description": "hdfs: User problem in queries",
        "Keywords": [
            "user",
            "groups",
            "permissions"
        ],
        "Owner": "u13",
        "Date": "20211017",
        "Status": "Resolved",
        "Info": "/dcd/BigStreamer/issues/20211701-IM1769421.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "SD1752317",
        "Description": "Can the functionality of Navigator be checked on the primary? It does not bring the information from Analytics.",
        "Keywords": [
            "primary",
            "navigator",
            "cloudera"
        ],
        "Owner": "u15",
        "Date": "20211105",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20211105-SD1752317.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1805149",
        "Description": "IBank_Ingetion batch job failed",
        "Keywords": [
            "dcd_bd_ibank_merge_batch"
        ],
        "Owner": "u15",
        "Date": "20220103",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20220103-IM1805149.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1851937",
        "Description": "PROD_IBANK_DWH_EXPORT_ServiceAudit batch job failed",
        "Keywords": [
            "PROD_IBANK_DWH_EXPORT",
            "Service Audit",
            "Datawarehouse"
        ],
        "Owner": "u140",
        "Date": "20220504",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20220504-IM1851937.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1868465",
        "Description": "Batch Job failed on Grafana",
        "Keywords": [
            "impala",
            "dcd_bd_ibank_spark"
        ],
        "Owner": "u77",
        "Date": "20220524",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20220524-IM1868465.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1893876",
        "Description": "hdfs - Data Directory Status",
        "Keywords": [
            "hdfs",
            "disk"
        ],
        "Owner": "u13",
        "Date": "20220620",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20220620-SD1951890.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1896751",
        "Description": "Alert in Grafana Application",
        "Keywords": [
            "hbase"
        ],
        "Owner": "u1",
        "Date": "20220630",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20220630-IM1805149.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "SD1949713",
        "Description": "Problem with dr cluster services",
        "Keywords": [
            "impala",
            "kudu",
            "dcd_bd_ibank_spark"
        ],
        "Owner": "u140",
        "Date": "20220617",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20220617-SD1949713.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2024442",
        "Description": "Critical alarm in Cloudera Application - dr1edge01",
        "Keywords": [
            "hive"
        ],
        "Owner": "u77",
        "Date": "20221119",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20221119-IM2024442.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2006951",
        "Description": "Failed job at Grafana",
        "Keywords": [
            "dcd_bd_ibank_dwh"
        ],
        "Owner": "u1",
        "Date": "20221027",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20221027-IM2006951.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2070630",
        "Description": "Failed batch Job on Grafana",
        "Keywords": [
            "dcd_bd_ibank_dwh"
        ],
        "Owner": "u77",
        "Date": "20230126",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230126-IM2070630.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2072206",
        "Description": "Batch Job Failed",
        "Keywords": [
            "dcd_bd_ibank_dwh"
        ],
        "Owner": "u77",
        "Date": "20230127",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230127-IM2072206.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2074270",
        "Description": "Failed Batch Job on Grafana",
        "Keywords": [
            "dcd_bd_ibank_dwh"
        ],
        "Owner": "u77",
        "Date": "20230131",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230131-IM2074270.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2095156",
        "Description": "Alarm on PRDBA  Cloudera Manager",
        "Keywords": [
            "yarn",
            "retention"
        ],
        "Owner": "u140",
        "Date": "20230301",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230301-IM2095156.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2098517",
        "Description": "Health issue on dr1edge01Cloudera Manager",
        "Keywords": [
            "spark on yarn",
            "history server"
        ],
        "Owner": "u140",
        "Date": "20230305",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230305-IM2098517.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2095966",
        "Description": "Failed Batch Job on Grafana",
        "Keywords": [
            "dcd_bd_ibank_spark",
            "HBase quotas"
        ],
        "Owner": "u140",
        "Date": "20230301",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230301-IM2095966-IM2097021.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2097021",
        "Description": "Multiple Health issues on PR Impala",
        "Keywords": [
            "dcd_bd_ibank_spark",
            "Impala",
            "HBase quotas"
        ],
        "Owner": "u140",
        "Date": "20230302",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230301-IM2095966-IM2097021.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "way4streams-venia",
        "Description": "Kerberos Authentication Errors on new Way4Streams installation",
        "Keywords": [
            "os"
        ],
        "Owner": "u1",
        "Date": "20230313",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230313-way4streams-venia.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2099957",
        "Description": "Alert on Grafana",
        "Keywords": [
            "dcd_bd_ibank_spark"
        ],
        "Owner": "u140",
        "Date": "20230307",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230307-IM2099957.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2117067",
        "Description": "Failed Batch Job appeared",
        "Keywords": [
            "ManDate",
            "DWH_IBank",
            "Extract"
        ],
        "Owner": "u7",
        "Date": "20232903",
        "Status": "Resolved",
        "Info": "https://metis.xyztel.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/dcd/BigStreamer/issues/20230329-IM2117067.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "SD2180781",
        "Description": "Failed Batch Job appeared",
        "Keywords": [
            "Enrich SA from SA_old",
            "Merge batch"
        ],
        "Owner": "u27",
        "Date": "20230331",
        "Status": "Resolved",
        "Info": "KnowledgeBase/dcd/BigStreamer/issues/20230331-SD2180781.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2158906",
        "Description": "Failed job at Grafana",
        "Keywords": [
            "failed job IBank_Ingestion",
            "Merge batch"
        ],
        "Owner": "u27",
        "Date": "20230531",
        "Status": "Resolved",
        "Info": "KnowledgeBase/dcd/BigStreamer/issues/20230531-IM2158906.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2165544",
        "Description": "Alert at Cloudera Manager (DR/PR)",
        "Keywords": [
            "clock offset",
            "ntpd service"
        ],
        "Owner": "u140",
        "Date": "20230611",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230611-IM2165544.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2165930",
        "Description": "Alert at Cloudera Manager (DR/PR)",
        "Keywords": [
            "Prod_Online_IngestStream",
            "distcp"
        ],
        "Owner": "u140",
        "Date": "20230611",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230611-IM2165930.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2165930",
        "Description": "Alert at Cloudera Manager (DR/PR)",
        "Keywords": [
            "dcd_bd_online_merge_batch"
        ],
        "Owner": "u1",
        "Date": "20230726",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20230726-IM2193241.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2241809",
        "Description": "Every Host on PR1 and DR1 are in critical state",
        "Keywords": [
            "nfs",
            "cloudera"
        ],
        "Owner": "u27",
        "Date": "20231117",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20231117-IM2241809.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2285747",
        "Description": "merge batch 29/2",
        "Keywords": [
            "dcd_bd_ibank_merge_batch"
        ],
        "Owner": "u44",
        "Date": "20240302",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20240302-IM2285747.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2271635",
        "Description": "PR NAVIGATOR CONNECTION ISSUES",
        "Keywords": [
            "cloudera_mgmt"
        ],
        "Owner": "user1",
        "Date": "20240129",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20240129-IM2271635.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2323192",
        "Description": "Disk replacement at DR",
        "Keywords": [
            "dcd",
            "disk"
        ],
        "Owner": "user1",
        "Date": "20240306",
        "Status": "Resolved",
        "Info": "dcd/BigStreamer/issues/20240306-IM2323192.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1333166",
        "Description": "Too many records with available speeds ='' (empty)",
        "Keywords": [
            "Query",
            "Rows",
            "BigStreamer"
        ],
        "Owner": "u27",
        "Date": "20201027",
        "Status": "Resolved",
        "Info": "def/BigData/issues/20201027-IM1333166.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1712804",
        "Description": "Queries from GDA Tool run forever",
        "Keywords": [
            "SpagoBi",
            "GDA Tool",
            "BigStreamer"
        ],
        "Owner": "u1",
        "Date": "20211117",
        "Status": "Resolved",
        "Info": "def/BigData/issues/20211117-IM1712804.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2087411",
        "Description": "SpagoBI web interface is not available",
        "Keywords": [
            "spagobi"
        ],
        "Owner": "u33",
        "Date": "20230217",
        "Status": "Resolved",
        "Info": "def/BigData/issues/20230217-IM2087411.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2186545",
        "Description": "The main web page of defBI is not responding.",
        "Keywords": [
            "spagobi"
        ],
        "Owner": "u3",
        "Date": "20230712",
        "Status": "Resolved",
        "Info": "KnowledgeBase/def/BigData/issues/2249242-IM2186545.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2189500",
        "Description": "All services in Cloudera Manager are stopped",
        "Keywords": [
            "mysql"
        ],
        "Owner": "u3",
        "Date": "20230718",
        "Status": "Resolved",
        "Info": "KnowledgeBase/def/BigData/issues/2253075-IM2189500.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2362406",
        "Description": "Sqoop loadings do not run",
        "Keywords": [
            "Yarn",
            "Cloudera Manager"
        ],
        "Owner": "user1",
        "Date": "20240814",
        "Status": "Resolved",
        "Info": "KnowledgeBase/def/BigData/issues/20240814-IM2362406.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "GI47",
        "Description": "ADSL service pod OOM when creating new thread",
        "Keywords": [
            "-"
        ],
        "Owner": "u15",
        "Date": "20210920",
        "Status": "Resolved",
        "Info": "KnowledgeBase/efg/actionStreamer/issues/20210920-GI47.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "GI48",
        "Description": "Pods evicted forcefully from kaasworker03",
        "Keywords": [
            "-"
        ],
        "Owner": "u15",
        "Date": "20211231",
        "Status": "Resolved",
        "Info": "KnowledgeBase/efg/actionStreamer/issues/20211231-GI48.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1855774",
        "Description": "Core files on smarts-console (SD1873844 old one)",
        "Keywords": [
            "core_files",
            "smarts-console"
        ],
        "Owner": "u13",
        "Date": "20220905",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20220905-IM1855774.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1875219",
        "Description": "no alarms/traps on Dashboard",
        "Keywords": [
            "alarms",
            "smarts"
        ],
        "Owner": "u13",
        "Date": "20220531",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20220531-IM1875219.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1997682",
        "Description": "Migration CARL Project to Smarts",
        "Keywords": [
            "carl",
            "alarms"
        ],
        "Owner": "u27",
        "Date": "20221017",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20221017-IM1997682.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2027989",
        "Description": "smarts SNMPTrap not coming",
        "Keywords": [
            "traps"
        ],
        "Owner": "u27",
        "Date": "20221124",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20221124-IM2027989.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1972080",
        "Description": "INCHARGE-AM issue",
        "Keywords": [
            "domain",
            "smarts",
            "critical"
        ],
        "Owner": "u27",
        "Date": "20220915",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20220915-IM1972080.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1972080",
        "Description": "trap service Stuck",
        "Keywords": [
            "smarts"
        ],
        "Owner": "u1",
        "Date": "20230304",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20230304-IM1972080.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2143540",
        "Description": "filter sub-interfaces for specific elements (router)",
        "Keywords": [
            "cde_smarts"
        ],
        "Owner": "u1",
        "Date": "20230515",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20230515-IM2143540.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2170823",
        "Description": "trap service Stuck - ( related ticket SD2159932 ))",
        "Keywords": [
            "cde_smarts"
        ],
        "Owner": "u13",
        "Date": "20230619",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20230619-IM2170823.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2050070",
        "Description": "snmpv3 HOSTS-SERVERS Discovery",
        "Keywords": [
            "cde_smarts",
            "snmpv3"
        ],
        "Owner": "u27",
        "Date": "20221220",
        "Status": "Resolved",
        "Info": "KnowledgeBase/cde/SMARTS/issues/20221220-IM2050070.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2132255",
        "Description": "certified DEVICE for SMARTS",
        "Keywords": [
            "cde_smarts",
            "certify devices"
        ],
        "Owner": "u27",
        "Date": "20230421",
        "Status": "Resolved",
        "Info": "KnowledgeBase/cde/SMARTS/issues/20230421-IM2132255.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2191585",
        "Description": "Router discovery problem on Smarts || 10.112.31.45",
        "Keywords": [
            "cde_smarts"
        ],
        "Owner": "u1",
        "Date": "20230718",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20230718-IM2191585.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2168454",
        "Description": "AR90 no properly discovered",
        "Keywords": [
            "cde_smarts"
        ],
        "Owner": "u13",
        "Date": "20231406",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/20231406-IM2168454.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2348452",
        "Description": "Smarts Console Memory Issue",
        "Keywords": [
            "cde_smarts"
        ],
        "Owner": "u2",
        "Date": "20241207",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/SD2415562_IM2348452.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM2334362",
        "Description": "Specific SNMPTrap presentation issue on console",
        "Keywords": [
            "cde_smarts"
        ],
        "Owner": "u27",
        "Date": "20240624",
        "Status": "Resolved",
        "Info": "cde/SMARTS/issues/SD2400787_IM2334362.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "obss/oss/sysadmin-group/kubernetes#26",
        "Description": "[kube-vip] Pod on barcelona.intranet.gr does not respect lock",
        "Keywords": [
            "kubernetes"
        ],
        "Owner": "u1",
        "Date": "20230208",
        "Status": "Resolved",
        "Info": "xyz/Kubernetes/issues/20230208-kubernetes26.md",
        "Detailed Description": {
            "Description": "Not specified",
            "Actions Taken": "Not specified",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified"
        }
    },
    {
        "Issue Number": "IM1299104",
        "Description": "Missing logs",
        "Keywords": [
            "logs"
        ],
        "Owner": "u27",
        "Date": "20201001",
        "Status": "Open",
        "Info": "abc/BigStreamer/issues/20201001-IM1299104.md",
        "Detailed Description": {
            "Description": "Good evening, the abc syslog administrators have noticed much lower than expected and irregular log reception times from server 172.25.37.236 for the period 7/23-27.\nWe would like to investigate the cause and if possible find the missing logs.\n\nThank you very much for your immediate actions.\n\nKeywords: logs\nOwner: u27\nDate: 20200929\nStatus: Open",
            "Actions Taken": "1. ssh un2 as root\n2. cat /etc/rsyslog.conf | more\n3. check the servers that messages transfered. ( only abc servers on this file)\n4. cat /etc/logrotate.conf | more ( check the rotate of messages)",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1629405",
        "Description": "radius.radarchive_hist  missing data",
        "Keywords": [
            ""
        ],
        "Owner": "",
        "Date": "20210831",
        "Status": "<br>\nInfo: [info](abc/BigStreamer/issues/20210831-IM1629405.md)<br>\n\n<b>Issue Number:</b> IM1630642<br>\nDescription: missing or corrupted statistics<br>\nKeywords: statistics<br>\nOwner: <br>\nDate: 20210901<br>\nStatus:",
        "Info": "abc/BigStreamer/issues/20210901-IM1630642.md",
        "Detailed Description": {
            "Description": "```\nGood evening,\nPlease for your actions\n\nWARNING: The following tables have pfghntially corrupt table statistics. Drop and re-compute statistics to resolve this problem. -> \n\nsai.voice_quality_hist\n\nsai.sms_raw, \n\nsai.voice_raw\n\nbrond.brond_retrains_hist,\n\nbrond.td_dslam_week,\n\nookla.ookla_android,\n\nookla.ookla_ios,\n\nookla.ookla_stnet\n\ntemip.temip_impala_terminated_alarms,\n\ntemip.temip_kudu_terminated_alarms\n```",
            "Actions Taken": "",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1726312",
        "Description": "abc NPCE :  BigStreamer - BackEnd",
        "Keywords": [
            ""
        ],
        "Owner": "",
        "Date": "20211220",
        "Status": "<br>\nInfo: [info](abc/BigStreamer/issues/20211220-IM1726312.md)<br>\n\n<b>Issue Number:</b> IM1757150<br>\nDescription: brond.brond_retrains_hist missing data<br>\nKeywords: <br>\nOwner: <br>\nDate: 20220103<br>\nStatus:",
        "Info": "abc/BigStreamer/issues/20220103-IM1757150.md",
        "Detailed Description": {
            "Description": "```\nPlease load brond.brond_retrains_hist table for 01/01, 02/01 and 03/01\n```\n\n0. Flow info:\n```runs every day via crontab at 08:00: \nun2:/shared/abc/brond/DataParser/scripts/brond_retrains.pl\nas intra \nConnects to sftp 172.16.166.30\n\ntakes parameters from :\n/shared/abc/brond/DataParser/scripts/transferlist/brond_retrains.trn\n\n/shared/abc/brond/bin/000_brond_retrains_ops.sh runs after /shared/abc/brond/DataParser/scripts/brond_retrains.pl\n\nLOGs: \n  - /shared/abc/brond/DataParser/scripts/log/brond_rollout_cron.* for /shared/abc/brond/DataParser/scripts/brond_retrains.pl\n  - /shared/abc/brond/log/000_brond_retrains_ops.* for /shared/abc/brond/bin/000_brond_retrains_ops.sh\n```",
            "Actions Taken": "1. Verify that there are no data for the corresponding par_dts (Impala shell) by choosing a convenient date:\n```\nselect count(*), par_dt from brond.brond_retrains_hist where par_dt >= '20221231' group by 2 order by 2;\n```\n   No data was found for 1/1, 2/1, and 3/1.\n\n2. By checking the **/shared/abc/brond/DataParser/scripts/log/brond_rollout_cron** log file, it was found out that the corresponding par_dts could not be created in Hive because their names, e.g. 20220101, could not be deduced. The brond_retrains.pl script deduces the name of the par_dt it will create by examining the name of the uploaded files on the SFTP server. The names of the uploaded files must have the following format:\nCounter_Collection_24H.<number>_<yyyy>_<mm>_<dd>.csv\nHowever, that was not the case for the files corresponding to those days.\n\n3. We asked abc to re-upload the files with correct names.",
            "Affected Systems": "Not specified",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM2104114",
        "Description": "<br>\nKeywords: <br>\nOwner: <br>\nDate: 20230313<br>\nStatus: <br>\nInfo: [info](abc/BigStreamer/issues/20230313-IM2104114.md)<br>\n\n<b>Issue Number:</b> IM2386183<br>\nDescription:",
        "Keywords": [
            ""
        ],
        "Owner": "<br>\nDate: <br>\nStatus: <br>\nInfo: [info](abc/BigStreamer/issues/SD2453618-IM2386183.md)<br>\n\n<b>Issue Number:</b> IM1353607<br>\nDescription: <br>\nKeywords: <br>\nOwner: <br>\nDate: <br>\nStatus: <br>\nInfo: [info](abc/BigStreamer/issues/X20201202-IM1353607.md)<br>\n\n<b>Issue Number:</b> GI13<br>\nDescription: <br>\nKeywords: <br>\nOwner:",
        "Date": "20210421",
        "Status": "Closed",
        "Info": "abc/BigStreamer/issues/20210506-GI13.md",
        "Detailed Description": {
            "Description": "```\nTicket/Question for Missing Data at Radius Flow\n```",
            "Actions Taken": "**Question** :  Does  **radius.radacct_orig_files** table contains missing data ?\n\n   **Compare files from SFTP Repository with files in Table**\n\n   e.g\n\n **Sftp Repo Contents :**\n\n```\n-rw-r--r-- 1 intra intra 219749225 May  6 11:40 radacct_2021-05-04_00-00.csv.bz2\n-rw-r--r-- 1 intra intra 219497773 May  6 11:40 radacct_2021-05-04_01-30.csv.bz2\n-rw-r--r-- 1 intra intra 219166609 May  6 11:40 radacct_2021-05-04_03-00.csv.bz2\n-rw-r--r-- 1 intra intra 219090980 May  6 11:40 radacct_2021-05-04_04-30.csv.bz2\n-rw-r--r-- 1 intra intra 218865632 May  6 11:40 radacct_2021-05-04_06-00.csv.bz2\n-rw-r--r-- 1 intra intra 219100909 May  6 11:41 radacct_2021-05-04_07-30.csv.bz2\n-rw-r--r-- 1 intra intra 219262945 May  6 11:41 radacct_2021-05-04_09-00.csv.bz2\n-rw-r--r-- 1 intra intra 219734952 May  6 11:41 radacct_2021-05-04_10-30.csv.bz2\n-rw-r--r-- 1 intra intra 219753745 May  6 11:41 radacct_2021-05-04_12-00.csv.bz2\n-rw-r--r-- 1 intra intra 219985878 May  6 11:41 radacct_2021-05-04_13-30.csv.bz2\n-rw-r--r-- 1 intra intra 220428037 May  6 11:41 radacct_2021-05-04_15-00.csv.bz2\n-rw-r--r-- 1 intra intra 220573605 May  6 11:42 radacct_2021-05-04_16-30.csv.bz2\n-rw-r--r-- 1 intra intra 220440718 May  6 11:42 radacct_2021-05-04_18-00.csv.bz2\n-rw-r--r-- 1 intra intra 220170325 May  6 11:42 radacct_2021-05-04_19-30.csv.bz2\n-rw-r--r-- 1 intra intra 220153678 May  6 11:42 radacct_2021-05-04_21-00.csv.bz2\n-rw-r--r-- 1 intra intra 220329041 May  6 11:42 radacct_2021-05-04_22-30.csv.bz2\n```\n\n**radius.radacct_orig_files**\n\nCommand : show files in <table>\n\n```\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_00-00.csv.20210504_001006.utc | 823.99MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_01-30.csv.20210504_021002.utc | 822.76MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_03-00.csv.20210504_031002.utc | 821.74MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_04-30.csv.20210504_051001.utc | 821.36MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_06-00.csv.20210504_061002.utc | 821.05MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_07-30.csv.20210504_081003.utc | 821.63MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_09-00.csv.20210504_091002.utc | 822.62MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_19-30.csv.20210504_221010.utc | 825.45MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_21-00.csv.20210504_221010.utc | 825.52MB |           |\n| hdfs://nameservice1/ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_2021-05-04_22-30.csv.20210504_231007.utc | 825.74MB |           |\n```\n\n---\n\n**Answer (Yes ) :**\n \nIf files exist at **radius.radacct_orig_files** \n\na. How to Detect Missing Files \n\n  1. Checking Hourly Data at radacct_hist table . \n\n\n| hour| rows |\n| ------ | ------ |\n| ..| .. |\n| 16 | 1489477 |\n| 17             | 1441560 |\t\t\n| **18**             | **732**     |\t\t\n| **19**             | **739837**  |\t\n| 20             | 1450106 |\t\n| ..| .. |\n\nIn this case missing file is the one with timestamp after the last hour with less data : ( 1930 ) \n\n\n  2. Checking the OPS cron log file \n\n    File : **/shared/abc/radius/log/000_radius_ops.20210506.log**\n  \n -   Normal Entry (1) ( file exists , impala table populated ) :\n\n```\nINFO: 2021-05-06 05:14:36 --> insert into radius.RADACCT_HIST completed. 2174145 rows\nINFO: 2021-05-06 05:14:36 --> HDFS:Clean-up RAD___radacct_*.utc files\nhdfs dfs -rm -skipTrash /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_04-30.csv.20210506_051001.utc 2>/dev/null\nDeleted /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_04-30.csv.20210506_051001.utc\n```\n\n -   Normal Entry (2) (No file exists , impala table not populated ) :\n\n```\nINFO: 2021-05-06 07:11:01 --> insert into radius.RADACCT_HIST completed. 0 rows\nINFO: 2021-05-06 07:11:01 --> HDFS:Clean-up RAD___radacct_*.utc files\n```\n\n- Abnormal Entry : ( File exists , Impala table NOT Populated ) \n\n\n```\nINFO: 2021-05-06 08:13:02 --> insert into radius.RADACCT_HIST\nINFO: 2021-05-06 08:13:26 --> insert into radius.RADACCT_HIST completed. 0 rows\nINFO: 2021-05-06 08:13:26 --> HDFS:Clean-up RAD___radacct_*.utc files\nhdfs dfs -rm -skipTrash /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_07-30.csv.20210506_081001.utc 2>/dev/null\nDeleted /ez/warehouse/radius.db/radacct_load/RAD___radacct_2021-05-06_07-30.csv.20210506_081001.utc\n```\n\n - **completed. 0 rows**\n - File **RAD___radacct_2021-05-06_07-30.csv.20210506_081001.utc** should be copied \n\nb. Copy missing hdfs files from radacct_orig_files to radacct_load\n\n```\n  hdfs dfs -cp /ez/warehouse/radius.db/radacct_orig_files/RAD___radacct_20xx-yy-zz_rr-mm.csv.yyyymmdd_hhmmss.utc /ez/warehouse/radius.db/radacct_load/\n ....\n```\n\nc. Execute post script :\n\n\n```\n/shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.`date '+\\%Y\\%m\\%d'`.manually.log\n```\n\n\n---\n**Answer (No ) :**  \n\nIf files do not exist at **radius.radacct_orig_files** , execute following steps : \n\n\n1. sftp Requested  Files from radius sftp server\n\n2. mv files to\n\n  [Remfgh]\n`local_spool_area=\"/shared/radius_repo/cdrs\"`\n\n3. Modify file  **radius.trn** :\n\nDefault Status :\n\n```\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n#-- local executions --\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n```\n\nWhen local file is used :\n\n```\n #file_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n #-- local executions --\n file_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n```\n\n4.Check if entry @ **/shared/radius_repo/radius_date.dat.local** , exist\n\ne.g\n\n```\n[File]\nlatest_file=\"/shared/radius_repo/cdrs/radarchive_2019-06-12.csv.bz2\"\n```\n\nand it is older than the new files arrived .\nIf not , create a dummy file on /shared/radius_repo/cdrs/ with older date from new files , and update /shared/radius_repo/radius_date.dat.local accordingly\n\n5.Then execute the respective commands :\n\n```\n-  /shared/abc/radius/DataParser/scripts/radius.pl  -l -d -D -o >> /shared/abc/radius/DataParser/scripts/log/radius_cron_manual.log  2>&1\n- /shared/abc/radius/bin/000_radius_ops.sh >> /shared/abc/radius/log/000_radius_ops.manual.log 2>&1\n```\n\n\n6. Rollback trn file to initial mode :\n\n\n```\nfile_latest_timestamp=\"/shared/radius_repo/radius_date.dat\"\n#-- local executions --\n#file_latest_timestamp=\"/shared/radius_repo/radius_date.dat.local\"\n```\n\n---\n**Important :**\n\n**All above actions should not be performed during scheduled crontab job for Radius ( Usually xx:10 ~ xx:15 ) \nCheck respective cron logs before manual executions described above .**\n\n---\n**Useful SQL statements :**\n\n```\nselect hour(acctupdatetime) acctupdatetime, count(*)\n cnt\nfrom radius.radacct_orig_files where acctupdatetime like '2021-05-04%'\ngroup by 1 order by 1;\n```\n\n```\nselect hour(acctupdatetime) acctupdatetime, count(*)\n cnt\nfrom radius.radacct_hist where par_dt='20210504'\ngroup by 1 order by 1;\n```",
            "Affected Systems": "abc Bigstreamer Radius",
            "Action Points": "```\nVerufy  Procedure Execution  by using mentioned Sql Statements\n```",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    },
    {
        "Issue Number": "IM1665032",
        "Description": "",
        "Keywords": [
            ""
        ],
        "Owner": "",
        "Date": "20211006",
        "Status": "",
        "Info": "abc/BigStreamer/issues/20211006-IM1665032.md",
        "Detailed Description": {
            "Description": "```\nGood morning, can you please check the brond.an_rollout_data_hist table because they have stopped loading data since 24/9.\nThanks\n```\n\n0. Flow info:\n```runs every day via crontab at 02:00: \nun2:/shared/abc/brond/DataParser/scriptsRollout/brond_rollout.pl\nas intra \nConnects to sftp 172.16.166.30\n\ntakes parameters from :\n/shared/abc/brond/DataParser/scriptsRollout/transferlist/brond_rollout.trn\n\n/shared/abc/brond/bin/000_brond_rollout_post.sh 20211006\n\nruns through:  brond_rollout.pl\n\nLOGs : /shared/abc/brond/log/brond_rollout_cron.*\n```",
            "Actions Taken": "1. Following query shows last 10 loads (it is Normal fow weekends to have no data):\n```select par_dt,count (*) from brond.an_rollout_data_hist group by 1 order by 1 desc limit 10; \nQuery: select par_dt,count (*) from brond.an_rollout_data_hist group by 1 order by 1 desc limit 10\nQuery submitted at: 2021-10-07 12:05:44 (Coordinator: http://sn65.bigdata.abc.gr:25000)\nQuery progress can be monitored at: http://sn65.bigdata.abc.gr:25000/query_plan?query_id=70404f65e4fa418c:fc1d536d00000000\n+----------+----------+\n| par_dt   | count(*) |\n+----------+----------+\n| 20211001 | 27673    |\n| 20210930 | 27673    |\n| 20210929 | 27673    |\n| 20210928 | 27673    |\n| 20210927 | 27671    |\n| 20210924 | 27671    |\n| 20210923 | 27671    |\n| 20210922 | 27671    |\n| 20210921 | 27671    |\n| 20210920 | 27671    |\n+----------+----------+\n```\n\n2. Checked log /shared/abc/brond/log/brond_rollout_cron.xxx  at un2, \nit shows : \n```\n...\nWARNING: Use \"yarn jar\" to launch YARN applications.\n...\nERROR: AnalysisException: Column/field reference is ambiguous\n...\nWARNINGS: No partitions selected for incremental stats update\n...\n```\n\n3. Due to upgrade, the following change was required at the \"/shared/abc/brond/bin/000_brond_rollout_post.sh\" cript:\n```  change :\nfrom :\n( select eett,dslam, *colid*,colvalue from brond.brond_rollout_data_hist where par_dt='20210927' ) d on c.colid=*d.colid*\n \nto :\n( select eett,dslam, **colid colid1**,colvalue from brond.brond_rollout_data_hist where par_dt='20210927' ) d on c.colid=**d.colid1**\n``` \n \n\n4. to reload missing data eg for dates 20211003-7 , run :\n```   \n /shared/abc/brond/bin/000_brond_rollout_post.sh 20211007\n /shared/abc/brond/bin/000_brond_rollout_post.sh 20211006\n /shared/abc/brond/bin/000_brond_rollout_post.sh 20211005\n /shared/abc/brond/bin/000_brond_rollout_post.sh 20211004\n /shared/abc/brond/bin/000_brond_rollout_post.sh 20211003\n``` \n \n5. Check again with following query shows last 10 loads (it is Normal fow weekends to have no data):\n```select par_dt,count (*) from brond.an_rollout_data_hist group by 1 order by 1 desc limit 10; \nQuery: select par_dt,count (*) from brond.an_rollout_data_hist group by 1 order by 1 desc limit 10\nQuery submitted at: 2021-10-07 12:05:44 (Coordinator: http://sn65.bigdata.abc.gr:25000)\nQuery progress can be monitored at: http://sn65.bigdata.abc.gr:25000/query_plan?query_id=70404f65e4fa418c:fc1d536d00000000\n+----------+----------+\n| par_dt   | count(*) |\n+----------+----------+\n| 20211007 | 27673    |\n| 20211006 | 27673    |\n| 20211005 | 27673    |\n| 20211004 | 27673    |\n| 20211001 | 27673    |\n| 20210930 | 27673    |\n| 20210929 | 27673    |\n| 20210928 | 27673    |\n| 20210927 | 27671    |\n| 20210924 | 27671    |\n+----------+----------+\n```",
            "Affected Systems": "abc Bigstreamer",
            "Action Points": "Not specified",
            "Customer Update": "Not specified",
            "Our Ticket Response": "Not specified",
            "Resolution": "Not specified",
            "Recommendations": "Not specified",
            "Root Cause Analysis": "Not specified",
            "Investigation": "Not specified",
            "References": "Not specified",
            "Nfgh": "Not specified"
        }
    }
]