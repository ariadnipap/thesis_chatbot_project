# mno - BigStreamer - IM1681883 - hdfs issue on cloudera manager


<b>Description:</b>

```
Today 20/10/2021 appeared the following issue on cloudera manager

hdfs: Datanode Data Directory Status
```

<b>Actions Taken:</b>

1. Login to Cloudera Manager in DR and check the alarm
2. It was a block count alarm on `dr1node09`
3. Login to `dr1edge01` with your personal account and execute `firefox` to view the Namenode UI.
4. Go to `https://dr1node02.mno.gr:50470` and from there in the tab `Datanodes`. Order the datanodes using the block count by desceding order to overview the situation.
5. Change user to PRODREST and use HDFS command or Impala query to check how many partitions exist in `prod_trlog_ibank.service_audit_old`. This is a big table that has no retention mechanism yet so its data are stored in many blocks. HDFS command is `hdfs dfs -ls /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit_old` and Impala query is `show partitions prod_trlog_ibank.service_audit_old`. In either case observe how many `par_dt` directories exist.
6. In our case there were at least 6 months in there so we informed mno. Example mail:
```
Good evening,
Regarding ticket SD1734269, the error shown in Cloudera Manager refers to a block count threshold exceeded on dr1node09 and is due to the increased number of blocks stored on the new nodes dr1node07-10. The impact on the datanode lies in the performance as the metadata for the blocks it has in memory does not fit.
From the investigation we did in the HDFS data, we saw that there are many files in the prod_trlog_ibank.service_audit_old table. Specifically, there is history in it from 01/03.
By keeping only recent data through a retention mechanism, the total block count in HDFS and on each datanode individually will be reduced.
As an immediate solution, we suggest deleting data older than 40 days, as has been done in the past. Consequently, it will not be possible to investigate problems that occurred older than 40 days.
Please let us know if you agree with the above and when you would like us to proceed with the immediate solution.
Thank you,
```
	Recipients are `ZEVGAROPOULOU.GEORGIA@mno.gr,krekoukias.konst@mno.gr,papakostas.athanasios@mno.gr`. Add our team and kbikos in CC.
5. When mno agreed to the immediate action, we informed the monitoring team to ignore alarms in Cloudera Manager of DR site regarding HDFS, Hive, Impala and YARN. Their email address is `csocmonitoringops@jkl-telecom.com`. Example mail:
```
Good evening,
We will proceed with actions on the National Bank of Greece Disaster Site infrastructure.
During this time, you will probably have alarms in the Cloudera Manager UI related mainly to the Health of HDFS, Hive, Impala, Yarn.
The work will start at 16:40 today. We will inform you after the work is completed.
Thank you,
```
8. When the time comes, login to Grafana and check charts of topologies running in DR. Login to Cloudera Manager and check throughout the process the health status of Cloudera services. Ndef that `Hive Metastore Canary` alerts will pop up as expected.
9. Login to dr1edge01 with your personal account and change user to `PRODREST`.
10. Open an impala shell `impala-shell -i pr1edge.mno.gr -k --ssl`.
11. Change database to `prod_trlog_ibank` with `use prod_trlog_ibank;` and `show tables;` to check that `service_audit_old` is here.
12. Drop partitions. Seperate the amount of days you have to delete in batches of 15 days in order to avoid disruptions of applications. `ALTER TABLE service_audit_old DROP IF EXISTS PARTITION(par_dt<=20200715) PURGE;`
12. When the query is done, execute the next one and so on: `ALTER TABLE service_audit_old DROP IF EXISTS PARTITION(par_dt<=20200801) PURGE;`
13. Can't stress this enough. **Throughout steps 12, 13 check Grafana and Cloudera Manager**.
14. When done, inform mno and the monitoring team that actions have been successfully finished.

<b>Affected Systems:</b>

mno Bigstreamer

<b>Action Points:</b>

Develop retention mechanism to automatically drop old partitions.


