# abc - BigStreamer - IM2172470 - abc (492) BigStreamer - CDH - energy_efficiency.pollaploi

<b>Description:</b>

```
Good morning,
we have a problem with the pollaploi job located in the Energy Bills project in impala
the frequency of it failing has increased quite a bit
```

<b>Actions Taken:</b>

After communication with customer we undestand that the issue occurs for job at workbench and not for flow. So:

1. Login to Cloudera Data Science Workbench with your personal account (https://mncdsw1.bigdata.abc.gr/)

2. Click on the left **Sessions** tab and then on **Scope** select **All Projects** and click on **Energy Bills** Project and find **Pollaploi** job.


3. Go on **History** tab and you will see that there are a lot of pollaploi jobs with status Failure

4. Click on one job with status Failure and then go to **See job details** and then click on **Script: Energy_Bills_Automation/Energy_Bills_Automation.py**

5. When investigated the script we saw below snippet of spark configuration:

```bash
spark = SparkSession.builder\
.master("yarn")\
.config("spark.submit.deployMode", "client")\
.config("spark.eventLog.enabled", "true")\
.config("spark.executor.instances", "100")\
.config("spark.executor.cores", "2")\
.config("spark.executor.memory", "4g")\
.config("spark.rpc.message.maxSize", "1024")\
.config("spark.executor.memoryOverhead", "800")\
.config("spark.driver.memory", "4g")\
.config("spark.driver.memoryOverhead", "800")\
.config("spark.spark.driver.maxResultSize", "4g")\
.config("spark.executor.dynamicAllocation.initialExecutors", "4")\
.config("spark.executor.dynamicAllocation.minExecutors", "4")\
.config("spark.executor.dynamicAllocation.maxExecutors", "4")\
.config("spark.sql.broadcastTimeout", "1000")\
.config("spark.kryoserializer.buffer.max", "1024m")\
.getOrCreate()
```

So, there are 100 instances * 2 cores = 200 vcores

and 100 instances * 4G ram = 400GB ram

The cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.

<b>Our Ticket Response:</b>

```bash
Good evening,

Upon investigation we noticed that the job you mentioned fails with an out-of-memory error.

Additionally, according to the spark configuration snippet below in your job:

```
.master("yarn")\
.config("spark.submit.deployMode", "client")\
.config("spark.eventLog.enabled", "true")\
.config("spark.executor.instances", "100")\
.config("spark.executor.cores", "2")\
.config("spark.executor.memory", "4g")\
```

We see that you have given 100 instances * 2 cores = 200 vcores and 100 instances * 4Gram = 400GB ram

The cluster currently has 1T of ram, and this job takes up almost 1/2 of the cluster.

Therefore, the problem concerns the specific job. For this reason, jobs should be optimized according to the guidance given in an earlier communication for a similar issue, taking into account the configuration that has already been done in the cluster.

If you don't need anything else please if we can proceed to close the ticket.

Thanks
```

<b>Affected Systems:</b>

abc Bigstreamer CDSW
