CDSW issue -  Engine exited with status 34

<b>Description:</b>

Good morning,

Noticed since 4/8 many job executions in CDSW fail with "Engine exited with status 34" message.
In the logs --> There are no logs for this engine node at this time.
With re-run the jobs are executed normally.
It is observed that many jobs fail during the day.

for example:

https://mncdsw1.bigdata.abc.gr/ccharisis/hardware_failures/engines/0nohdhssxz6uebit

https://mncdsw1.bigdata.abc.gr/ccharisis/forecasting/engines/kx29vx7l91i7x567


<b>Actions Taken:</b>

1. Connect with you personal ldap account in 'https://mncdsw1.bigdata.abc.gr/'
2. Select the `Site Administration` tab (must have admin privileges)
3. Select the `Usage` tab which displays the jobs
4. Inspect the jobs in question.

The jobs are in `FAILED` status. The logs for the failed applications are missing.

5. Troubleshoot from the command line:

From `mncdsw1` as root (use personal account and then sudo):

```bash
kubectl get pods -w -A # Wait a pod to fail (namespace should be like default-user-XXX)

# After a while, a pod has failed, describe it

kubectl describe pod -n default-user-XXX XXXXXXXX
```

In some cases the pod has failed but it cannot be seen by the `kubectl describe pod` command. In those cases, we use the `kubectl get events` command on the same namespace and search for the appropriate pod name.

```bash
kubectl get events -n default-user-XXX
```

In our case, we used the `kubectl get events` command:

```logs
60m         Normal    Scheduled                pod/t804rlnpej08xzcg                  Successfully assigned default-user-49/t804rlnpej08xzcg to wrkcdsw1.bigdata.abc.gr
60m         Warning   FailedCreatePodSandBox   pod/t804rlnpej08xzcg                  Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container "166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e" network for pod "t804rlnpej08xzcg": networkPlugin cni failed to set up pod "t804rlnpej08xzcg_default-user-49" network: unable to allocate IP address: Post "
http://127.0.0.1:6784/ip/166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e":
dial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container "166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e" network for pod "t804rlnpej08xzcg": networkPlugin cni failed to teardown pod "t804rlnpej08xzcg_default-user-49" network: Delete "
http://127.0.0.1:6784/ip/166b7ec672e07672ebcff4c19baebe04b45b86cbd6535107f04ca78379ad5b1e":
dial tcp 127.0.0.1:6784: connect: connection refused]
33s         Normal    SandboxChanged           pod/t804rlnpej08xzcg                  Pod sandbox changed, it will be killed and re-created.
```

The jobs were running for about an hour before failing with the message `Pod sandbox changed, it will be killed and re-created.`

6. Restart the docker daemon to restart all containers on `wrkcdsw1`

_At the time of the issue, CDSW had stale configuration that required full restart (outage) which was not desirable_

To avoid applying the settings, restart the service with the same configuration by triggering a restart by `supervisord` deployed as part of the Cloudera agent

<details> ![Danger ahead](https://media3.giphy.com/media/vvzMdSygQejBIejeRO/200w.gif?cid=6c09b952aacsm9yssw6k6q0z5v8ejuy82rjpvw6qdhglcwpu&rid=200w.gif&ct=g) </details>

From wrkcdsw4 as root (use personal account and then sudo):

``` bash
/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf status | grep DOCKER
# Sample
# 145071-cdsw-CDSW_DOCKER          RUNNING   pid 39353, uptime 29 days, 0:40:20
/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart 145071-cdsw-CDSW_DOCKER
```

8. Check that the node is operational after the restart

From `mncdsw1` as root (use personal account and then sudo):

```bash
cdsw status # You might have to wait a few minutes
```

9. Inform the customer about the problem

```text
Good morning,

We noticed that some jobs on the wrkcdsw1 node were running for about an hour before they failed with the error "Pod sandbox changed, it will be killed and re-created". To solve this particular error, we restarted the cdsw service on the wrkcdsw1 node and noticed that the failures with a duration of 1 hour stopped and the corresponding jobs were executed normally.

Please let us know if you need anything else or if we can proceed to close the ticket.
```

<b>Affected Systems:</b>

abc Bigstreamer CDSW

<b>Action Points:</b>
