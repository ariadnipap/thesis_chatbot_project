# abc - BigStreamer - IM2073052 - Application not working

<b>Description:</b>

```bash
Good morning,

In CDSW we get error "Unexpected Error. An unexpected error occurred" when connecting. We saw that the node mncdsw1.bigdata.abc.gr was down. We did a restrart, just that, and it now appears to be in good health status.

However, we still get the same error.

In CDSW status it has the following message

Failed to run CDSW Nodes Check. * Failed to run CDSW system pods check. * Failed to run CDSW application pods check. * Failed to run CDSW services check. * Failed to run CDSW secrets check. * Failed to run CDSW persistent volumes check. * Failed to run...

Please for your checks.

Thanks
```

<b>Actions Taken:</b>

1. Restart CDSW

   The customer had already restarted CDSW, so we tried it once more in order to live monitor it.

   ```bash
   Cloudera Manager -> CDSW -> Restart
   ```

2. Check status

   We followed the logs until CDSW was available again.

   ```bash
   #from mncdsw1
   cdsw status
   ...
   Cloudera Data Science Workbench is ready!
   ```

   Since CDSW was up and running, we continued with root cause analysis.

3. Check logs

   ```bash
   less /var/log/cdsw/cdsw_health.log
   ```

   Firstly, we noticed an abnormal behavior with some of the control plane pods:

   ```bash
   2023-01-29 05:50:53,868 ERROR cdsw.status:Pods not ready in cluster kube-system ['component/kube-controller-manager', 'component/kube-scheduler'].
   ```

   And after that, CDSW lost connection with apiserver pod completely:

   ```bash
   2023-01-29 05:51:42,392 WARNING urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549bb50>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes
   2023-01-29 05:51:42,735 WARNING urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b710>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes
   2023-01-29 05:51:43,065 WARNING urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f740549b050>: Failed to establish a new connection: [Errno 111] Connection refused',)': /api/v1/nodes
   2023-01-29 05:51:43,371 ERROR cdsw.status:Failed to run CDSW Nodes Check.
   ```

4. Check node resources

   From Cloudera Manager we saw that CPU and Memory were not increased but Disk I/O reached 100%.

   ![IM2073052_diskio](.media/IM2073052_diskio.png)

   From the image above we noticed that the issue occured on dm-7.

   ```bash
   [root@mncdsw1 ~]# ll /dev/mapper/cdsw-var_lib_cdsw
   lrwxrwxrwx 1 root root 7 Dec 16  2021 /dev/mapper/cdsw-var_lib_cdsw -> ../dm-7
   ```

   ```bash
   [root@mncdsw1 ~]# lsblk | grep cdsw-var
   └─cdsw-var_lib_cdsw                                                                         253:7    0   931G  0 lvm  /var/lib/cdsw
   ```

   ```bash
   [root@mncdsw1 ~]# kubectl get pv
   NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS                 REASON   AGE
   0c9df8bb         1Ti        RWX            Retain           Bound    default-user-120/b128af5f   cdsw-storageclass-whiteout            83m
   1214923b         1Ti        RWX            Retain           Bound    default-user-98/1ec1e99a    cdsw-storageclass-whiteout            11m
   1297834a         1Ti        RWX            Retain           Bound    default-user-9/740094c3     cdsw-storageclass-whiteout            54s
   1a2f7a8a         1Ti        RWX            Retain           Bound    default-user-9/92acb87f     cdsw-storageclass-whiteout            55s
   1f498fe8         1Ti        RWX            Retain           Bound    default-user-120/588500de   cdsw-storageclass-whiteout            106s 
   ```

   ```bash
   [root@mncdsw1 ~]# kubectl get pv 0c9df8bb -o yaml

   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: 0c9df8bb
   spec:
     accessModes:
     - ReadWriteMany
     capacity:
       storage: 1Ti
     mountOptions:
     - nfsvers=4.1
     nfs:
       path: /var/lib/cdsw/current/projects/cdn/4xsyzsv0lnij00ob
       server: 10.255.241.130
   persistentVolumeReclaimPolicy: Retain
     storageClassName: cdsw-storageclass-whiteout
     volumeMode: Filesystem 
   ```

   It seems that every CDSW project uses mncdsw1:/var/lib/cdsw for storage.

5. Check kubelet logs

   ```bash
   ll /run/cloudera-scm-agent/process/ | grep -i master
   ```

   ```bash
   [root@mncdsw1 ~]# ll /run/cloudera-scm-agent/process/145081-cdsw-CDSW_MASTER/logs/
   total 111880
   -rw-r--r-- 1 root root  9658036 Jan 30 10:24 stderr.log
   -rw-r--r-- 1 root root 10485841 Jan 30 05:42 stderr.log.1
   -rw-r--r-- 1 root root 10485989 Jan  4 19:40 stderr.log.10
   -rw-r--r-- 1 root root 10485928 Jan 30 00:20 stderr.log.2
   -rw-r--r-- 1 root root 10486166 Jan 29 18:58 stderr.log.3
   -rw-r--r-- 1 root root 10485841 Jan 29 13:36 stderr.log.4
   -rw-r--r-- 1 root root 10485790 Jan 29 08:06 stderr.log.5
   -rw-r--r-- 1 root root 10485858 Jan 25 16:41 stderr.log.6
   -rw-r--r-- 1 root root 10485835 Jan 21 08:56 stderr.log.7
   -rw-r--r-- 1 root root 10485760 Jan 15 14:47 stderr.log.8
   -rw-r--r-- 1 root root 10485805 Jan 10 11:57 stderr.log.9
   -rw-r--r-- 1 root root    12055 Nov 21 14:58 stdout.log
   ```

   In stderr.log.5 file there were many log entries indicating a problem with etcd.

   ```bash
   I0129 05:50:11.022246   89953 prober.go:117] Liveness probe for "etcd-mncdsw1.bigdata.abc.gr_kube-system(ef618d8c591c98ed7bd7d66b177d34f7):etcd" failed (failure): HTTP probe failed with statuscode: 503
   ```

   ```bash
   E0129 05:51:22.881553   89953 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-mncdsw1.bigdata.abc.gr.17299b09446544c4", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"27938507", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"etcd-mncdsw1.bigdata.abc.gr", UID:"ef618d8c591c98ed7bd7d66b177d34f7", APIVersion:"v1", ResourceVersion:"", FieldPath:"spec.containers{etcd}"}, Reason:"Unhealthy", Message:"Liveness probe failed: HTTP probe failed with statuscode: 503", Source:v1.EventSource{Component:"kubelet", Host:"mncdsw1.bigdata.abc.gr"}, FirstTimestamp:time.Date(2022, time.November, 21, 15, 0, 1, 0, time.Local), LastTimestamp:time.Date(2023, time.January, 29, 5, 50, 41, 21788692, time.Local), Count:700, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'rpc error: code = Unknown desc = OK: HTTP status code 200; transport: missing content-type field' (will not retry!)
   ```

   Apparently the high DiskI/O was affecting the etcd server.

6. Check warn file logs

   ```bash
   less /var/log/warn
   ...
   Jan 29 05:51:22 mncdsw1.bigdata.abc.gr dockerd[86247]: W0129 03:51:22.867126       1 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {https://127.0.0.1:2379  <nil> 0 <nil>}. E
   rr :connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting..
   ```

   Run the following to check if this error occured in the past.

   ```bash
   cat /var/log/warn | grep  -e "Jan.*29.*grpc: addrConn.createTransport failed to connect to" | less
   ```

   ![IM2073052_warn_logs1](.media/IM2073052_warn_logs1.png)

   ![IM2073052_warn_logs2](.media/IM2073052_warn_logs2.png)

   The errors appear every Sunday morning.

<b>Our Ticket Response:</b>

```bash
Good evening,

from the analysis we saw that CDSW crashed as there was a problem in the Control Plane pods of the Kubernetes Cluster in which CDSW is deployed.  We notice in the logs that the problem started with timeouts in requests to the etcd of the cluster, which seem to be due to a high Disk I/O of the sdb disk of mncdsw1 at that moment. As a result we have the inability to synchronize the control plane pods with the base of the cluster, which led to their termination and by extension the entire service. Attached you will also find the screenshot that describes the high Disk I/O at that time.

Continuing the analysis we noticed that this behavior is periodic, and more specifically it happens every Sunday at 6 am. Attached you will also find the screenshots that show that there was the same problem on January 15, 22 and 29. Before January the logs are clean. Could you tell us if you have any job set up every Sunday morning that needs a lot of Disk I/O?

Thank you
```

<b>Action Points:</b>

We opened [this](https://metis.xyztel.com/obss/oss/sysadmin-group/abc/BigStreamer/bigstreamer/-/issues/25) issue to re-evaluate CDSW disk architecture.
