# mno - IBANK - IM1868465 - Batch Job failed on Grafana

<b>Description:</b>

``` bash
We have the following failed messages on Grafana

application: iBank_Migration
job_name: Historical
component: Impala_Insert
date: 23/05/2022
status: Failed
description:
host: pr1edge01.mno.gr

application: iBank_Migration
job_name: Historical
component: JOB
date: 23/05/2022
status: Failed
description: Impala rcords are less than retrieved sqoop records
host: pr1edge01.mno.gr
```

<b>Actions Taken:</b>

Workaround Steps

1. Check Grafana

   Grafana -> LOCAL MONITOR/ Batch Jobs PR

   ```bash
   Historical | Sqoop_Import : SUCCESS 
   Historical | Impala_Insert : FAILED
   ```

2. Check Main script logs

   ```bash
   sudo su - PRODREST
   crontab -l #find log file
   less /var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log
   ```

   We saw the following:

   ```bash
   restval_sqoop_2=1
   sqoop import failed again
   ```

   According to the log file, the first step of the Main Script failed with exit code 1.
   More Info for the Steps here:[<https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md#mssql-sqoop-import>]

3. Check "MSSQL Sqoop Import" step logs:

   ```bash
   less /opt/icomdev/ibank_hist/prod_migrate/sqoop_`date +%Y%m%d`_1_1_`date +%Y%m%d_%H%M`.log
   ```

   We found out the the INSERT Impala query failed with error: Memory limit exceeded.

4. Backup prod_trlog_ibank.historical_service_audit_raw_v2 files

   ```bash
   hdfs dfs -mkdir /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/20220524_import_historical_service_audit_raw_v2_dvoul

   hdfs dfs -mv /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/import_historical_service_audit_raw_v2/* /mno_data/hive/warehouse/prod_trlog_ibank.db/landing_zone/20220524_import_historical_service_audit_raw_v2_dvoul/
   ```

5. Run Main script with PRDREST user

   ```bash
   screen

   /opt/ingestion/PRODREST/historical/ibank_histMigrate_aggr_MergeBatchWithLock_STABLE_v2.sh >> /var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log
   ```

Root Cause Analysis Steps

Now that we are certain that we have an Impala OOM issue, we have to check Impala resources specifically.

1. From Cloudera Manager select Impala Service and check the Out of Memory Impala Queries Chart

   In our case we had a large number of OOM quries between 23:00 and 02:00

2. Check for large queries in the corresponding time slot

   Cloudera Manager -> Impala -> Queries

   Select those that have a lot of Aggregate Peak Memory Usage from the filters on the left.

<b> Response to the customer </b>

```
Good morning,
The topology has been re-executed and is in running status. The analysis revealed that there was an Out of Memory problem in Impala due to some queries that ran between 23:00 and 02:00 and requested many MB of memory.
These specific queries are hindering the production process. Attached you will find the relevant screenshots.
We ask for your immediate actions as well as for the control of your own flows during the above period.
```

![PR_OOM_Impala_Queries.png](.media/PR_OOM_Impala_Queries.png)
![PR_Queries1.png](.media/PR_Queries1.png)
![PR_Queries2.png](.media/PR_Queries2.png)

<b>Affected Systems:</b>

Primary Site IBank Batch
