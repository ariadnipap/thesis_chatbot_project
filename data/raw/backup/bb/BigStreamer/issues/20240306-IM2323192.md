# mno - BigStreamer - SD2389640 - hdfs - Data Directory Status

<b>Description:</b>

```
Good morning,

The following alert has appeared in Cloudera Manager (DR):
DataNode (dr1node02)
Data Directory Status

Thank you.
```

<b>Actions Taken:</b>

There are references from the similar issue [20220620-SD1951890.md](20220620-SD1951890.md).

After investigation we saw that the problem occurred due to disk issue on dr1node02.

We communicated with Oracle and disk replacement was scheduled.

> Ndef that disk replacement perfomerd online so there was no downtime.

We followed the steps as described at [20220620-SD1951890.md](20220620-SD1951890.md) and [sync_mysql.md](sync_mysql.md), which include the following:

1. Stopping the processes that specifically run at the disk slots `s1` and `s7` of the server `dr1node02`. On our case was the hdfs datanode and some yarn applications . We identified them with:

2. Stopping the mysql slaves using the command:
```
mysql -u root -p
SHOW SLAVE STATUS\G;
```

3. Ensuring that the no processes are running at the partitions with the following commands:
```bash
lsof /u02
```

```bash
lsof /u08
```

4. Unmounting the two partitions, so the disks can be replaced.

```bash
umount <mountpoint>
```

5. Once the disks have been replaced we ran the following command for both partitions:

```bash
bdadiskutility /u02
```

6. After running the command, we got the following error:
```
Virtual Drive <VIRTUAL_DRIVE_NUMBER> is incorrectly mapped.
<TIMESTAMP> : Error executing 'MegaCli64 CfgLdAdd r0[<ENCLOSURE>:<SLOT>] a0'
<TIMESTAMP> : Error code is 84 . Response is <<
Adapter 0: Configure Adapter Failed

FW error description:
The current operation is not allowed because the controller has data in cache for offline or missing virtual disks.

Exit Code: 0x54>>
Found a disk with a Firmware State of Unconfigured(good).
Successfully cleared the cache for the logical drive.
Successfully added the disk to its own RAID(0) volume.
```

7. After communicating with Oracle Support [SR 3-36895603206 : Wrong disk status after replacement](https://support.oracle.com/epmos/faces/SrDetail?_afrLoop=206254157461870&srNumber=3-36895603206&queryModeName=Technical&needSrDetailRefresh=true&_afrvwxowMode=0&_adf.ctrl-state=iwvcvrye_184), we ran the following commands to solve the issue:


- `For s1 # The disk slot 1 of the server that corresponds to mount point /u02`
- `For s7 # The disk slot 7 of the server that corresponds to mount point /u08`

- Validated if there is a cache pinned for any device, running command:

```bash
MegaCli64 -GetPreservedCacheList -a0 
 ```

If the old disk has pinned the cache, the command will return something like:

```
Adapter #0
 
Virtual Drive(Target ID 07): Missing.
 
Exit Code: 0x00
```
- In this case, the disk in slot 7 had the pinned cache and had to clear.

Remove the pinned cache by running command:

```bash
#MegaCli64 -DiscardPreservedCache -L7 -force -a0 <<<< where -LX should be replaced by the Target ID number reported in previous step.
```
Get the `ENCLOSURE_NUMBER`
```bash
MegaCli64 LdPdInfo a0 | more
```
- Added the virtual disk back

```bash
MegaCli64 CfgLdAdd r0[ENCLOSURE_NUMBER:slot] a0
```
On our case was:

For `s1`
```bash
MegaCli64 CfgLdAdd r0[252:1] a0
```

For `s7`

```bash
MegaCli64 CfgLdAdd r0[252:7] a0
```

Started configuring the disk at `slot1`

```bash
bdadiskutility -f /u02
```

Wait until the mirroring is finished and after that.

Started configuring the disk at `slot7`

```bash
bdadiskutility -f /u08
```

- Checks:

For `s1`:

```bash
parted /dev/disk/by-hba-slot/s1 -s unit chs print
iscsi # Check that all disks appeared
lsblk # Check that all disks appeared
```

For `s7`:

```bash
parted /dev/disk/by-hba-slot/s7 -s unit chs print
iscsi # Check that all disks appeared
lsblk # Check that all disks appeared
```

8. We proceed with the start of the `datanode` role of `dr1node02`
