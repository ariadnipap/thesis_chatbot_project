# mno - BigStreamer - IM2285747 - merge batch 29/2

<b>Description:</b>

```
Good evening,

could you please check the merge batch in DR/pr for 2/29. It seems to be still running. We have disabled it on 1/31 until this is finished.

Thanks,
Thanos
```

<b>Investigation:</b>

**Ndef**: All log paths and query executions are found/executed from pr1edge01/dr1edge01.

1. Login to Grafana and make sure that the failed step is the Merge Batch.
2. Login to DR/PR edge nodes and through the node's firefox check YARN at https://dr1node03:8090 and https://pr1node03:8090 for the PROD_Ibank merge batch job.
3. Check the stages tab for stages that have been completed for this job. At PR the `insert into` stages had completed after 8h. The RDD stages had failed and continued failing.
![Yarn UI](.media/IM2285747_1.png)
![Spark Stages](.media/IM2285747_2.png)
4. The 8 hour mark gives us some clues as to the failure. In essence after 8 hours kerberos tickets are dropped, leading to continuous authentication failures and timeouts as shown in the logs `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log`.
5. Checked the merge batch logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_histMigrate_aggr_MergeBatchWithLock_v2.log` and `/var/log/ingestion/PRODREST/ibank/log/cronExecutor_ibankBatch_full.log` and indeed we found authorization errors.
6. Checked the size and count of the `service_audit_old` table through Impala and HDFS and it was found to be among the largest ever both in size (over 115 GB) and count (60 mil). The query used is `select  count(*) from prod_trlog_ibank.service_audit_old where par_dt=20240229`.
7. Inspecting the submition script we can see that for each individual sub-script the date is generated anew in the sub-script invocation. This means that since the merge in question spanned multiple dates, all steps started after the day's end wouldn't have completed succesfully.
8. Inspecting the range partitions through Impala with the query `show range partitions prod_trlog_ibank.service_audit_stream`, we can find multiple leftover range partitions since November. This added further computation time for the already large table.

Investigation for 05/03:

For this day, while the size and count were not unusually large the last step of the process, ie. the
hbase upsert took unusually long and couldn't complete on its own. The process seemed to get stuck on 1
singular task as shown by the logs at `/var/log/ingestion/PRODREST/ibank/log/ibank_visible_trn_hbase_daily_upsert.log`, which leads us to conclude that a certain record was problematic because all tasks completed
normally except a specific one that kept failling even on re-runs.

Investigation - Summary:

- Greater than most end of the month dates size(over 115 GB) and count (60 mil)
- Multiple leftover range partitions
- Execution spanning multiple days, triggering the dynamic date issue with the submition script
- Stage execution surpassing the 8 hour mark, triggering a known kerberos authorization bug
- Problematic record for 05/03

<b>Resolution:</b>

1. Since the data was already loaded into PR for 29/2, the rest of the steps were executed manually on it
   according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#batch instructions for
   steps beyond the Merge Batch .
2. For all the remaining dates since 29/2 and because the cron jobs were stopped, each day was executed
   manually in full in each site, half of them (days) in PR and half of them (days) in DR following
   all the steps in the sub-steps guide from https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md .
3. For each job completed in one site it was replicated over to the other using HDFS replication, through
   the destination's Cloudera Manager for both `service_audit` and `service_audit_old` tables. **A similar
   procedure for table replication exists in [Table Replication](./20201218-IM1389913.md) but not for HDFS replication.**
   - /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit_old/par_dt=$date
   - /mno_data/hive/warehouse/prod_trlog_ibank.db/service_audit/par_dt=$date
4. Once replicated the HBase Upsert step was run on the destination site according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .
5. HBase markers were manually set for each job that didn't complete automatically according to https://metis.ghi.com/obss/oss/sysadmin-group/support/-/blob/master/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/ibank.md?ref_type=heads#upsert-to-hbase-migration .
6. Kudu range partitions were cleaned up manually for all previous days and 3 new partitions were created
   for the 3 previous days, in order for the cleanup script to continue functioning as nornal. More details
   on this can be found at the end of this document.


Resolution for 05/03 HBase Upsert:

For this operation specifically more resources had to be allocated to the spark-submit job at `/opt/ingestion/PRODREST/ibank/spark/submit/visible_trn_hbase_daily_upsert/submitVisibleTrnToHbaseIndexesDailyUpsert_STABLE.sh`. After its completion resources were reverted back to normal:
```
Increase node count to 12
Decrease core count to 1
Increase tasks to 40
```

Kudu Range Partitions:

Range partitions are created for the `service_audit_stream` table on the `u_timestamp` column. The commands to create and delete them
can be found below for some example dates. Ndef that those partitions are in UTC time, so the time to
create/drop must be converted to local time, taking into account DST. For winter we are at GMT+2 so in order
to include a full day it must range for 22:00 of the previous to 22:00 of the current (where current is the day you want to delete).

```sh
# Drop
alter table prod_trlog_ibank.service_audit_stream drop range partition '2023-02-26T22:00:00.000000Z' <= VALUES < '2023-02-27T22:00:00.000000Z' ;

# Create
alter table prod_trlog_ibank.service_audit_stream add range partition '1970-01-01T00:00:00.000000Z' <= VALUES < '2023-02-28T22:00:00.000000Z' ;
```

The example above includes the first partition, which tracks from 1970 to the date in question. When deleting previous partitions, the first one
must be recreated in order to include the time from 1970 to the first date, and then you can create the daily partitions. For example, lets say we have
the following partitions:

```
| 1970-01-01T00:00:00.000000Z <= VALUES < 2023-11-29T22:00:00.000000Z | # We want to drop this one in order to include a bigger range
| 2023-11-29T22:00:00.000000Z <= VALUES < 2023-11-30T22:00:00.000000Z | # We want to drop this one due to policy
| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.
| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.
| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # We want to drop this one because it contains trash. Ndef the merge has completed but didn't cleanup.
| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This must not be touched
| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |
| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |
| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |
```

The above will become:

```
| 2023-11-30T22:00:00.000000Z <= VALUES < 2023-12-01T22:00:00.000000Z | # This was created by including all dates from 1970 to this one
| 2023-12-01T22:00:00.000000Z <= VALUES < 2023-12-02T22:00:00.000000Z | # This was dropped and recreated
| 2023-12-02T22:00:00.000000Z <= VALUES < 2023-12-03T22:00:00.000000Z | # This was dropped and recreated
| 2023-12-03T22:00:00.000000Z <= VALUES < 2023-12-04T22:00:00.000000Z | # Current. This was not touched
| 2023-12-04T22:00:00.000000Z <= VALUES < 2023-12-05T22:00:00.000000Z |
| 2023-12-05T22:00:00.000000Z <= VALUES < 2023-12-06T22:00:00.000000Z |
| 2023-12-06T22:00:00.000000Z <= VALUES < 2023-12-07T22:00:00.000000Z |
```
