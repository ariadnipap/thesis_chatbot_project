# mno - BigStreamer - IM1896751 - Alert in Grafana Application

<b>Description:</b>

```
An Alert appeared in Grafana Application.

[DR][ONLINE] Spark Waiting Batches alert.

Please for your attention.

Thank you.
```

<b>Actions Taken:</b>

1. Login to `dr1edge01` and open firefox
2. At the YARN UI search for `PRODREST` and sort by End date. You will find the failed application.
3. From the UI we saw that Spark exited due to errors related to HBase timeouts.
4. From DR site's Cloudera Manager we saw that HBase Regionservers restarted due to Out-Of-Memory errors and are now healthy again.
5. Using this [document](/KnowledgeBase/mno/BigStreamer/supportDocuments/applicationFlows/online.md#spark-streaming) we re-submitted the failed topology and the alarm was cleared:
    ```
    /opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh
    ```
6. We informed the customer that a **workaround** has been implemented. The ticket status from this point is "Workaround Provided".
7. The logs and resources charts from Cloudera Manager did not indicate a specific reason for the restarts.
8. After investigating HBase tables we identified that 2 tables from the `PROD_BANK` namespace although they have low traffic overall, they have high traffic per region. Since each region is hosted on only one Regionserver, so this application pattern creates hotspots on specific servers and does not utilize the whole cluster properly. Below you can find the charts that show this behavior.

Workload per region

```
select delete_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'
```

![DR_DELETE.PNG](.media/DR_DELETE.PNG)
![DR_DELETE_12H.PNG](.media/DR_DELETE_12H.PNG)

```
select get_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'
```

![DR_GET.PNG](.media/DR_GET.PNG)
![DR_GET_12H.PNG](.media/DR_GET_12H.PNG)

```
select increment_rate_across_hregions + append_rate_across_hregions + mutate_rate_across_hregions WHERE category = HTABLE and htabIeName RLIKE 'PROD*'
```

![DR_WRITE.PNG](.media/DR_WRITE.PNG)
![DR_WRITE_12H.PNG](.media/DR_WRITE_12H.PNG)

Uneven workload between Regionservers

```
select read_requests_rate, write_requests_rate
```

![DR_LOAD_REGIONS.PNG](.media/DR_LOAD_REGIONS.PNG)
![DR_LOAD_REGIONS_12H.PNG](.media/DR_LOAD_REGIONS_12H.PNG)

9. Inform the customer of the findings
```
The problem did not occur today. Looking at the overall picture of HBase, we notice that its usage has increased significantly. Specifically, we see tables in the Namespace "PROD:BANK" with a high workload throughout the day and with an application-level configuration that creates uneven load distribution between Region Servers.

You should immediately proceed with the actions that have been suggested for benchmarking and tuning HBase as a service and the applications that use it in order to optimize its use. If there is nothing else, please let us know if we can proceed with closing the ticket.
```

<b>Affected Systems:</b>

Disaster Site HBase

<b>Action Points:</b>
