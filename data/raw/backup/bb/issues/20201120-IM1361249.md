# mno - BigStreamer - IM1361249 - Spark Waiting Batches Alert - Grafana 

<b>Description:</b>

```
[DR][IBANK] Spark Waiting Batches alert
[DR][ONLINE] Spark Waiting Batches alert

```

<b>Actions Taken:</b>

1. Login to `https://dr1edge01.mno.gr:3000` with personal account
2. Inspected `Monitoring Alerts` and `Monitoring DR\PR`
3. From the `Monitoring Alerts` check the graphs `spark waiting batches` to find which topology has delays`
4. Open MobaXterm `dr1edge01` ssh with your personal account
5. Execute `firefox`
6. Click the `DR` bookmark
7. Check the logs of failed spark topology.
8. Login to `DR cloudera manager` with your personal account
9. Go to `CHARTS-->KAFKA_KUDU_DISK_UTIL` and see if abnormal rates on disk util exists
10. Go to `firefox from moba` and check for every kudu tablet on `DR SITE` the `memory(detail)`. If its greater than 90% then restart the tablet instance on `DR KUDU`.
11. Check again the graphs `Monitoring Alerts` and `Monitoring DR\PR`
12. If the alert appeared `(in our case appeared on PROD_Online_IngestStream topology)` restart the specific topology
13. Open a new tab on MobaXterm `dr1edge01` ssh with your personal account
14. sudo -iu `PRODREST`
15. yarn application -list | grep "PROD_Online_IngestStream"
16. yarn application -kill `<application_id>`
17. Start again the topology `/opt/ingestion/PRODREST/online/spark/submit/submitmnoSparkTopology_stream_cluster_mno_STABLE.sh`
18. Check from Graphana again the graphs `Monitoring Alerts` and `Monitoring DR\PR`.
19. Go to spark jobs history and click running.
20. Click `Running-->Online_Ibank_IngestStream-->Streaming`
21. Refresh every 1sec to until the `active batches` is 0.


<b>Affected Systems:</b>

Disaster Site IBANK