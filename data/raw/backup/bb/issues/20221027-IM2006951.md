# mno - BigStreamer - IM2024442 - Failed job at Grafana

<b>Description:</b>

```bash
Application: DWH_IBank
Job Name: EXPORT
Component: CARD
Date: 26/10/2022
Status: FAILED
Description: Code:6
```

<b>Actions Taken:</b>

1. Login to `https://dr1edge01.mno.gr:3000` with personal account and confirm that Datawarehouse Flows have failed from `Monitoring/Monitoring PR/DR` dashboard.
2. All flows have failed with `Code: 6` which means that the control script has timed-out while monitoring the `EXTRACT` script. The `EXTRACT` step has 2 sub-steps: `Impala Insert` and `Sqoop Export`.
3. Check logs

    From `dr1edge01.mno.gr` with personal account:

    ``` bash
    less /var/log/datawarehouse-ibank/PRODUSER/sched_extract.log
    ```

    In this file for all flows that failed we see that the last log entry is the submission of the `Impala Insert` part of the `EXTRACT`, which was still running. This means that another query is hogging all resources for Impala and our flows are waiting to be executed.

4. Login to Disaster Site Cloudera Manager `https://dr1edge01.mno.mno.gr:7183` and check for resource intensive Impala queries `Clusters > Impala > Queries`. The key resource here is memory as this is the only metric that can be defined in Resource Pools.

   The query that created the problem was `COMPUTE STATS prod_trlog_ibank.service_audit`. Pictures are not included because Impala does not report statistics for the `COMPUTE STATS` queries, but given the size of the table and the time of execution it matched. This hypothesis was later confirmed when the same problem appeared on a later `COMPUTE STATS` execution.

5. We informed the customer to re-run the failed jobs and proposed to stop computing statistics for that table as they did not impact our application.

    ``` text
    27/10/22 13:16:01 Europe/Eastern (POULAS GIORGOS):
	Good evening,

	Please rerun the flow steps that encountered a problem.

	We are continuing to investigate the root cause of the problem.

    **Workaround**

    27/10/22 13:32:44 Europe/Eastern (POULAS GIORGOS):
    Following the previous answer, the compute statistics on the prod_trlog_ibank.service_audit table committed many resources on the Calatog Server and the query Coordinator (dr1node02), resulting in REFRESH/INVALIDATE METADATA operations experiencing long execution times and causing jobs to time out. After the compute statistics were completed, Impala released the resources and resumed.
    
    As we have communicated in the past, from our perspective the statistics of the table are not necessary. Please let us know if we can disable the production of statistics for this particular table.

    **Resolved**
    ```

6. The changes for the statistics were implemented as part of [this ticket](obss/oss/sysadmin-group/mno/cloudera-cluster#180).

<b>Affected Systems:</b>

Disaster Recovery Site Datawarehouse

<b>Action Points:</b>
