# mno - BigStreamer - IM2099957 - Alert on Grafana

<b>Description:</b>

```text
Good evening,

The following alert appeared in the grafana system:

[DR][IBANK] Spark Waiting Batches Alert
```

<b>Actions Taken:</b>

1. The following text has been sent to mno/PM and explains the problem, as well as the recommended actions

```text
Spark Waiting Batches Problem: The first and most important problem we have is the "Spark Waiting Batches" which opens a ticket for this monitoring. This is due to physical user actions (queries/jobs) that occupy/bind production resources on the disaster site mainly. This results in there being no resources available for the MySQL process (a central point that the entire cluster has a dependency on), the service that does the authorization is unable to process its data in the database and thus causes a delay in the spark topologies until the "permission denied" error is resolved. The spark topologies, while up and running, are unable to process the data, as a result of which they continue to execute after the execution of the jobs that occupied the resources on the server where the MySQL service is located has finished. We do not take any action in this nor can we do anything and mno has asked us to close the ticket directly.
Suggestion: Disable Impala Daemon and YARN Node Manager on dr1node03.mno.gr, pr1node03.mno.gr where the primary MySQL service is located. This will not affect our cluster workload as, as you will see in the attached screenshots:
1. impala_mean.png: The average memory occupied by Impala Daemon is much smaller than the limit (150GB) that we have set even in the evening hours when all the flows "close" the previous day and the largest load is concentrated on the cluster.
```

![impala mean usage](.media/IM2099957/impala_mean.PNG)

``` text
2. impala_total.png: The total memory commitment from Impala cumulatively for all nodes is at its peak about 500GB less than the total available, which means that by removing a node there will still be room for resources even with the addition of new flows
```

![impala total usage](.media/IM2099957/impala_total.PNG)

```
3. yarn.png: throughout the day, the available yarn resources are sufficient even with the removal of more than one node
The above applies to both sites and the screenshots are from the Disaster site, where the most resources are reserved compared to the 2 sites. It is important as in this MySQL has a dependency on the entire cluster.
```
![yarn usage](.media/IM2099957/yarn.PNG)

``` text
By monitoring the remaining nodes of the 2 clusters, we see how they can manage the workload at CPU levels and with the above proposal we will reduce the CPU levels on critical node03 which anyway uses increased CPU for cluster management processes.
```

<b>Affected Systems:</b>

mno Disaster Site

<b>Action Points:</b>

https://metis.ghi.com/obss/bigdata/mno/devops-mno/-/issues/66  
https://metis.ghi.com/obss/oss/sysadmin-group/mno/cloudera-cluster/-/issues/196  
