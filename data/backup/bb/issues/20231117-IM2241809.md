# mno - BigStreamer - IM2241809 - Every Host on PR1 and DR1 are in critical state

<b>Description:</b>

```
Good evening,

All Hosts in Dr1 and PR1 are in critical state

Please take action.
```

<b>Actions Taken:</b>

**Steps in order to investigate and make sure that the table is not created**
1. Login to `PR` and `DR` cloudera manager in order to check the health of each cluster. The status was unhealthy for all services on both clusters.
2. Login to `Grafana` in order to check that applications running. All the applications were running without errors.
3. ssh to `pr1edge01.mno.gr` with personal account
4. sudo to root
5. Move to the log folder:
```bash
cd /var/log
```
6. Check messages file
```bash
less messages
```
The output was:
![image](.media/IM2241809/pr1edge01_messages.png)

7. From the above output we saw that at `22:13:02 pr1edge01_kernel: nfs: server 999.999.999.999 not responding`.
8. Now lets check the `agent logs` of an internal node.
9. ssh to `pr1node03.mno.gr` with personal account
10. sudo to root
11. Move to the log folder:
```bash
cd /var/log/cloudera-scm-agent
```
12. Check `cloudera-scm-agent.log` file
```bash
less cloudera-scm-agent.log
```
The output was:
![image](.media/IM2241809/pr1node03_agent_logs.png)

13. Due to unavaliability of `nfs storage`(responisibility of the customer to maintain), `Host Monitor` service of Cloudera management services had `timeout` errors because couldn't collect metrics from each filesystem of the nodes.
14. Customer informed that `nfs storage` caused the issue on both clusters and the unhealthy state of all services was not real because `Host Monitor` was not able to collect metrics in order to be appeared on `CM`. Also all flows ran without errors during the issue.
15. Customer informed us that the `nfs` storage was full and after their actions it's ok. We checked the `CM` and all the services now is healthy.

<b>Root Cause Analysis:</b>

This problem occurred due to `nfs` unavaliability.

<b>Our Ticket Response:</b>

```
Good evening,

The issue was caused by the nfs storage used on the nodes of both clusters becoming full. This resulted in the host monitor of the cloudera management services timeouting as it was unable to collect metrics for each filesystem of the nodes.

Relevant screenshots are attached showing the above causes of the issue.

Throughout the issue, the flows were up and running as seen in grafana as it was a malfunction of the management services resulting in the incorrect image of all services in Cloudera Manager PR & DR respectively.

After space was freed up on the nfs, both clusters returned to good health.

Thank you.
```

<b>Affected Systems:</b>

Disaster/Primary Site


**Well Done!!**




