# mno - BigStreamer - IM2095156 - Alarm on PRDBA  Cloudera Manager

</b>Description:</b>

```
We have a critical alarm in PRDBA Cloudera Manager -

YARM -- Node Manager (pr1node03)
Process Status

Thank you
```

<b>Actions Taken:</b>

1. Login to Cloudera UI for the PR Site
2. Cloudera > Yarn
3. Upon inspection we noticed that the alert was about pr1node01 (Node Manager) and not pr1node03 (JobHistory Server)
4. Ssh pr1node01 and inspect logs at /var/loh/hadoop-yarn. We could not find the root cause from logs
5. Restart the Node Manager role for the specific node. After the restart the alert disappeared.
6. During further investigation, from Cloudera UI we saw that prior to ```Process Status``` alert there was a ```NODE_MANAGER_LOG_FREE_SPACE``` alert
7. From pr1node01 as root `df -h /`. The usage of `/` was at 98% at that time
8. Upon inspection we noticed that the krb5kdc logs had increased over the last months peaking the monthly log file to ~80G.
9. We proceeded to the removal of krb5kdc log files for 2022.
10. As a permanent solution, we implemented changes to retention policy for krb5kdc logs. Specifically, we changed the rotation to weekly from monthly and the storage to 7 old logs from 12 logs files that it was prior the change.  This change was implemented at pr1node02, dr1node01 and dr1node02 as well.


    ![logrotate_krb5kdc](.media/IM2095156/IM2095156_logrotate_krb5kdc.PNG)

<b>Our Ticket Response:</b>

```
09/03/23 15:47:41 Europe/Eastern (MASTROKOSTA MARIA):
Good evening,

Following the investigation, we have changed the retention for krb5kdc logs. Specifically, we have set the rotation to be weekly instead of monthly and to keep 7 log files. Note that the monthly krb5kdc log file had reached 80G.

Please let us know if we can proceed with closing the ticket.

Thank you

01/03/23 07:16:41 Europe/Eastern (MASTROKOSTA MARIA):
Good morning,

There was a malfunction in the yarn node manager since 3.42, resulting in the ibank and online streaming topologies falling as recorded in ticket SD2157107.

We proceeded to restart at 4:53 to get it back up. During the restart, the online merge batch crashed, which was resubmitted (related ticket SD2157111).

At this time, yarn and the flows are running normally.

From the investigation it appears that the root partition on pr1node01 had filled up, which was caused by the local kdc logs. We have proceeded to clean the corresponding log files and are investigating changes to the retention of the logs to avoid future problems.

Please let us know if you consider the workaround acceptable.

Thank you
```
